{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Füge das übergeordnete Verzeichnis zu sys.path hinzu\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy \u001b[38;5;28;01mas\u001b[39;00m dc\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_data_into_sequences, load_sequential_time_series, reconstruct_sequential_data, Scaler, extract_features_and_targets_reg\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvisual_evaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m visualize\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpredictive_evaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m predictive_evaluation\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utilities'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "from utilities import split_data_into_sequences, load_sequential_time_series, reconstruct_sequential_data, Scaler, extract_features_and_targets_reg, get_discriminative_test_performance\n",
    "from visual_evaluation import visualize\n",
    "from predictive_evaluation import predictive_evaluation\n",
    "\n",
    "from discriminative_model import LSTMClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../data\")\n",
    "REAL_DATA_FOLDER = DATA_FOLDER / \"real\"\n",
    "SYNTHETIC_DATA_FOLDER = DATA_FOLDER / \"synthetic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of loading data\n",
    "- Laden der Originaldaten: als pd dataframe \n",
    "- Laden der synthetischen, sequentiellen Daten: als np array (GAN, (V)AE)\n",
    "- Laden der synthetischen, sequentiellen Daten: als pd dataframe (brownian, algorithmit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible types: 'timegan_lstm', 'timegan_gru', 'jitter', 'timewarp', 'autoencoder', 'all'\n",
    "syn_data_type = 'jitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if syn_data_type == 'all':\n",
    "    # load all results\n",
    "    pass\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load real time series\n",
    "    data_real_df = pd.read_csv(REAL_DATA_FOLDER/'metro_interstate_traffic_volume_label_encoded_no_categorical.csv')\n",
    "    data_real_numpy = dc(data_real_df).to_numpy()\n",
    "\n",
    "    if syn_data_type == 'timegan_lstm':\n",
    "        # load sequential data (which should already be scaled)\n",
    "        data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_lstm_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "    elif syn_data_type == 'timegan_gru':\n",
    "        data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_gru_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "    elif syn_data_type == 'autoencoder':\n",
    "        data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28478_12_5_autoencoder_unscaled.csv', shape=(28478, 12, 5))\n",
    "\n",
    "    elif syn_data_type == 'jitter':\n",
    "        jitter_factor = 0.1\n",
    "        data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'mitv_jittered_{str(jitter_factor).replace(\".\", \"\")}.csv')\n",
    "        data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "    elif syn_data_type == 'timewarp':\n",
    "        data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'mitv_time_warped.csv')\n",
    "        data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "    # Loot at real and syn data\n",
    "    df = pd.DataFrame(data_syn_numpy.reshape(-1, data_syn_numpy.shape[-1]), columns=data_real_df.columns)\n",
    "    df.describe()\n",
    "    data_real_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Predictive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_model_hyperparameters = {\n",
    "    \"seq_len\": 12,\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 32,\n",
    "    \"hidden_size\": 4,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"num_evaluation_runs\": 10,\n",
    "    \"num_epochs\": 200,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETERS:\n",
      "seq_len :  12\n",
      "lr :  0.0001\n",
      "batch_size :  32\n",
      "hidden_size :  4\n",
      "num_layers :  1\n",
      "num_evaluation_runs :  10\n",
      "num_epochs :  200\n",
      "device :  cpu\n",
      "Synthetic Data is sequential: False\n",
      "Shape of the data after splitting into sequences: (22797, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2841, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2840, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Loss: 0.5521320375800133\n",
      "Training Loss: 0.4500365850329399\n",
      "Training Loss: 0.35785618633031846\n",
      "Validation Loss: 0.22566637028469128\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.20727982167154552\n",
      "Training Loss: 0.14357522808015347\n",
      "Training Loss: 0.11016296058893203\n",
      "Validation Loss: 0.0763319128852212\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.08103626372292638\n",
      "Training Loss: 0.07519765118137002\n",
      "Training Loss: 0.07341821977868676\n",
      "Validation Loss: 0.06599259179797065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.06865114687010646\n",
      "Training Loss: 0.06850901484489441\n",
      "Training Loss: 0.06827586220577359\n",
      "Validation Loss: 0.06276435798473573\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.06425764945335687\n",
      "Training Loss: 0.06377812531776726\n",
      "Training Loss: 0.0630571318231523\n",
      "Validation Loss: 0.05739058181643486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.05842366363853216\n",
      "Training Loss: 0.05720728891901672\n",
      "Training Loss: 0.055802933592349294\n",
      "Validation Loss: 0.05010249980547455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.05047617672942579\n",
      "Training Loss: 0.048431802447885273\n",
      "Training Loss: 0.046444925991818306\n",
      "Validation Loss: 0.041451459352889755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.04119233822450042\n",
      "Training Loss: 0.03866771909408271\n",
      "Training Loss: 0.03648498598486185\n",
      "Validation Loss: 0.03215530353566904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.03182745785918087\n",
      "Training Loss: 0.029121477021835745\n",
      "Training Loss: 0.027000426575541497\n",
      "Validation Loss: 0.02349291717780105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.023578920434229076\n",
      "Training Loss: 0.02177265173755586\n",
      "Training Loss: 0.02035698898602277\n",
      "Validation Loss: 0.018015914021080798\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.018690796038135886\n",
      "Training Loss: 0.018033733752090483\n",
      "Training Loss: 0.017237180846277626\n",
      "Validation Loss: 0.015419829239168863\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.01649455919628963\n",
      "Training Loss: 0.016297606113366784\n",
      "Training Loss: 0.015560717363841832\n",
      "Validation Loss: 0.013749229533367612\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.015016948741395026\n",
      "Training Loss: 0.014975613006390632\n",
      "Training Loss: 0.014195892000570894\n",
      "Validation Loss: 0.01236341463067056\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.013778327619656921\n",
      "Training Loss: 0.013838919117115439\n",
      "Training Loss: 0.013035701920744032\n",
      "Validation Loss: 0.011209749433557304\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.012754748975858092\n",
      "Training Loss: 0.012883670101873577\n",
      "Training Loss: 0.012079648077487945\n",
      "Validation Loss: 0.010276208657855064\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.01193461162270978\n",
      "Training Loss: 0.012098379656672478\n",
      "Training Loss: 0.011309298790292814\n",
      "Validation Loss: 0.00952971808277489\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.011285128213930875\n",
      "Training Loss: 0.011455173180438578\n",
      "Training Loss: 0.010689517363207415\n",
      "Validation Loss: 0.008925969231078464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.010765269432449714\n",
      "Training Loss: 0.010922019770368933\n",
      "Training Loss: 0.010183448372408747\n",
      "Validation Loss: 0.00842580196887171\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.010339430869789794\n",
      "Training Loss: 0.010471907791215927\n",
      "Training Loss: 0.009761468260549008\n",
      "Validation Loss: 0.008000851572187764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.009981912440853193\n",
      "Training Loss: 0.01008512943983078\n",
      "Training Loss: 0.009402591375401243\n",
      "Validation Loss: 0.007632173448696398\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.009675603387877344\n",
      "Training Loss: 0.00974810148589313\n",
      "Training Loss: 0.0090926437091548\n",
      "Validation Loss: 0.007307376770114296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.009409336973913014\n",
      "Training Loss: 0.009451526862103493\n",
      "Training Loss: 0.008822055504424497\n",
      "Validation Loss: 0.007018193934803431\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.00917567388038151\n",
      "Training Loss: 0.009188868663040922\n",
      "Training Loss: 0.008584153314586729\n",
      "Validation Loss: 0.0067588959302585785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008969437398482114\n",
      "Training Loss: 0.008955337654333562\n",
      "Training Loss: 0.008374059002380817\n",
      "Validation Loss: 0.006525289356248097\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.00878680289722979\n",
      "Training Loss: 0.008747229089494794\n",
      "Training Loss: 0.008188004281837493\n",
      "Validation Loss: 0.006314135407715031\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008624772825278341\n",
      "Training Loss: 0.008561525768600404\n",
      "Training Loss: 0.008022935466142371\n",
      "Validation Loss: 0.00612281539655301\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008480890364153311\n",
      "Training Loss: 0.008395701867993922\n",
      "Training Loss: 0.007876296859467401\n",
      "Validation Loss: 0.005949146603495719\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008353064928669482\n",
      "Training Loss: 0.008247570293024182\n",
      "Training Loss: 0.007745894578984007\n",
      "Validation Loss: 0.005791242637284351\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008239475925220176\n",
      "Training Loss: 0.008115210288669914\n",
      "Training Loss: 0.007629814023384824\n",
      "Validation Loss: 0.005647476498309648\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008138509978307411\n",
      "Training Loss: 0.007996912164380775\n",
      "Training Loss: 0.007526371229905635\n",
      "Validation Loss: 0.005516400709852911\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008048739701043814\n",
      "Training Loss: 0.007891160427825526\n",
      "Training Loss: 0.007434090101160109\n",
      "Validation Loss: 0.005396745879244939\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007968887782189994\n",
      "Training Loss: 0.007796588906785473\n",
      "Training Loss: 0.007351655513048172\n",
      "Validation Loss: 0.0052873667491746415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007897811578586697\n",
      "Training Loss: 0.007711981295142323\n",
      "Training Loss: 0.007277915775775909\n",
      "Validation Loss: 0.005187256588715683\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007834492517868058\n",
      "Training Loss: 0.007636245972244069\n",
      "Training Loss: 0.007211848982842639\n",
      "Validation Loss: 0.005095506110490205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007778024417348206\n",
      "Training Loss: 0.007568401728058234\n",
      "Training Loss: 0.007152552156476304\n",
      "Validation Loss: 0.00501130319336492\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.00772760103805922\n",
      "Training Loss: 0.007507576713105663\n",
      "Training Loss: 0.007099238923983648\n",
      "Validation Loss: 0.00493393204007507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007682510535232723\n",
      "Training Loss: 0.0074529918853659186\n",
      "Training Loss: 0.0070512073265854265\n",
      "Validation Loss: 0.0048627386424230055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007642111426685005\n",
      "Training Loss: 0.007403938954230398\n",
      "Training Loss: 0.0070078378834296014\n",
      "Validation Loss: 0.0047971353881856365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.00760584841365926\n",
      "Training Loss: 0.007359800240956247\n",
      "Training Loss: 0.006968594370409847\n",
      "Validation Loss: 0.004736609042758185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007573225789237767\n",
      "Training Loss: 0.007320015261648223\n",
      "Training Loss: 0.006932993865339085\n",
      "Validation Loss: 0.004680684911547585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007543803736334667\n",
      "Training Loss: 0.007284085515420884\n",
      "Training Loss: 0.006900614212499931\n",
      "Validation Loss: 0.004628938989023145\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007517198083223775\n",
      "Training Loss: 0.007251572175882757\n",
      "Training Loss: 0.006871083956211805\n",
      "Validation Loss: 0.004580992773656597\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007493069680640474\n",
      "Training Loss: 0.007222080837236717\n",
      "Training Loss: 0.006844072206877172\n",
      "Validation Loss: 0.004536501053469569\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007471118511166424\n",
      "Training Loss: 0.007195261652814224\n",
      "Training Loss: 0.006819287459366024\n",
      "Validation Loss: 0.004495156369056929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007451082326006144\n",
      "Training Loss: 0.007170807855436579\n",
      "Training Loss: 0.006796474099392071\n",
      "Validation Loss: 0.00445667451006894\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.0074327270861249415\n",
      "Training Loss: 0.007148440027376637\n",
      "Training Loss: 0.006775401529157534\n",
      "Validation Loss: 0.004420799598244302\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007415848363889381\n",
      "Training Loss: 0.00712791983038187\n",
      "Training Loss: 0.006755871573695913\n",
      "Validation Loss: 0.004387310002354926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007400266465265304\n",
      "Training Loss: 0.007109033084707335\n",
      "Training Loss: 0.006737705726409331\n",
      "Validation Loss: 0.004355994649650006\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007385822074720636\n",
      "Training Loss: 0.007091585450107232\n",
      "Training Loss: 0.006720743125770241\n",
      "Validation Loss: 0.0043266599819985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0073723725916352125\n",
      "Training Loss: 0.007075407997472211\n",
      "Training Loss: 0.006704845569329336\n",
      "Validation Loss: 0.004299134656975276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007359794721705839\n",
      "Training Loss: 0.007060353760607541\n",
      "Training Loss: 0.006689888803521171\n",
      "Validation Loss: 0.004273265346445227\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007347978698089719\n",
      "Training Loss: 0.007046288755955175\n",
      "Training Loss: 0.006675762307131663\n",
      "Validation Loss: 0.004248908250017113\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007336827956605702\n",
      "Training Loss: 0.007033098706742748\n",
      "Training Loss: 0.006662370609119534\n",
      "Validation Loss: 0.004225936689508263\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007326256650267169\n",
      "Training Loss: 0.007020678743720055\n",
      "Training Loss: 0.006649625273421406\n",
      "Validation Loss: 0.004204230974271392\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007316189598059282\n",
      "Training Loss: 0.00700893548084423\n",
      "Training Loss: 0.006637450834969059\n",
      "Validation Loss: 0.004183682497967495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007306558833224699\n",
      "Training Loss: 0.006997790619498119\n",
      "Training Loss: 0.00662577934213914\n",
      "Validation Loss: 0.004164194358575545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007297307819826528\n",
      "Training Loss: 0.0069871728611178695\n",
      "Training Loss: 0.006614551160018891\n",
      "Validation Loss: 0.004145680063435536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007288382534170523\n",
      "Training Loss: 0.006977016060845926\n",
      "Training Loss: 0.006603711674688384\n",
      "Validation Loss: 0.004128052549583189\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.0072797373740468175\n",
      "Training Loss: 0.00696726662106812\n",
      "Training Loss: 0.006593215132597834\n",
      "Validation Loss: 0.004111244882132565\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007271331842057407\n",
      "Training Loss: 0.006957874768413603\n",
      "Training Loss: 0.006583020653342828\n",
      "Validation Loss: 0.004095188890102456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007263132870430127\n",
      "Training Loss: 0.006948798140510917\n",
      "Training Loss: 0.006573090543970466\n",
      "Validation Loss: 0.004079822405094929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007255105385556817\n",
      "Training Loss: 0.006939997512381524\n",
      "Training Loss: 0.006563391683157533\n",
      "Validation Loss: 0.004065084203115005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007247223226586357\n",
      "Training Loss: 0.0069314356101676826\n",
      "Training Loss: 0.006553894821554423\n",
      "Validation Loss: 0.0040509294956566745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007239461574936285\n",
      "Training Loss: 0.006923086459282786\n",
      "Training Loss: 0.006544574924046174\n",
      "Validation Loss: 0.004037307996533058\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007231798266293481\n",
      "Training Loss: 0.00691491967998445\n",
      "Training Loss: 0.006535407397896051\n",
      "Validation Loss: 0.004024173907433333\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007224214526358992\n",
      "Training Loss: 0.006906914053251967\n",
      "Training Loss: 0.006526374095119536\n",
      "Validation Loss: 0.004011496154622834\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007216694132657722\n",
      "Training Loss: 0.006899049633648247\n",
      "Training Loss: 0.006517457201844081\n",
      "Validation Loss: 0.003999237196122328\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007209223184036091\n",
      "Training Loss: 0.006891305724857375\n",
      "Training Loss: 0.006508640706306324\n",
      "Validation Loss: 0.0039873576887851855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007201786120422184\n",
      "Training Loss: 0.006883665475761518\n",
      "Training Loss: 0.006499908218393103\n",
      "Validation Loss: 0.003975836441098723\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007194372434169054\n",
      "Training Loss: 0.006876115906052292\n",
      "Training Loss: 0.006491249426035211\n",
      "Validation Loss: 0.003964638236774069\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.00718697251402773\n",
      "Training Loss: 0.006868644408532418\n",
      "Training Loss: 0.006482653594575822\n",
      "Validation Loss: 0.003953742224388243\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007179576760390774\n",
      "Training Loss: 0.00686123859253712\n",
      "Training Loss: 0.006474110739072785\n",
      "Validation Loss: 0.003943125090792022\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007172178907785565\n",
      "Training Loss: 0.006853889903286472\n",
      "Training Loss: 0.006465613276232034\n",
      "Validation Loss: 0.0039327713998369455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007164771277457476\n",
      "Training Loss: 0.006846586840110831\n",
      "Training Loss: 0.006457150677451864\n",
      "Validation Loss: 0.00392265296070261\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007157347549218684\n",
      "Training Loss: 0.006839324289467185\n",
      "Training Loss: 0.006448718616738915\n",
      "Validation Loss: 0.00391275804945048\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007149903301615268\n",
      "Training Loss: 0.006832093236735091\n",
      "Training Loss: 0.0064403107890393585\n",
      "Validation Loss: 0.0039030652783622736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007142434435663745\n",
      "Training Loss: 0.0068248906172811985\n",
      "Training Loss: 0.006431923848576843\n",
      "Validation Loss: 0.0038935664852850892\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00713493644609116\n",
      "Training Loss: 0.006817708918824792\n",
      "Training Loss: 0.006423550969921052\n",
      "Validation Loss: 0.0038842441869920558\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007127407331718132\n",
      "Training Loss: 0.006810543254832737\n",
      "Training Loss: 0.006415189055260271\n",
      "Validation Loss: 0.0038750834653174943\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007119843463879078\n",
      "Training Loss: 0.006803389095002785\n",
      "Training Loss: 0.006406834553927183\n",
      "Validation Loss: 0.0038660782038788784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007112243000883609\n",
      "Training Loss: 0.006796243946300819\n",
      "Training Loss: 0.006398485816316679\n",
      "Validation Loss: 0.0038572147140656125\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.00710460580768995\n",
      "Training Loss: 0.0067891056055668746\n",
      "Training Loss: 0.006390141552546993\n",
      "Validation Loss: 0.003848487206504502\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007096928594401106\n",
      "Training Loss: 0.0067819709493778645\n",
      "Training Loss: 0.00638179816189222\n",
      "Validation Loss: 0.003839879861792152\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007089212550781667\n",
      "Training Loss: 0.006774837807752192\n",
      "Training Loss: 0.006373455359134823\n",
      "Validation Loss: 0.003831393330023111\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007081456830492243\n",
      "Training Loss: 0.0067677033110521735\n",
      "Training Loss: 0.006365110789192841\n",
      "Validation Loss: 0.003823013292112796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007073659801390022\n",
      "Training Loss: 0.006760567944729701\n",
      "Training Loss: 0.006356765786185861\n",
      "Validation Loss: 0.003814735957369041\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.0070658242446370425\n",
      "Training Loss: 0.0067534295638324695\n",
      "Training Loss: 0.006348418708657846\n",
      "Validation Loss: 0.0038065539824703102\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.0070579481055028735\n",
      "Training Loss: 0.0067462876054923985\n",
      "Training Loss: 0.00634006908396259\n",
      "Validation Loss: 0.0037984654576821105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.00705003276001662\n",
      "Training Loss: 0.0067391421447973695\n",
      "Training Loss: 0.006331719359150156\n",
      "Validation Loss: 0.0037904593340131673\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007042080813553184\n",
      "Training Loss: 0.00673199214390479\n",
      "Training Loss: 0.006323367711156607\n",
      "Validation Loss: 0.0037825339476400045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0070340921368915585\n",
      "Training Loss: 0.006724837591173127\n",
      "Training Loss: 0.006315015394939109\n",
      "Validation Loss: 0.0037746844836772323\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0070260689524002376\n",
      "Training Loss: 0.006717679533758201\n",
      "Training Loss: 0.006306663426803425\n",
      "Validation Loss: 0.003766907410424077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007018010665196925\n",
      "Training Loss: 0.006710515378508717\n",
      "Training Loss: 0.006298310614656657\n",
      "Validation Loss: 0.003759195001304066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.00700992141966708\n",
      "Training Loss: 0.006703347638249397\n",
      "Training Loss: 0.006289962144801393\n",
      "Validation Loss: 0.0037515472144600045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007001802063314244\n",
      "Training Loss: 0.006696177353151142\n",
      "Training Loss: 0.006281616854248568\n",
      "Validation Loss: 0.0037439607797985835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006993654172401875\n",
      "Training Loss: 0.006689003868959844\n",
      "Training Loss: 0.006273276787251234\n",
      "Validation Loss: 0.0037364301460093997\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006985481864539907\n",
      "Training Loss: 0.006681829625740647\n",
      "Training Loss: 0.0062649437156505885\n",
      "Validation Loss: 0.0037289552646950722\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006977286384208128\n",
      "Training Loss: 0.006674654838861898\n",
      "Training Loss: 0.006256619084160775\n",
      "Validation Loss: 0.0037215343124515723\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0069690703868400304\n",
      "Training Loss: 0.006667482256307267\n",
      "Training Loss: 0.006248305690241977\n",
      "Validation Loss: 0.003714163876609521\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.00696083671413362\n",
      "Training Loss: 0.00666030946013052\n",
      "Training Loss: 0.006240005388390273\n",
      "Validation Loss: 0.003706838496113091\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006952587426640094\n",
      "Training Loss: 0.006653141269343905\n",
      "Training Loss: 0.006231718669878319\n",
      "Validation Loss: 0.0036995602049603223\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006944326157681644\n",
      "Training Loss: 0.006645978437154554\n",
      "Training Loss: 0.0062234498525504025\n",
      "Validation Loss: 0.0036923273798768943\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.00693605454871431\n",
      "Training Loss: 0.006638821410015225\n",
      "Training Loss: 0.006215199115686119\n",
      "Validation Loss: 0.003685140138586167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006927775933872908\n",
      "Training Loss: 0.006631672408548184\n",
      "Training Loss: 0.006206970772473142\n",
      "Validation Loss: 0.003677995464754071\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006919494416797534\n",
      "Training Loss: 0.006624533097492531\n",
      "Training Loss: 0.0061987661430612205\n",
      "Validation Loss: 0.0036708912720610754\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006911211266415193\n",
      "Training Loss: 0.00661740499373991\n",
      "Training Loss: 0.006190586982993409\n",
      "Validation Loss: 0.0036638247259902987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006902931754011661\n",
      "Training Loss: 0.006610290486132726\n",
      "Training Loss: 0.006182439207332208\n",
      "Validation Loss: 0.0036567983759064854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006894657133379951\n",
      "Training Loss: 0.006603191493777558\n",
      "Training Loss: 0.0061743211839348075\n",
      "Validation Loss: 0.0036498116528134\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006886392055312171\n",
      "Training Loss: 0.006596109848469495\n",
      "Training Loss: 0.0061662391049321745\n",
      "Validation Loss: 0.0036428647346041176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006878139221807942\n",
      "Training Loss: 0.006589047702727839\n",
      "Training Loss: 0.006158193227602169\n",
      "Validation Loss: 0.003635954518614107\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006869901423342526\n",
      "Training Loss: 0.0065820060868281875\n",
      "Training Loss: 0.0061501864669844504\n",
      "Validation Loss: 0.0036290825300148866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006861682400340214\n",
      "Training Loss: 0.0065749885799596085\n",
      "Training Loss: 0.006142221591435373\n",
      "Validation Loss: 0.0036222483397702152\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006853485207539051\n",
      "Training Loss: 0.006567995231016539\n",
      "Training Loss: 0.0061343019478954375\n",
      "Validation Loss: 0.003615451440361611\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006845314067322761\n",
      "Training Loss: 0.00656103158602491\n",
      "Training Loss: 0.00612643045024015\n",
      "Validation Loss: 0.00360869633536158\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006837170217186213\n",
      "Training Loss: 0.006554096360923722\n",
      "Training Loss: 0.006118608972756192\n",
      "Validation Loss: 0.0036019770378832904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006829058600123972\n",
      "Training Loss: 0.006547194640152156\n",
      "Training Loss: 0.006110840516630561\n",
      "Validation Loss: 0.0035952998055773003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006820981847122312\n",
      "Training Loss: 0.006540325960959308\n",
      "Training Loss: 0.006103127028327435\n",
      "Validation Loss: 0.003588662321909509\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006812943003606051\n",
      "Training Loss: 0.0065334942436311395\n",
      "Training Loss: 0.006095472190063447\n",
      "Validation Loss: 0.0035820672513519444\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006804946713382378\n",
      "Training Loss: 0.006526701557449996\n",
      "Training Loss: 0.006087880359264091\n",
      "Validation Loss: 0.003575514239426493\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006796994181349873\n",
      "Training Loss: 0.0065199501160532235\n",
      "Training Loss: 0.006080350724514574\n",
      "Validation Loss: 0.0035690003914465563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006789091659011319\n",
      "Training Loss: 0.0065132430772064254\n",
      "Training Loss: 0.006072887985501438\n",
      "Validation Loss: 0.003562531359441411\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006781239687697962\n",
      "Training Loss: 0.006506581552675925\n",
      "Training Loss: 0.006065495170187205\n",
      "Validation Loss: 0.003556110393883807\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006773443646961823\n",
      "Training Loss: 0.006499969092546962\n",
      "Training Loss: 0.006058174166828394\n",
      "Validation Loss: 0.003549733909443439\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006765703620621935\n",
      "Training Loss: 0.006493407988455146\n",
      "Training Loss: 0.006050927710020915\n",
      "Validation Loss: 0.0035434074770512706\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006758026942843571\n",
      "Training Loss: 0.006486900291056372\n",
      "Training Loss: 0.006043760097818449\n",
      "Validation Loss: 0.0035371280332838887\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00675041395588778\n",
      "Training Loss: 0.006480449204100296\n",
      "Training Loss: 0.006036671630572528\n",
      "Validation Loss: 0.0035309000097717464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006742868103319779\n",
      "Training Loss: 0.006474055888829753\n",
      "Training Loss: 0.0060296653106343\n",
      "Validation Loss: 0.0035247250664142077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006735392352566123\n",
      "Training Loss: 0.006467724132235162\n",
      "Training Loss: 0.006022744183428586\n",
      "Validation Loss: 0.003518598890612216\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006727990891085937\n",
      "Training Loss: 0.006461456049582921\n",
      "Training Loss: 0.006015911389840767\n",
      "Validation Loss: 0.0035125290925269214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006720665921457112\n",
      "Training Loss: 0.0064552539977012205\n",
      "Training Loss: 0.006009168800665066\n",
      "Validation Loss: 0.003506517104459194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006713421569438651\n",
      "Training Loss: 0.006449122345657088\n",
      "Training Loss: 0.006002519257599488\n",
      "Validation Loss: 0.003500564854456049\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006706258369376883\n",
      "Training Loss: 0.006443061380996369\n",
      "Training Loss: 0.0059959641576278954\n",
      "Validation Loss: 0.0034946706067519578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.00669918120955117\n",
      "Training Loss: 0.0064370732940733436\n",
      "Training Loss: 0.005989506159676239\n",
      "Validation Loss: 0.003488836110192822\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006692191833863035\n",
      "Training Loss: 0.006431161029031501\n",
      "Training Loss: 0.005983147643273696\n",
      "Validation Loss: 0.0034830651803235157\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006685292444890365\n",
      "Training Loss: 0.006425327402539551\n",
      "Training Loss: 0.005976890156744048\n",
      "Validation Loss: 0.0034773567615579187\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006678486612509004\n",
      "Training Loss: 0.006419575158506632\n",
      "Training Loss: 0.005970736953895539\n",
      "Validation Loss: 0.0034717147256865095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006671775312861427\n",
      "Training Loss: 0.006413905534427613\n",
      "Training Loss: 0.005964687000960112\n",
      "Validation Loss: 0.003466138310123528\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006665161069249735\n",
      "Training Loss: 0.0064083210891112685\n",
      "Training Loss: 0.005958745228126645\n",
      "Validation Loss: 0.0034606289288573217\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006658645982970484\n",
      "Training Loss: 0.006402823561802507\n",
      "Training Loss: 0.005952910520136356\n",
      "Validation Loss: 0.003455184857894698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006652230633189902\n",
      "Training Loss: 0.006397415043320507\n",
      "Training Loss: 0.0059471858409233394\n",
      "Validation Loss: 0.0034498127132884405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.0066459187626605855\n",
      "Training Loss: 0.006392096906201914\n",
      "Training Loss: 0.0059415700333192946\n",
      "Validation Loss: 0.003444505732485585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006639710686868056\n",
      "Training Loss: 0.006386870168498717\n",
      "Training Loss: 0.005936064762063324\n",
      "Validation Loss: 0.003439269422323265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006633605764363892\n",
      "Training Loss: 0.006381737503106706\n",
      "Training Loss: 0.005930671663954854\n",
      "Validation Loss: 0.0034341027245992856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00662760651728604\n",
      "Training Loss: 0.00637669819756411\n",
      "Training Loss: 0.005925390255870297\n",
      "Validation Loss: 0.0034290083809598777\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.0066217132320161905\n",
      "Training Loss: 0.0063717553520109506\n",
      "Training Loss: 0.005920220447005704\n",
      "Validation Loss: 0.0034239885614389615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006615925616933964\n",
      "Training Loss: 0.0063669074111385275\n",
      "Training Loss: 0.005915161464363336\n",
      "Validation Loss: 0.0034190333694261448\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006610244185430929\n",
      "Training Loss: 0.0063621563103515655\n",
      "Training Loss: 0.005910213212482631\n",
      "Validation Loss: 0.003414149902331946\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006604667869978585\n",
      "Training Loss: 0.006357502013561316\n",
      "Training Loss: 0.005905376196606085\n",
      "Validation Loss: 0.003409335282474254\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006599196282913908\n",
      "Training Loss: 0.006352944725076668\n",
      "Training Loss: 0.005900647708913311\n",
      "Validation Loss: 0.0034045885249009626\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006593828320037573\n",
      "Training Loss: 0.006348484337795526\n",
      "Training Loss: 0.0058960283058695495\n",
      "Validation Loss: 0.003399911763020948\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006588563650730066\n",
      "Training Loss: 0.006344120713765733\n",
      "Training Loss: 0.005891514002578333\n",
      "Validation Loss: 0.003395298446183292\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0065833999071037395\n",
      "Training Loss: 0.006339851825032383\n",
      "Training Loss: 0.005887104341527447\n",
      "Validation Loss: 0.0033907520799280216\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006578335423837416\n",
      "Training Loss: 0.006335678184987046\n",
      "Training Loss: 0.005882796943187714\n",
      "Validation Loss: 0.0033862700037072214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006573367987875826\n",
      "Training Loss: 0.006331598404794931\n",
      "Training Loss: 0.005878590604988858\n",
      "Validation Loss: 0.003381851624979983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.0065684945101384075\n",
      "Training Loss: 0.0063276092370506375\n",
      "Training Loss: 0.0058744799892883745\n",
      "Validation Loss: 0.0033774909581675123\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006563712891656906\n",
      "Training Loss: 0.006323710469296202\n",
      "Training Loss: 0.005870466064661742\n",
      "Validation Loss: 0.0033731902949512005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006559019389096647\n",
      "Training Loss: 0.0063199003151385115\n",
      "Training Loss: 0.005866542072035372\n",
      "Validation Loss: 0.0033689489786963116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006554412834811956\n",
      "Training Loss: 0.00631617603066843\n",
      "Training Loss: 0.005862706993939355\n",
      "Validation Loss: 0.003364759605911592\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006549887160072103\n",
      "Training Loss: 0.006312535803299397\n",
      "Training Loss: 0.00585895654046908\n",
      "Validation Loss: 0.0033606266278480546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.0065454397676512595\n",
      "Training Loss: 0.0063089779647998515\n",
      "Training Loss: 0.005855288102757186\n",
      "Validation Loss: 0.0033565475513865606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006541067071957514\n",
      "Training Loss: 0.0063054980657761914\n",
      "Training Loss: 0.005851699319900944\n",
      "Validation Loss: 0.003352514188213462\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006536765532218851\n",
      "Training Loss: 0.006302094641141593\n",
      "Training Loss: 0.00584818413015455\n",
      "Validation Loss: 0.003348530667802591\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006532530115218833\n",
      "Training Loss: 0.006298764062230475\n",
      "Training Loss: 0.0058447389514185485\n",
      "Validation Loss: 0.0033445877200934324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006528356046765112\n",
      "Training Loss: 0.0062955030874582004\n",
      "Training Loss: 0.005841360610211268\n",
      "Validation Loss: 0.0033406878251247525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006524241755250841\n",
      "Training Loss: 0.0062923087039962414\n",
      "Training Loss: 0.005838046227581799\n",
      "Validation Loss: 0.0033368287723134745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.0065201812470331785\n",
      "Training Loss: 0.006289177357684821\n",
      "Training Loss: 0.005834792355308309\n",
      "Validation Loss: 0.003333007818705329\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006516170182731002\n",
      "Training Loss: 0.006286106543848291\n",
      "Training Loss: 0.005831592483446002\n",
      "Validation Loss: 0.0033292259100113023\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006512203775346279\n",
      "Training Loss: 0.006283092431258411\n",
      "Training Loss: 0.005828447019448504\n",
      "Validation Loss: 0.0033254754726952883\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.00650828032346908\n",
      "Training Loss: 0.0062801305780885745\n",
      "Training Loss: 0.005825349794467911\n",
      "Validation Loss: 0.003321756650641393\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006504395175143145\n",
      "Training Loss: 0.006277218786999583\n",
      "Training Loss: 0.005822298992425204\n",
      "Validation Loss: 0.003318071224088414\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0065005432657198985\n",
      "Training Loss: 0.006274353688349947\n",
      "Training Loss: 0.005819291101070121\n",
      "Validation Loss: 0.003314415159049245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006496722751180641\n",
      "Training Loss: 0.0062715316825779155\n",
      "Training Loss: 0.005816323510371149\n",
      "Validation Loss: 0.003310784068366701\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006492930522072129\n",
      "Training Loss: 0.006268749407026917\n",
      "Training Loss: 0.005813393716234714\n",
      "Validation Loss: 0.0033071800932978814\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006489162349025719\n",
      "Training Loss: 0.006266002447810024\n",
      "Training Loss: 0.005810499102808535\n",
      "Validation Loss: 0.0033036018381669614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006485417416552081\n",
      "Training Loss: 0.00626328953076154\n",
      "Training Loss: 0.005807636654935777\n",
      "Validation Loss: 0.0033000460041038107\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006481691551744007\n",
      "Training Loss: 0.006260606170981191\n",
      "Training Loss: 0.005804806855740026\n",
      "Validation Loss: 0.003296517148310465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006477984796511009\n",
      "Training Loss: 0.006257952321902849\n",
      "Training Loss: 0.005802006479352712\n",
      "Validation Loss: 0.0032930108954020766\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006474293734063394\n",
      "Training Loss: 0.006255322351935319\n",
      "Training Loss: 0.005799234473379329\n",
      "Validation Loss: 0.0032895291132559435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006470618337043561\n",
      "Training Loss: 0.006252715010195971\n",
      "Training Loss: 0.005796490574721247\n",
      "Validation Loss: 0.003286066959517893\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006466957597876899\n",
      "Training Loss: 0.006250127828679979\n",
      "Training Loss: 0.005793771913740784\n",
      "Validation Loss: 0.0032826302418736426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006463309685932473\n",
      "Training Loss: 0.006247558483155444\n",
      "Training Loss: 0.005791080079507083\n",
      "Validation Loss: 0.0032792151500104687\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006459675283404067\n",
      "Training Loss: 0.0062450055469525974\n",
      "Training Loss: 0.005788412619149312\n",
      "Validation Loss: 0.003275824194051995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006456053180154413\n",
      "Training Loss: 0.006242466515977867\n",
      "Training Loss: 0.005785768926143646\n",
      "Validation Loss: 0.0032724513413300844\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.0064524444262497125\n",
      "Training Loss: 0.006239939929218963\n",
      "Training Loss: 0.0057831501797772945\n",
      "Validation Loss: 0.0032691060541867375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006448847408173606\n",
      "Training Loss: 0.0062374252773588525\n",
      "Training Loss: 0.005780554661760107\n",
      "Validation Loss: 0.0032657830769279865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.0064452626573620365\n",
      "Training Loss: 0.006234920088900253\n",
      "Training Loss: 0.005777982922736556\n",
      "Validation Loss: 0.0032624819622050676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006441691598156467\n",
      "Training Loss: 0.006232423972105608\n",
      "Training Loss: 0.0057754334690980616\n",
      "Validation Loss: 0.0032592031024291776\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006438133334740997\n",
      "Training Loss: 0.006229935155715793\n",
      "Training Loss: 0.0057729073241353035\n",
      "Validation Loss: 0.0032559518893301655\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006434587255935185\n",
      "Training Loss: 0.006227454046602361\n",
      "Training Loss: 0.00577040349249728\n",
      "Validation Loss: 0.0032527213824619905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006431054578861221\n",
      "Training Loss: 0.006224979009130038\n",
      "Training Loss: 0.005767921466613189\n",
      "Validation Loss: 0.003249515270489906\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.00642753588501364\n",
      "Training Loss: 0.006222510565421544\n",
      "Training Loss: 0.0057654614245984705\n",
      "Validation Loss: 0.003246330832696279\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006424030069028959\n",
      "Training Loss: 0.006220046965172515\n",
      "Training Loss: 0.005763023175531999\n",
      "Validation Loss: 0.0032431707453254737\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.0064205378177575765\n",
      "Training Loss: 0.006217588835279457\n",
      "Training Loss: 0.005760605398099869\n",
      "Validation Loss: 0.0032400315957081117\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006417060120147653\n",
      "Training Loss: 0.006215135470265523\n",
      "Training Loss: 0.0057582076720427726\n",
      "Validation Loss: 0.0032369163648612548\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006413594604819082\n",
      "Training Loss: 0.006212687400402501\n",
      "Training Loss: 0.005755831276765093\n",
      "Validation Loss: 0.0032338262339426057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006410143695538864\n",
      "Training Loss: 0.00621024338353891\n",
      "Training Loss: 0.005753474086523056\n",
      "Validation Loss: 0.003230756856344138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006406706603011117\n",
      "Training Loss: 0.006207803567522205\n",
      "Training Loss: 0.005751136082690209\n",
      "Validation Loss: 0.003227707016899177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006403282460523769\n",
      "Training Loss: 0.006205368483788334\n",
      "Training Loss: 0.005748816959094256\n",
      "Validation Loss: 0.0032246832858792013\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.0063998718187212944\n",
      "Training Loss: 0.006202936963527464\n",
      "Training Loss: 0.005746516755316407\n",
      "Validation Loss: 0.003221679482807855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006396474482025951\n",
      "Training Loss: 0.006200510798371397\n",
      "Training Loss: 0.00574423412210308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive_evaluation.py:255: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([{'Model': evaluation_method, 'Metric': 'MAE', 'Error': mae}])], ignore_index=True)\n",
      " 10%|█         | 1/10 [02:31<22:39, 151.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.00321870066586547\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.09658761993050576\n",
      "Training Loss: 0.08031178018078208\n",
      "Training Loss: 0.07402516283094883\n",
      "Validation Loss: 0.06814003727409278\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06863283475860954\n",
      "Training Loss: 0.06777406817302108\n",
      "Training Loss: 0.06681125290691853\n",
      "Validation Loss: 0.06310274054327708\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06353999324142932\n",
      "Training Loss: 0.06244449030607939\n",
      "Training Loss: 0.06089218690991402\n",
      "Validation Loss: 0.05644018064891355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.056569183208048344\n",
      "Training Loss: 0.05465948013588786\n",
      "Training Loss: 0.05232197614386678\n",
      "Validation Loss: 0.04708953436171071\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.04706743910908699\n",
      "Training Loss: 0.044617739822715524\n",
      "Training Loss: 0.04214690173044801\n",
      "Validation Loss: 0.03713330528123325\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.037462326623499395\n",
      "Training Loss: 0.035277815870940685\n",
      "Training Loss: 0.03336550730280578\n",
      "Validation Loss: 0.029124709285711974\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.029881397080607712\n",
      "Training Loss: 0.028010969767346978\n",
      "Training Loss: 0.02650121579412371\n",
      "Validation Loss: 0.022902769402841503\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.02400259752292186\n",
      "Training Loss: 0.022410559640266\n",
      "Training Loss: 0.021176432152278723\n",
      "Validation Loss: 0.018094583006387346\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.019505883718375118\n",
      "Training Loss: 0.018264389883261175\n",
      "Training Loss: 0.017292788987979292\n",
      "Validation Loss: 0.014618658476373118\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.01633943059714511\n",
      "Training Loss: 0.015485706611070783\n",
      "Training Loss: 0.014731345907784998\n",
      "Validation Loss: 0.012280748639088334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.014245910921599715\n",
      "Training Loss: 0.013660355610772968\n",
      "Training Loss: 0.013003362249583006\n",
      "Validation Loss: 0.010643912055477333\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.012761498971376567\n",
      "Training Loss: 0.01233922827988863\n",
      "Training Loss: 0.011722292185295373\n",
      "Validation Loss: 0.00942938690063324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.011643583772238343\n",
      "Training Loss: 0.011330842383904383\n",
      "Training Loss: 0.01073781835497357\n",
      "Validation Loss: 0.00850280930466029\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.010792753996793181\n",
      "Training Loss: 0.010552622245159\n",
      "Training Loss: 0.009980185544118286\n",
      "Validation Loss: 0.007791029964312074\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.010148168625310063\n",
      "Training Loss: 0.009953152070520445\n",
      "Training Loss: 0.00940145785571076\n",
      "Validation Loss: 0.007246190517240863\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.009661859357729554\n",
      "Training Loss: 0.00949217802262865\n",
      "Training Loss: 0.008960682395845652\n",
      "Validation Loss: 0.0068274611329890034\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.009293315851828084\n",
      "Training Loss: 0.009135161197045818\n",
      "Training Loss: 0.008622341386508197\n",
      "Validation Loss: 0.0065001272674901096\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.009009716875152663\n",
      "Training Loss: 0.008853857888607309\n",
      "Training Loss: 0.008357958734268322\n",
      "Validation Loss: 0.006237435826948017\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.00878633267013356\n",
      "Training Loss: 0.008626997848041355\n",
      "Training Loss: 0.008146400660043582\n",
      "Validation Loss: 0.006020276093060214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.008605621168389916\n",
      "Training Loss: 0.008439534080680459\n",
      "Training Loss: 0.007972794151864946\n",
      "Validation Loss: 0.005835644312182002\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008455614537233487\n",
      "Training Loss: 0.008281203535152598\n",
      "Training Loss: 0.007826965351123363\n",
      "Validation Loss: 0.005674888688568654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.008328286114847287\n",
      "Training Loss: 0.008145080114481972\n",
      "Training Loss: 0.007702034601243213\n",
      "Validation Loss: 0.005532319947888845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008218288229545578\n",
      "Training Loss: 0.008026483388384804\n",
      "Training Loss: 0.007593352477997541\n",
      "Validation Loss: 0.005404174981869004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008122021991293877\n",
      "Training Loss: 0.007922182471957058\n",
      "Training Loss: 0.007497731826733797\n",
      "Validation Loss: 0.005287923400154274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.008037019085604698\n",
      "Training Loss: 0.007829878324409947\n",
      "Training Loss: 0.007412945900578052\n",
      "Validation Loss: 0.005181832511103555\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007961530154570937\n",
      "Training Loss: 0.007747870403109119\n",
      "Training Loss: 0.00733737409580499\n",
      "Validation Loss: 0.0050846541959666805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.007894255262799561\n",
      "Training Loss: 0.007674839673563838\n",
      "Training Loss: 0.00726979568367824\n",
      "Validation Loss: 0.004995462437568421\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007834184122039005\n",
      "Training Loss: 0.00760971866780892\n",
      "Training Loss: 0.007209238151554018\n",
      "Validation Loss: 0.004913516277238057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.007780483760870993\n",
      "Training Loss: 0.007551603973843158\n",
      "Training Loss: 0.007154891462996602\n",
      "Validation Loss: 0.0048381981984032\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.0077324361749924716\n",
      "Training Loss: 0.007499701349297538\n",
      "Training Loss: 0.007106054569594562\n",
      "Validation Loss: 0.0047689758246408755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007689409187296405\n",
      "Training Loss: 0.007453312581637874\n",
      "Training Loss: 0.0070621077809482815\n",
      "Validation Loss: 0.0047053525831257355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.00765082535916008\n",
      "Training Loss: 0.007411792795173824\n",
      "Training Loss: 0.007022486764471978\n",
      "Validation Loss: 0.004646870932866181\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.0076161597506143154\n",
      "Training Loss: 0.007374565891223028\n",
      "Training Loss: 0.0069866848783567545\n",
      "Validation Loss: 0.004593110033781843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.00758493579691276\n",
      "Training Loss: 0.007341103529324755\n",
      "Training Loss: 0.006954237218014896\n",
      "Validation Loss: 0.004543665840991595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007556716380640864\n",
      "Training Loss: 0.007310927230864763\n",
      "Training Loss: 0.006924727709265426\n",
      "Validation Loss: 0.004498163125676553\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.0075311084988061334\n",
      "Training Loss: 0.0072836100426502525\n",
      "Training Loss: 0.006897778495913371\n",
      "Validation Loss: 0.004456250051957335\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007507759819272906\n",
      "Training Loss: 0.007258769378531724\n",
      "Training Loss: 0.006873055521864444\n",
      "Validation Loss: 0.00441759859284993\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007486359268659726\n",
      "Training Loss: 0.007236069508362562\n",
      "Training Loss: 0.006850263435626403\n",
      "Validation Loss: 0.0043819000436976716\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007466629118425772\n",
      "Training Loss: 0.007215209092246369\n",
      "Training Loss: 0.006829139840556308\n",
      "Validation Loss: 0.004348881479896856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007448330889455974\n",
      "Training Loss: 0.00719593221321702\n",
      "Training Loss: 0.006809459774522111\n",
      "Validation Loss: 0.0043182802841778885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007431257520802319\n",
      "Training Loss: 0.007178018171107397\n",
      "Training Loss: 0.0067910257703624665\n",
      "Validation Loss: 0.0042898624303510016\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007415229049511254\n",
      "Training Loss: 0.007161270916694776\n",
      "Training Loss: 0.006773668163223192\n",
      "Validation Loss: 0.004263412178588215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007400092150783166\n",
      "Training Loss: 0.007145525178639218\n",
      "Training Loss: 0.006757239327998832\n",
      "Validation Loss: 0.004238735902740547\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007385717501165345\n",
      "Training Loss: 0.007130644882563502\n",
      "Training Loss: 0.006741620810935274\n",
      "Validation Loss: 0.004215659673085038\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.00737199664930813\n",
      "Training Loss: 0.007116509650368244\n",
      "Training Loss: 0.006726703587919474\n",
      "Validation Loss: 0.004194022883566913\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.0073588351195212455\n",
      "Training Loss: 0.007103017648914829\n",
      "Training Loss: 0.006712395647773519\n",
      "Validation Loss: 0.004173678230870975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007346154410624877\n",
      "Training Loss: 0.007090086711104959\n",
      "Training Loss: 0.006698624348500743\n",
      "Validation Loss: 0.004154509702550896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0073338910669554025\n",
      "Training Loss: 0.007077644158853218\n",
      "Training Loss: 0.0066853219643235205\n",
      "Validation Loss: 0.00413638913187753\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007321987679461017\n",
      "Training Loss: 0.007065628581913188\n",
      "Training Loss: 0.006672433846397326\n",
      "Validation Loss: 0.0041192185049981215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007310399055713787\n",
      "Training Loss: 0.007053988691186532\n",
      "Training Loss: 0.006659912002505735\n",
      "Validation Loss: 0.0041029012861420935\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007299085193080828\n",
      "Training Loss: 0.007042681784369052\n",
      "Training Loss: 0.006647716958541423\n",
      "Validation Loss: 0.0040873576065504486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007288014621008187\n",
      "Training Loss: 0.007031672589946538\n",
      "Training Loss: 0.006635814335895702\n",
      "Validation Loss: 0.0040725123351753765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007277157506905496\n",
      "Training Loss: 0.007020927455741912\n",
      "Training Loss: 0.00662417403771542\n",
      "Validation Loss: 0.004058296941325403\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.0072664928541053084\n",
      "Training Loss: 0.007010420365259051\n",
      "Training Loss: 0.006612768849590793\n",
      "Validation Loss: 0.004044650520166654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007255997320171446\n",
      "Training Loss: 0.0070001281681470575\n",
      "Training Loss: 0.006601577419787646\n",
      "Validation Loss: 0.004031520229607319\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007245655893348158\n",
      "Training Loss: 0.00699003012268804\n",
      "Training Loss: 0.00659057977492921\n",
      "Validation Loss: 0.004018856474104222\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.00723545256536454\n",
      "Training Loss: 0.006980109207797795\n",
      "Training Loss: 0.00657975874375552\n",
      "Validation Loss: 0.004006614105441095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007225375270936638\n",
      "Training Loss: 0.006970350320916623\n",
      "Training Loss: 0.006569099043263122\n",
      "Validation Loss: 0.003994757186600499\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007215411770157516\n",
      "Training Loss: 0.006960740684298798\n",
      "Training Loss: 0.006558588637271896\n",
      "Validation Loss: 0.003983250267154882\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007205553518142551\n",
      "Training Loss: 0.0069512670621043075\n",
      "Training Loss: 0.006548213887726888\n",
      "Validation Loss: 0.003972058589936474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007195791995618492\n",
      "Training Loss: 0.006941918173106387\n",
      "Training Loss: 0.006537964346352965\n",
      "Validation Loss: 0.003961154853590251\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007186118386453017\n",
      "Training Loss: 0.006932687883963808\n",
      "Training Loss: 0.006527832985157147\n",
      "Validation Loss: 0.003950520546772004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007176529355347156\n",
      "Training Loss: 0.006923567580524832\n",
      "Training Loss: 0.006517809258075431\n",
      "Validation Loss: 0.0039401261837043785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007167016614694148\n",
      "Training Loss: 0.006914546759799123\n",
      "Training Loss: 0.006507886372273788\n",
      "Validation Loss: 0.0039299507442275795\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007157576259924098\n",
      "Training Loss: 0.006905620004981756\n",
      "Training Loss: 0.006498057300923392\n",
      "Validation Loss: 0.003919978437715032\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007148203527322039\n",
      "Training Loss: 0.006896782313706353\n",
      "Training Loss: 0.006488317161565647\n",
      "Validation Loss: 0.003910191898663225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007138894886011258\n",
      "Training Loss: 0.006888026357628405\n",
      "Training Loss: 0.006478660823777318\n",
      "Validation Loss: 0.003900576150044799\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007129648647969588\n",
      "Training Loss: 0.0068793483858462425\n",
      "Training Loss: 0.006469083225820214\n",
      "Validation Loss: 0.0038911198799529774\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007120460018049926\n",
      "Training Loss: 0.006870744036277756\n",
      "Training Loss: 0.006459580673836171\n",
      "Validation Loss: 0.0038818099321483563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0071113282558508215\n",
      "Training Loss: 0.0068622083985246714\n",
      "Training Loss: 0.00645015093497932\n",
      "Validation Loss: 0.0038726379744331825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007102252217009664\n",
      "Training Loss: 0.0068537391524296255\n",
      "Training Loss: 0.006440789526095614\n",
      "Validation Loss: 0.003863592841400859\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.0070932298735715445\n",
      "Training Loss: 0.006845330566866323\n",
      "Training Loss: 0.006431493621785194\n",
      "Validation Loss: 0.0038546660622016766\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.0070842585933860395\n",
      "Training Loss: 0.0068369819165673106\n",
      "Training Loss: 0.006422262821579352\n",
      "Validation Loss: 0.0038458516748015132\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.0070753400458488614\n",
      "Training Loss: 0.006828688956447877\n",
      "Training Loss: 0.0064130935026332735\n",
      "Validation Loss: 0.0038371442900865934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007066473262384534\n",
      "Training Loss: 0.006820449686492793\n",
      "Training Loss: 0.006403986654477194\n",
      "Validation Loss: 0.0038285378387637355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00705765750957653\n",
      "Training Loss: 0.006812262731837109\n",
      "Training Loss: 0.006394939959282055\n",
      "Validation Loss: 0.0038200270115094407\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007048892476595938\n",
      "Training Loss: 0.006804124837508425\n",
      "Training Loss: 0.006385952373966575\n",
      "Validation Loss: 0.003811605757343049\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007040180364856497\n",
      "Training Loss: 0.00679603407625109\n",
      "Training Loss: 0.006377023751847446\n",
      "Validation Loss: 0.003803274133818203\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007031519560841843\n",
      "Training Loss: 0.006787989425938576\n",
      "Training Loss: 0.006368155259406194\n",
      "Validation Loss: 0.0037950272187904526\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007022914014523849\n",
      "Training Loss: 0.006779989997157827\n",
      "Training Loss: 0.006359345418168232\n",
      "Validation Loss: 0.003786862438042345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007014360984321684\n",
      "Training Loss: 0.006772034062305465\n",
      "Training Loss: 0.006350595990661532\n",
      "Validation Loss: 0.003778779858283782\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007005864684469998\n",
      "Training Loss: 0.006764121000887826\n",
      "Training Loss: 0.006341904602013528\n",
      "Validation Loss: 0.003770772641095636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006997423514258116\n",
      "Training Loss: 0.006756248530000448\n",
      "Training Loss: 0.006333275045035407\n",
      "Validation Loss: 0.0037628418852292587\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006989039754262194\n",
      "Training Loss: 0.006748417021008209\n",
      "Training Loss: 0.0063247060857247565\n",
      "Validation Loss: 0.0037549874847336265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.00698071553139016\n",
      "Training Loss: 0.006740624806843698\n",
      "Training Loss: 0.006316199194407091\n",
      "Validation Loss: 0.0037472092708719248\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006972450453322381\n",
      "Training Loss: 0.006732871495187282\n",
      "Training Loss: 0.0063077557599171994\n",
      "Validation Loss: 0.003739503677934408\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006964246787829325\n",
      "Training Loss: 0.00672515650105197\n",
      "Training Loss: 0.006299375707749277\n",
      "Validation Loss: 0.0037318683933110887\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006956105761928484\n",
      "Training Loss: 0.006717477973434143\n",
      "Training Loss: 0.006291060018120334\n",
      "Validation Loss: 0.003724305949362225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006948028153274209\n",
      "Training Loss: 0.0067098373622866345\n",
      "Training Loss: 0.006282809625845403\n",
      "Validation Loss: 0.003716812709744057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006940014732535928\n",
      "Training Loss: 0.006702232243260369\n",
      "Training Loss: 0.006274624931393191\n",
      "Validation Loss: 0.0037093913088437546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006932066666195169\n",
      "Training Loss: 0.00669466486084275\n",
      "Training Loss: 0.006266509098932147\n",
      "Validation Loss: 0.003702040438623994\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006924185079988092\n",
      "Training Loss: 0.006687132986262441\n",
      "Training Loss: 0.006258459938690067\n",
      "Validation Loss: 0.0036947578322560934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006916370997205376\n",
      "Training Loss: 0.006679635505424813\n",
      "Training Loss: 0.006250480321468786\n",
      "Validation Loss: 0.003687544188231983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006908623670460657\n",
      "Training Loss: 0.006672173728002235\n",
      "Training Loss: 0.006242571065668017\n",
      "Validation Loss: 0.003680399921199495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006900945656234399\n",
      "Training Loss: 0.006664746445603669\n",
      "Training Loss: 0.006234732168959454\n",
      "Validation Loss: 0.0036733216433419607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006893337041838095\n",
      "Training Loss: 0.006657353141345084\n",
      "Training Loss: 0.006226964812958613\n",
      "Validation Loss: 0.0036663105345090454\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006885797766735777\n",
      "Training Loss: 0.006649993673781864\n",
      "Training Loss: 0.006219269030261785\n",
      "Validation Loss: 0.003659364924337087\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006878328082384541\n",
      "Training Loss: 0.00664266922743991\n",
      "Training Loss: 0.006211645607836544\n",
      "Validation Loss: 0.003652482685957397\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006870927166892216\n",
      "Training Loss: 0.006635377284837886\n",
      "Training Loss: 0.006204094578279183\n",
      "Validation Loss: 0.0036456639985710886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006863597609335557\n",
      "Training Loss: 0.0066281189676374195\n",
      "Training Loss: 0.006196616939269007\n",
      "Validation Loss: 0.003638909062307872\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006856336591299623\n",
      "Training Loss: 0.006620894586667418\n",
      "Training Loss: 0.006189212260069326\n",
      "Validation Loss: 0.0036322168909076057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.0068491448787972335\n",
      "Training Loss: 0.006613704048795625\n",
      "Training Loss: 0.006181881908560172\n",
      "Validation Loss: 0.003625585310412257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006842023660428822\n",
      "Training Loss: 0.006606546974508092\n",
      "Training Loss: 0.006174625109415501\n",
      "Validation Loss: 0.0036190130494095467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006834970735944808\n",
      "Training Loss: 0.006599423383595422\n",
      "Training Loss: 0.0061674414353910835\n",
      "Validation Loss: 0.0036124996383140764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006827987002907321\n",
      "Training Loss: 0.0065923348808428275\n",
      "Training Loss: 0.00616033268510364\n",
      "Validation Loss: 0.0036060472144588324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0068210706382524225\n",
      "Training Loss: 0.006585281006409787\n",
      "Training Loss: 0.006153297402197495\n",
      "Validation Loss: 0.0035996504227390116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006814223545370623\n",
      "Training Loss: 0.006578261874383315\n",
      "Training Loss: 0.006146337731042877\n",
      "Validation Loss: 0.0035933111297238745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.00680744449258782\n",
      "Training Loss: 0.006571279157069512\n",
      "Training Loss: 0.006139450811315328\n",
      "Validation Loss: 0.00358702494171605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006800731754628942\n",
      "Training Loss: 0.006564331970294006\n",
      "Training Loss: 0.006132638419512659\n",
      "Validation Loss: 0.003580792528430649\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006794086348963902\n",
      "Training Loss: 0.006557421828038059\n",
      "Training Loss: 0.0061258994275704024\n",
      "Validation Loss: 0.0035746112162393802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006787508380366489\n",
      "Training Loss: 0.006550549564417451\n",
      "Training Loss: 0.006119234673678875\n",
      "Validation Loss: 0.0035684800214981767\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006780996003653854\n",
      "Training Loss: 0.006543715723673813\n",
      "Training Loss: 0.006112643510568887\n",
      "Validation Loss: 0.003562400163297824\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006774549692054279\n",
      "Training Loss: 0.00653692198859062\n",
      "Training Loss: 0.006106125598307699\n",
      "Validation Loss: 0.003556371704424114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006768168820417486\n",
      "Training Loss: 0.0065301698877010496\n",
      "Training Loss: 0.006099681145278737\n",
      "Validation Loss: 0.0035503900994473462\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006761853704228997\n",
      "Training Loss: 0.006523458992596716\n",
      "Training Loss: 0.0060933097579982136\n",
      "Validation Loss: 0.0035444549114830542\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.0067556033923756335\n",
      "Training Loss: 0.006516792080947198\n",
      "Training Loss: 0.006087011955678463\n",
      "Validation Loss: 0.0035385658972362957\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006749418826657348\n",
      "Training Loss: 0.006510169905377552\n",
      "Training Loss: 0.006080787198152393\n",
      "Validation Loss: 0.003532720023368517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006743299171794206\n",
      "Training Loss: 0.006503592414665036\n",
      "Training Loss: 0.006074633217649534\n",
      "Validation Loss: 0.003526920300981637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006737243614043109\n",
      "Training Loss: 0.006497063937713392\n",
      "Training Loss: 0.006068552528740838\n",
      "Validation Loss: 0.0035211643612200623\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.0067312525381566955\n",
      "Training Loss: 0.006490583387785591\n",
      "Training Loss: 0.006062542590079829\n",
      "Validation Loss: 0.003515447993511648\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.00672532529802993\n",
      "Training Loss: 0.006484153186902404\n",
      "Training Loss: 0.006056604558834806\n",
      "Validation Loss: 0.003509772549058949\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.00671946233662311\n",
      "Training Loss: 0.006477774058585056\n",
      "Training Loss: 0.006050736639881506\n",
      "Validation Loss: 0.003504136740753239\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0067136631457833575\n",
      "Training Loss: 0.006471448297379539\n",
      "Training Loss: 0.006044938308186829\n",
      "Validation Loss: 0.003498539764151563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006707927612587809\n",
      "Training Loss: 0.006465176606434397\n",
      "Training Loss: 0.006039210010785609\n",
      "Validation Loss: 0.0034929808579761993\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006702255465788767\n",
      "Training Loss: 0.006458960750023834\n",
      "Training Loss: 0.0060335509362630544\n",
      "Validation Loss: 0.0034874608685272965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006696647079661488\n",
      "Training Loss: 0.006452801195555366\n",
      "Training Loss: 0.00602795987855643\n",
      "Validation Loss: 0.0034819753275493557\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006691100837197155\n",
      "Training Loss: 0.006446700186352246\n",
      "Training Loss: 0.006022436709026806\n",
      "Validation Loss: 0.0034765259001738906\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006685616887989454\n",
      "Training Loss: 0.006440658344654366\n",
      "Training Loss: 0.006016979950945824\n",
      "Validation Loss: 0.003471110221469419\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.0066801956115523356\n",
      "Training Loss: 0.00643467589863576\n",
      "Training Loss: 0.006011589290574193\n",
      "Validation Loss: 0.0034657265098891063\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006674836219171993\n",
      "Training Loss: 0.00642875486984849\n",
      "Training Loss: 0.006006263233721256\n",
      "Validation Loss: 0.003460377248087793\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006669536386034451\n",
      "Training Loss: 0.00642289464478381\n",
      "Training Loss: 0.006001001014956273\n",
      "Validation Loss: 0.003455058024055586\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006664298042305745\n",
      "Training Loss: 0.006417097515659407\n",
      "Training Loss: 0.005995801771641709\n",
      "Validation Loss: 0.0034497727605964194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006659118084353394\n",
      "Training Loss: 0.0064113635092508045\n",
      "Training Loss: 0.005990664576529525\n",
      "Validation Loss: 0.0034445186375818226\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006653997857938521\n",
      "Training Loss: 0.006405692283879034\n",
      "Training Loss: 0.005985587187460624\n",
      "Validation Loss: 0.0034392919755028037\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006648935612756759\n",
      "Training Loss: 0.006400084710912779\n",
      "Training Loss: 0.005980569932144135\n",
      "Validation Loss: 0.003434096761257126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.00664392992388457\n",
      "Training Loss: 0.006394541498157196\n",
      "Training Loss: 0.005975609769229777\n",
      "Validation Loss: 0.003428930654766017\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006638980920542963\n",
      "Training Loss: 0.006389061504160054\n",
      "Training Loss: 0.005970707855885849\n",
      "Validation Loss: 0.0034237951275714663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006634085980476811\n",
      "Training Loss: 0.006383645380265079\n",
      "Training Loss: 0.005965860270662233\n",
      "Validation Loss: 0.003418686454383175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006629245809745043\n",
      "Training Loss: 0.0063782924774568525\n",
      "Training Loss: 0.005961067577009089\n",
      "Validation Loss: 0.003413603680333897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00662445678783115\n",
      "Training Loss: 0.0063730019459035244\n",
      "Training Loss: 0.005956327454186976\n",
      "Validation Loss: 0.0034085502311733836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006619720862945541\n",
      "Training Loss: 0.006367774846730754\n",
      "Training Loss: 0.005951639103586786\n",
      "Validation Loss: 0.003403522337138067\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006615034661372192\n",
      "Training Loss: 0.0063626085978467015\n",
      "Training Loss: 0.005947001638123766\n",
      "Validation Loss: 0.0033985235992546998\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006610398230259306\n",
      "Training Loss: 0.0063575042720185595\n",
      "Training Loss: 0.005942412607255392\n",
      "Validation Loss: 0.0033935501718935506\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.0066058092139428485\n",
      "Training Loss: 0.00635246119229123\n",
      "Training Loss: 0.005937872054055333\n",
      "Validation Loss: 0.0033886049615135523\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006601266931975261\n",
      "Training Loss: 0.006347478096140549\n",
      "Training Loss: 0.005933378249173984\n",
      "Validation Loss: 0.003383687883092279\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.00659677000483498\n",
      "Training Loss: 0.006342553798458539\n",
      "Training Loss: 0.005928929228102789\n",
      "Validation Loss: 0.0033787940014049075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006592317264876329\n",
      "Training Loss: 0.006337686971528455\n",
      "Training Loss: 0.00592452433949802\n",
      "Validation Loss: 0.0033739274236886355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006587907193461433\n",
      "Training Loss: 0.006332878067623824\n",
      "Training Loss: 0.005920162536203861\n",
      "Validation Loss: 0.003369087045959961\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006583538571139798\n",
      "Training Loss: 0.00632812510652002\n",
      "Training Loss: 0.0059158420312451195\n",
      "Validation Loss: 0.0033642733077194247\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006579210157506168\n",
      "Training Loss: 0.006323427716270089\n",
      "Training Loss: 0.0059115626168204476\n",
      "Validation Loss: 0.0033594881213175947\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.00657492158934474\n",
      "Training Loss: 0.006318783948081545\n",
      "Training Loss: 0.005907323293504305\n",
      "Validation Loss: 0.003354728058388645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.00657067006919533\n",
      "Training Loss: 0.006314194626756944\n",
      "Training Loss: 0.005903122473973781\n",
      "Validation Loss: 0.0033499965512225134\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006566455593565479\n",
      "Training Loss: 0.006309656133526005\n",
      "Training Loss: 0.005898960288031958\n",
      "Validation Loss: 0.0033452922355362708\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006562277796911075\n",
      "Training Loss: 0.006305169008555822\n",
      "Training Loss: 0.005894834285136312\n",
      "Validation Loss: 0.003340615353316822\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006558134324732237\n",
      "Training Loss: 0.0063007327244849875\n",
      "Training Loss: 0.0058907450328115375\n",
      "Validation Loss: 0.003335965542237829\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006554024086217396\n",
      "Training Loss: 0.006296344973379746\n",
      "Training Loss: 0.005886691053165123\n",
      "Validation Loss: 0.0033313420554099793\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006549946847953834\n",
      "Training Loss: 0.006292005334398709\n",
      "Training Loss: 0.005882671436411329\n",
      "Validation Loss: 0.003326748147230135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006545901648350992\n",
      "Training Loss: 0.006287712870398536\n",
      "Training Loss: 0.005878686134237796\n",
      "Validation Loss: 0.0033221843893106065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006541886579361745\n",
      "Training Loss: 0.00628346654411871\n",
      "Training Loss: 0.005874733806704171\n",
      "Validation Loss: 0.0033176478778085346\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006537901972769759\n",
      "Training Loss: 0.006279264694894664\n",
      "Training Loss: 0.0058708134817425164\n",
      "Validation Loss: 0.0033131364910873803\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006533947093994357\n",
      "Training Loss: 0.006275105559034273\n",
      "Training Loss: 0.005866925495793112\n",
      "Validation Loss: 0.0033086544109424683\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006530019965721294\n",
      "Training Loss: 0.006270990822231397\n",
      "Training Loss: 0.005863069252227433\n",
      "Validation Loss: 0.0033042011756366225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0065261200250824915\n",
      "Training Loss: 0.006266917601460591\n",
      "Training Loss: 0.005859242443111725\n",
      "Validation Loss: 0.0032997757714409172\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.00652224789839238\n",
      "Training Loss: 0.006262885349569842\n",
      "Training Loss: 0.00585544683795888\n",
      "Validation Loss: 0.003295378603846923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0065184012951795016\n",
      "Training Loss: 0.00625889393966645\n",
      "Training Loss: 0.005851681039202959\n",
      "Validation Loss: 0.00329101060156114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006514580760267563\n",
      "Training Loss: 0.006254941693041474\n",
      "Training Loss: 0.005847944795386866\n",
      "Validation Loss: 0.0032866717842041275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006510784583515488\n",
      "Training Loss: 0.006251027464168146\n",
      "Training Loss: 0.005844237544806674\n",
      "Validation Loss: 0.0032823621884009307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.0065070124791236595\n",
      "Training Loss: 0.006247151148854755\n",
      "Training Loss: 0.0058405588677851485\n",
      "Validation Loss: 0.003278081098655134\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006503264885395765\n",
      "Training Loss: 0.0062433125881943856\n",
      "Training Loss: 0.005836908629862592\n",
      "Validation Loss: 0.0032738294737580955\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006499540092772804\n",
      "Training Loss: 0.006239510092418641\n",
      "Training Loss: 0.005833285997505299\n",
      "Validation Loss: 0.003269605315028784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006495837864349596\n",
      "Training Loss: 0.006235742894932628\n",
      "Training Loss: 0.0058296912087826055\n",
      "Validation Loss: 0.0032654090004899865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.00649215885612648\n",
      "Training Loss: 0.006232009847299196\n",
      "Training Loss: 0.005826121922000311\n",
      "Validation Loss: 0.003261241686446697\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.00648849961697124\n",
      "Training Loss: 0.006228311135200784\n",
      "Training Loss: 0.005822580583626404\n",
      "Validation Loss: 0.003257104577601291\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.0064848631201311945\n",
      "Training Loss: 0.006224645265610888\n",
      "Training Loss: 0.005819065928808414\n",
      "Validation Loss: 0.003252996372456631\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006481246831826866\n",
      "Training Loss: 0.006221012577880174\n",
      "Training Loss: 0.005815576936583966\n",
      "Validation Loss: 0.0032489165255611533\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.0064776503486791625\n",
      "Training Loss: 0.006217411364777945\n",
      "Training Loss: 0.005812114593572915\n",
      "Validation Loss: 0.003244862562108241\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006474074696889147\n",
      "Training Loss: 0.006213841318385676\n",
      "Training Loss: 0.005808676469605416\n",
      "Validation Loss: 0.0032408374016372004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006470518163987435\n",
      "Training Loss: 0.006210302153485827\n",
      "Training Loss: 0.005805264611262828\n",
      "Validation Loss: 0.003236839123949241\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006466980635887012\n",
      "Training Loss: 0.006206793235614896\n",
      "Training Loss: 0.00580187737068627\n",
      "Validation Loss: 0.0032328678833927664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.0064634616358671335\n",
      "Training Loss: 0.006203312667785212\n",
      "Training Loss: 0.005798514873022214\n",
      "Validation Loss: 0.003228925891858892\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006459961417131126\n",
      "Training Loss: 0.00619986233767122\n",
      "Training Loss: 0.0057951772306114435\n",
      "Validation Loss: 0.003225011051255749\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006456479118205607\n",
      "Training Loss: 0.0061964394274400545\n",
      "Training Loss: 0.00579186346149072\n",
      "Validation Loss: 0.003221122616002064\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006453014624421485\n",
      "Training Loss: 0.006193045450490899\n",
      "Training Loss: 0.005788573927711696\n",
      "Validation Loss: 0.0032172587417652098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006449566894443706\n",
      "Training Loss: 0.006189677807851695\n",
      "Training Loss: 0.005785307713085786\n",
      "Validation Loss: 0.003213423248014172\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006446136084850878\n",
      "Training Loss: 0.006186336996615864\n",
      "Training Loss: 0.005782065362436697\n",
      "Validation Loss: 0.003209612969298627\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006442721748608164\n",
      "Training Loss: 0.006183022829936817\n",
      "Training Loss: 0.0057788459165021774\n",
      "Validation Loss: 0.003205829916071942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006439324368257076\n",
      "Training Loss: 0.006179734694887884\n",
      "Training Loss: 0.005775649708230048\n",
      "Validation Loss: 0.0032020725657786716\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006435941985691898\n",
      "Training Loss: 0.006176472340594046\n",
      "Training Loss: 0.0057724767096806314\n",
      "Validation Loss: 0.0031983413513791696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006432576045044698\n",
      "Training Loss: 0.006173234278685413\n",
      "Training Loss: 0.005769325036671944\n",
      "Validation Loss: 0.0031946347163090212\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006429225109750405\n",
      "Training Loss: 0.006170020562713034\n",
      "Training Loss: 0.005766195940668695\n",
      "Validation Loss: 0.003190951433629216\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006425889709498733\n",
      "Training Loss: 0.006166830640868284\n",
      "Training Loss: 0.005763089307001792\n",
      "Validation Loss: 0.0031872934850163005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006422568790148944\n",
      "Training Loss: 0.006163665377534926\n",
      "Training Loss: 0.005760004640906118\n",
      "Validation Loss: 0.003183658491150382\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006419262040872127\n",
      "Training Loss: 0.0061605214641895145\n",
      "Training Loss: 0.005756940825958737\n",
      "Validation Loss: 0.0031800491348160115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006415969919762574\n",
      "Training Loss: 0.006157401701202616\n",
      "Training Loss: 0.005753898412804119\n",
      "Validation Loss: 0.0031764630563138577\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006412691160221584\n",
      "Training Loss: 0.006154304133960977\n",
      "Training Loss: 0.0057508772815344856\n",
      "Validation Loss: 0.0031729010954038815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.0064094260532874614\n",
      "Training Loss: 0.006151229350944049\n",
      "Training Loss: 0.005747876676032319\n",
      "Validation Loss: 0.003169363516309623\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0064061747369123625\n",
      "Training Loss: 0.00614817509194836\n",
      "Training Loss: 0.005744896678370423\n",
      "Validation Loss: 0.003165847132652161\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006402935988153331\n",
      "Training Loss: 0.006145142333116383\n",
      "Training Loss: 0.005741937697748654\n",
      "Validation Loss: 0.00316235495291734\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.0063997102889698\n",
      "Training Loss: 0.006142131024389528\n",
      "Training Loss: 0.005738998124725185\n",
      "Validation Loss: 0.003158884090266787\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0063964972435496745\n",
      "Training Loss: 0.006139140258310363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [05:02<20:08, 151.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0057360784168122335\n",
      "Validation Loss: 0.003155435617291107\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.08412747969850898\n",
      "Training Loss: 0.07487512188032269\n",
      "Training Loss: 0.07221389051526785\n",
      "Validation Loss: 0.06563495606016577\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06717987544834614\n",
      "Training Loss: 0.06644755318760871\n",
      "Training Loss: 0.06575750015676021\n",
      "Validation Loss: 0.05922129067979502\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06124685214832425\n",
      "Training Loss: 0.06007185539230704\n",
      "Training Loss: 0.05878416320309043\n",
      "Validation Loss: 0.051334404041258136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.053227047761902214\n",
      "Training Loss: 0.05120619846507907\n",
      "Training Loss: 0.04919970951974392\n",
      "Validation Loss: 0.04174463750187601\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.043278562119230625\n",
      "Training Loss: 0.04094663788564503\n",
      "Training Loss: 0.03893230054527521\n",
      "Validation Loss: 0.032910503760984776\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.03413597943261266\n",
      "Training Loss: 0.03216618608683348\n",
      "Training Loss: 0.030645285742357375\n",
      "Validation Loss: 0.026341992995461052\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.027470285594463347\n",
      "Training Loss: 0.026003634049557148\n",
      "Training Loss: 0.024955699048005046\n",
      "Validation Loss: 0.02187426280481427\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.023082079323939977\n",
      "Training Loss: 0.021981940190307796\n",
      "Training Loss: 0.021207900629378856\n",
      "Validation Loss: 0.018780577404612905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.020140849305316805\n",
      "Training Loss: 0.019261004514992238\n",
      "Training Loss: 0.018610869939438997\n",
      "Validation Loss: 0.016478904083454878\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.01799986845580861\n",
      "Training Loss: 0.01726124695967883\n",
      "Training Loss: 0.016664193505421282\n",
      "Validation Loss: 0.01464636486253879\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.016316663462202997\n",
      "Training Loss: 0.015684565985575317\n",
      "Training Loss: 0.015112960578408092\n",
      "Validation Loss: 0.01312152903234021\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.014929745139088482\n",
      "Training Loss: 0.014388227113522589\n",
      "Training Loss: 0.013832600216846912\n",
      "Validation Loss: 0.011823461542764072\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.013762793359346688\n",
      "Training Loss: 0.013303839323343709\n",
      "Training Loss: 0.012764212798792869\n",
      "Validation Loss: 0.010715572958749331\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.01278330871486105\n",
      "Training Loss: 0.012402118659811095\n",
      "Training Loss: 0.011884724411647767\n",
      "Validation Loss: 0.009787557432114074\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.011982118519954383\n",
      "Training Loss: 0.011672332877060399\n",
      "Training Loss: 0.011184898087522014\n",
      "Validation Loss: 0.00903549598885721\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.011353357057087123\n",
      "Training Loss: 0.011103027714416385\n",
      "Training Loss: 0.010649353340268135\n",
      "Validation Loss: 0.008443358772842403\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.01087864581728354\n",
      "Training Loss: 0.010670565795153379\n",
      "Training Loss: 0.01024877403047867\n",
      "Validation Loss: 0.007980016054941362\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.010526161522138864\n",
      "Training Loss: 0.010342166257323697\n",
      "Training Loss: 0.009947077103424818\n",
      "Validation Loss: 0.00761035695839464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.010261038389289752\n",
      "Training Loss: 0.010086169630521908\n",
      "Training Loss: 0.009712331786286086\n",
      "Validation Loss: 0.007306003239361591\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.01005457635736093\n",
      "Training Loss: 0.009878460053587333\n",
      "Training Loss: 0.00952150280936621\n",
      "Validation Loss: 0.007047903739711207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.009886568203801288\n",
      "Training Loss: 0.009702645587967709\n",
      "Training Loss: 0.00935926312347874\n",
      "Validation Loss: 0.006823829788547219\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.009743228778243065\n",
      "Training Loss: 0.009547255773795768\n",
      "Training Loss: 0.00921474201604724\n",
      "Validation Loss: 0.006624929593834147\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.009614144284278155\n",
      "Training Loss: 0.009402808177983389\n",
      "Training Loss: 0.009078705358551814\n",
      "Validation Loss: 0.006442986227299893\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.009489943254739047\n",
      "Training Loss: 0.009260084802517667\n",
      "Training Loss: 0.008942412425531074\n",
      "Validation Loss: 0.00626912024499995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.009362190370447934\n",
      "Training Loss: 0.00911180388997309\n",
      "Training Loss: 0.008799890611553564\n",
      "Validation Loss: 0.006096090729023968\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.009227002973202615\n",
      "Training Loss: 0.00895752977230586\n",
      "Training Loss: 0.008651581708109006\n",
      "Validation Loss: 0.005922961101103364\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.009087224397808314\n",
      "Training Loss: 0.008802654533647002\n",
      "Training Loss: 0.008501954032108188\n",
      "Validation Loss: 0.005753789889741312\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008947363252518699\n",
      "Training Loss: 0.00865138195687905\n",
      "Training Loss: 0.008354436946101487\n",
      "Validation Loss: 0.005592563166021464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008809733316302299\n",
      "Training Loss: 0.008504935121163727\n",
      "Training Loss: 0.008210564004257322\n",
      "Validation Loss: 0.005441656014411135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008675255322596059\n",
      "Training Loss: 0.00836335840052925\n",
      "Training Loss: 0.008071221405407414\n",
      "Validation Loss: 0.0053025268101960085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008545052997069433\n",
      "Training Loss: 0.008227093482855708\n",
      "Training Loss: 0.007938008812488987\n",
      "Validation Loss: 0.0051762787677515085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008421667078509927\n",
      "Training Loss: 0.00809852200676687\n",
      "Training Loss: 0.007814187842886894\n",
      "Validation Loss: 0.005063898639041889\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.00830891881370917\n",
      "Training Loss: 0.007981717537622899\n",
      "Training Loss: 0.00770325546967797\n",
      "Validation Loss: 0.004965773859490337\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008209600823465734\n",
      "Training Loss: 0.00787935065338388\n",
      "Training Loss: 0.0076067997212521735\n",
      "Validation Loss: 0.0048809844733665834\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008124362338567153\n",
      "Training Loss: 0.007791592329740524\n",
      "Training Loss: 0.007524428491014987\n",
      "Validation Loss: 0.004807547830945153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008052235655486583\n",
      "Training Loss: 0.007717031813226641\n",
      "Training Loss: 0.007454507887596265\n",
      "Validation Loss: 0.004743196988436445\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007991380889434367\n",
      "Training Loss: 0.007653599249897525\n",
      "Training Loss: 0.007394891848089173\n",
      "Validation Loss: 0.004685918167442753\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007939714484964498\n",
      "Training Loss: 0.007599178645759821\n",
      "Training Loss: 0.007343482155702077\n",
      "Validation Loss: 0.004634174986564544\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.00789533182804007\n",
      "Training Loss: 0.007551924539729953\n",
      "Training Loss: 0.007298504127538763\n",
      "Validation Loss: 0.00458688494812153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007856669332832099\n",
      "Training Loss: 0.007510350012453273\n",
      "Training Loss: 0.007258566007367335\n",
      "Validation Loss: 0.004543296856742897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007822513118735514\n",
      "Training Loss: 0.0074733010167256\n",
      "Training Loss: 0.007222617723746225\n",
      "Validation Loss: 0.00450289223521027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007791939337621443\n",
      "Training Loss: 0.007439899421297014\n",
      "Training Loss: 0.007189876008778811\n",
      "Validation Loss: 0.004465293217701524\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0077642524865223096\n",
      "Training Loss: 0.00740947739337571\n",
      "Training Loss: 0.0071597564057447015\n",
      "Validation Loss: 0.004430217686726638\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007738923315773718\n",
      "Training Loss: 0.007381519465707242\n",
      "Training Loss: 0.007131816316978075\n",
      "Validation Loss: 0.004397433068017277\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007715544299571775\n",
      "Training Loss: 0.007355628043878824\n",
      "Training Loss: 0.00710571399948094\n",
      "Validation Loss: 0.004366754164416971\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007693797450046986\n",
      "Training Loss: 0.007331489019561559\n",
      "Training Loss: 0.007081184682319872\n",
      "Validation Loss: 0.004338015701485735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007673432471929118\n",
      "Training Loss: 0.007308859815821051\n",
      "Training Loss: 0.007058021005941555\n",
      "Validation Loss: 0.0043110832312991945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007654250038322061\n",
      "Training Loss: 0.007287540785036981\n",
      "Training Loss: 0.0070360495045315476\n",
      "Validation Loss: 0.004285820919413413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007636084863333963\n",
      "Training Loss: 0.007267373835202306\n",
      "Training Loss: 0.007015136259142309\n",
      "Validation Loss: 0.00426211954601988\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007618803939549252\n",
      "Training Loss: 0.007248232670826837\n",
      "Training Loss: 0.006995168953435496\n",
      "Validation Loss: 0.004239873610060202\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.0076023001183057205\n",
      "Training Loss: 0.007230010298080743\n",
      "Training Loss: 0.00697605264489539\n",
      "Validation Loss: 0.004218984949873404\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007586479460005648\n",
      "Training Loss: 0.0072126219642814245\n",
      "Training Loss: 0.006957711675204337\n",
      "Validation Loss: 0.004199371006067717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007571267391322181\n",
      "Training Loss: 0.00719599373289384\n",
      "Training Loss: 0.006940078117186204\n",
      "Validation Loss: 0.004180949283345194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007556598106166348\n",
      "Training Loss: 0.007180065648863092\n",
      "Training Loss: 0.006923095880774781\n",
      "Validation Loss: 0.004163642008422634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0075424144114367665\n",
      "Training Loss: 0.007164783792104572\n",
      "Training Loss: 0.006906711560441181\n",
      "Validation Loss: 0.004147375617280937\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007528669274179265\n",
      "Training Loss: 0.007150101026054472\n",
      "Training Loss: 0.0068908838473726065\n",
      "Validation Loss: 0.004132084919955958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007515319364028983\n",
      "Training Loss: 0.007135974789271131\n",
      "Training Loss: 0.006875568599207326\n",
      "Validation Loss: 0.004117699040445301\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.00750232381396927\n",
      "Training Loss: 0.007122365426039323\n",
      "Training Loss: 0.006860728609608486\n",
      "Validation Loss: 0.004104156240695313\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007489651910145767\n",
      "Training Loss: 0.007109238982666284\n",
      "Training Loss: 0.006846328994724899\n",
      "Validation Loss: 0.004091397617169226\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007477269983501174\n",
      "Training Loss: 0.007096558741759509\n",
      "Training Loss: 0.006832337729865685\n",
      "Validation Loss: 0.004079366543623252\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007465147936018184\n",
      "Training Loss: 0.007084292819490657\n",
      "Training Loss: 0.006818723182659596\n",
      "Validation Loss: 0.00406799831014294\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007453262553317472\n",
      "Training Loss: 0.00707240924355574\n",
      "Training Loss: 0.0068054541968740525\n",
      "Validation Loss: 0.0040572429824034485\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007441585693159141\n",
      "Training Loss: 0.007060876678442582\n",
      "Training Loss: 0.006792502705939114\n",
      "Validation Loss: 0.0040470466326419895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007430096719181165\n",
      "Training Loss: 0.007049666700186208\n",
      "Training Loss: 0.006779840951785445\n",
      "Validation Loss: 0.004037359221890736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007418772949022241\n",
      "Training Loss: 0.007038751252694055\n",
      "Training Loss: 0.006767444555880502\n",
      "Validation Loss: 0.00402813493614242\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007407594752730801\n",
      "Training Loss: 0.007028102084295824\n",
      "Training Loss: 0.006755288902204484\n",
      "Validation Loss: 0.0040193316238942775\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007396544180810451\n",
      "Training Loss: 0.007017696226248518\n",
      "Training Loss: 0.006743352082557976\n",
      "Validation Loss: 0.004010908535859558\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007385603212169372\n",
      "Training Loss: 0.007007506841327995\n",
      "Training Loss: 0.006731611444847658\n",
      "Validation Loss: 0.00400281931650354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007374758049845696\n",
      "Training Loss: 0.00699751170235686\n",
      "Training Loss: 0.006720045324182138\n",
      "Validation Loss: 0.003995027971255143\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007363993232720531\n",
      "Training Loss: 0.006987690558889881\n",
      "Training Loss: 0.006708636985858902\n",
      "Validation Loss: 0.003987505660507451\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007353294719941914\n",
      "Training Loss: 0.006978021905524656\n",
      "Training Loss: 0.006697368138702586\n",
      "Validation Loss: 0.003980220368738924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.0073426520341308785\n",
      "Training Loss: 0.0069684896722901615\n",
      "Training Loss: 0.00668622252997011\n",
      "Validation Loss: 0.003973143043132562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007332053549471311\n",
      "Training Loss: 0.0069590737839462236\n",
      "Training Loss: 0.006675186043139547\n",
      "Validation Loss: 0.003966244367955859\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007321488258894533\n",
      "Training Loss: 0.006949760071001947\n",
      "Training Loss: 0.006664242260158062\n",
      "Validation Loss: 0.003959500141902252\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007310946111683734\n",
      "Training Loss: 0.006940533244051039\n",
      "Training Loss: 0.006653381616342813\n",
      "Validation Loss: 0.003952891958270515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007300418907543644\n",
      "Training Loss: 0.0069313804880948736\n",
      "Training Loss: 0.006642589098773897\n",
      "Validation Loss: 0.003946400832206932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.0072898977797012775\n",
      "Training Loss: 0.006922288276255131\n",
      "Training Loss: 0.00663185553625226\n",
      "Validation Loss: 0.003939999582695911\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007279376499936916\n",
      "Training Loss: 0.0069132445566356185\n",
      "Training Loss: 0.006621169855352491\n",
      "Validation Loss: 0.0039336766937768525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.0072688452922739085\n",
      "Training Loss: 0.0069042402785271404\n",
      "Training Loss: 0.006610523668350652\n",
      "Validation Loss: 0.003927415538987333\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007258298537344672\n",
      "Training Loss: 0.006895264014601708\n",
      "Training Loss: 0.006599904543254525\n",
      "Validation Loss: 0.003921203610874461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007247727349167689\n",
      "Training Loss: 0.006886307119275443\n",
      "Training Loss: 0.006589308858383447\n",
      "Validation Loss: 0.003915027604082578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007237126015825197\n",
      "Training Loss: 0.006877360731596127\n",
      "Training Loss: 0.0065787268418353055\n",
      "Validation Loss: 0.00390887481233712\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.0072264877241104844\n",
      "Training Loss: 0.006868416992947459\n",
      "Training Loss: 0.006568151895189658\n",
      "Validation Loss: 0.0039027333808958195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007215806333697401\n",
      "Training Loss: 0.006859467456815764\n",
      "Training Loss: 0.006557577436324209\n",
      "Validation Loss: 0.003896598316980212\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007205075020901858\n",
      "Training Loss: 0.006850507767521777\n",
      "Training Loss: 0.0065469990309793506\n",
      "Validation Loss: 0.0038904595290823432\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007194287804304622\n",
      "Training Loss: 0.006841528227669187\n",
      "Training Loss: 0.006536409414839\n",
      "Validation Loss: 0.0038843065555196968\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007183440868393518\n",
      "Training Loss: 0.006832526082871482\n",
      "Training Loss: 0.006525805291021243\n",
      "Validation Loss: 0.0038781377560134682\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007172526739304885\n",
      "Training Loss: 0.006823495319113135\n",
      "Training Loss: 0.006515181653667241\n",
      "Validation Loss: 0.003871945835639503\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.0071615408430807295\n",
      "Training Loss: 0.006814432804239914\n",
      "Training Loss: 0.0065045386482961475\n",
      "Validation Loss: 0.0038657253111053385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007150483942241408\n",
      "Training Loss: 0.006805334240780212\n",
      "Training Loss: 0.006493872732389719\n",
      "Validation Loss: 0.0038594730758223306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007139352241647429\n",
      "Training Loss: 0.006796202205587179\n",
      "Training Loss: 0.0064831841958221045\n",
      "Validation Loss: 0.0038531896189964387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007128146304748953\n",
      "Training Loss: 0.00678703319048509\n",
      "Training Loss: 0.006472472392488271\n",
      "Validation Loss: 0.0038468713971545523\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.0071168671373743565\n",
      "Training Loss: 0.00677783124090638\n",
      "Training Loss: 0.006461740991799161\n",
      "Validation Loss: 0.003840518263796491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007105522769270465\n",
      "Training Loss: 0.006768600589130074\n",
      "Training Loss: 0.006450991517631337\n",
      "Validation Loss: 0.0038341365903720595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007094117382657714\n",
      "Training Loss: 0.006759347526822239\n",
      "Training Loss: 0.0064402319607324895\n",
      "Validation Loss: 0.0038277256810054015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007082662115572021\n",
      "Training Loss: 0.006750076748430729\n",
      "Training Loss: 0.006429461993975565\n",
      "Validation Loss: 0.0038212813198922222\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007071170496055856\n",
      "Training Loss: 0.0067407981713768095\n",
      "Training Loss: 0.006418690289137885\n",
      "Validation Loss: 0.0038148183061668044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007059652426396497\n",
      "Training Loss: 0.006731524267815985\n",
      "Training Loss: 0.0064079244364984335\n",
      "Validation Loss: 0.003808335588167139\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007048123279237189\n",
      "Training Loss: 0.0067222593846963715\n",
      "Training Loss: 0.006397165468661115\n",
      "Validation Loss: 0.003801839355525843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007036593449884094\n",
      "Training Loss: 0.006713015274726786\n",
      "Training Loss: 0.006386420626658946\n",
      "Validation Loss: 0.003795331858067114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007025075403507799\n",
      "Training Loss: 0.006703797292429954\n",
      "Training Loss: 0.006375692191068083\n",
      "Validation Loss: 0.003788815052614788\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007013578034820967\n",
      "Training Loss: 0.006694612502469681\n",
      "Training Loss: 0.00636498233070597\n",
      "Validation Loss: 0.003782293046406062\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007002107632579282\n",
      "Training Loss: 0.006685461383312941\n",
      "Training Loss: 0.00635429157409817\n",
      "Validation Loss: 0.0037757622253338104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006990667106001638\n",
      "Training Loss: 0.006676346822059713\n",
      "Training Loss: 0.006343617410166189\n",
      "Validation Loss: 0.0037692235730421007\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0069792574387975035\n",
      "Training Loss: 0.006667268357123248\n",
      "Training Loss: 0.00633295914856717\n",
      "Validation Loss: 0.00376267339563353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006967876476701349\n",
      "Training Loss: 0.006658222439000383\n",
      "Training Loss: 0.006322311805561185\n",
      "Validation Loss: 0.0037561070600398974\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006956521380343475\n",
      "Training Loss: 0.006649207905284129\n",
      "Training Loss: 0.006311674760654569\n",
      "Validation Loss: 0.0037495289821952078\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006945185788208619\n",
      "Training Loss: 0.00664022151962854\n",
      "Training Loss: 0.006301045713480562\n",
      "Validation Loss: 0.0037429282923093004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006933864668244496\n",
      "Training Loss: 0.006631257346016355\n",
      "Training Loss: 0.006290417008567601\n",
      "Validation Loss: 0.003736306478928649\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006922550371964462\n",
      "Training Loss: 0.006622312552062795\n",
      "Training Loss: 0.0062797864293679595\n",
      "Validation Loss: 0.003729659293548026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006911238253815099\n",
      "Training Loss: 0.0066133832768537106\n",
      "Training Loss: 0.00626915218308568\n",
      "Validation Loss: 0.0037229815341029942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.00689991886203643\n",
      "Training Loss: 0.00660446455469355\n",
      "Training Loss: 0.006258511093910783\n",
      "Validation Loss: 0.0037162657971023007\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.0068885870865779\n",
      "Training Loss: 0.006595553493825719\n",
      "Training Loss: 0.006247857806738466\n",
      "Validation Loss: 0.003709514537732002\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006877237398875877\n",
      "Training Loss: 0.00658664736954961\n",
      "Training Loss: 0.0062371909793000665\n",
      "Validation Loss: 0.003702721949614417\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006865861605037935\n",
      "Training Loss: 0.006577740648644977\n",
      "Training Loss: 0.006226506618550048\n",
      "Validation Loss: 0.0036958857135993713\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006854454696294852\n",
      "Training Loss: 0.006568832275806926\n",
      "Training Loss: 0.006215803903760388\n",
      "Validation Loss: 0.0036890015144717325\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.0068430105003062635\n",
      "Training Loss: 0.0065599172271322455\n",
      "Training Loss: 0.006205077854683622\n",
      "Validation Loss: 0.0036820673378540223\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.0068315229570725935\n",
      "Training Loss: 0.006550992361735552\n",
      "Training Loss: 0.00619432678911835\n",
      "Validation Loss: 0.003675077249180902\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.0068199863831978295\n",
      "Training Loss: 0.006542055012541823\n",
      "Training Loss: 0.006183546776883304\n",
      "Validation Loss: 0.0036680320306586916\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006808395033003762\n",
      "Training Loss: 0.006533102093380876\n",
      "Training Loss: 0.0061727346677798774\n",
      "Validation Loss: 0.003660917988444647\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006796739597339183\n",
      "Training Loss: 0.006524127425509505\n",
      "Training Loss: 0.006161887366324663\n",
      "Validation Loss: 0.0036537411486667194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006785017758957111\n",
      "Training Loss: 0.006515128081082367\n",
      "Training Loss: 0.0061509974405635146\n",
      "Validation Loss: 0.0036464887252600675\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006773218922317028\n",
      "Training Loss: 0.006506098379613832\n",
      "Training Loss: 0.0061400617007166145\n",
      "Validation Loss: 0.003639151755552948\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.0067613365378929305\n",
      "Training Loss: 0.0064970320794964205\n",
      "Training Loss: 0.006129074993077665\n",
      "Validation Loss: 0.0036317315802836185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006749360231915489\n",
      "Training Loss: 0.006487924117827788\n",
      "Training Loss: 0.006118031068472192\n",
      "Validation Loss: 0.003624212688745491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0067372820258606225\n",
      "Training Loss: 0.006478764527710155\n",
      "Training Loss: 0.006106921094469726\n",
      "Validation Loss: 0.003616582638887542\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.00672508932184428\n",
      "Training Loss: 0.006469547047745436\n",
      "Training Loss: 0.006095735322451219\n",
      "Validation Loss: 0.0036088306917233413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006712771122111007\n",
      "Training Loss: 0.006460260027670302\n",
      "Training Loss: 0.006084464124869555\n",
      "Validation Loss: 0.00360094298990548\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006700311852619052\n",
      "Training Loss: 0.006450890489504673\n",
      "Training Loss: 0.006073096456239\n",
      "Validation Loss: 0.0035928951490021657\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006687696216395125\n",
      "Training Loss: 0.006441425021039322\n",
      "Training Loss: 0.00606161673553288\n",
      "Validation Loss: 0.003584676558214627\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006674904791871086\n",
      "Training Loss: 0.006431847172207199\n",
      "Training Loss: 0.006050010205945\n",
      "Validation Loss: 0.003576246659229562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006661918532336131\n",
      "Training Loss: 0.006422137925983407\n",
      "Training Loss: 0.0060382561001461\n",
      "Validation Loss: 0.0035675852127854575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006648712743190117\n",
      "Training Loss: 0.006412275122711435\n",
      "Training Loss: 0.006026335273636505\n",
      "Validation Loss: 0.0035586561235923615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006635259778122417\n",
      "Training Loss: 0.006402234239503741\n",
      "Training Loss: 0.006014223452657461\n",
      "Validation Loss: 0.0035494207430714635\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006621533146826551\n",
      "Training Loss: 0.006391986021772027\n",
      "Training Loss: 0.006001893168431706\n",
      "Validation Loss: 0.0035398345312438488\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006607499318779447\n",
      "Training Loss: 0.006381499802228063\n",
      "Training Loss: 0.005989315145416185\n",
      "Validation Loss: 0.0035298497290507463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00659312489151489\n",
      "Training Loss: 0.006370740680140443\n",
      "Training Loss: 0.005976458553923294\n",
      "Validation Loss: 0.00351941425960218\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006578376117977314\n",
      "Training Loss: 0.006359672058024444\n",
      "Training Loss: 0.005963289002538659\n",
      "Validation Loss: 0.003508477058429062\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006563216224894859\n",
      "Training Loss: 0.0063482556020608176\n",
      "Training Loss: 0.005949773988104426\n",
      "Validation Loss: 0.003496982669981008\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006547612661961466\n",
      "Training Loss: 0.00633645388064906\n",
      "Training Loss: 0.005935878356103786\n",
      "Validation Loss: 0.003484885909416702\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006531538016861305\n",
      "Training Loss: 0.006324231427861378\n",
      "Training Loss: 0.005921572868828662\n",
      "Validation Loss: 0.003472148829265341\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006514969883719459\n",
      "Training Loss: 0.006311556769651361\n",
      "Training Loss: 0.005906831138418056\n",
      "Validation Loss: 0.0034587430660455915\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006497897693188861\n",
      "Training Loss: 0.006298403577529825\n",
      "Training Loss: 0.0058916304248850795\n",
      "Validation Loss: 0.003444646496922196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006480321463895961\n",
      "Training Loss: 0.006284754488733597\n",
      "Training Loss: 0.005875954233342782\n",
      "Validation Loss: 0.0034298421762716234\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.00646225429023616\n",
      "Training Loss: 0.0062706000875914466\n",
      "Training Loss: 0.005859793584095314\n",
      "Validation Loss: 0.003414342436269763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.00644371974398382\n",
      "Training Loss: 0.006255937432288192\n",
      "Training Loss: 0.005843145645339973\n",
      "Validation Loss: 0.0033981479479397616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006424750933074392\n",
      "Training Loss: 0.006240770818549208\n",
      "Training Loss: 0.005826009861193597\n",
      "Validation Loss: 0.003381261053976467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006405387714039534\n",
      "Training Loss: 0.006225106277270243\n",
      "Training Loss: 0.005808383175171912\n",
      "Validation Loss: 0.0033636681651801207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006385668643051758\n",
      "Training Loss: 0.00620894410123583\n",
      "Training Loss: 0.005790264478418976\n",
      "Validation Loss: 0.0033453513065018177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006365630882792175\n",
      "Training Loss: 0.006192288545425981\n",
      "Training Loss: 0.0057716472883475945\n",
      "Validation Loss: 0.003326267780987232\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006345300428802147\n",
      "Training Loss: 0.006175129893817939\n",
      "Training Loss: 0.005752517539658584\n",
      "Validation Loss: 0.0033063390488432868\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0063246994488872586\n",
      "Training Loss: 0.006157460302347317\n",
      "Training Loss: 0.005732857309048996\n",
      "Validation Loss: 0.003285494969184563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006303844338981434\n",
      "Training Loss: 0.0061392714706016704\n",
      "Training Loss: 0.005712657034164295\n",
      "Validation Loss: 0.003263649135681518\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006282753645209595\n",
      "Training Loss: 0.00612056709418539\n",
      "Training Loss: 0.005691917182411998\n",
      "Validation Loss: 0.0032407513628661465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006261459641391412\n",
      "Training Loss: 0.0061013767868280415\n",
      "Training Loss: 0.005670667646336369\n",
      "Validation Loss: 0.003216791994806923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.0062400197272654626\n",
      "Training Loss: 0.006081766535644419\n",
      "Training Loss: 0.005648976921802387\n",
      "Validation Loss: 0.003191848947363121\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.0062185233854688705\n",
      "Training Loss: 0.006061849727993831\n",
      "Training Loss: 0.005626963254762814\n",
      "Validation Loss: 0.0031660916702298636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006197096472606063\n",
      "Training Loss: 0.006041788415168412\n",
      "Training Loss: 0.005604790196521207\n",
      "Validation Loss: 0.00313979546733075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.00617589674599003\n",
      "Training Loss: 0.006021780931041576\n",
      "Training Loss: 0.005582663296954706\n",
      "Validation Loss: 0.0031133030836857606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006155097775044851\n",
      "Training Loss: 0.006002048194059171\n",
      "Training Loss: 0.005560811376199126\n",
      "Validation Loss: 0.0030870268584406945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006134876259602606\n",
      "Training Loss: 0.005982813416630961\n",
      "Training Loss: 0.005539470204384997\n",
      "Validation Loss: 0.003061371274669184\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006115390881313942\n",
      "Training Loss: 0.005964274795842357\n",
      "Training Loss: 0.0055188556417124345\n",
      "Validation Loss: 0.003036711762627859\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006096770012518391\n",
      "Training Loss: 0.005946589370141737\n",
      "Training Loss: 0.005499143141205423\n",
      "Validation Loss: 0.003013345220628498\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.00607909903221298\n",
      "Training Loss: 0.0059298510989174245\n",
      "Training Loss: 0.005480453125783242\n",
      "Validation Loss: 0.002991470309455743\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.00606241311703343\n",
      "Training Loss: 0.005914094788604416\n",
      "Training Loss: 0.005462845438160002\n",
      "Validation Loss: 0.0029711864098613517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.0060467028606217355\n",
      "Training Loss: 0.005899292404064909\n",
      "Training Loss: 0.0054463173111435025\n",
      "Validation Loss: 0.0029524947278557366\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.00603192146809306\n",
      "Training Loss: 0.005885368880117312\n",
      "Training Loss: 0.005430818410823122\n",
      "Validation Loss: 0.002935327750400462\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006017991118133068\n",
      "Training Loss: 0.0058722150512039665\n",
      "Training Loss: 0.0054162594862282275\n",
      "Validation Loss: 0.0029195712505201443\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006004815416526981\n",
      "Training Loss: 0.005859701014123857\n",
      "Training Loss: 0.005402526775142178\n",
      "Validation Loss: 0.002905087614840085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005992288226843811\n",
      "Training Loss: 0.005847690054215491\n",
      "Training Loss: 0.005389496191055514\n",
      "Validation Loss: 0.002891727922002921\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005980303900432773\n",
      "Training Loss: 0.0058360515703679994\n",
      "Training Loss: 0.005377036029822193\n",
      "Validation Loss: 0.002879347139446253\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005968755250214599\n",
      "Training Loss: 0.005824663851526566\n",
      "Training Loss: 0.00536502234521322\n",
      "Validation Loss: 0.002867815133152802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.0059575473732547835\n",
      "Training Loss: 0.005813425102969632\n",
      "Training Loss: 0.005353336096741259\n",
      "Validation Loss: 0.0028570154956489634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005946588704246096\n",
      "Training Loss: 0.005802254346199334\n",
      "Training Loss: 0.005341873809811659\n",
      "Validation Loss: 0.0028468382693931796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005935806042398326\n",
      "Training Loss: 0.005791094554588198\n",
      "Training Loss: 0.00533054830157198\n",
      "Validation Loss: 0.0028371775412952965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005925135716097429\n",
      "Training Loss: 0.005779907045071014\n",
      "Training Loss: 0.005319287303136661\n",
      "Validation Loss: 0.0028279322432354093\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005914531183661893\n",
      "Training Loss: 0.005768670142279006\n",
      "Training Loss: 0.005308036467758939\n",
      "Validation Loss: 0.002818993584441251\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005903958649141714\n",
      "Training Loss: 0.005757366770994849\n",
      "Training Loss: 0.005296753988368437\n",
      "Validation Loss: 0.0028102535733513616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005893395398743451\n",
      "Training Loss: 0.005745985574903898\n",
      "Training Loss: 0.005285411136574112\n",
      "Validation Loss: 0.00280160701891219\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005882823985884898\n",
      "Training Loss: 0.005734516144730151\n",
      "Training Loss: 0.0052739872259553525\n",
      "Validation Loss: 0.0027929497503392984\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005872234886628576\n",
      "Training Loss: 0.00572294854791835\n",
      "Training Loss: 0.0052624683221802115\n",
      "Validation Loss: 0.0027841951376287623\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005861616930342279\n",
      "Training Loss: 0.005711273680790327\n",
      "Training Loss: 0.00525084329477977\n",
      "Validation Loss: 0.0027752559830884586\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005850958378287033\n",
      "Training Loss: 0.005699480455950834\n",
      "Training Loss: 0.005239102626801468\n",
      "Validation Loss: 0.0027660618580647566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.0058402493596076965\n",
      "Training Loss: 0.005687560609076172\n",
      "Training Loss: 0.005227237786748447\n",
      "Validation Loss: 0.002756548379652537\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005829472346231341\n",
      "Training Loss: 0.00567550310457591\n",
      "Training Loss: 0.00521524474141188\n",
      "Validation Loss: 0.0027466597299750767\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005818608582485467\n",
      "Training Loss: 0.005663299811421893\n",
      "Training Loss: 0.0052031157293822615\n",
      "Validation Loss: 0.002736350919088621\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005807635996607132\n",
      "Training Loss: 0.005650940971099772\n",
      "Training Loss: 0.005190845053875819\n",
      "Validation Loss: 0.0027255787228998006\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005796526496415027\n",
      "Training Loss: 0.005638417167356238\n",
      "Training Loss: 0.005178424561163411\n",
      "Validation Loss: 0.002714310861663537\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005785245536244475\n",
      "Training Loss: 0.005625719000818208\n",
      "Training Loss: 0.005165848837932572\n",
      "Validation Loss: 0.002702517034695222\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005773759216535837\n",
      "Training Loss: 0.00561284032126423\n",
      "Training Loss: 0.005153110024984926\n",
      "Validation Loss: 0.002690175064222029\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.0057620256522204726\n",
      "Training Loss: 0.005599780764314346\n",
      "Training Loss: 0.005140199792804196\n",
      "Validation Loss: 0.0026772724702598507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005750007869792171\n",
      "Training Loss: 0.005586542736273259\n",
      "Training Loss: 0.005127115512732416\n",
      "Validation Loss: 0.0026638191742717884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005737671788083389\n",
      "Training Loss: 0.005573144205263816\n",
      "Training Loss: 0.005113857553806156\n",
      "Validation Loss: 0.002649839351403663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005724997855140828\n",
      "Training Loss: 0.005559613392688334\n",
      "Training Loss: 0.005100436748471111\n",
      "Validation Loss: 0.0026354037560103985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005711995011079125\n",
      "Training Loss: 0.005546001332113519\n",
      "Training Loss: 0.005086875953129493\n",
      "Validation Loss: 0.002620623283496315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005698703562957235\n",
      "Training Loss: 0.005532377886702307\n",
      "Training Loss: 0.005073224692605436\n",
      "Validation Loss: 0.002605670539850599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005685215244884603\n",
      "Training Loss: 0.005518832203815691\n",
      "Training Loss: 0.00505955042433925\n",
      "Validation Loss: 0.0025907617933483102\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.0056716630654409525\n",
      "Training Loss: 0.005505461705033667\n",
      "Training Loss: 0.005045951376669109\n",
      "Validation Loss: 0.0025761373683789307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005658203553175554\n",
      "Training Loss: 0.005492371038999408\n",
      "Training Loss: 0.00503254137642216\n",
      "Validation Loss: 0.0025620296351569756\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005644996363553218\n",
      "Training Loss: 0.005479658581898548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [07:32<17:35, 150.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.005019438926829025\n",
      "Validation Loss: 0.0025486233003249163\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.07858436740934849\n",
      "Training Loss: 0.07201367937028408\n",
      "Training Loss: 0.06891362505033612\n",
      "Validation Loss: 0.062339771706401634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06363692801445722\n",
      "Training Loss: 0.06200914217159152\n",
      "Training Loss: 0.05980329712852836\n",
      "Validation Loss: 0.05318022933736276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.05411225249059498\n",
      "Training Loss: 0.051562070082873106\n",
      "Training Loss: 0.048637196756899356\n",
      "Validation Loss: 0.04195287055597546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.04268436682410538\n",
      "Training Loss: 0.03990974804386496\n",
      "Training Loss: 0.037309535378590226\n",
      "Validation Loss: 0.03169153074032805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.03260359033942223\n",
      "Training Loss: 0.030415616035461425\n",
      "Training Loss: 0.028616116107441483\n",
      "Validation Loss: 0.024254333939445153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.025391549924388527\n",
      "Training Loss: 0.023748204745352268\n",
      "Training Loss: 0.02244384245481342\n",
      "Validation Loss: 0.01893594077254614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.020237184958532453\n",
      "Training Loss: 0.01891245011938736\n",
      "Training Loss: 0.017829005958046765\n",
      "Validation Loss: 0.014855010058270411\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.016347850651945918\n",
      "Training Loss: 0.01536791333463043\n",
      "Training Loss: 0.014584421282634139\n",
      "Validation Loss: 0.012076188967134176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.013876799284480512\n",
      "Training Loss: 0.013254612328018993\n",
      "Training Loss: 0.012687299880199135\n",
      "Validation Loss: 0.010314928655502167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.012356751812621952\n",
      "Training Loss: 0.011895814182935283\n",
      "Training Loss: 0.011381933747325093\n",
      "Validation Loss: 0.009013816688126057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.011233351583359762\n",
      "Training Loss: 0.010877571889432147\n",
      "Training Loss: 0.01038269019802101\n",
      "Validation Loss: 0.008016740939883369\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.010372317999135704\n",
      "Training Loss: 0.010097054955549538\n",
      "Training Loss: 0.009610760583309457\n",
      "Validation Loss: 0.007250816629299622\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.009716373167466373\n",
      "Training Loss: 0.009499591963831336\n",
      "Training Loss: 0.009016623018542304\n",
      "Validation Loss: 0.006658151709171159\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.009219984377268701\n",
      "Training Loss: 0.009042192345950752\n",
      "Training Loss: 0.008559838820947335\n",
      "Validation Loss: 0.00619224749055555\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.00884496126556769\n",
      "Training Loss: 0.008690653233788908\n",
      "Training Loss: 0.008207891987403855\n",
      "Validation Loss: 0.005818963689164499\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.008561115727061405\n",
      "Training Loss: 0.008419010901125148\n",
      "Training Loss: 0.007935776090016588\n",
      "Validation Loss: 0.005515072615084688\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.008345499346032738\n",
      "Training Loss: 0.008207820125389844\n",
      "Training Loss: 0.007724495120346547\n",
      "Validation Loss: 0.005265233193241646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.008180762545671315\n",
      "Training Loss: 0.00804229614092037\n",
      "Training Loss: 0.007559420933248475\n",
      "Validation Loss: 0.005058959756946463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.008053679395234213\n",
      "Training Loss: 0.007911011320538818\n",
      "Training Loss: 0.007429192665731534\n",
      "Validation Loss: 0.004888493256392271\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.007954198947409168\n",
      "Training Loss: 0.007805175946559757\n",
      "Training Loss: 0.007325043356977403\n",
      "Validation Loss: 0.004747539677583937\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.00787481463397853\n",
      "Training Loss: 0.007718162754317745\n",
      "Training Loss: 0.007240330475615338\n",
      "Validation Loss: 0.004630747818900795\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.007810049568070099\n",
      "Training Loss: 0.0076451024843845515\n",
      "Training Loss: 0.007170116553315893\n",
      "Validation Loss: 0.004533517041002934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.00775597853702493\n",
      "Training Loss: 0.007582483313744888\n",
      "Training Loss: 0.007110783488024026\n",
      "Validation Loss: 0.004451989083749692\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.007709825766505673\n",
      "Training Loss: 0.007527796312933788\n",
      "Training Loss: 0.007059700451791286\n",
      "Validation Loss: 0.004383006180228477\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0076696328795515\n",
      "Training Loss: 0.007479258372914046\n",
      "Training Loss: 0.007014962509274482\n",
      "Validation Loss: 0.004324051826208663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.0076340142544358966\n",
      "Training Loss: 0.0074355957400985065\n",
      "Training Loss: 0.006975187339121476\n",
      "Validation Loss: 0.0042731648604019306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.007601986537920311\n",
      "Training Loss: 0.007395889310864732\n",
      "Training Loss: 0.006939365119906143\n",
      "Validation Loss: 0.00422882516816091\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007572838568594306\n",
      "Training Loss: 0.007359468633076176\n",
      "Training Loss: 0.006906752499053255\n",
      "Validation Loss: 0.004189851657183987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.0075460505380760875\n",
      "Training Loss: 0.007325831764610484\n",
      "Training Loss: 0.00687679422320798\n",
      "Validation Loss: 0.004155330085247922\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.007521233943989501\n",
      "Training Loss: 0.007294592809630558\n",
      "Training Loss: 0.006849065916612745\n",
      "Validation Loss: 0.0041245448434453335\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007498091631568968\n",
      "Training Loss: 0.007265449881087988\n",
      "Training Loss: 0.00682323805638589\n",
      "Validation Loss: 0.004096922739356589\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007476389664225281\n",
      "Training Loss: 0.007238158468389884\n",
      "Training Loss: 0.006799050346016884\n",
      "Validation Loss: 0.004072010163950284\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007455941501539201\n",
      "Training Loss: 0.00721251271199435\n",
      "Training Loss: 0.006776290283305571\n",
      "Validation Loss: 0.004049429765462959\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007436592192389071\n",
      "Training Loss: 0.007188341633882374\n",
      "Training Loss: 0.0067547850799746815\n",
      "Validation Loss: 0.0040288785815218026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007418212704360485\n",
      "Training Loss: 0.007165494171204045\n",
      "Training Loss: 0.00673438860103488\n",
      "Validation Loss: 0.004010100913066627\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007400691274087876\n",
      "Training Loss: 0.007143839381169528\n",
      "Training Loss: 0.0067149767035152765\n",
      "Validation Loss: 0.003992880196205937\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007383934007957577\n",
      "Training Loss: 0.007123263360699639\n",
      "Training Loss: 0.006696445294655859\n",
      "Validation Loss: 0.003977032421481157\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007367859103251249\n",
      "Training Loss: 0.007103664566529915\n",
      "Training Loss: 0.00667870557284914\n",
      "Validation Loss: 0.003962405154146672\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007352395446505397\n",
      "Training Loss: 0.007084954137681052\n",
      "Training Loss: 0.006661680892575532\n",
      "Validation Loss: 0.003948859152321317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007337481619324535\n",
      "Training Loss: 0.00706705279299058\n",
      "Training Loss: 0.006645305076381192\n",
      "Validation Loss: 0.003936282594510344\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.0073230635758955035\n",
      "Training Loss: 0.007049892081413418\n",
      "Training Loss: 0.006629520987626165\n",
      "Validation Loss: 0.00392456912289091\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007309094591764733\n",
      "Training Loss: 0.007033411575248465\n",
      "Training Loss: 0.006614280941430479\n",
      "Validation Loss: 0.00391362878504429\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007295536691090092\n",
      "Training Loss: 0.007017558104125783\n",
      "Training Loss: 0.006599544325144962\n",
      "Validation Loss: 0.0039033841608573545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0072823522880207745\n",
      "Training Loss: 0.00700228562578559\n",
      "Training Loss: 0.0065852742805145685\n",
      "Validation Loss: 0.0038937672336973\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.00726951397722587\n",
      "Training Loss: 0.006987553611397743\n",
      "Training Loss: 0.006571438993560151\n",
      "Validation Loss: 0.003884712430345041\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007256996178766712\n",
      "Training Loss: 0.006973326378501951\n",
      "Training Loss: 0.0065580136782955375\n",
      "Validation Loss: 0.0038761612880723866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0072447773907333615\n",
      "Training Loss: 0.006959574377397075\n",
      "Training Loss: 0.006544974429998547\n",
      "Validation Loss: 0.003868065514141338\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007232837959891185\n",
      "Training Loss: 0.00694626996293664\n",
      "Training Loss: 0.006532299894606694\n",
      "Validation Loss: 0.0038603757641518886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007221162614878267\n",
      "Training Loss: 0.0069333872303832325\n",
      "Training Loss: 0.006519971175584942\n",
      "Validation Loss: 0.003853053992530436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007209736319491639\n",
      "Training Loss: 0.006920905120205134\n",
      "Training Loss: 0.00650797134032473\n",
      "Validation Loss: 0.0038460626468334474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.00719854608294554\n",
      "Training Loss: 0.006908802964026108\n",
      "Training Loss: 0.006496284600580111\n",
      "Validation Loss: 0.003839363897313479\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.00718758137547411\n",
      "Training Loss: 0.0068970622611232105\n",
      "Training Loss: 0.00648489628569223\n",
      "Validation Loss: 0.0038329272183035027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007176830995595082\n",
      "Training Loss: 0.00688566590542905\n",
      "Training Loss: 0.006473792538745329\n",
      "Validation Loss: 0.0038267266112536696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007166286058491096\n",
      "Training Loss: 0.006874596773413942\n",
      "Training Loss: 0.006462959814816713\n",
      "Validation Loss: 0.00382073458121943\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007155937051866203\n",
      "Training Loss: 0.0068638385529629885\n",
      "Training Loss: 0.006452385262819007\n",
      "Validation Loss: 0.0038149252316778464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007145775207318366\n",
      "Training Loss: 0.00685337679926306\n",
      "Training Loss: 0.0064420550956856455\n",
      "Validation Loss: 0.003809281903465561\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007135791841428727\n",
      "Training Loss: 0.006843195175752044\n",
      "Training Loss: 0.006431956994347275\n",
      "Validation Loss: 0.003803777784433509\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007125979268457741\n",
      "Training Loss: 0.0068332789943087845\n",
      "Training Loss: 0.006422079475596547\n",
      "Validation Loss: 0.0037984001434544163\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007116328646661714\n",
      "Training Loss: 0.006823614495806396\n",
      "Training Loss: 0.006412410356570035\n",
      "Validation Loss: 0.003793132751709206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007106833151774481\n",
      "Training Loss: 0.00681418847758323\n",
      "Training Loss: 0.0064029381098225716\n",
      "Validation Loss: 0.0037879611135282544\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007097483279649168\n",
      "Training Loss: 0.006804985555354506\n",
      "Training Loss: 0.006393650958780199\n",
      "Validation Loss: 0.003782870835268849\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007088271867251024\n",
      "Training Loss: 0.006795992470579222\n",
      "Training Loss: 0.0063845369592309\n",
      "Validation Loss: 0.0037778516122129527\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.0070791918179020285\n",
      "Training Loss: 0.006787196145160124\n",
      "Training Loss: 0.006375586655922234\n",
      "Validation Loss: 0.0037728945026174188\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.0070702354575041685\n",
      "Training Loss: 0.006778585190186277\n",
      "Training Loss: 0.006366789106978104\n",
      "Validation Loss: 0.0037679866955646975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0070613956614397465\n",
      "Training Loss: 0.006770147453062236\n",
      "Training Loss: 0.0063581347360741345\n",
      "Validation Loss: 0.0037631223009627185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007052666861563921\n",
      "Training Loss: 0.006761870213085786\n",
      "Training Loss: 0.0063496144965756686\n",
      "Validation Loss: 0.003758295467960533\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007044040601467714\n",
      "Training Loss: 0.006753742813598365\n",
      "Training Loss: 0.006341219301102683\n",
      "Validation Loss: 0.0037534990612145386\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007035512123256922\n",
      "Training Loss: 0.006745754446601495\n",
      "Training Loss: 0.0063329398073256014\n",
      "Validation Loss: 0.0037487235948380627\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007027074471116066\n",
      "Training Loss: 0.006737895293626934\n",
      "Training Loss: 0.006324768975609914\n",
      "Validation Loss: 0.003743966325876837\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007018722333014011\n",
      "Training Loss: 0.0067301556642632935\n",
      "Training Loss: 0.006316698044538498\n",
      "Validation Loss: 0.00373922421445212\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0070104504190385344\n",
      "Training Loss: 0.006722525805234909\n",
      "Training Loss: 0.0063087210559751835\n",
      "Validation Loss: 0.0037344913103021264\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007002253462560475\n",
      "Training Loss: 0.006714997444069013\n",
      "Training Loss: 0.006300830848049372\n",
      "Validation Loss: 0.0037297621785317746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.0069941268162801865\n",
      "Training Loss: 0.006707562706433237\n",
      "Training Loss: 0.006293021716410294\n",
      "Validation Loss: 0.003725036345631554\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006986066640820354\n",
      "Training Loss: 0.006700213495641947\n",
      "Training Loss: 0.006285286645870656\n",
      "Validation Loss: 0.003720312115077055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.0069780666520819065\n",
      "Training Loss: 0.006692942389054224\n",
      "Training Loss: 0.0062776202545501295\n",
      "Validation Loss: 0.0037155818570865674\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006970125115476549\n",
      "Training Loss: 0.006685742787085474\n",
      "Training Loss: 0.006270018288632855\n",
      "Validation Loss: 0.0037108448182305927\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00696223733597435\n",
      "Training Loss: 0.006678609261289239\n",
      "Training Loss: 0.0062624754093121736\n",
      "Validation Loss: 0.0037061006845801733\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0069543999480083584\n",
      "Training Loss: 0.00667153388261795\n",
      "Training Loss: 0.006254987579304725\n",
      "Validation Loss: 0.0037013438289587417\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006946610519662499\n",
      "Training Loss: 0.006664512432180345\n",
      "Training Loss: 0.006247549265390262\n",
      "Validation Loss: 0.003696572902779817\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006938863783143461\n",
      "Training Loss: 0.006657537878490985\n",
      "Training Loss: 0.006240157911088317\n",
      "Validation Loss: 0.0036917884998923438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006931159385712818\n",
      "Training Loss: 0.0066506067279260605\n",
      "Training Loss: 0.006232809299835935\n",
      "Validation Loss: 0.003686988672628748\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0069234926556237045\n",
      "Training Loss: 0.0066437134786974635\n",
      "Training Loss: 0.006225498747080564\n",
      "Validation Loss: 0.003682167453722756\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006915862838504836\n",
      "Training Loss: 0.006636854024836793\n",
      "Training Loss: 0.0062182238546665755\n",
      "Validation Loss: 0.0036773247176027867\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006908265880774707\n",
      "Training Loss: 0.0066300236294046045\n",
      "Training Loss: 0.006210981509648264\n",
      "Validation Loss: 0.003672458924708909\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006900700256228447\n",
      "Training Loss: 0.0066232178651262075\n",
      "Training Loss: 0.006203768424456939\n",
      "Validation Loss: 0.003667570496229141\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006893163736676798\n",
      "Training Loss: 0.0066164326434955\n",
      "Training Loss: 0.006196581709664315\n",
      "Validation Loss: 0.0036626515826315023\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006885653873905539\n",
      "Training Loss: 0.006609666524454951\n",
      "Training Loss: 0.006189418695867061\n",
      "Validation Loss: 0.003657709392509685\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006878169301198795\n",
      "Training Loss: 0.006602914659306407\n",
      "Training Loss: 0.006182277695043013\n",
      "Validation Loss: 0.0036527365485331817\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006870708459755406\n",
      "Training Loss: 0.006596174029400572\n",
      "Training Loss: 0.006175153879448772\n",
      "Validation Loss: 0.003647732935594709\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006863268750021234\n",
      "Training Loss: 0.00658944095717743\n",
      "Training Loss: 0.006168046271195635\n",
      "Validation Loss: 0.0036426981168097994\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00685584842809476\n",
      "Training Loss: 0.006582713463576511\n",
      "Training Loss: 0.006160951568745077\n",
      "Validation Loss: 0.003637629809653324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006848446816438809\n",
      "Training Loss: 0.006575989194680006\n",
      "Training Loss: 0.0061538689828012135\n",
      "Validation Loss: 0.0036325250710413028\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006841061719460413\n",
      "Training Loss: 0.006569264498539269\n",
      "Training Loss: 0.006146795729873702\n",
      "Validation Loss: 0.0036273839742238267\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006833693124353886\n",
      "Training Loss: 0.0065625371946953235\n",
      "Training Loss: 0.00613972864462994\n",
      "Validation Loss: 0.0036222076820460764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0068263379763811825\n",
      "Training Loss: 0.006555805741227232\n",
      "Training Loss: 0.006132666212506592\n",
      "Validation Loss: 0.003616992479682052\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006818995645735413\n",
      "Training Loss: 0.006549067262676544\n",
      "Training Loss: 0.006125606276327744\n",
      "Validation Loss: 0.003611738475698852\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006811664567794651\n",
      "Training Loss: 0.006542319679283537\n",
      "Training Loss: 0.006118547087535262\n",
      "Validation Loss: 0.0036064422364985005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006804343576077372\n",
      "Training Loss: 0.006535560776246712\n",
      "Training Loss: 0.006111484669381753\n",
      "Validation Loss: 0.003601100985117759\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006797031036112458\n",
      "Training Loss: 0.006528787820716388\n",
      "Training Loss: 0.00610441904515028\n",
      "Validation Loss: 0.003595717656792382\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006789726049173623\n",
      "Training Loss: 0.006522001115954481\n",
      "Training Loss: 0.00609734739176929\n",
      "Validation Loss: 0.0035902908990117772\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00678242756635882\n",
      "Training Loss: 0.006515195976244286\n",
      "Training Loss: 0.006090266132960096\n",
      "Validation Loss: 0.0035848161611140945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006775133210467175\n",
      "Training Loss: 0.006508372598909773\n",
      "Training Loss: 0.006083174858940765\n",
      "Validation Loss: 0.003579293725635396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006767842250410467\n",
      "Training Loss: 0.006501527155051008\n",
      "Training Loss: 0.006076070320559666\n",
      "Validation Loss: 0.0035737185540159096\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006760552119230851\n",
      "Training Loss: 0.006494659119052813\n",
      "Training Loss: 0.006068950848421082\n",
      "Validation Loss: 0.0035680945141220027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006753262957790867\n",
      "Training Loss: 0.006487766761565581\n",
      "Training Loss: 0.0060618137044366445\n",
      "Validation Loss: 0.003562419939514124\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006745970889460296\n",
      "Training Loss: 0.006480846682679839\n",
      "Training Loss: 0.006054657120257616\n",
      "Validation Loss: 0.0035566886418676945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006738675654632971\n",
      "Training Loss: 0.006473898399854079\n",
      "Training Loss: 0.006047477526590228\n",
      "Validation Loss: 0.003550901629679491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006731374857481569\n",
      "Training Loss: 0.006466916813515126\n",
      "Training Loss: 0.006040271414676681\n",
      "Validation Loss: 0.003545056490928688\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006724065574817359\n",
      "Training Loss: 0.006459902332280763\n",
      "Training Loss: 0.006033037290908396\n",
      "Validation Loss: 0.003539150909081185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.00671674600802362\n",
      "Training Loss: 0.006452851379872299\n",
      "Training Loss: 0.00602577222045511\n",
      "Validation Loss: 0.003533181046355474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006709413466742262\n",
      "Training Loss: 0.006445762739167549\n",
      "Training Loss: 0.006018473779549822\n",
      "Validation Loss: 0.003527147365796767\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006702064821729437\n",
      "Training Loss: 0.006438632999779656\n",
      "Training Loss: 0.006011137147434056\n",
      "Validation Loss: 0.0035210475286343293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006694697868078947\n",
      "Training Loss: 0.006431460547028109\n",
      "Training Loss: 0.006003759654704482\n",
      "Validation Loss: 0.003514876820701645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006687309003900736\n",
      "Training Loss: 0.006424241812783294\n",
      "Training Loss: 0.005996339421253652\n",
      "Validation Loss: 0.0035086336056440234\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006679893813561648\n",
      "Training Loss: 0.0064169732038863005\n",
      "Training Loss: 0.005988868972053751\n",
      "Validation Loss: 0.003502312558441517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006672449534526095\n",
      "Training Loss: 0.00640965222788509\n",
      "Training Loss: 0.005981347402557731\n",
      "Validation Loss: 0.003495910806644164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006664971841964871\n",
      "Training Loss: 0.006402275807922706\n",
      "Training Loss: 0.005973769584670663\n",
      "Validation Loss: 0.003489426052030385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006657455213135108\n",
      "Training Loss: 0.006394839536515064\n",
      "Training Loss: 0.0059661288815550505\n",
      "Validation Loss: 0.003482855970217857\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006649894013535231\n",
      "Training Loss: 0.006387338967178948\n",
      "Training Loss: 0.005958422550465911\n",
      "Validation Loss: 0.003476190223787608\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006642284383997321\n",
      "Training Loss: 0.006379770376370288\n",
      "Training Loss: 0.0059506421675905585\n",
      "Validation Loss: 0.003469422161369846\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006634617798263207\n",
      "Training Loss: 0.006372128574876115\n",
      "Training Loss: 0.005942784142680466\n",
      "Validation Loss: 0.003462551440258793\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006626889126491733\n",
      "Training Loss: 0.006364409243687987\n",
      "Training Loss: 0.005934840894769877\n",
      "Validation Loss: 0.003455572921175803\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006619089232408441\n",
      "Training Loss: 0.006356605840846896\n",
      "Training Loss: 0.005926806178176775\n",
      "Validation Loss: 0.0034484765047646977\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006611209613038227\n",
      "Training Loss: 0.00634871153102722\n",
      "Training Loss: 0.005918670673854649\n",
      "Validation Loss: 0.0034412535278942813\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006603242759592831\n",
      "Training Loss: 0.0063407206349074844\n",
      "Training Loss: 0.005910426563350484\n",
      "Validation Loss: 0.0034338960612422965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006595176123082638\n",
      "Training Loss: 0.006332625359063968\n",
      "Training Loss: 0.005902062265668064\n",
      "Validation Loss: 0.0034263915999719267\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006586998978164047\n",
      "Training Loss: 0.00632441732857842\n",
      "Training Loss: 0.00589357009739615\n",
      "Validation Loss: 0.003418734366482312\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006578698445810005\n",
      "Training Loss: 0.0063160858995979655\n",
      "Training Loss: 0.005884937476366758\n",
      "Validation Loss: 0.003410909989367375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006570260800654069\n",
      "Training Loss: 0.006307620981242507\n",
      "Training Loss: 0.005876149696996435\n",
      "Validation Loss: 0.0034029000226847746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006561668671783991\n",
      "Training Loss: 0.0062990123667987065\n",
      "Training Loss: 0.00586719332728535\n",
      "Validation Loss: 0.0033946971819746527\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006552906564902514\n",
      "Training Loss: 0.0062902475538430735\n",
      "Training Loss: 0.00585805622395128\n",
      "Validation Loss: 0.00338627956022791\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00654395492922049\n",
      "Training Loss: 0.006281311423517764\n",
      "Training Loss: 0.005848716815235093\n",
      "Validation Loss: 0.0033776303740176423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006534792049787939\n",
      "Training Loss: 0.0062721891130786385\n",
      "Training Loss: 0.00583915920346044\n",
      "Validation Loss: 0.0033687302905522037\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006525394727941602\n",
      "Training Loss: 0.006262863071169704\n",
      "Training Loss: 0.005829361891373992\n",
      "Validation Loss: 0.0033595543864884236\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006515739587484859\n",
      "Training Loss: 0.006253316793590784\n",
      "Training Loss: 0.0058193031582050025\n",
      "Validation Loss: 0.0033500822198190045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006505797237041406\n",
      "Training Loss: 0.006243528272607364\n",
      "Training Loss: 0.0058089604857377705\n",
      "Validation Loss: 0.0033402840143823055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006495539457537234\n",
      "Training Loss: 0.006233476981869899\n",
      "Training Loss: 0.0057983077550306916\n",
      "Validation Loss: 0.0033301292327389625\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006484934210311622\n",
      "Training Loss: 0.006223138782661408\n",
      "Training Loss: 0.005787319467635825\n",
      "Validation Loss: 0.003319594471485176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006473949475330301\n",
      "Training Loss: 0.006212489247554913\n",
      "Training Loss: 0.005775967808440328\n",
      "Validation Loss: 0.003308644921131683\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006462549895513803\n",
      "Training Loss: 0.006201504516066052\n",
      "Training Loss: 0.005764225173043087\n",
      "Validation Loss: 0.0032972481397367764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006450699674896896\n",
      "Training Loss: 0.006190157264354639\n",
      "Training Loss: 0.005752063881373033\n",
      "Validation Loss: 0.0032853699103520995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006438364663044922\n",
      "Training Loss: 0.006178422179073095\n",
      "Training Loss: 0.005739458529278636\n",
      "Validation Loss: 0.0032729838485567927\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006425511397537775\n",
      "Training Loss: 0.0061662739759776744\n",
      "Training Loss: 0.005726386429159902\n",
      "Validation Loss: 0.0032600564842918114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006412110507953912\n",
      "Training Loss: 0.006153691997169517\n",
      "Training Loss: 0.00571283062454313\n",
      "Validation Loss: 0.0032465707685986763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006398137586656958\n",
      "Training Loss: 0.006140658404910937\n",
      "Training Loss: 0.005698777493089438\n",
      "Validation Loss: 0.003232507996543656\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006383575017098337\n",
      "Training Loss: 0.00612715904135257\n",
      "Training Loss: 0.005684225835721008\n",
      "Validation Loss: 0.0032178673152864147\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006368415922042914\n",
      "Training Loss: 0.006113192875636742\n",
      "Training Loss: 0.005669183031423017\n",
      "Validation Loss: 0.0032026520106167105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006352666103630326\n",
      "Training Loss: 0.006098762600449845\n",
      "Training Loss: 0.005653668369050137\n",
      "Validation Loss: 0.003186888719406607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006336346537573263\n",
      "Training Loss: 0.006083885085536167\n",
      "Training Loss: 0.005637717273202725\n",
      "Validation Loss: 0.0031706096743118395\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.0063194918760564175\n",
      "Training Loss: 0.006068585357279516\n",
      "Training Loss: 0.005621377762290649\n",
      "Validation Loss: 0.00315387634393037\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006302156032761559\n",
      "Training Loss: 0.0060529033007333055\n",
      "Training Loss: 0.005604710630723275\n",
      "Validation Loss: 0.0031367532785960967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006284407373168505\n",
      "Training Loss: 0.006036887984955683\n",
      "Training Loss: 0.0055877890792908145\n",
      "Validation Loss: 0.003119329844727024\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006266325442120433\n",
      "Training Loss: 0.006020596165326424\n",
      "Training Loss: 0.005570694163325242\n",
      "Validation Loss: 0.003101695669040586\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006248001480125822\n",
      "Training Loss: 0.006004092186340131\n",
      "Training Loss: 0.005553510778117925\n",
      "Validation Loss: 0.0030839500511902268\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006229531964054331\n",
      "Training Loss: 0.005987441786564887\n",
      "Training Loss: 0.005536323548294603\n",
      "Validation Loss: 0.0030661992260432812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006211011316627264\n",
      "Training Loss: 0.005970708688837476\n",
      "Training Loss: 0.005519210007041693\n",
      "Validation Loss: 0.0030485251355979047\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006192525936639868\n",
      "Training Loss: 0.0059539486624998975\n",
      "Training Loss: 0.005502236881293357\n",
      "Validation Loss: 0.0030310094327760046\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006174152104067616\n",
      "Training Loss: 0.005937209611875005\n",
      "Training Loss: 0.005485457043396309\n",
      "Validation Loss: 0.0030137201629872067\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006155948807718232\n",
      "Training Loss: 0.005920528555870988\n",
      "Training Loss: 0.005468904033186846\n",
      "Validation Loss: 0.0029966989898363525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006137956348829902\n",
      "Training Loss: 0.005903927636099979\n",
      "Training Loss: 0.005452597891562618\n",
      "Validation Loss: 0.0029799894382653946\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006120200842851773\n",
      "Training Loss: 0.005887418464990333\n",
      "Training Loss: 0.005436540495720692\n",
      "Validation Loss: 0.002963615016321118\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006102686504600569\n",
      "Training Loss: 0.005871002041967586\n",
      "Training Loss: 0.005420722636627033\n",
      "Validation Loss: 0.0029475989325097604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006085408231010661\n",
      "Training Loss: 0.005854673604480922\n",
      "Training Loss: 0.005405125109828077\n",
      "Validation Loss: 0.002931963020590333\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006068351952708326\n",
      "Training Loss: 0.00583842538879253\n",
      "Training Loss: 0.005389725844725035\n",
      "Validation Loss: 0.0029167332097867066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006051501101464965\n",
      "Training Loss: 0.005822256643441505\n",
      "Training Loss: 0.005374507908127271\n",
      "Validation Loss: 0.002901955745758468\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006034844144596718\n",
      "Training Loss: 0.005806172500015237\n",
      "Training Loss: 0.005359461999614723\n",
      "Validation Loss: 0.0028876870153785755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006018383153132163\n",
      "Training Loss: 0.005790194983128458\n",
      "Training Loss: 0.0053445906471461056\n",
      "Validation Loss: 0.0028740009135995686\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006002136162715032\n",
      "Training Loss: 0.005774360081995837\n",
      "Training Loss: 0.0053299151372630145\n",
      "Validation Loss: 0.0028609679030840484\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005986135089187883\n",
      "Training Loss: 0.005758716549607925\n",
      "Training Loss: 0.005315469359047711\n",
      "Validation Loss: 0.0028486501391495715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005970426627318375\n",
      "Training Loss: 0.0057433212467003615\n",
      "Training Loss: 0.005301297082332894\n",
      "Validation Loss: 0.002837093649013491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005955069485353306\n",
      "Training Loss: 0.00572823945316486\n",
      "Training Loss: 0.005287448365124874\n",
      "Validation Loss: 0.0028263017720950956\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.0059401208657072855\n",
      "Training Loss: 0.005713529671775177\n",
      "Training Loss: 0.005273970229318366\n",
      "Validation Loss: 0.002816246652431535\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.0059256315685343\n",
      "Training Loss: 0.005699239405221306\n",
      "Training Loss: 0.00526090019964613\n",
      "Validation Loss: 0.0028068612570340714\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005911641624406911\n",
      "Training Loss: 0.005685406900011003\n",
      "Training Loss: 0.005248265931149945\n",
      "Validation Loss: 0.0027980565242573955\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005898172795423306\n",
      "Training Loss: 0.005672052336740308\n",
      "Training Loss: 0.005236081041512079\n",
      "Validation Loss: 0.0027897277437516728\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005885230993153527\n",
      "Training Loss: 0.005659182516974397\n",
      "Training Loss: 0.005224342875299044\n",
      "Validation Loss: 0.0027817682149882815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005872807527775876\n",
      "Training Loss: 0.005646792639745399\n",
      "Training Loss: 0.005213041927781888\n",
      "Validation Loss: 0.002774080664463592\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005860883420682512\n",
      "Training Loss: 0.005634867553017103\n",
      "Training Loss: 0.005202159059699625\n",
      "Validation Loss: 0.0027665820821669747\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.0058494301571045074\n",
      "Training Loss: 0.005623388111125678\n",
      "Training Loss: 0.005191669302876107\n",
      "Validation Loss: 0.0027592071192077457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.0058384169661439955\n",
      "Training Loss: 0.005612330442527309\n",
      "Training Loss: 0.0051815470238216225\n",
      "Validation Loss: 0.0027519079092680736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005827812078641728\n",
      "Training Loss: 0.005601667358423584\n",
      "Training Loss: 0.005171766597777605\n",
      "Validation Loss: 0.002744649379943194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005817582400632091\n",
      "Training Loss: 0.005591375437797978\n",
      "Training Loss: 0.00516230161243584\n",
      "Validation Loss: 0.002737409055881788\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005807697174604982\n",
      "Training Loss: 0.005581428966252133\n",
      "Training Loss: 0.0051531279919436205\n",
      "Validation Loss: 0.0027301782831089215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005798128889873624\n",
      "Training Loss: 0.00557180556235835\n",
      "Training Loss: 0.005144223572569899\n",
      "Validation Loss: 0.00272295429708771\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005788853186531924\n",
      "Training Loss: 0.005562485367408954\n",
      "Training Loss: 0.005135569106787443\n",
      "Validation Loss: 0.0027157366315028473\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005779848180827685\n",
      "Training Loss: 0.005553445725818165\n",
      "Training Loss: 0.0051271500729490075\n",
      "Validation Loss: 0.002708531814114599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005771091466885992\n",
      "Training Loss: 0.005544671604875475\n",
      "Training Loss: 0.005118945797439664\n",
      "Validation Loss: 0.002701344238620335\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.0057625683298101645\n",
      "Training Loss: 0.005536146169761196\n",
      "Training Loss: 0.005110947387292981\n",
      "Validation Loss: 0.002694182232639679\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005754264688002877\n",
      "Training Loss: 0.005527857181732543\n",
      "Training Loss: 0.005103141703875736\n",
      "Validation Loss: 0.0026870553742759347\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005746167502948083\n",
      "Training Loss: 0.005519789886893705\n",
      "Training Loss: 0.0050955180480377745\n",
      "Validation Loss: 0.002679969690965091\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005738262899685651\n",
      "Training Loss: 0.005511933860834688\n",
      "Training Loss: 0.0050880673388019205\n",
      "Validation Loss: 0.0026729346411250364\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005730543616809882\n",
      "Training Loss: 0.005504278578446247\n",
      "Training Loss: 0.005080781656433828\n",
      "Validation Loss: 0.002665954751872949\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005722999356221408\n",
      "Training Loss: 0.0054968147573526945\n",
      "Training Loss: 0.005073652963619679\n",
      "Validation Loss: 0.002659036964308889\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.0057156228041276335\n",
      "Training Loss: 0.005489533648942597\n",
      "Training Loss: 0.005066675572306849\n",
      "Validation Loss: 0.0026521835204088287\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005708406738121994\n",
      "Training Loss: 0.005482427901006304\n",
      "Training Loss: 0.00505984328687191\n",
      "Validation Loss: 0.0026454043632184857\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005701344384578988\n",
      "Training Loss: 0.005475489698001184\n",
      "Training Loss: 0.005053148629376665\n",
      "Validation Loss: 0.0026386975627287887\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005694429783034138\n",
      "Training Loss: 0.005468712497968226\n",
      "Training Loss: 0.005046588912955485\n",
      "Validation Loss: 0.0026320680489324115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005687657911912538\n",
      "Training Loss: 0.0054620902030728755\n",
      "Training Loss: 0.005040156596223823\n",
      "Validation Loss: 0.0026255150271966804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005681024768273346\n",
      "Training Loss: 0.005455616641556844\n",
      "Training Loss: 0.005033848680905066\n",
      "Validation Loss: 0.0026190445720469264\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005674523681518622\n",
      "Training Loss: 0.005449285252252594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [10:03<15:04, 150.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.005027661338681355\n",
      "Validation Loss: 0.0026126508103955664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.3427133320271969\n",
      "Training Loss: 0.2556185609102249\n",
      "Training Loss: 0.19207063373178243\n",
      "Validation Loss: 0.13518705729688152\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.12381840623915195\n",
      "Training Loss: 0.10214487995952368\n",
      "Training Loss: 0.09243503592908382\n",
      "Validation Loss: 0.08587341462628226\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.08480286940932275\n",
      "Training Loss: 0.08360745238140226\n",
      "Training Loss: 0.08251411160454154\n",
      "Validation Loss: 0.08000405396470862\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.07875292597338557\n",
      "Training Loss: 0.07789763139560818\n",
      "Training Loss: 0.0768245223723352\n",
      "Validation Loss: 0.0740033933955632\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.07290056409314274\n",
      "Training Loss: 0.07199862336739898\n",
      "Training Loss: 0.0708609270490706\n",
      "Validation Loss: 0.06768107146359562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.06667065590620042\n",
      "Training Loss: 0.06561159916222095\n",
      "Training Loss: 0.06428123405203223\n",
      "Validation Loss: 0.060745298946171665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.059747636532410976\n",
      "Training Loss: 0.05840477633289993\n",
      "Training Loss: 0.05675217356532812\n",
      "Validation Loss: 0.052916015383232845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.05187005075626075\n",
      "Training Loss: 0.050117594730108975\n",
      "Training Loss: 0.048033048398792744\n",
      "Validation Loss: 0.04401414198905564\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.04296311690472066\n",
      "Training Loss: 0.040845451802015306\n",
      "Training Loss: 0.03850818747654557\n",
      "Validation Loss: 0.03476656648968713\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.03398065846413374\n",
      "Training Loss: 0.0319524514535442\n",
      "Training Loss: 0.029849265748634935\n",
      "Validation Loss: 0.02681090511130483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.026470233676955102\n",
      "Training Loss: 0.024802116863429546\n",
      "Training Loss: 0.023111689388751983\n",
      "Validation Loss: 0.020734196350899306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.020893688877113164\n",
      "Training Loss: 0.019688687790185212\n",
      "Training Loss: 0.018440963570028543\n",
      "Validation Loss: 0.016540932055730165\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.01725341738201678\n",
      "Training Loss: 0.01657467890763655\n",
      "Training Loss: 0.015698458780534566\n",
      "Validation Loss: 0.014072252432240194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.015209748002234846\n",
      "Training Loss: 0.014894140865653754\n",
      "Training Loss: 0.014164172403980047\n",
      "Validation Loss: 0.012615041833454639\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.013972596805542708\n",
      "Training Loss: 0.013819018364883959\n",
      "Training Loss: 0.013103965144837275\n",
      "Validation Loss: 0.011559876550533128\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.013036350547336041\n",
      "Training Loss: 0.012951804480981082\n",
      "Training Loss: 0.012232238829601555\n",
      "Validation Loss: 0.010674424626435457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.012244760352186858\n",
      "Training Loss: 0.012188557728659361\n",
      "Training Loss: 0.011474154067691415\n",
      "Validation Loss: 0.00988916087937489\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.011554212016053498\n",
      "Training Loss: 0.011503695198334753\n",
      "Training Loss: 0.01080638634855859\n",
      "Validation Loss: 0.009181953893367494\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.010948108979500831\n",
      "Training Loss: 0.010889491006964817\n",
      "Training Loss: 0.010218565746909007\n",
      "Validation Loss: 0.00854472979756721\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.010417629192816094\n",
      "Training Loss: 0.010342780727660284\n",
      "Training Loss: 0.009704070491716265\n",
      "Validation Loss: 0.007973439238044652\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.00995631452766247\n",
      "Training Loss: 0.009861024756683037\n",
      "Training Loss: 0.009257132109487429\n",
      "Validation Loss: 0.007464679591148422\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.009558233552379534\n",
      "Training Loss: 0.009441025655250996\n",
      "Training Loss: 0.008871952525805683\n",
      "Validation Loss: 0.007014647696502088\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.009217435899190605\n",
      "Training Loss: 0.009078670111484825\n",
      "Training Loss: 0.008542582837399094\n",
      "Validation Loss: 0.00661899381725306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008927955471444876\n",
      "Training Loss: 0.008769122331868858\n",
      "Training Loss: 0.008263077123556286\n",
      "Validation Loss: 0.006273043789283446\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.008683930089464411\n",
      "Training Loss: 0.008507115619722753\n",
      "Training Loss: 0.008027628605486826\n",
      "Validation Loss: 0.005971993031922016\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008479725698707625\n",
      "Training Loss: 0.008287219548365102\n",
      "Training Loss: 0.007830676366575062\n",
      "Validation Loss: 0.005711129094798411\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008310025554383173\n",
      "Training Loss: 0.008104074825532735\n",
      "Training Loss: 0.007666994725586847\n",
      "Validation Loss: 0.005485908055071081\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.00816988721373491\n",
      "Training Loss: 0.00795255100936629\n",
      "Training Loss: 0.007531730195041746\n",
      "Validation Loss: 0.005292073284600223\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008054786849534139\n",
      "Training Loss: 0.007827869773609563\n",
      "Training Loss: 0.007420457757543773\n",
      "Validation Loss: 0.00512570032609229\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.007960639061639085\n",
      "Training Loss: 0.007725674913963303\n",
      "Training Loss: 0.007329198122024536\n",
      "Validation Loss: 0.004983212422119097\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007883804489392788\n",
      "Training Loss: 0.007642070696456358\n",
      "Training Loss: 0.007254428706364707\n",
      "Validation Loss: 0.004861389250191075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007821096839616076\n",
      "Training Loss: 0.007573644496733323\n",
      "Training Loss: 0.007193081283476204\n",
      "Validation Loss: 0.004757338486025843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007769770682789385\n",
      "Training Loss: 0.007517460345989093\n",
      "Training Loss: 0.00714253586018458\n",
      "Validation Loss: 0.004668499296007866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007727513054851443\n",
      "Training Loss: 0.007471051514148713\n",
      "Training Loss: 0.00710060017532669\n",
      "Validation Loss: 0.004592604746811845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007692416795762256\n",
      "Training Loss: 0.00743238870985806\n",
      "Training Loss: 0.007065481032477692\n",
      "Validation Loss: 0.00452768867783081\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007662941197631881\n",
      "Training Loss: 0.007399837920675054\n",
      "Training Loss: 0.007035735559184104\n",
      "Validation Loss: 0.00447204720563768\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007637870126636699\n",
      "Training Loss: 0.007372102995868772\n",
      "Training Loss: 0.007010225378908217\n",
      "Validation Loss: 0.0044242217024955685\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007616254843305796\n",
      "Training Loss: 0.007348174798535183\n",
      "Training Loss: 0.006988063369644806\n",
      "Validation Loss: 0.0043829718285522765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007597364609828219\n",
      "Training Loss: 0.007327271376270801\n",
      "Training Loss: 0.0069685654831118885\n",
      "Validation Loss: 0.004347257267715138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007580646836431697\n",
      "Training Loss: 0.007308794793207198\n",
      "Training Loss: 0.006951205256627873\n",
      "Validation Loss: 0.004316195918854033\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.00756567744188942\n",
      "Training Loss: 0.007292284849099815\n",
      "Training Loss: 0.006935579640557989\n",
      "Validation Loss: 0.004289062309152122\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0075521356577519325\n",
      "Training Loss: 0.00727738918736577\n",
      "Training Loss: 0.006921378709375858\n",
      "Validation Loss: 0.004265239145711399\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007539774841861799\n",
      "Training Loss: 0.007263831918826327\n",
      "Training Loss: 0.006908362144604325\n",
      "Validation Loss: 0.004244220681132727\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007528403856558725\n",
      "Training Loss: 0.007251400257227942\n",
      "Training Loss: 0.006896339759696275\n",
      "Validation Loss: 0.004225581259142314\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007517872174503282\n",
      "Training Loss: 0.007239921760046854\n",
      "Training Loss: 0.006885162487160415\n",
      "Validation Loss: 0.004208968770480976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007508057896047831\n",
      "Training Loss: 0.0072292597498744724\n",
      "Training Loss: 0.00687470905832015\n",
      "Validation Loss: 0.0041940828780068105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007498864281224087\n",
      "Training Loss: 0.007219300497090444\n",
      "Training Loss: 0.006864880988141522\n",
      "Validation Loss: 0.0041806760578929035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.00749020878225565\n",
      "Training Loss: 0.007209951702971011\n",
      "Training Loss: 0.006855595608940348\n",
      "Validation Loss: 0.004168536578137637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007482023378834128\n",
      "Training Loss: 0.007201134380884469\n",
      "Training Loss: 0.006846783735090867\n",
      "Validation Loss: 0.004157492791543181\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.00747424955945462\n",
      "Training Loss: 0.007192781482590362\n",
      "Training Loss: 0.006838387175230309\n",
      "Validation Loss: 0.0041473928912218365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007466835691593588\n",
      "Training Loss: 0.007184834345243871\n",
      "Training Loss: 0.006830354779958725\n",
      "Validation Loss: 0.004138110498531481\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.0074597389437258245\n",
      "Training Loss: 0.007177244594786316\n",
      "Training Loss: 0.0068226450495421885\n",
      "Validation Loss: 0.004129539362908331\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007452919165370986\n",
      "Training Loss: 0.007169966996880248\n",
      "Training Loss: 0.006815217843977734\n",
      "Validation Loss: 0.004121591224533956\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007446342703187839\n",
      "Training Loss: 0.007162963687442243\n",
      "Training Loss: 0.006808040468022227\n",
      "Validation Loss: 0.004114178184608228\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007439978970214724\n",
      "Training Loss: 0.00715620135422796\n",
      "Training Loss: 0.00680108224391006\n",
      "Validation Loss: 0.004107238225157509\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0074337994412053375\n",
      "Training Loss: 0.007149648338090628\n",
      "Training Loss: 0.006794317847816273\n",
      "Validation Loss: 0.004100714466178768\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007427779914578423\n",
      "Training Loss: 0.007143278826260939\n",
      "Training Loss: 0.0067877243855036795\n",
      "Validation Loss: 0.004094548527350168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.00742189931566827\n",
      "Training Loss: 0.007137067957082763\n",
      "Training Loss: 0.006781278665876016\n",
      "Validation Loss: 0.004088702667870799\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007416136373067274\n",
      "Training Loss: 0.007130994828185066\n",
      "Training Loss: 0.006774963883217424\n",
      "Validation Loss: 0.004083133904326163\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.0074104725499637425\n",
      "Training Loss: 0.007125038696685806\n",
      "Training Loss: 0.006768761378480121\n",
      "Validation Loss: 0.004077813442890647\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007404891933547333\n",
      "Training Loss: 0.007119182717287913\n",
      "Training Loss: 0.006762656039791181\n",
      "Validation Loss: 0.004072705702332968\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007399379613343626\n",
      "Training Loss: 0.007113410078454762\n",
      "Training Loss: 0.006756633181357757\n",
      "Validation Loss: 0.004067786912998876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.0073939210607204585\n",
      "Training Loss: 0.007107706132810563\n",
      "Training Loss: 0.0067506808112375435\n",
      "Validation Loss: 0.004063027211088227\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.00738850289140828\n",
      "Training Loss: 0.007102056096773595\n",
      "Training Loss: 0.006744786152848974\n",
      "Validation Loss: 0.004058412666003523\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0073831143334973605\n",
      "Training Loss: 0.007096449136734009\n",
      "Training Loss: 0.006738936386536807\n",
      "Validation Loss: 0.004053919611425464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007377742772223428\n",
      "Training Loss: 0.007090871356194839\n",
      "Training Loss: 0.0067331224423833195\n",
      "Validation Loss: 0.004049533572613021\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007372377827996388\n",
      "Training Loss: 0.007085313157876954\n",
      "Training Loss: 0.006727334107272327\n",
      "Validation Loss: 0.004045232171663658\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007367009335430339\n",
      "Training Loss: 0.007079761675558984\n",
      "Training Loss: 0.006721559904981404\n",
      "Validation Loss: 0.0040410006447601015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007361627221107483\n",
      "Training Loss: 0.0070742078882176426\n",
      "Training Loss: 0.006715792148606852\n",
      "Validation Loss: 0.004036827046905508\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0073562220728490505\n",
      "Training Loss: 0.007068642051890493\n",
      "Training Loss: 0.006710020587779582\n",
      "Validation Loss: 0.004032697941940487\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007350783323636279\n",
      "Training Loss: 0.007063052853336558\n",
      "Training Loss: 0.006704236902296543\n",
      "Validation Loss: 0.0040285973678855745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007345303787151352\n",
      "Training Loss: 0.00705743110505864\n",
      "Training Loss: 0.00669843110954389\n",
      "Validation Loss: 0.004024515339183841\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007339772521518171\n",
      "Training Loss: 0.007051766943186521\n",
      "Training Loss: 0.006692594774067402\n",
      "Validation Loss: 0.004020439373235103\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007334180789766834\n",
      "Training Loss: 0.007046050402568654\n",
      "Training Loss: 0.006686717802658677\n",
      "Validation Loss: 0.004016355506740929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007328518346184865\n",
      "Training Loss: 0.007040271771838889\n",
      "Training Loss: 0.006680792010156438\n",
      "Validation Loss: 0.004012254337529034\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007322776349028572\n",
      "Training Loss: 0.007034421297721565\n",
      "Training Loss: 0.006674806571099907\n",
      "Validation Loss: 0.004008122281631811\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007316943234764039\n",
      "Training Loss: 0.007028486829949543\n",
      "Training Loss: 0.006668751895194874\n",
      "Validation Loss: 0.004003943607284363\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0073110092210117725\n",
      "Training Loss: 0.007022458299761638\n",
      "Training Loss: 0.006662616499233991\n",
      "Validation Loss: 0.003999712624524333\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007304963666247204\n",
      "Training Loss: 0.007016324046999216\n",
      "Training Loss: 0.006656390645075589\n",
      "Validation Loss: 0.00399541022161838\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.0072987936995923515\n",
      "Training Loss: 0.0070100720576010645\n",
      "Training Loss: 0.006650061494437978\n",
      "Validation Loss: 0.003991023636046337\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0072924874234013256\n",
      "Training Loss: 0.007003688639961183\n",
      "Training Loss: 0.0066436169599182904\n",
      "Validation Loss: 0.003986541176570601\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.00728603114373982\n",
      "Training Loss: 0.006997160487808287\n",
      "Training Loss: 0.006637044249800965\n",
      "Validation Loss: 0.003981946044067821\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007279411712661385\n",
      "Training Loss: 0.006990472918841988\n",
      "Training Loss: 0.006630328395403922\n",
      "Validation Loss: 0.003977218675531782\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007272612571250647\n",
      "Training Loss: 0.006983609197195619\n",
      "Training Loss: 0.006623453422216699\n",
      "Validation Loss: 0.003972345984049058\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007265616710064933\n",
      "Training Loss: 0.006976553092245013\n",
      "Training Loss: 0.0066164035059046\n",
      "Validation Loss: 0.003967307250308438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007258408053312451\n",
      "Training Loss: 0.006969286306994036\n",
      "Training Loss: 0.006609161883825436\n",
      "Validation Loss: 0.003962080957095945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007250966373831034\n",
      "Training Loss: 0.006961788929766044\n",
      "Training Loss: 0.006601708199596033\n",
      "Validation Loss: 0.0039566498487832\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007243269922910258\n",
      "Training Loss: 0.006954039734555408\n",
      "Training Loss: 0.0065940225566737355\n",
      "Validation Loss: 0.003950987173390858\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007235297188162804\n",
      "Training Loss: 0.0069460158317815515\n",
      "Training Loss: 0.006586081673158333\n",
      "Validation Loss: 0.003945065902943691\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007227022941224277\n",
      "Training Loss: 0.006937692812643945\n",
      "Training Loss: 0.0065778624580707405\n",
      "Validation Loss: 0.003938860643205097\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007218422308797017\n",
      "Training Loss: 0.00692904329742305\n",
      "Training Loss: 0.006569338863482699\n",
      "Validation Loss: 0.0039323377069818335\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007209464287152514\n",
      "Training Loss: 0.006920039640972391\n",
      "Training Loss: 0.00656048426986672\n",
      "Validation Loss: 0.00392546639463791\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007200118700275197\n",
      "Training Loss: 0.00691065194667317\n",
      "Training Loss: 0.006551268480252475\n",
      "Validation Loss: 0.003918209173350355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007190353003097698\n",
      "Training Loss: 0.006900848014047369\n",
      "Training Loss: 0.006541661189403385\n",
      "Validation Loss: 0.003910527603826329\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007180133917136118\n",
      "Training Loss: 0.006890594378346577\n",
      "Training Loss: 0.00653163057519123\n",
      "Validation Loss: 0.0039023817189854015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007169423416489735\n",
      "Training Loss: 0.006879856555024162\n",
      "Training Loss: 0.006521143433637917\n",
      "Validation Loss: 0.0038937306243486784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007158183096908033\n",
      "Training Loss: 0.006868599036242813\n",
      "Training Loss: 0.0065101657877676185\n",
      "Validation Loss: 0.0038845204222382286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007146374228177592\n",
      "Training Loss: 0.006856787655269727\n",
      "Training Loss: 0.00649866548134014\n",
      "Validation Loss: 0.003874708456248882\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007133960467763245\n",
      "Training Loss: 0.0068443883094005285\n",
      "Training Loss: 0.006486610888969153\n",
      "Validation Loss: 0.0038642472289293333\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007120902321767062\n",
      "Training Loss: 0.006831370479194448\n",
      "Training Loss: 0.006473973542451859\n",
      "Validation Loss: 0.003853087738585355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007107167288195342\n",
      "Training Loss: 0.006817706507863477\n",
      "Training Loss: 0.0064607297407928855\n",
      "Validation Loss: 0.00384119131570954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007092724300455302\n",
      "Training Loss: 0.00680337903322652\n",
      "Training Loss: 0.0064468620566185564\n",
      "Validation Loss: 0.0038285143372571366\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.0070775519125163555\n",
      "Training Loss: 0.006788375144824385\n",
      "Training Loss: 0.006432360897306353\n",
      "Validation Loss: 0.003815030676490638\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007061636105645448\n",
      "Training Loss: 0.006772696325788275\n",
      "Training Loss: 0.006417229051003233\n",
      "Validation Loss: 0.0038007205532696307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007044974828604608\n",
      "Training Loss: 0.006756357665872201\n",
      "Training Loss: 0.0064014820952434095\n",
      "Validation Loss: 0.003785580821764352\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007027581947622821\n",
      "Training Loss: 0.006739392690360546\n",
      "Training Loss: 0.0063851470849476755\n",
      "Validation Loss: 0.0037696271483378297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007009486819151789\n",
      "Training Loss: 0.0067218497756402935\n",
      "Training Loss: 0.006368269972736016\n",
      "Validation Loss: 0.003752899200529948\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006990734488936141\n",
      "Training Loss: 0.006703796506626531\n",
      "Training Loss: 0.006350909640314057\n",
      "Validation Loss: 0.0037354467413126587\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006971388480160386\n",
      "Training Loss: 0.0066853190760593866\n",
      "Training Loss: 0.006333136722678319\n",
      "Validation Loss: 0.003717360358810827\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006951524701435119\n",
      "Training Loss: 0.006666517819976434\n",
      "Training Loss: 0.0063150321959983555\n",
      "Validation Loss: 0.003698734940668003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006931234953226522\n",
      "Training Loss: 0.0066475023771636185\n",
      "Training Loss: 0.006296685020206496\n",
      "Validation Loss: 0.0036796865267993023\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.00691061447840184\n",
      "Training Loss: 0.00662838697899133\n",
      "Training Loss: 0.006278182652313263\n",
      "Validation Loss: 0.00366034335718396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006889766328968107\n",
      "Training Loss: 0.006609284837031737\n",
      "Training Loss: 0.0062596110708545896\n",
      "Validation Loss: 0.0036408351695039467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006868786960840225\n",
      "Training Loss: 0.006590302815893665\n",
      "Training Loss: 0.006241048211231828\n",
      "Validation Loss: 0.0036212818244514004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006847768091829493\n",
      "Training Loss: 0.006571533245733008\n",
      "Training Loss: 0.006222561760805548\n",
      "Validation Loss: 0.0036017935254788968\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006826790400082245\n",
      "Training Loss: 0.006553053120151162\n",
      "Training Loss: 0.006204208905692212\n",
      "Validation Loss: 0.0035824692203778397\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006805923623032868\n",
      "Training Loss: 0.006534920891863294\n",
      "Training Loss: 0.006186034288839437\n",
      "Validation Loss: 0.0035633899342122203\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006785223161568865\n",
      "Training Loss: 0.006517178887734189\n",
      "Training Loss: 0.006168069457053207\n",
      "Validation Loss: 0.003544610294544797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006764732273877598\n",
      "Training Loss: 0.00649985097523313\n",
      "Training Loss: 0.006150336321443319\n",
      "Validation Loss: 0.0035261822304573287\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006744482441572473\n",
      "Training Loss: 0.006482947585172951\n",
      "Training Loss: 0.006132851965376176\n",
      "Validation Loss: 0.0035081331898050194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006724497055984102\n",
      "Training Loss: 0.0064664689009077845\n",
      "Training Loss: 0.0061156248324550685\n",
      "Validation Loss: 0.0034904860488526273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006704795524710789\n",
      "Training Loss: 0.006450406349031255\n",
      "Training Loss: 0.006098662193398923\n",
      "Validation Loss: 0.0034732462006380384\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006685387344332412\n",
      "Training Loss: 0.0064347489946521815\n",
      "Training Loss: 0.006081970476079732\n",
      "Validation Loss: 0.003456431277504379\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006666290368884802\n",
      "Training Loss: 0.006419483782956377\n",
      "Training Loss: 0.006065555663080886\n",
      "Validation Loss: 0.003440044795455976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006647514948854223\n",
      "Training Loss: 0.00640459775866475\n",
      "Training Loss: 0.006049427046673372\n",
      "Validation Loss: 0.003424093093241701\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006629078247351572\n",
      "Training Loss: 0.006390082373982296\n",
      "Training Loss: 0.006033595804474316\n",
      "Validation Loss: 0.003408599404220501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006610997053794563\n",
      "Training Loss: 0.0063759293255861845\n",
      "Training Loss: 0.006018075090833008\n",
      "Validation Loss: 0.00339356087163886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006593292205361649\n",
      "Training Loss: 0.006362134639639408\n",
      "Training Loss: 0.006002877476857975\n",
      "Validation Loss: 0.0033789981310936006\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006575981957139447\n",
      "Training Loss: 0.006348695440683514\n",
      "Training Loss: 0.005988021774101071\n",
      "Validation Loss: 0.003364919782930127\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006559090548544191\n",
      "Training Loss: 0.0063356117467628795\n",
      "Training Loss: 0.005973522033891641\n",
      "Validation Loss: 0.003351336844746819\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006542640086845495\n",
      "Training Loss: 0.006322886096313596\n",
      "Training Loss: 0.005959395689424127\n",
      "Validation Loss: 0.0033382521883396203\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006526651097810827\n",
      "Training Loss: 0.006310519545222632\n",
      "Training Loss: 0.0059456565964501355\n",
      "Validation Loss: 0.003325675718821167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006511141392984428\n",
      "Training Loss: 0.006298514221562072\n",
      "Training Loss: 0.0059323167259572075\n",
      "Validation Loss: 0.0033136052661575377\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006496127645950764\n",
      "Training Loss: 0.00628686947573442\n",
      "Training Loss: 0.005919385128072463\n",
      "Validation Loss: 0.003302035301928984\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006481620375416242\n",
      "Training Loss: 0.006275584585382603\n",
      "Training Loss: 0.005906865946017205\n",
      "Validation Loss: 0.003290951378209244\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006467625229852274\n",
      "Training Loss: 0.006264655787381343\n",
      "Training Loss: 0.005894760845694691\n",
      "Validation Loss: 0.0032803411543201864\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006454144098097458\n",
      "Training Loss: 0.00625407715968322\n",
      "Training Loss: 0.0058830680977553125\n",
      "Validation Loss: 0.0032701838946001334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006441174126812257\n",
      "Training Loss: 0.006243842031108215\n",
      "Training Loss: 0.005871781193418428\n",
      "Validation Loss: 0.0032604556985911023\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006428710563923233\n",
      "Training Loss: 0.006233940962702036\n",
      "Training Loss: 0.005860890060430393\n",
      "Validation Loss: 0.0032511377299016112\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00641674036916811\n",
      "Training Loss: 0.006224362526554614\n",
      "Training Loss: 0.005850383199285716\n",
      "Validation Loss: 0.0032421946846042876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.0064052496285876255\n",
      "Training Loss: 0.00621509324060753\n",
      "Training Loss: 0.005840244340943173\n",
      "Validation Loss: 0.0032336069545656275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006394220381043852\n",
      "Training Loss: 0.0062061209190869705\n",
      "Training Loss: 0.0058304570917971435\n",
      "Validation Loss: 0.0032253464902332577\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006383633429068141\n",
      "Training Loss: 0.00619743068004027\n",
      "Training Loss: 0.005821004036115482\n",
      "Validation Loss: 0.003217381131547514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.0063734677876345815\n",
      "Training Loss: 0.006189008645014838\n",
      "Training Loss: 0.005811865002033301\n",
      "Validation Loss: 0.003209685626847858\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.0063637020922033116\n",
      "Training Loss: 0.006180839345324785\n",
      "Training Loss: 0.005803021283936687\n",
      "Validation Loss: 0.0032022393830486813\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006354312432231382\n",
      "Training Loss: 0.006172908098669723\n",
      "Training Loss: 0.005794452164554969\n",
      "Validation Loss: 0.003195016739073764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006345277715008706\n",
      "Training Loss: 0.006165201757103205\n",
      "Training Loss: 0.0057861404918367046\n",
      "Validation Loss: 0.0031880033503317935\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.00633657420927193\n",
      "Training Loss: 0.006157705528894439\n",
      "Training Loss: 0.005778067082283087\n",
      "Validation Loss: 0.0031811751332395614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006328181642456912\n",
      "Training Loss: 0.006150407801615074\n",
      "Training Loss: 0.005770213682553731\n",
      "Validation Loss: 0.0031745224920352606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.0063200762652559205\n",
      "Training Loss: 0.006143295471556485\n",
      "Training Loss: 0.0057625640282640235\n",
      "Validation Loss: 0.0031680233758256842\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006312242149724625\n",
      "Training Loss: 0.0061363570811226965\n",
      "Training Loss: 0.005755101387039758\n",
      "Validation Loss: 0.0031616665498782576\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006304658771841787\n",
      "Training Loss: 0.0061295804928522555\n",
      "Training Loss: 0.005747811471228488\n",
      "Validation Loss: 0.003155440676125457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006297307170461864\n",
      "Training Loss: 0.0061229566385736685\n",
      "Training Loss: 0.005740681365132332\n",
      "Validation Loss: 0.003149340098613894\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006290172777953558\n",
      "Training Loss: 0.006116474869195372\n",
      "Training Loss: 0.00573369606398046\n",
      "Validation Loss: 0.0031433481516400246\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006283237849711441\n",
      "Training Loss: 0.006110125980339945\n",
      "Training Loss: 0.005726844992022961\n",
      "Validation Loss: 0.003137465001324589\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006276489747688174\n",
      "Training Loss: 0.0061039010086096825\n",
      "Training Loss: 0.005720118375611491\n",
      "Validation Loss: 0.0031316785287017817\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006269913067808375\n",
      "Training Loss: 0.006097792035434395\n",
      "Training Loss: 0.005713503457373008\n",
      "Validation Loss: 0.0031259861098487307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.0062634982418967415\n",
      "Training Loss: 0.0060917936614714565\n",
      "Training Loss: 0.005706993478816003\n",
      "Validation Loss: 0.0031203834118918\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006257229644106701\n",
      "Training Loss: 0.006085895611904562\n",
      "Training Loss: 0.005700578721007332\n",
      "Validation Loss: 0.0031148615146704603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006251101202215068\n",
      "Training Loss: 0.006080091858748347\n",
      "Training Loss: 0.0056942513230023905\n",
      "Validation Loss: 0.003109411085338405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.0062451001576846465\n",
      "Training Loss: 0.006074377503828146\n",
      "Training Loss: 0.005688005609554239\n",
      "Validation Loss: 0.003104039390696987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006239218160044402\n",
      "Training Loss: 0.006068745758966543\n",
      "Training Loss: 0.005681832869886421\n",
      "Validation Loss: 0.0030987371351789642\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006233445613761432\n",
      "Training Loss: 0.006063193011214025\n",
      "Training Loss: 0.005675729064387269\n",
      "Validation Loss: 0.003093497541188943\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006227777146268636\n",
      "Training Loss: 0.006057711500907317\n",
      "Training Loss: 0.005669688098714687\n",
      "Validation Loss: 0.0030883275681395043\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006222204740624875\n",
      "Training Loss: 0.006052299195434898\n",
      "Training Loss: 0.005663706748746336\n",
      "Validation Loss: 0.0030832138753579823\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.0062167225609300655\n",
      "Training Loss: 0.006046951026073657\n",
      "Training Loss: 0.005657779359607957\n",
      "Validation Loss: 0.0030781591129279955\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006211322555900551\n",
      "Training Loss: 0.006041663305368274\n",
      "Training Loss: 0.005651900917873718\n",
      "Validation Loss: 0.0030731618400464318\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006206000368110836\n",
      "Training Loss: 0.006036430896492675\n",
      "Training Loss: 0.005646069027134218\n",
      "Validation Loss: 0.0030682145655835363\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.0062007519695907835\n",
      "Training Loss: 0.006031251199310646\n",
      "Training Loss: 0.005640280181542039\n",
      "Validation Loss: 0.0030633182561788917\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006195571711286902\n",
      "Training Loss: 0.006026121991453692\n",
      "Training Loss: 0.005634531882824376\n",
      "Validation Loss: 0.003058472060954219\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006190455088508315\n",
      "Training Loss: 0.006021039014449343\n",
      "Training Loss: 0.005628820392303169\n",
      "Validation Loss: 0.0030536705005411685\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006185397527879104\n",
      "Training Loss: 0.0060160001472104345\n",
      "Training Loss: 0.005623142900294625\n",
      "Validation Loss: 0.0030489163545190535\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006180397649295628\n",
      "Training Loss: 0.0060110030340729285\n",
      "Training Loss: 0.005617497857892886\n",
      "Validation Loss: 0.003044205760253656\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.0061754496797220785\n",
      "Training Loss: 0.006006045811809599\n",
      "Training Loss: 0.005611883582896553\n",
      "Validation Loss: 0.003039535205664762\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.0061705507646547635\n",
      "Training Loss: 0.006001125800539739\n",
      "Training Loss: 0.00560629824467469\n",
      "Validation Loss: 0.0030349086168263988\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006165698536788114\n",
      "Training Loss: 0.0059962400008225815\n",
      "Training Loss: 0.005600738581852056\n",
      "Validation Loss: 0.0030303164882313333\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006160889949533157\n",
      "Training Loss: 0.005991387923713774\n",
      "Training Loss: 0.005595204244600609\n",
      "Validation Loss: 0.0030257604974374343\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.0061561224894830955\n",
      "Training Loss: 0.005986567513900809\n",
      "Training Loss: 0.005589692844077945\n",
      "Validation Loss: 0.003021245266394585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006151393977925182\n",
      "Training Loss: 0.0059817758575081825\n",
      "Training Loss: 0.005584204368642531\n",
      "Validation Loss: 0.0030167614838390957\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006146701892721467\n",
      "Training Loss: 0.005977014160016551\n",
      "Training Loss: 0.005578736355528235\n",
      "Validation Loss: 0.003012313928685329\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006142044251319021\n",
      "Training Loss: 0.005972277742112056\n",
      "Training Loss: 0.005573289041640237\n",
      "Validation Loss: 0.003007901179096714\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006137419334845617\n",
      "Training Loss: 0.005967567921616137\n",
      "Training Loss: 0.00556785925058648\n",
      "Validation Loss: 0.0030035201938864724\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006132825617096387\n",
      "Training Loss: 0.005962883271276951\n",
      "Training Loss: 0.005562448487617075\n",
      "Validation Loss: 0.002999170750688259\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.00612826086464338\n",
      "Training Loss: 0.005958222219487652\n",
      "Training Loss: 0.005557053718948737\n",
      "Validation Loss: 0.0029948492864084043\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006123723987839185\n",
      "Training Loss: 0.005953583195223473\n",
      "Training Loss: 0.005551675233291462\n",
      "Validation Loss: 0.002990561897808874\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006119212448247709\n",
      "Training Loss: 0.00594896610127762\n",
      "Training Loss: 0.005546312778606079\n",
      "Validation Loss: 0.002986301574463632\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006114726950763725\n",
      "Training Loss: 0.005944370146025903\n",
      "Training Loss: 0.005540964615065604\n",
      "Validation Loss: 0.002982069522383089\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006110265406314284\n",
      "Training Loss: 0.005939794600126333\n",
      "Training Loss: 0.005535630632657558\n",
      "Validation Loss: 0.0029778654544530543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0061058251722715794\n",
      "Training Loss: 0.005935237003141083\n",
      "Training Loss: 0.005530309949535877\n",
      "Validation Loss: 0.0029736890789812034\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006101407359237782\n",
      "Training Loss: 0.0059306990366894755\n",
      "Training Loss: 0.005525001880014316\n",
      "Validation Loss: 0.002969539515004399\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006097009202931076\n",
      "Training Loss: 0.005926178958034143\n",
      "Training Loss: 0.005519707228522748\n",
      "Validation Loss: 0.002965419332161964\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006092631364008411\n",
      "Training Loss: 0.005921676610014402\n",
      "Training Loss: 0.005514424630673602\n",
      "Validation Loss: 0.0029613217332379455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006088272418128327\n",
      "Training Loss: 0.005917191629996523\n",
      "Training Loss: 0.005509154311730526\n",
      "Validation Loss: 0.0029572508882553306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006083930847235024\n",
      "Training Loss: 0.0059127237438224255\n",
      "Training Loss: 0.005503895047004334\n",
      "Validation Loss: 0.0029532062236397537\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006079607076826505\n",
      "Training Loss: 0.005908271018997766\n",
      "Training Loss: 0.00549864748492837\n",
      "Validation Loss: 0.002949187868232891\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.00607529905217234\n",
      "Training Loss: 0.005903835981735028\n",
      "Training Loss: 0.005493410776252859\n",
      "Validation Loss: 0.0029451934237483095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006071007225546055\n",
      "Training Loss: 0.005899415313033387\n",
      "Training Loss: 0.005488185213180259\n",
      "Validation Loss: 0.002941223492537196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006066730991587974\n",
      "Training Loss: 0.005895011071697809\n",
      "Training Loss: 0.005482970075099729\n",
      "Validation Loss: 0.002937281771113029\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006062469478347338\n",
      "Training Loss: 0.0058906227350234985\n",
      "Training Loss: 0.005477764934184961\n",
      "Validation Loss: 0.002933360499544383\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.00605822226672899\n",
      "Training Loss: 0.005886249329196289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [12:34<12:34, 150.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.005472571206628345\n",
      "Validation Loss: 0.0029294641016134886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.7189407405257225\n",
      "Training Loss: 0.6143703299760819\n",
      "Training Loss: 0.5078303414583206\n",
      "Validation Loss: 0.37199543902043547\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.3145623592287302\n",
      "Training Loss: 0.21076797895133495\n",
      "Training Loss: 0.1429582539573312\n",
      "Validation Loss: 0.09405989836106139\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.08637635296210647\n",
      "Training Loss: 0.07292447550222278\n",
      "Training Loss: 0.06781622197479009\n",
      "Validation Loss: 0.06121627249744501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.06275764927268028\n",
      "Training Loss: 0.06276895569637418\n",
      "Training Loss: 0.06283809814602137\n",
      "Validation Loss: 0.05911472322565786\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.060734227672219275\n",
      "Training Loss: 0.06103730820119381\n",
      "Training Loss: 0.06100273050367832\n",
      "Validation Loss: 0.0573074050415098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.0588257022947073\n",
      "Training Loss: 0.059001632388681176\n",
      "Training Loss: 0.05882091568782925\n",
      "Validation Loss: 0.05509711672248465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.05653351142071188\n",
      "Training Loss: 0.05658014576882124\n",
      "Training Loss: 0.05625701429322362\n",
      "Validation Loss: 0.05251482447211662\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.05387482894584537\n",
      "Training Loss: 0.053785299053415656\n",
      "Training Loss: 0.05331565359607339\n",
      "Validation Loss: 0.04955713070008192\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.05083679651841521\n",
      "Training Loss: 0.050580533603206274\n",
      "Training Loss: 0.04993647936731577\n",
      "Validation Loss: 0.046145983458904735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.04731613986194134\n",
      "Training Loss: 0.04682541365735233\n",
      "Training Loss: 0.04595593691803515\n",
      "Validation Loss: 0.04215375514010365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.04316930853761733\n",
      "Training Loss: 0.042409642059355976\n",
      "Training Loss: 0.041340989712625745\n",
      "Validation Loss: 0.0376954282100281\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.038563310015015305\n",
      "Training Loss: 0.037626280374825\n",
      "Training Loss: 0.03651430788449943\n",
      "Validation Loss: 0.0332424837802903\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.03405856119468808\n",
      "Training Loss: 0.03309585299342871\n",
      "Training Loss: 0.0321105002053082\n",
      "Validation Loss: 0.029285313917344874\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.030166757483966647\n",
      "Training Loss: 0.029270181595347822\n",
      "Training Loss: 0.02846062334254384\n",
      "Validation Loss: 0.02599089312252034\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.02698700004722923\n",
      "Training Loss: 0.026169786294922233\n",
      "Training Loss: 0.02549717065412551\n",
      "Validation Loss: 0.02327551855967286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.024385491111315785\n",
      "Training Loss: 0.023645731336437166\n",
      "Training Loss: 0.02307198049966246\n",
      "Validation Loss: 0.021040717589804966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.02225085597485304\n",
      "Training Loss: 0.02158960675355047\n",
      "Training Loss: 0.021094416938722133\n",
      "Validation Loss: 0.019221696071326733\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.020518029881641267\n",
      "Training Loss: 0.019931971253827216\n",
      "Training Loss: 0.019499243246391417\n",
      "Validation Loss: 0.01775039164244794\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.01912139993160963\n",
      "Training Loss: 0.01859897024463862\n",
      "Training Loss: 0.018208816363476217\n",
      "Validation Loss: 0.016541116075653037\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.017979398344177753\n",
      "Training Loss: 0.01750570269767195\n",
      "Training Loss: 0.01713682954432443\n",
      "Validation Loss: 0.015507208999623073\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.017010021950118245\n",
      "Training Loss: 0.01657195377862081\n",
      "Training Loss: 0.016204945580102503\n",
      "Validation Loss: 0.014574905151103655\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.01614333300618455\n",
      "Training Loss: 0.015731298585888\n",
      "Training Loss: 0.01534986042417586\n",
      "Validation Loss: 0.013686611139205065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.01532457016641274\n",
      "Training Loss: 0.014932787700090558\n",
      "Training Loss: 0.014524486074224114\n",
      "Validation Loss: 0.012800692884104976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.014514536450151353\n",
      "Training Loss: 0.014140368411317468\n",
      "Training Loss: 0.013695839706342668\n",
      "Validation Loss: 0.011888117089951307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.01368496110662818\n",
      "Training Loss: 0.01332671094685793\n",
      "Training Loss: 0.012836427527945489\n",
      "Validation Loss: 0.010926174470799023\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.01281061495654285\n",
      "Training Loss: 0.012472656922182069\n",
      "Training Loss: 0.011933504512999207\n",
      "Validation Loss: 0.009929158557415678\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.011908361669629813\n",
      "Training Loss: 0.011628022494260221\n",
      "Training Loss: 0.011071372518781573\n",
      "Validation Loss: 0.009034696448426903\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.011118822754360736\n",
      "Training Loss: 0.010939650579821318\n",
      "Training Loss: 0.010401514146942646\n",
      "Validation Loss: 0.00837116175739283\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.010548347657313571\n",
      "Training Loss: 0.01044704817701131\n",
      "Training Loss: 0.009927283241413534\n",
      "Validation Loss: 0.007897159393374504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.010145510141737759\n",
      "Training Loss: 0.010081896662013605\n",
      "Training Loss: 0.009574175897287205\n",
      "Validation Loss: 0.007536412828884433\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.009841423456091434\n",
      "Training Loss: 0.009793462342349813\n",
      "Training Loss: 0.00929466126835905\n",
      "Validation Loss: 0.007245816007842508\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.009599417492281645\n",
      "Training Loss: 0.00955710315145552\n",
      "Training Loss: 0.009065304087707773\n",
      "Validation Loss: 0.0070036307285968845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.009400860330788418\n",
      "Training Loss: 0.009359315399779007\n",
      "Training Loss: 0.008873062953352929\n",
      "Validation Loss: 0.006797513097859501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.009234838822158054\n",
      "Training Loss: 0.009191345828585327\n",
      "Training Loss: 0.008709525247104466\n",
      "Validation Loss: 0.006619463706116998\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.009094094645697624\n",
      "Training Loss: 0.009046930482145398\n",
      "Training Loss: 0.008568740547634662\n",
      "Validation Loss: 0.006463826890365126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008973418754758314\n",
      "Training Loss: 0.008921375924255699\n",
      "Training Loss: 0.008446279161144049\n",
      "Validation Loss: 0.006326371419316764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.00886888497392647\n",
      "Training Loss: 0.008811064121546223\n",
      "Training Loss: 0.008338731346884742\n",
      "Validation Loss: 0.006203821648351765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.0087774716259446\n",
      "Training Loss: 0.008713184825610369\n",
      "Training Loss: 0.008243441439699382\n",
      "Validation Loss: 0.006093609596738655\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008696799639146775\n",
      "Training Loss: 0.00862551888800226\n",
      "Training Loss: 0.008158295969478787\n",
      "Validation Loss: 0.005993671853911508\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008624975843122228\n",
      "Training Loss: 0.008546305348863826\n",
      "Training Loss: 0.008081597357522697\n",
      "Validation Loss: 0.005902337938484349\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008560470195952803\n",
      "Training Loss: 0.008474125532666221\n",
      "Training Loss: 0.008011961620068177\n",
      "Validation Loss: 0.005818241877544127\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008502026773057877\n",
      "Training Loss: 0.008407821982400492\n",
      "Training Loss: 0.007948236829834058\n",
      "Validation Loss: 0.005740233286273446\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.008448594047222287\n",
      "Training Loss: 0.008346422261092811\n",
      "Training Loss: 0.007889445624314248\n",
      "Validation Loss: 0.005667345056812582\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.008399273371323943\n",
      "Training Loss: 0.008289098821114749\n",
      "Training Loss: 0.007834749156609177\n",
      "Validation Loss: 0.005598734917767932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.008353277797577902\n",
      "Training Loss: 0.008235118376323953\n",
      "Training Loss: 0.007783405582886189\n",
      "Validation Loss: 0.005533661300923382\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.008309907463844865\n",
      "Training Loss: 0.008183816277887673\n",
      "Training Loss: 0.0077347520121838894\n",
      "Validation Loss: 0.005471461716767275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.00826852316269651\n",
      "Training Loss: 0.008134560429025441\n",
      "Training Loss: 0.00768817619420588\n",
      "Validation Loss: 0.00541153588865915\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.008228527524042874\n",
      "Training Loss: 0.008086737034609541\n",
      "Training Loss: 0.007643116558901966\n",
      "Validation Loss: 0.005353356949629241\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.008189367565792053\n",
      "Training Loss: 0.008039748698938637\n",
      "Training Loss: 0.007599061352666467\n",
      "Validation Loss: 0.005296464572566446\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.008150560418143868\n",
      "Training Loss: 0.007993068493669852\n",
      "Training Loss: 0.007555604019435122\n",
      "Validation Loss: 0.0052405410168922684\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.008111781083280221\n",
      "Training Loss: 0.007946388351265342\n",
      "Training Loss: 0.007512551739346236\n",
      "Validation Loss: 0.005185472983029786\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.008072983977617696\n",
      "Training Loss: 0.007899783600587398\n",
      "Training Loss: 0.007470027492381633\n",
      "Validation Loss: 0.00513141537137497\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.008034452679567039\n",
      "Training Loss: 0.007853764870669694\n",
      "Training Loss: 0.007428448117570952\n",
      "Validation Loss: 0.005078773652569631\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007996689047431574\n",
      "Training Loss: 0.007809070395305753\n",
      "Training Loss: 0.007388333230046555\n",
      "Validation Loss: 0.005028039627718959\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007960196775384247\n",
      "Training Loss: 0.007766362861730158\n",
      "Training Loss: 0.007350114687578752\n",
      "Validation Loss: 0.004979637543032511\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007925345393596217\n",
      "Training Loss: 0.007726063570007682\n",
      "Training Loss: 0.007314051712164656\n",
      "Validation Loss: 0.004933831159004502\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007892356626689434\n",
      "Training Loss: 0.007688355745049193\n",
      "Training Loss: 0.007280263896100223\n",
      "Validation Loss: 0.004890733864158392\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007861323601100595\n",
      "Training Loss: 0.007653254197211936\n",
      "Training Loss: 0.007248763382667675\n",
      "Validation Loss: 0.004850335458419129\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007832251361105592\n",
      "Training Loss: 0.007620667241280898\n",
      "Training Loss: 0.007219488341361284\n",
      "Validation Loss: 0.004812549650564455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.00780507274903357\n",
      "Training Loss: 0.007590440097264946\n",
      "Training Loss: 0.007192327476805076\n",
      "Validation Loss: 0.004777240797123882\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007779679988743737\n",
      "Training Loss: 0.0075623892818111925\n",
      "Training Loss: 0.0071671398659236725\n",
      "Validation Loss: 0.00474424705202325\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007755940718343481\n",
      "Training Loss: 0.007536320794606581\n",
      "Training Loss: 0.007143767541274429\n",
      "Validation Loss: 0.004713392947287707\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.00773371658520773\n",
      "Training Loss: 0.007512048714561388\n",
      "Training Loss: 0.007122051286278293\n",
      "Validation Loss: 0.004684511613979769\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007712878804886714\n",
      "Training Loss: 0.007489407362882048\n",
      "Training Loss: 0.007101842147530988\n",
      "Validation Loss: 0.0046574337475964525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007693307279841974\n",
      "Training Loss: 0.007468249950325117\n",
      "Training Loss: 0.007083001551218331\n",
      "Validation Loss: 0.004632006137726012\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007674901220016181\n",
      "Training Loss: 0.007448455193080008\n",
      "Training Loss: 0.007065407709451393\n",
      "Validation Loss: 0.004608089347589719\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007657575132325292\n",
      "Training Loss: 0.007429921401198953\n",
      "Training Loss: 0.0070489524037111555\n",
      "Validation Loss: 0.0045855511686230975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007641259231604636\n",
      "Training Loss: 0.007412561975652352\n",
      "Training Loss: 0.007033538959221914\n",
      "Validation Loss: 0.0045642746753697646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0076258948759641496\n",
      "Training Loss: 0.007396308113820851\n",
      "Training Loss: 0.007019084959756583\n",
      "Validation Loss: 0.004544145752037509\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007611431650584564\n",
      "Training Loss: 0.007381095850141719\n",
      "Training Loss: 0.007005514319753274\n",
      "Validation Loss: 0.004525068148007888\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007597827760037035\n",
      "Training Loss: 0.007366870711557567\n",
      "Training Loss: 0.0069927630387246606\n",
      "Validation Loss: 0.004506950324212902\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007585042089922354\n",
      "Training Loss: 0.007353577979374677\n",
      "Training Loss: 0.0069807683140970765\n",
      "Validation Loss: 0.00448970745704817\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.0075730389938689765\n",
      "Training Loss: 0.007341166528640315\n",
      "Training Loss: 0.006969476881204173\n",
      "Validation Loss: 0.00447327772082154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007561781934928149\n",
      "Training Loss: 0.007329582221573218\n",
      "Training Loss: 0.006958840154111385\n",
      "Validation Loss: 0.004457591261107768\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007551230455283076\n",
      "Training Loss: 0.007318770514102652\n",
      "Training Loss: 0.006948808772722259\n",
      "Validation Loss: 0.0044425924549276906\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007541344988858327\n",
      "Training Loss: 0.007308675321983173\n",
      "Training Loss: 0.006939342119731009\n",
      "Validation Loss: 0.004428238759675388\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007532085524871945\n",
      "Training Loss: 0.0072992400440853086\n",
      "Training Loss: 0.006930396156385541\n",
      "Validation Loss: 0.004414486730199182\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0075234074774198235\n",
      "Training Loss: 0.007290408053668216\n",
      "Training Loss: 0.006921936488943174\n",
      "Validation Loss: 0.0044013065537123865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.0075152688997332006\n",
      "Training Loss: 0.007282125076744706\n",
      "Training Loss: 0.006913926647976041\n",
      "Validation Loss: 0.004388667603829101\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007507624226855114\n",
      "Training Loss: 0.007274335904512554\n",
      "Training Loss: 0.006906329105840996\n",
      "Validation Loss: 0.004376541352255291\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007500430550426245\n",
      "Training Loss: 0.007266990232747048\n",
      "Training Loss: 0.006899116284912452\n",
      "Validation Loss: 0.004364909224860956\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007493647878291086\n",
      "Training Loss: 0.0072600400622468446\n",
      "Training Loss: 0.006892257374711335\n",
      "Validation Loss: 0.004353746565142542\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007487234086729586\n",
      "Training Loss: 0.007253442549845204\n",
      "Training Loss: 0.006885722456499935\n",
      "Validation Loss: 0.004343031434698052\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007481155321002007\n",
      "Training Loss: 0.007247157660312951\n",
      "Training Loss: 0.006879487682599574\n",
      "Validation Loss: 0.004332749044857501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007475375311914831\n",
      "Training Loss: 0.007241152483038605\n",
      "Training Loss: 0.00687352676410228\n",
      "Validation Loss: 0.004322878888211726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.0074698654445819555\n",
      "Training Loss: 0.007235394158633426\n",
      "Training Loss: 0.006867818512255326\n",
      "Validation Loss: 0.004313401077361254\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007464596001664176\n",
      "Training Loss: 0.007229857803322375\n",
      "Training Loss: 0.006862343683606014\n",
      "Validation Loss: 0.0043042978622396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007459545088931918\n",
      "Training Loss: 0.007224518648581579\n",
      "Training Loss: 0.006857080466579646\n",
      "Validation Loss: 0.004295552620308452\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007454690188169479\n",
      "Training Loss: 0.007219356956193223\n",
      "Training Loss: 0.006852012764429673\n",
      "Validation Loss: 0.0042871458618116845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007450011337641627\n",
      "Training Loss: 0.007214354863390326\n",
      "Training Loss: 0.006847125442000106\n",
      "Validation Loss: 0.004279059776513094\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00744549220544286\n",
      "Training Loss: 0.007209495947463438\n",
      "Training Loss: 0.006842402737820521\n",
      "Validation Loss: 0.004271280457359854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007441118948627264\n",
      "Training Loss: 0.007204769873060286\n",
      "Training Loss: 0.006837832560995594\n",
      "Validation Loss: 0.004263792071868194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007436877229483798\n",
      "Training Loss: 0.007200162196531892\n",
      "Training Loss: 0.006833403720520437\n",
      "Validation Loss: 0.004256578256491195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007432756742928177\n",
      "Training Loss: 0.007195663438178599\n",
      "Training Loss: 0.006829102362971753\n",
      "Validation Loss: 0.004249623121191444\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0074287463363725695\n",
      "Training Loss: 0.007191265450092033\n",
      "Training Loss: 0.006824922080850229\n",
      "Validation Loss: 0.0042429141951410955\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.0074248382728546855\n",
      "Training Loss: 0.007186960084363818\n",
      "Training Loss: 0.006820851467782631\n",
      "Validation Loss: 0.004236438149439819\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007421021522022783\n",
      "Training Loss: 0.007182739462004975\n",
      "Training Loss: 0.006816883313003928\n",
      "Validation Loss: 0.004230181192142073\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007417293695034459\n",
      "Training Loss: 0.00717859904631041\n",
      "Training Loss: 0.006813010519836098\n",
      "Validation Loss: 0.004224133842593331\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007413645153865218\n",
      "Training Loss: 0.007174531921045855\n",
      "Training Loss: 0.006809226400218904\n",
      "Validation Loss: 0.004218284032841245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007410070589976385\n",
      "Training Loss: 0.007170533302705735\n",
      "Training Loss: 0.006805523470975458\n",
      "Validation Loss: 0.004212622311008111\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00740656558307819\n",
      "Training Loss: 0.0071666010550688955\n",
      "Training Loss: 0.006801898212870583\n",
      "Validation Loss: 0.004207135638577885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007403125781565905\n",
      "Training Loss: 0.007162728838156909\n",
      "Training Loss: 0.006798345388378948\n",
      "Validation Loss: 0.004201815531620484\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007399746571900323\n",
      "Training Loss: 0.0071589137846603985\n",
      "Training Loss: 0.006794858772773296\n",
      "Validation Loss: 0.0041966546729751185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007396424114704132\n",
      "Training Loss: 0.0071551538142375646\n",
      "Training Loss: 0.006791435156483203\n",
      "Validation Loss: 0.004191645135935606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0073931565915700045\n",
      "Training Loss: 0.007151447171345353\n",
      "Training Loss: 0.006788072476629168\n",
      "Validation Loss: 0.004186777565419004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007389939104905352\n",
      "Training Loss: 0.0071477884007617835\n",
      "Training Loss: 0.0067847654735669494\n",
      "Validation Loss: 0.004182043780959891\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007386770874727517\n",
      "Training Loss: 0.007144177306909114\n",
      "Training Loss: 0.006781511472072452\n",
      "Validation Loss: 0.004177442893092887\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.007383647309616208\n",
      "Training Loss: 0.007140612547518685\n",
      "Training Loss: 0.006778307817876339\n",
      "Validation Loss: 0.004172962334195382\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007380567947402597\n",
      "Training Loss: 0.0071370911493431775\n",
      "Training Loss: 0.006775150123285129\n",
      "Validation Loss: 0.00416859822724475\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.007377530703088268\n",
      "Training Loss: 0.00713361180271022\n",
      "Training Loss: 0.006772038326598704\n",
      "Validation Loss: 0.004164344793332092\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.0073745330318342895\n",
      "Training Loss: 0.0071301718370523305\n",
      "Training Loss: 0.006768970256671309\n",
      "Validation Loss: 0.0041601946786715745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.007371573275886476\n",
      "Training Loss: 0.0071267721219919625\n",
      "Training Loss: 0.006765941706253216\n",
      "Validation Loss: 0.004156144084722808\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.0073686497926246375\n",
      "Training Loss: 0.007123408917104826\n",
      "Training Loss: 0.00676295229117386\n",
      "Validation Loss: 0.004152189696919215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007365762229310349\n",
      "Training Loss: 0.0071200839034281675\n",
      "Training Loss: 0.006760000919457525\n",
      "Validation Loss: 0.004148324025439077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007362907828064635\n",
      "Training Loss: 0.007116794032044709\n",
      "Training Loss: 0.0067570842255372555\n",
      "Validation Loss: 0.00414454143787368\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007360086082480848\n",
      "Training Loss: 0.007113538015400991\n",
      "Training Loss: 0.006754201356088742\n",
      "Validation Loss: 0.004140843417537347\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.007357296561822295\n",
      "Training Loss: 0.0071103161887731406\n",
      "Training Loss: 0.00675135157071054\n",
      "Validation Loss: 0.0041372204968559275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.007354535876074806\n",
      "Training Loss: 0.007107127590570599\n",
      "Training Loss: 0.006748533105710521\n",
      "Validation Loss: 0.004133672445614854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.0073518065235111865\n",
      "Training Loss: 0.007103971529286355\n",
      "Training Loss: 0.006745744068175554\n",
      "Validation Loss: 0.004130195488818408\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.00734910559724085\n",
      "Training Loss: 0.007100847332039848\n",
      "Training Loss: 0.006742984803859145\n",
      "Validation Loss: 0.0041267886846797185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0073464327852707356\n",
      "Training Loss: 0.007097754615824669\n",
      "Training Loss: 0.006740253247553483\n",
      "Validation Loss: 0.004123445090136669\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007343786773271859\n",
      "Training Loss: 0.007094691812526434\n",
      "Training Loss: 0.006737548809032887\n",
      "Validation Loss: 0.004120161798730326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.007341167385457084\n",
      "Training Loss: 0.0070916590874549\n",
      "Training Loss: 0.006734872452216223\n",
      "Validation Loss: 0.004116939082532451\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.007338575557805598\n",
      "Training Loss: 0.007088656574487686\n",
      "Training Loss: 0.0067322201712522655\n",
      "Validation Loss: 0.0041137736583693645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.0073360087745822965\n",
      "Training Loss: 0.007085683452896774\n",
      "Training Loss: 0.006729592821793631\n",
      "Validation Loss: 0.0041106615445754505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.007333466723794118\n",
      "Training Loss: 0.0070827384025324135\n",
      "Training Loss: 0.006726990290917456\n",
      "Validation Loss: 0.004107600823567992\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.00733094816445373\n",
      "Training Loss: 0.007079820189392194\n",
      "Training Loss: 0.006724410303868353\n",
      "Validation Loss: 0.004104589488817735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.007328454847447574\n",
      "Training Loss: 0.007076930574839935\n",
      "Training Loss: 0.0067218535579741\n",
      "Validation Loss: 0.00410162400077568\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.007325983833288774\n",
      "Training Loss: 0.007074067477369681\n",
      "Training Loss: 0.0067193176131695505\n",
      "Validation Loss: 0.004098705578532614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007323537121992558\n",
      "Training Loss: 0.007071231917943805\n",
      "Training Loss: 0.006716804167954251\n",
      "Validation Loss: 0.004095830711316359\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007321112169884145\n",
      "Training Loss: 0.007068423457676545\n",
      "Training Loss: 0.006714312019757926\n",
      "Validation Loss: 0.004092995124460941\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.007318708874518052\n",
      "Training Loss: 0.00706563965883106\n",
      "Training Loss: 0.006711839661002159\n",
      "Validation Loss: 0.004090201716577069\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.0073163283045869325\n",
      "Training Loss: 0.007062882313039154\n",
      "Training Loss: 0.00670938748633489\n",
      "Validation Loss: 0.004087447097232\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.007313968836097046\n",
      "Training Loss: 0.007060150705510751\n",
      "Training Loss: 0.00670695558656007\n",
      "Validation Loss: 0.004084726390062591\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.0073116308578755704\n",
      "Training Loss: 0.007057443723315373\n",
      "Training Loss: 0.006704541232902556\n",
      "Validation Loss: 0.004082041308597735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.0073093137401156125\n",
      "Training Loss: 0.007054761241888628\n",
      "Training Loss: 0.006702145001618191\n",
      "Validation Loss: 0.00407939120405092\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.007307015500264242\n",
      "Training Loss: 0.007052103275200352\n",
      "Training Loss: 0.006699768069665879\n",
      "Validation Loss: 0.0040767736827138435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.007304738451493904\n",
      "Training Loss: 0.0070494696951936935\n",
      "Training Loss: 0.006697409332264215\n",
      "Validation Loss: 0.004074186492146233\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.007302481076912955\n",
      "Training Loss: 0.007046859728870913\n",
      "Training Loss: 0.006695067071123048\n",
      "Validation Loss: 0.004071631324948387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.007300243430072442\n",
      "Training Loss: 0.007044272883795202\n",
      "Training Loss: 0.00669274281593971\n",
      "Validation Loss: 0.004069108429647396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.007298025219934061\n",
      "Training Loss: 0.007041709881741553\n",
      "Training Loss: 0.006690435110358522\n",
      "Validation Loss: 0.004066611532896255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.0072958252253010865\n",
      "Training Loss: 0.007039168608607724\n",
      "Training Loss: 0.006688142843777314\n",
      "Validation Loss: 0.004064142342991709\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.007293644088786095\n",
      "Training Loss: 0.007036650732625276\n",
      "Training Loss: 0.00668586743879132\n",
      "Validation Loss: 0.004061698021492764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.007291481256252155\n",
      "Training Loss: 0.007034153993008658\n",
      "Training Loss: 0.0066836071701254695\n",
      "Validation Loss: 0.00405928043627672\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.007289336124667898\n",
      "Training Loss: 0.007031680343206972\n",
      "Training Loss: 0.006681362607050687\n",
      "Validation Loss: 0.004056886278009147\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.007287209158530459\n",
      "Training Loss: 0.007029227354796603\n",
      "Training Loss: 0.006679133279249072\n",
      "Validation Loss: 0.004054518691211772\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.007285099276341498\n",
      "Training Loss: 0.007026796278078109\n",
      "Training Loss: 0.00667691875831224\n",
      "Validation Loss: 0.004052172964334153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.007283007838996127\n",
      "Training Loss: 0.007024386380799115\n",
      "Training Loss: 0.006674718383001163\n",
      "Validation Loss: 0.004049850680101453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.0072809318767394875\n",
      "Training Loss: 0.00702199624851346\n",
      "Training Loss: 0.006672532360535115\n",
      "Validation Loss: 0.0040475521995319745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.007278874201001599\n",
      "Training Loss: 0.0070196269534062595\n",
      "Training Loss: 0.0066703607630915936\n",
      "Validation Loss: 0.004045273899362328\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.007276832427596674\n",
      "Training Loss: 0.0070172779459971935\n",
      "Training Loss: 0.006668203084263951\n",
      "Validation Loss: 0.004043014877046762\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.007274807683425024\n",
      "Training Loss: 0.007014949009753763\n",
      "Training Loss: 0.006666057989932596\n",
      "Validation Loss: 0.004040776390917181\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.00727279874146916\n",
      "Training Loss: 0.0070126397034619\n",
      "Training Loss: 0.006663927310146391\n",
      "Validation Loss: 0.004038558663339929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.007270805474836379\n",
      "Training Loss: 0.007010349216870964\n",
      "Training Loss: 0.006661809262586757\n",
      "Validation Loss: 0.004036360093277324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.007268827332882211\n",
      "Training Loss: 0.007008076952770352\n",
      "Training Loss: 0.006659705152269452\n",
      "Validation Loss: 0.004034179563665491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.007266864996636286\n",
      "Training Loss: 0.007005824644584209\n",
      "Training Loss: 0.00665761157637462\n",
      "Validation Loss: 0.004032016355083899\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.007264917152933776\n",
      "Training Loss: 0.007003589916275814\n",
      "Training Loss: 0.006655531455762685\n",
      "Validation Loss: 0.004029869724567352\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.0072629853431135415\n",
      "Training Loss: 0.007001372170634568\n",
      "Training Loss: 0.0066534625028725715\n",
      "Validation Loss: 0.004027739967732282\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.007261067169019952\n",
      "Training Loss: 0.006999173611402512\n",
      "Training Loss: 0.00665140611003153\n",
      "Validation Loss: 0.004025628512955449\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.007259163185954094\n",
      "Training Loss: 0.006996992020867765\n",
      "Training Loss: 0.006649361241143197\n",
      "Validation Loss: 0.004023533960636915\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.007257273800205439\n",
      "Training Loss: 0.006994827551534399\n",
      "Training Loss: 0.006647328666877002\n",
      "Validation Loss: 0.004021455070757297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.007255398472771049\n",
      "Training Loss: 0.006992679662071169\n",
      "Training Loss: 0.006645306741120294\n",
      "Validation Loss: 0.0040193905457435695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0072535375645384195\n",
      "Training Loss: 0.006990548482863232\n",
      "Training Loss: 0.006643296184483916\n",
      "Validation Loss: 0.004017341832285014\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.007251688753021881\n",
      "Training Loss: 0.006988433523802086\n",
      "Training Loss: 0.006641296287998557\n",
      "Validation Loss: 0.004015307122674049\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.007249854466645047\n",
      "Training Loss: 0.006986335025867447\n",
      "Training Loss: 0.006639307589503005\n",
      "Validation Loss: 0.004013286620964495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.007248031644849107\n",
      "Training Loss: 0.006984251767862588\n",
      "Training Loss: 0.006637329624500126\n",
      "Validation Loss: 0.004011282558668028\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.007246223498368636\n",
      "Training Loss: 0.006982184175867588\n",
      "Training Loss: 0.006635360626969487\n",
      "Validation Loss: 0.004009292400808314\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.0072444279596675185\n",
      "Training Loss: 0.006980131827294827\n",
      "Training Loss: 0.006633402783190832\n",
      "Validation Loss: 0.004007316312198056\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.007242643996141851\n",
      "Training Loss: 0.0069780940527562054\n",
      "Training Loss: 0.006631454493617639\n",
      "Validation Loss: 0.004005351823262787\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.007240872433176265\n",
      "Training Loss: 0.006976071825483814\n",
      "Training Loss: 0.006629516835091636\n",
      "Validation Loss: 0.004003402209327964\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.007239113469840959\n",
      "Training Loss: 0.006974063861416652\n",
      "Training Loss: 0.006627588026458397\n",
      "Validation Loss: 0.00400146519702472\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.007237366830231622\n",
      "Training Loss: 0.006972071457421407\n",
      "Training Loss: 0.006625669460045174\n",
      "Validation Loss: 0.0039995427091679215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.007235631201183423\n",
      "Training Loss: 0.006970092934789136\n",
      "Training Loss: 0.00662375999498181\n",
      "Validation Loss: 0.003997633084550165\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.007233907841145993\n",
      "Training Loss: 0.006968126490246505\n",
      "Training Loss: 0.006621859589358792\n",
      "Validation Loss: 0.003995735071379733\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.007232195342658088\n",
      "Training Loss: 0.006966173928231001\n",
      "Training Loss: 0.006619969051098451\n",
      "Validation Loss: 0.003993849980310024\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.0072304928745143115\n",
      "Training Loss: 0.006964234565384686\n",
      "Training Loss: 0.006618086672388017\n",
      "Validation Loss: 0.00399197591861103\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.007228802734753117\n",
      "Training Loss: 0.006962308710208163\n",
      "Training Loss: 0.006616212909575552\n",
      "Validation Loss: 0.003990114632512579\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.00722712489310652\n",
      "Training Loss: 0.00696039678179659\n",
      "Training Loss: 0.006614348411094397\n",
      "Validation Loss: 0.003988265483692455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.007225455967709422\n",
      "Training Loss: 0.006958495822036639\n",
      "Training Loss: 0.0066124921361915764\n",
      "Validation Loss: 0.003986427352470712\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.007223796445177868\n",
      "Training Loss: 0.006956606972962618\n",
      "Training Loss: 0.006610643190797418\n",
      "Validation Loss: 0.003984598241474354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.007222147748107091\n",
      "Training Loss: 0.006954731056466699\n",
      "Training Loss: 0.006608802763512358\n",
      "Validation Loss: 0.003982780454157109\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.00722050966694951\n",
      "Training Loss: 0.006952866561478004\n",
      "Training Loss: 0.006606971009168774\n",
      "Validation Loss: 0.003980973756380296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.0072188812238164245\n",
      "Training Loss: 0.00695101356250234\n",
      "Training Loss: 0.006605146570364013\n",
      "Validation Loss: 0.003979179007524436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.007217263298807666\n",
      "Training Loss: 0.006949172132881359\n",
      "Training Loss: 0.006603330542566255\n",
      "Validation Loss: 0.003977393586282733\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0072156547638587655\n",
      "Training Loss: 0.00694734213873744\n",
      "Training Loss: 0.006601521390257404\n",
      "Validation Loss: 0.003975618335031224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.0072140552336350085\n",
      "Training Loss: 0.0069455215288326145\n",
      "Training Loss: 0.006599719430087134\n",
      "Validation Loss: 0.003973852425782282\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.007212464129552245\n",
      "Training Loss: 0.006943713730433956\n",
      "Training Loss: 0.006597924841335043\n",
      "Validation Loss: 0.003972097623078257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.007210883417865261\n",
      "Training Loss: 0.00694191611954011\n",
      "Training Loss: 0.0065961386414710435\n",
      "Validation Loss: 0.003970353524043654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0072093112592119726\n",
      "Training Loss: 0.006940130040748045\n",
      "Training Loss: 0.006594358797883615\n",
      "Validation Loss: 0.003968618170546598\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.007207748778164387\n",
      "Training Loss: 0.0069383530190680175\n",
      "Training Loss: 0.0065925861150026325\n",
      "Validation Loss: 0.003966894241447529\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.007206194172613323\n",
      "Training Loss: 0.006936586125520989\n",
      "Training Loss: 0.00659082030877471\n",
      "Validation Loss: 0.003965178103018762\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.007204648300539702\n",
      "Training Loss: 0.006934829093515873\n",
      "Training Loss: 0.006589060830883682\n",
      "Validation Loss: 0.003963471717316281\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.007203109802212566\n",
      "Training Loss: 0.0069330825330689546\n",
      "Training Loss: 0.00658730921568349\n",
      "Validation Loss: 0.003961774808344211\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.007201581055996939\n",
      "Training Loss: 0.0069313443161081525\n",
      "Training Loss: 0.006585562833352015\n",
      "Validation Loss: 0.003960087132807611\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.007200059624155983\n",
      "Training Loss: 0.00692961614811793\n",
      "Training Loss: 0.006583824445260689\n",
      "Validation Loss: 0.003958408460491912\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.007198546630097553\n",
      "Training Loss: 0.006927897750865668\n",
      "Training Loss: 0.006582090760348365\n",
      "Validation Loss: 0.003956737601083149\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.007197040852624923\n",
      "Training Loss: 0.006926187807694078\n",
      "Training Loss: 0.006580363780958578\n",
      "Validation Loss: 0.003955078928658132\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.007195543005364016\n",
      "Training Loss: 0.006924486550269648\n",
      "Training Loss: 0.0065786428085993975\n",
      "Validation Loss: 0.003953426648611516\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.007194052974227816\n",
      "Training Loss: 0.006922793108969927\n",
      "Training Loss: 0.006576928102876991\n",
      "Validation Loss: 0.003951781239684964\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.007192570058396086\n",
      "Training Loss: 0.006921108696842566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [15:04<10:03, 150.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.006575218753423542\n",
      "Validation Loss: 0.00395014404784888\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.10739677529782057\n",
      "Training Loss: 0.09069374470040202\n",
      "Training Loss: 0.07869932986795902\n",
      "Validation Loss: 0.0680212223630273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06706428792327643\n",
      "Training Loss: 0.06590791145339608\n",
      "Training Loss: 0.06525563983246684\n",
      "Validation Loss: 0.06107131203406312\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06225955419242382\n",
      "Training Loss: 0.06146451260894537\n",
      "Training Loss: 0.06049423540011048\n",
      "Validation Loss: 0.05622601237022475\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.05717246325686574\n",
      "Training Loss: 0.05585582533851266\n",
      "Training Loss: 0.05439279362559318\n",
      "Validation Loss: 0.04992615820819073\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.050596202248707416\n",
      "Training Loss: 0.048632128331810236\n",
      "Training Loss: 0.04658296652138233\n",
      "Validation Loss: 0.04175624272294259\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.042126366915181276\n",
      "Training Loss: 0.03940740338526666\n",
      "Training Loss: 0.03661749294959009\n",
      "Validation Loss: 0.03158665639923865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.03204466348513961\n",
      "Training Loss: 0.029753730557858946\n",
      "Training Loss: 0.027626487985253335\n",
      "Validation Loss: 0.024146119770960192\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.024918841030448675\n",
      "Training Loss: 0.023673212067224086\n",
      "Training Loss: 0.022458442449569703\n",
      "Validation Loss: 0.01986636193155238\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.02094163150526583\n",
      "Training Loss: 0.02024283881299198\n",
      "Training Loss: 0.019476839173585177\n",
      "Validation Loss: 0.017221557395009513\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.01852993855252862\n",
      "Training Loss: 0.01807000783504918\n",
      "Training Loss: 0.01747954376041889\n",
      "Validation Loss: 0.015373507368095805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.01686325960094109\n",
      "Training Loss: 0.016524799526669087\n",
      "Training Loss: 0.016011607826221734\n",
      "Validation Loss: 0.013974471687433425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.015610400440637022\n",
      "Training Loss: 0.01532806468429044\n",
      "Training Loss: 0.014841742932330816\n",
      "Validation Loss: 0.012824498702970784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.014589778750669212\n",
      "Training Loss: 0.014329273069743067\n",
      "Training Loss: 0.013847130793146789\n",
      "Validation Loss: 0.01182373126006026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.013712743599899112\n",
      "Training Loss: 0.013461391527671366\n",
      "Training Loss: 0.012977617245633155\n",
      "Validation Loss: 0.010939484634802918\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.012946093692444266\n",
      "Training Loss: 0.012702596737071872\n",
      "Training Loss: 0.012218409711495041\n",
      "Validation Loss: 0.01016951095982549\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.012279006480239331\n",
      "Training Loss: 0.012045866372063755\n",
      "Training Loss: 0.011562594182323664\n",
      "Validation Loss: 0.00951349183268259\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.011702826810069383\n",
      "Training Loss: 0.011482533372472972\n",
      "Training Loss: 0.010999422885943204\n",
      "Validation Loss: 0.008960937568394656\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.011205133228795603\n",
      "Training Loss: 0.010998985110782088\n",
      "Training Loss: 0.010514143334003166\n",
      "Validation Loss: 0.008492603652137384\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.010771285817027092\n",
      "Training Loss: 0.010579165106173604\n",
      "Training Loss: 0.010090988989686593\n",
      "Validation Loss: 0.008086599315401543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.010387077587656677\n",
      "Training Loss: 0.01020769371651113\n",
      "Training Loss: 0.009715738834347575\n",
      "Validation Loss: 0.0077237283910384005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.010040774254593997\n",
      "Training Loss: 0.009872253856156022\n",
      "Training Loss: 0.009377428273437545\n",
      "Validation Loss: 0.007390419063106012\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.00972420652047731\n",
      "Training Loss: 0.009564641292672604\n",
      "Training Loss: 0.009068838277598842\n",
      "Validation Loss: 0.007078841503897912\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.009432562440633774\n",
      "Training Loss: 0.009280278792139142\n",
      "Training Loss: 0.008785744127817453\n",
      "Validation Loss: 0.006785265047082238\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.00916324851103127\n",
      "Training Loss: 0.009016923640156166\n",
      "Training Loss: 0.008525708181550726\n",
      "Validation Loss: 0.006508188730294115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.00891474525909871\n",
      "Training Loss: 0.008773445511469618\n",
      "Training Loss: 0.00828710095374845\n",
      "Validation Loss: 0.006247006233760647\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008685852017952128\n",
      "Training Loss: 0.008549076893832535\n",
      "Training Loss: 0.008068591330666095\n",
      "Validation Loss: 0.006001372222547953\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008475407290970906\n",
      "Training Loss: 0.00834310892270878\n",
      "Training Loss: 0.007868980627972633\n",
      "Validation Loss: 0.005771004982041509\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008282292360672728\n",
      "Training Loss: 0.008154828348197043\n",
      "Training Loss: 0.00768720249645412\n",
      "Validation Loss: 0.00555569846758598\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008105535538634285\n",
      "Training Loss: 0.00798356462502852\n",
      "Training Loss: 0.007522379769943655\n",
      "Validation Loss: 0.005355416284350867\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.007944432096555829\n",
      "Training Loss: 0.007828737909439951\n",
      "Training Loss: 0.0073738260811660435\n",
      "Validation Loss: 0.005170292916885587\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007798521843506023\n",
      "Training Loss: 0.007689826652640477\n",
      "Training Loss: 0.007240946999518201\n",
      "Validation Loss: 0.0050005369552861105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007667457415955141\n",
      "Training Loss: 0.007566265655914322\n",
      "Training Loss: 0.007123093637637794\n",
      "Validation Loss: 0.004846225301682782\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007550817055162042\n",
      "Training Loss: 0.007457316091749817\n",
      "Training Loss: 0.007019424259196967\n",
      "Validation Loss: 0.004707134883390383\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007447942561702803\n",
      "Training Loss: 0.007361990150529891\n",
      "Training Loss: 0.006928849862888455\n",
      "Validation Loss: 0.004582651817564214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007357880265917629\n",
      "Training Loss: 0.007279047798947431\n",
      "Training Loss: 0.006850056205876171\n",
      "Validation Loss: 0.00447179910365827\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.0072794051619712264\n",
      "Training Loss: 0.00720705019950401\n",
      "Training Loss: 0.006781570790335536\n",
      "Validation Loss: 0.0043733313841891755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0072111148398835215\n",
      "Training Loss: 0.007144477524561808\n",
      "Training Loss: 0.006721882220590487\n",
      "Validation Loss: 0.004285879587503464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007151537220925093\n",
      "Training Loss: 0.00708982887910679\n",
      "Training Loss: 0.006669536426197737\n",
      "Validation Loss: 0.004208066815960357\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.0070992498169653116\n",
      "Training Loss: 0.007041715942905285\n",
      "Training Loss: 0.006623216203879565\n",
      "Validation Loss: 0.004138605703149786\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007052947070915252\n",
      "Training Loss: 0.006998916284064762\n",
      "Training Loss: 0.006581773470388725\n",
      "Validation Loss: 0.004076334303820484\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007011487368727103\n",
      "Training Loss: 0.00696039313566871\n",
      "Training Loss: 0.006544248330756091\n",
      "Validation Loss: 0.004020239549605281\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006973906815983355\n",
      "Training Loss: 0.006925288816564717\n",
      "Training Loss: 0.006509854895411991\n",
      "Validation Loss: 0.003969452646037752\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006939412591746077\n",
      "Training Loss: 0.006892912332550623\n",
      "Training Loss: 0.006477958322502673\n",
      "Validation Loss: 0.00392322613350168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006907365451334044\n",
      "Training Loss: 0.00686270916077774\n",
      "Training Loss: 0.006448057554662228\n",
      "Validation Loss: 0.003880923425548532\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006877257265732624\n",
      "Training Loss: 0.0068342461204156275\n",
      "Training Loss: 0.00641975759412162\n",
      "Validation Loss: 0.003842010192855607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006848692601779476\n",
      "Training Loss: 0.006807181519689038\n",
      "Training Loss: 0.006392749705119058\n",
      "Validation Loss: 0.0038060184118201895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006821359939058311\n",
      "Training Loss: 0.006781248109182343\n",
      "Training Loss: 0.006366795601788908\n",
      "Validation Loss: 0.0037725589701152417\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0067950207192916424\n",
      "Training Loss: 0.006756240741815418\n",
      "Training Loss: 0.006341708048712463\n",
      "Validation Loss: 0.003741293041600605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006769492410239763\n",
      "Training Loss: 0.006731997421011329\n",
      "Training Loss: 0.006317344170529395\n",
      "Validation Loss: 0.003711933683436573\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006744632627232932\n",
      "Training Loss: 0.006708391617867165\n",
      "Training Loss: 0.006293590572895482\n",
      "Validation Loss: 0.003684228897000548\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006720335326390341\n",
      "Training Loss: 0.006685327002778649\n",
      "Training Loss: 0.006270361344795674\n",
      "Validation Loss: 0.0036579644061583145\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006696518487296998\n",
      "Training Loss: 0.006662724249763414\n",
      "Training Loss: 0.006247587383259087\n",
      "Validation Loss: 0.0036329524447681026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.0066731207742122935\n",
      "Training Loss: 0.006640521524241194\n",
      "Training Loss: 0.006225214374717325\n",
      "Validation Loss: 0.0036090295356332085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006650091162882745\n",
      "Training Loss: 0.006618670325842686\n",
      "Training Loss: 0.00620320005225949\n",
      "Validation Loss: 0.003586058187026405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006627394296228886\n",
      "Training Loss: 0.006597130878362805\n",
      "Training Loss: 0.006181508967420086\n",
      "Validation Loss: 0.003563911368902982\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.006605002324213274\n",
      "Training Loss: 0.006575869336375035\n",
      "Training Loss: 0.006160113130463287\n",
      "Validation Loss: 0.0035424818653676114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006582890110439621\n",
      "Training Loss: 0.0065548565069912\n",
      "Training Loss: 0.006138986886944622\n",
      "Validation Loss: 0.0035216743937494716\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006561040402157232\n",
      "Training Loss: 0.00653407028876245\n",
      "Training Loss: 0.006118110442184843\n",
      "Validation Loss: 0.003501407064371899\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006539437879109755\n",
      "Training Loss: 0.0065134915011003615\n",
      "Training Loss: 0.006097464705817401\n",
      "Validation Loss: 0.0034816030522787506\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006518070156453178\n",
      "Training Loss: 0.0064930986659601335\n",
      "Training Loss: 0.00607703254907392\n",
      "Validation Loss: 0.0034621970131574723\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.0064969265478430314\n",
      "Training Loss: 0.006472876025363803\n",
      "Training Loss: 0.006056798745994456\n",
      "Validation Loss: 0.003443129611818978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006475995755172334\n",
      "Training Loss: 0.0064528077177237715\n",
      "Training Loss: 0.006036747514153831\n",
      "Validation Loss: 0.003424348637953484\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006455270390724763\n",
      "Training Loss: 0.0064328819169895725\n",
      "Training Loss: 0.006016866654390469\n",
      "Validation Loss: 0.0034058088727107042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.0064347419777186585\n",
      "Training Loss: 0.006413082004874013\n",
      "Training Loss: 0.005997140751569532\n",
      "Validation Loss: 0.003387464741454198\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006414403088856488\n",
      "Training Loss: 0.006393397419597022\n",
      "Training Loss: 0.0059775555873056874\n",
      "Validation Loss: 0.0033692817734763698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006394245909759775\n",
      "Training Loss: 0.006373813019017689\n",
      "Training Loss: 0.005958097319817171\n",
      "Validation Loss: 0.003351223919268572\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0063742634688969705\n",
      "Training Loss: 0.006354317307122983\n",
      "Training Loss: 0.005938752382062376\n",
      "Validation Loss: 0.0033332589330946965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0063544470863416795\n",
      "Training Loss: 0.006334897353663109\n",
      "Training Loss: 0.005919504797202535\n",
      "Validation Loss: 0.0033153611041731043\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006334791640401818\n",
      "Training Loss: 0.006315540772629902\n",
      "Training Loss: 0.0059003418521024285\n",
      "Validation Loss: 0.0032975020167009717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006315286254975945\n",
      "Training Loss: 0.0062962325353873895\n",
      "Training Loss: 0.0058812447160016745\n",
      "Validation Loss: 0.003279659228681932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0062959246517857535\n",
      "Training Loss: 0.0062769611942349\n",
      "Training Loss: 0.005862198150134645\n",
      "Validation Loss: 0.003261811931204218\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006276698185247369\n",
      "Training Loss: 0.006257708345074207\n",
      "Training Loss: 0.0058431840396951885\n",
      "Validation Loss: 0.0032439341662671376\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006257598022348248\n",
      "Training Loss: 0.00623846132773906\n",
      "Training Loss: 0.00582418471458368\n",
      "Validation Loss: 0.003226006691987637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.00623861514905002\n",
      "Training Loss: 0.006219202146166935\n",
      "Training Loss: 0.005805180633906275\n",
      "Validation Loss: 0.003208010081395381\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006219740173546598\n",
      "Training Loss: 0.006199914944591001\n",
      "Training Loss: 0.005786151528009213\n",
      "Validation Loss: 0.003189922846053214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.0062009653553832325\n",
      "Training Loss: 0.006180580275249667\n",
      "Training Loss: 0.005767075920011848\n",
      "Validation Loss: 0.0031717284412510443\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006182283265516162\n",
      "Training Loss: 0.006161182040232234\n",
      "Training Loss: 0.005747934523387812\n",
      "Validation Loss: 0.003153408568854747\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.006163684025523253\n",
      "Training Loss: 0.006141700368607417\n",
      "Training Loss: 0.005728704767534509\n",
      "Validation Loss: 0.0031349455200010137\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006145164485787973\n",
      "Training Loss: 0.006122118270141073\n",
      "Training Loss: 0.005709367225063033\n",
      "Validation Loss: 0.003116322216300524\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006126721686450764\n",
      "Training Loss: 0.006102419192902744\n",
      "Training Loss: 0.0056899041333235804\n",
      "Validation Loss: 0.0030975262196263655\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006108355462783948\n",
      "Training Loss: 0.006082590434234589\n",
      "Training Loss: 0.005670299924677238\n",
      "Validation Loss: 0.003078549788359636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006090072017395869\n",
      "Training Loss: 0.006062625496415421\n",
      "Training Loss: 0.00565054704551585\n",
      "Validation Loss: 0.003059394495415219\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006071884570410475\n",
      "Training Loss: 0.006042524673976004\n",
      "Training Loss: 0.005630644868360833\n",
      "Validation Loss: 0.003040065075888225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006053813128964976\n",
      "Training Loss: 0.006022295909933746\n",
      "Training Loss: 0.005610600361833349\n",
      "Validation Loss: 0.0030205807899742316\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.0060358884563902395\n",
      "Training Loss: 0.006001962284208275\n",
      "Training Loss: 0.005590435076737776\n",
      "Validation Loss: 0.0030009761371576553\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006018153160694055\n",
      "Training Loss: 0.005981561752269045\n",
      "Training Loss: 0.005570187891134992\n",
      "Validation Loss: 0.0029813075814856572\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006000659949495457\n",
      "Training Loss: 0.005961151176597923\n",
      "Training Loss: 0.005549909508554265\n",
      "Validation Loss: 0.002961635236429532\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.005983473868691362\n",
      "Training Loss: 0.005940799819072708\n",
      "Training Loss: 0.005529668559902347\n",
      "Validation Loss: 0.002942053259093984\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.005966669239569455\n",
      "Training Loss: 0.005920596910873428\n",
      "Training Loss: 0.00550954780599568\n",
      "Validation Loss: 0.0029226585253392962\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.005950323310680687\n",
      "Training Loss: 0.005900646009249613\n",
      "Training Loss: 0.005489641260355711\n",
      "Validation Loss: 0.002903563988112499\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.005934517519781366\n",
      "Training Loss: 0.005881055038189515\n",
      "Training Loss: 0.005470047295675613\n",
      "Validation Loss: 0.002884887259589571\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.005919322057743557\n",
      "Training Loss: 0.00586193046416156\n",
      "Training Loss: 0.0054508603835711255\n",
      "Validation Loss: 0.0028667400899772227\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.005904795380774886\n",
      "Training Loss: 0.0058433724922360855\n",
      "Training Loss: 0.005432163089280948\n",
      "Validation Loss: 0.002849216732520903\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.0058909775031497705\n",
      "Training Loss: 0.005825459403567947\n",
      "Training Loss: 0.005414023434277624\n",
      "Validation Loss: 0.002832399770157056\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.005877884533256292\n",
      "Training Loss: 0.0058082509879022835\n",
      "Training Loss: 0.005396488650585524\n",
      "Validation Loss: 0.0028163413471395797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.005865508340648375\n",
      "Training Loss: 0.005791778072016314\n",
      "Training Loss: 0.005379580820444972\n",
      "Validation Loss: 0.0028010664273608018\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.0058538238320033995\n",
      "Training Loss: 0.005776047965046018\n",
      "Training Loss: 0.005363302566111088\n",
      "Validation Loss: 0.002786583186053995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.005842783694388345\n",
      "Training Loss: 0.005761046700063162\n",
      "Training Loss: 0.005347641207044944\n",
      "Validation Loss: 0.002772873219134014\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.00583233471494168\n",
      "Training Loss: 0.005746743126073852\n",
      "Training Loss: 0.005332568268058822\n",
      "Validation Loss: 0.002759899811301213\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.005822414739523083\n",
      "Training Loss: 0.005733094451134093\n",
      "Training Loss: 0.005318050693604164\n",
      "Validation Loss: 0.002747625620052051\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.005812964345095679\n",
      "Training Loss: 0.005720054617268033\n",
      "Training Loss: 0.005304051927523688\n",
      "Validation Loss: 0.002736005499786331\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.0058039268618449566\n",
      "Training Loss: 0.005707575923297554\n",
      "Training Loss: 0.005290535818785429\n",
      "Validation Loss: 0.0027249895861592103\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.005795254800468683\n",
      "Training Loss: 0.005695611074916087\n",
      "Training Loss: 0.005277467090054415\n",
      "Validation Loss: 0.002714530983416552\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.00578690436843317\n",
      "Training Loss: 0.00568411975284107\n",
      "Training Loss: 0.005264817407005466\n",
      "Validation Loss: 0.002704588179685845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.005778842830914073\n",
      "Training Loss: 0.005673064607544802\n",
      "Training Loss: 0.0052525589801371095\n",
      "Validation Loss: 0.00269512037597492\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.005771043008426204\n",
      "Training Loss: 0.005662413592799567\n",
      "Training Loss: 0.005240669048507698\n",
      "Validation Loss: 0.002686090526050689\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.005763482017209753\n",
      "Training Loss: 0.005652138852747157\n",
      "Training Loss: 0.005229130430961959\n",
      "Validation Loss: 0.0026774675784151207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.005756144508486613\n",
      "Training Loss: 0.005642218526918441\n",
      "Training Loss: 0.005217926384648308\n",
      "Validation Loss: 0.0026692281787158148\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.005749018641072325\n",
      "Training Loss: 0.0056326325098052625\n",
      "Training Loss: 0.005207045905408449\n",
      "Validation Loss: 0.002661345134164845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.005742094575543888\n",
      "Training Loss: 0.005623365642386489\n",
      "Training Loss: 0.005196475608972833\n",
      "Validation Loss: 0.002653793761443891\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.005735361704719253\n",
      "Training Loss: 0.005614400128251873\n",
      "Training Loss: 0.0051862045133020725\n",
      "Validation Loss: 0.0026465570541056855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0057288155949208885\n",
      "Training Loss: 0.005605723998160101\n",
      "Training Loss: 0.005176225024624728\n",
      "Validation Loss: 0.0026396180011248323\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.005722451728652231\n",
      "Training Loss: 0.00559732488298323\n",
      "Training Loss: 0.005166526505490765\n",
      "Validation Loss: 0.002632954345592245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.005716261112247593\n",
      "Training Loss: 0.005589191602775827\n",
      "Training Loss: 0.0051571012905333195\n",
      "Validation Loss: 0.002626555555845412\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.0057102416746784\n",
      "Training Loss: 0.005581315109157004\n",
      "Training Loss: 0.005147943273768761\n",
      "Validation Loss: 0.0026204080381064436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.00570438748865854\n",
      "Training Loss: 0.005573683521943167\n",
      "Training Loss: 0.0051390420953975995\n",
      "Validation Loss: 0.0026144945857983627\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.005698692867881618\n",
      "Training Loss: 0.005566288157715462\n",
      "Training Loss: 0.005130392531282268\n",
      "Validation Loss: 0.002608809918374493\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.00569315436470788\n",
      "Training Loss: 0.00555912126088515\n",
      "Training Loss: 0.005121985672740265\n",
      "Validation Loss: 0.0026033375977297848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.005687766326009296\n",
      "Training Loss: 0.00555217292974703\n",
      "Training Loss: 0.005113813701318577\n",
      "Validation Loss: 0.0025980633283243255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.00568252255092375\n",
      "Training Loss: 0.005545434842351824\n",
      "Training Loss: 0.005105871328269131\n",
      "Validation Loss: 0.0025929845974184155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0056774208589922635\n",
      "Training Loss: 0.005538898330414667\n",
      "Training Loss: 0.005098148975521326\n",
      "Validation Loss: 0.002588087070885041\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.005672453350271099\n",
      "Training Loss: 0.005532556584803387\n",
      "Training Loss: 0.005090640629059635\n",
      "Validation Loss: 0.002583358996008847\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.00566761537687853\n",
      "Training Loss: 0.005526399915688671\n",
      "Training Loss: 0.005083338971599005\n",
      "Validation Loss: 0.0025787956272304226\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.005662903760094195\n",
      "Training Loss: 0.005520421926630661\n",
      "Training Loss: 0.005076237083412707\n",
      "Validation Loss: 0.0025743846055591123\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.005658311411389149\n",
      "Training Loss: 0.005514613558771089\n",
      "Training Loss: 0.005069326169905253\n",
      "Validation Loss: 0.0025701181318223726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00565383522422053\n",
      "Training Loss: 0.0055089693074114625\n",
      "Training Loss: 0.005062601094250567\n",
      "Validation Loss: 0.0025659905598770954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.005649469007039443\n",
      "Training Loss: 0.005503482308122329\n",
      "Training Loss: 0.005056054149172269\n",
      "Validation Loss: 0.0025619935928425344\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.0056452070258092134\n",
      "Training Loss: 0.00549814501835499\n",
      "Training Loss: 0.005049679867806844\n",
      "Validation Loss: 0.0025581174145525927\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00564104784221854\n",
      "Training Loss: 0.005492950438056141\n",
      "Training Loss: 0.005043471166281961\n",
      "Validation Loss: 0.0025543626737937835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.005636984258890152\n",
      "Training Loss: 0.0054878944338997825\n",
      "Training Loss: 0.005037421635934152\n",
      "Validation Loss: 0.002550714978231431\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.0056330123246880245\n",
      "Training Loss: 0.005482969149015844\n",
      "Training Loss: 0.005031524928635917\n",
      "Validation Loss: 0.002547174393921421\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.005629128997097724\n",
      "Training Loss: 0.005478169379639439\n",
      "Training Loss: 0.005025774647947401\n",
      "Validation Loss: 0.0025437308280477616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.005625330756301992\n",
      "Training Loss: 0.005473488923162222\n",
      "Training Loss: 0.005020165432943031\n",
      "Validation Loss: 0.002540380788150798\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.005621612560353242\n",
      "Training Loss: 0.005468922980362549\n",
      "Training Loss: 0.005014692202094011\n",
      "Validation Loss: 0.0025371201172879154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005617972072213888\n",
      "Training Loss: 0.005464466359699145\n",
      "Training Loss: 0.005009350342443213\n",
      "Validation Loss: 0.0025339420476739997\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.005614405123051256\n",
      "Training Loss: 0.00546011470083613\n",
      "Training Loss: 0.005004132479662075\n",
      "Validation Loss: 0.002530841221588172\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00561090704461094\n",
      "Training Loss: 0.0054558623419143255\n",
      "Training Loss: 0.00499903557356447\n",
      "Validation Loss: 0.0025278162996002127\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005607477668090724\n",
      "Training Loss: 0.005451705838786438\n",
      "Training Loss: 0.00499405488371849\n",
      "Validation Loss: 0.0025248639848020472\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.005604111342690885\n",
      "Training Loss: 0.005447640575002879\n",
      "Training Loss: 0.004989184101577848\n",
      "Validation Loss: 0.0025219755375422955\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005600806350121275\n",
      "Training Loss: 0.005443662684992887\n",
      "Training Loss: 0.004984421232948079\n",
      "Validation Loss: 0.0025191545771602332\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.0055975615442730485\n",
      "Training Loss: 0.005439768679207191\n",
      "Training Loss: 0.004979759683483281\n",
      "Validation Loss: 0.0025163932920569615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.00559437177318614\n",
      "Training Loss: 0.005435953619307838\n",
      "Training Loss: 0.004975196520099416\n",
      "Validation Loss: 0.002513688357201604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00559123647166416\n",
      "Training Loss: 0.005432215039036237\n",
      "Training Loss: 0.0049707279307767745\n",
      "Validation Loss: 0.002511037253968292\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005588150829426013\n",
      "Training Loss: 0.00542854996281676\n",
      "Training Loss: 0.004966350898030214\n",
      "Validation Loss: 0.0025084401857568307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005585116271977313\n",
      "Training Loss: 0.005424954477930442\n",
      "Training Loss: 0.00496206171286758\n",
      "Validation Loss: 0.002505893272928517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005582128620008006\n",
      "Training Loss: 0.005421427176333964\n",
      "Training Loss: 0.004957856485270895\n",
      "Validation Loss: 0.002503392737871559\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005579185959068127\n",
      "Training Loss: 0.005417964540538378\n",
      "Training Loss: 0.004953732052235864\n",
      "Validation Loss: 0.0025009385576953043\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.0055762870894977825\n",
      "Training Loss: 0.005414562577498145\n",
      "Training Loss: 0.004949685094761663\n",
      "Validation Loss: 0.002498525550609894\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.0055734287883387875\n",
      "Training Loss: 0.005411220142850653\n",
      "Training Loss: 0.004945713963825256\n",
      "Validation Loss: 0.002496158494875672\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005570611185394227\n",
      "Training Loss: 0.005407934674876742\n",
      "Training Loss: 0.004941813884652219\n",
      "Validation Loss: 0.0024938267816559233\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005567833266686648\n",
      "Training Loss: 0.005404703300446272\n",
      "Training Loss: 0.004937983545823954\n",
      "Validation Loss: 0.0024915337045886294\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005565090457675979\n",
      "Training Loss: 0.005401524375192821\n",
      "Training Loss: 0.004934220387949609\n",
      "Validation Loss: 0.0024892779732949696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.0055623841588385405\n",
      "Training Loss: 0.005398397069075145\n",
      "Training Loss: 0.0049305216746870426\n",
      "Validation Loss: 0.002487058287585845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005559713301481679\n",
      "Training Loss: 0.005395316928625107\n",
      "Training Loss: 0.004926886656321585\n",
      "Validation Loss: 0.0024848717841675525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005557074139360339\n",
      "Training Loss: 0.005392283771652728\n",
      "Training Loss: 0.00492331130313687\n",
      "Validation Loss: 0.0024827162132158957\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005554467097972519\n",
      "Training Loss: 0.0053892953752074394\n",
      "Training Loss: 0.0049197936645941805\n",
      "Validation Loss: 0.002480592810826146\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005551890693604946\n",
      "Training Loss: 0.005386350357439369\n",
      "Training Loss: 0.004916332492721267\n",
      "Validation Loss: 0.002478500060983044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005549343830789439\n",
      "Training Loss: 0.005383447392378002\n",
      "Training Loss: 0.004912924359086901\n",
      "Validation Loss: 0.002476437704041182\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005546825483324937\n",
      "Training Loss: 0.005380583859514445\n",
      "Training Loss: 0.004909569033770822\n",
      "Validation Loss: 0.0024744023103266954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005544335804879666\n",
      "Training Loss: 0.005377759003313259\n",
      "Training Loss: 0.004906265180325136\n",
      "Validation Loss: 0.0024723923239291885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005541871637688018\n",
      "Training Loss: 0.005374972326098942\n",
      "Training Loss: 0.0049030089110601695\n",
      "Validation Loss: 0.0024704092589018637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005539433548110537\n",
      "Training Loss: 0.005372221496654675\n",
      "Training Loss: 0.004899800706189126\n",
      "Validation Loss: 0.002468454429822231\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005537020995398052\n",
      "Training Loss: 0.005369505712296814\n",
      "Training Loss: 0.004896638112841174\n",
      "Validation Loss: 0.0024665250911199478\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005534633242059499\n",
      "Training Loss: 0.005366824357188307\n",
      "Training Loss: 0.0048935204930603505\n",
      "Validation Loss: 0.0024646176692370452\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005532266551163048\n",
      "Training Loss: 0.00536417534109205\n",
      "Training Loss: 0.004890445515047759\n",
      "Validation Loss: 0.0024627329234891897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005529923252179287\n",
      "Training Loss: 0.005361557657597587\n",
      "Training Loss: 0.004887411475647241\n",
      "Validation Loss: 0.0024608717001765295\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005527602413203567\n",
      "Training Loss: 0.0053589697403367605\n",
      "Training Loss: 0.00488441705761943\n",
      "Validation Loss: 0.0024590337651603845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.0055253022065153345\n",
      "Training Loss: 0.005356412472901866\n",
      "Training Loss: 0.00488146290241275\n",
      "Validation Loss: 0.002457216384642747\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005523023046553135\n",
      "Training Loss: 0.00535388350719586\n",
      "Training Loss: 0.004878545030369424\n",
      "Validation Loss: 0.0024554194291279224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.0055207652813987805\n",
      "Training Loss: 0.005351381541113369\n",
      "Training Loss: 0.004875664777355269\n",
      "Validation Loss: 0.0024536449319599303\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005518525067600422\n",
      "Training Loss: 0.005348906607250683\n",
      "Training Loss: 0.00487281946639996\n",
      "Validation Loss: 0.0024518870606003436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.0055163051694398745\n",
      "Training Loss: 0.0053464579675346615\n",
      "Training Loss: 0.004870010176091455\n",
      "Validation Loss: 0.002450150071402614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005514102290035226\n",
      "Training Loss: 0.005344034271547571\n",
      "Training Loss: 0.0048672328790416945\n",
      "Validation Loss: 0.0024484330565888394\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.0055119187367381525\n",
      "Training Loss: 0.005341635872027837\n",
      "Training Loss: 0.004864488340099342\n",
      "Validation Loss: 0.002446737116872427\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005509750774945132\n",
      "Training Loss: 0.005339260226464831\n",
      "Training Loss: 0.004861774892779067\n",
      "Validation Loss: 0.0024450557114127397\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005507600066484883\n",
      "Training Loss: 0.005336907635210082\n",
      "Training Loss: 0.004859093290288001\n",
      "Validation Loss: 0.0024433938087895513\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005505465921596624\n",
      "Training Loss: 0.005334578458569012\n",
      "Training Loss: 0.004856441678712144\n",
      "Validation Loss: 0.0024417492376609044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005503348224447108\n",
      "Training Loss: 0.005332270959042944\n",
      "Training Loss: 0.00485381837177556\n",
      "Validation Loss: 0.002440121793972977\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.0055012452811934055\n",
      "Training Loss: 0.005329984314739704\n",
      "Training Loss: 0.00485122308251448\n",
      "Validation Loss: 0.002438510403827126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005499156990554184\n",
      "Training Loss: 0.005327718871994875\n",
      "Training Loss: 0.004848655359237455\n",
      "Validation Loss: 0.002436914110394048\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005497084747184999\n",
      "Training Loss: 0.005325472247204743\n",
      "Training Loss: 0.004846113777603023\n",
      "Validation Loss: 0.0024353341111819143\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005495025769923814\n",
      "Training Loss: 0.005323244843748398\n",
      "Training Loss: 0.004843598363804631\n",
      "Validation Loss: 0.0024337691295462966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.00549298188299872\n",
      "Training Loss: 0.005321036044624634\n",
      "Training Loss: 0.004841108450200408\n",
      "Validation Loss: 0.0024322213100143866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.0054909492057049645\n",
      "Training Loss: 0.005318846568115987\n",
      "Training Loss: 0.004838643128168769\n",
      "Validation Loss: 0.002430686869742244\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005488931931904517\n",
      "Training Loss: 0.0053166747628711165\n",
      "Training Loss: 0.004836201251600869\n",
      "Validation Loss: 0.00242916689832496\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.0054869261209387335\n",
      "Training Loss: 0.0053145197912817825\n",
      "Training Loss: 0.004833783799549565\n",
      "Validation Loss: 0.0024276621753527793\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005484932953841053\n",
      "Training Loss: 0.005312381741823628\n",
      "Training Loss: 0.004831387078738771\n",
      "Validation Loss: 0.002426172400631136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005482952870661393\n",
      "Training Loss: 0.00531026134849526\n",
      "Training Loss: 0.004829014642746187\n",
      "Validation Loss: 0.002424694193537567\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005480983584420756\n",
      "Training Loss: 0.005308155426755548\n",
      "Training Loss: 0.0048266628431156275\n",
      "Validation Loss: 0.002423229735224309\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005479027141700499\n",
      "Training Loss: 0.005306065725744702\n",
      "Training Loss: 0.00482433189812582\n",
      "Validation Loss: 0.002421777351403588\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005477081694407388\n",
      "Training Loss: 0.0053039907565107566\n",
      "Training Loss: 0.004822020321153104\n",
      "Validation Loss: 0.002420338131016476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.0054751471016788855\n",
      "Training Loss: 0.005301930673886091\n",
      "Training Loss: 0.0048197290272219105\n",
      "Validation Loss: 0.0024189126365190224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005473223760491237\n",
      "Training Loss: 0.005299884924315847\n",
      "Training Loss: 0.004817458083271049\n",
      "Validation Loss: 0.002417498183818639\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005471310181892477\n",
      "Training Loss: 0.00529785301128868\n",
      "Training Loss: 0.004815206367056817\n",
      "Validation Loss: 0.002416095946224804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005469407351338305\n",
      "Training Loss: 0.0052958348102401945\n",
      "Training Loss: 0.004812972089857794\n",
      "Validation Loss: 0.0024147040362396603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005467514237388968\n",
      "Training Loss: 0.00529382974433247\n",
      "Training Loss: 0.004810756567167118\n",
      "Validation Loss: 0.002413325306038592\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0054656314314343035\n",
      "Training Loss: 0.005291837737313471\n",
      "Training Loss: 0.004808557933429256\n",
      "Validation Loss: 0.0024119578753179463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.00546375747944694\n",
      "Training Loss: 0.005289858914329671\n",
      "Training Loss: 0.00480637661239598\n",
      "Validation Loss: 0.0024106018205979065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005461893857573159\n",
      "Training Loss: 0.005287892211927101\n",
      "Training Loss: 0.0048042113654082645\n",
      "Validation Loss: 0.0024092540235174926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005460037744487636\n",
      "Training Loss: 0.0052859354845713824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [17:35<07:32, 150.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.004802062826929614\n",
      "Validation Loss: 0.0024079160550295304\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.41966582417488096\n",
      "Training Loss: 0.35058857418596745\n",
      "Training Loss: 0.29117364950478075\n",
      "Validation Loss: 0.1996717191814037\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.18020024243742228\n",
      "Training Loss: 0.12049757219851016\n",
      "Training Loss: 0.09200996337458492\n",
      "Validation Loss: 0.07440277117859112\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.07793560709804297\n",
      "Training Loss: 0.07667856317013502\n",
      "Training Loss: 0.07593371042981743\n",
      "Validation Loss: 0.0700097649954678\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.07199015267193318\n",
      "Training Loss: 0.07140544408932328\n",
      "Training Loss: 0.07077793696895242\n",
      "Validation Loss: 0.0650583789422271\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.0670493870973587\n",
      "Training Loss: 0.06645885249599814\n",
      "Training Loss: 0.06578167820349336\n",
      "Validation Loss: 0.060072824065939764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.06202777377329767\n",
      "Training Loss: 0.061371339065954086\n",
      "Training Loss: 0.060565212089568374\n",
      "Validation Loss: 0.05469043247318\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.056515045072883366\n",
      "Training Loss: 0.05556893086992204\n",
      "Training Loss: 0.0543808357976377\n",
      "Validation Loss: 0.04821029083614939\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.04974196462891996\n",
      "Training Loss: 0.048211303343996405\n",
      "Training Loss: 0.0463498751167208\n",
      "Validation Loss: 0.04007595258482387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.041219984674826265\n",
      "Training Loss: 0.03908373110927641\n",
      "Training Loss: 0.03675385762006044\n",
      "Validation Loss: 0.03086455599561836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.032255598069168626\n",
      "Training Loss: 0.030084520587697627\n",
      "Training Loss: 0.0279581191111356\n",
      "Validation Loss: 0.022768645065972645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.024816579557955264\n",
      "Training Loss: 0.023034258568659424\n",
      "Training Loss: 0.021341047836467623\n",
      "Validation Loss: 0.01709739241246762\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.019461204919498414\n",
      "Training Loss: 0.018344153393991293\n",
      "Training Loss: 0.01709360138978809\n",
      "Validation Loss: 0.013849815955448352\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.016197262769564985\n",
      "Training Loss: 0.015620329838711769\n",
      "Training Loss: 0.014617318504024297\n",
      "Validation Loss: 0.012003382631274087\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.014285966395400465\n",
      "Training Loss: 0.013982440470717847\n",
      "Training Loss: 0.013101315593812614\n",
      "Validation Loss: 0.010824051026380463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.01309476041700691\n",
      "Training Loss: 0.012924220343120397\n",
      "Training Loss: 0.01211208243155852\n",
      "Validation Loss: 0.01000637419291594\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.012292192217428237\n",
      "Training Loss: 0.012180532761849463\n",
      "Training Loss: 0.011413882849738002\n",
      "Validation Loss: 0.009400284059052721\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.01170677348272875\n",
      "Training Loss: 0.011618603693787009\n",
      "Training Loss: 0.010888437640387565\n",
      "Validation Loss: 0.008929767614502586\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.011254679977428168\n",
      "Training Loss: 0.011173063095193357\n",
      "Training Loss: 0.010474082760047167\n",
      "Validation Loss: 0.008549378750573718\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.010889613549225033\n",
      "Training Loss: 0.010805920879356563\n",
      "Training Loss: 0.010133662864100189\n",
      "Validation Loss: 0.00822887204461888\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.010583001233171671\n",
      "Training Loss: 0.010492896880023181\n",
      "Training Loss: 0.00984362171497196\n",
      "Validation Loss: 0.007948832167835717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.010317087119910867\n",
      "Training Loss: 0.01021859223023057\n",
      "Training Loss: 0.00958938367664814\n",
      "Validation Loss: 0.0076976029537199585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.010081071935128421\n",
      "Training Loss: 0.009973426463548095\n",
      "Training Loss: 0.00936209190520458\n",
      "Validation Loss: 0.0074683189695554505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.009868394390214235\n",
      "Training Loss: 0.009751407355070114\n",
      "Training Loss: 0.00915633060852997\n",
      "Validation Loss: 0.0072568756648538155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.009674927950836718\n",
      "Training Loss: 0.009548659669235349\n",
      "Training Loss: 0.008968647648580373\n",
      "Validation Loss: 0.007060680096299293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.00949794952524826\n",
      "Training Loss: 0.009362555713159963\n",
      "Training Loss: 0.008796702402178199\n",
      "Validation Loss: 0.00687796798838156\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.009335537157021463\n",
      "Training Loss: 0.009191211302531883\n",
      "Training Loss: 0.008638803844805807\n",
      "Validation Loss: 0.006707486063451245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.009186247379984707\n",
      "Training Loss: 0.009033206837484613\n",
      "Training Loss: 0.008493630078155547\n",
      "Validation Loss: 0.0065482447415674\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.009048917008331046\n",
      "Training Loss: 0.00888739179354161\n",
      "Training Loss: 0.00836007933365181\n",
      "Validation Loss: 0.006399438541194194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.00892256424529478\n",
      "Training Loss: 0.008752803524257615\n",
      "Training Loss: 0.008237194046378135\n",
      "Validation Loss: 0.006260357726511828\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.00880631405627355\n",
      "Training Loss: 0.0086285874445457\n",
      "Training Loss: 0.008124108030460774\n",
      "Validation Loss: 0.006130349712467261\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008699371812399477\n",
      "Training Loss: 0.008513967366889119\n",
      "Training Loss: 0.00802002078620717\n",
      "Validation Loss: 0.006008801288546973\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008600996138993651\n",
      "Training Loss: 0.008408226111205295\n",
      "Training Loss: 0.0079241896700114\n",
      "Validation Loss: 0.005895141111896112\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008510498993564397\n",
      "Training Loss: 0.008310687073972076\n",
      "Training Loss: 0.007835922695230693\n",
      "Validation Loss: 0.005788809967781888\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008427231372334063\n",
      "Training Loss: 0.00822071986971423\n",
      "Training Loss: 0.0077545804809778926\n",
      "Validation Loss: 0.005689291532015365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008350599018158392\n",
      "Training Loss: 0.00813773977337405\n",
      "Training Loss: 0.007679577430244535\n",
      "Validation Loss: 0.0055961069739894585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.00828004666720517\n",
      "Training Loss: 0.00806118882028386\n",
      "Training Loss: 0.0076103646738920365\n",
      "Validation Loss: 0.005508785125579727\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008215060798684136\n",
      "Training Loss: 0.007990555844735354\n",
      "Training Loss: 0.007546451613306999\n",
      "Validation Loss: 0.005426916612960984\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008155175063293427\n",
      "Training Loss: 0.007925362897804007\n",
      "Training Loss: 0.007487381971441209\n",
      "Validation Loss: 0.005350105168842031\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.00809995606658049\n",
      "Training Loss: 0.007865165962139145\n",
      "Training Loss: 0.007432742525124923\n",
      "Validation Loss: 0.005278001719918312\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.00804900906747207\n",
      "Training Loss: 0.0078095559554640205\n",
      "Training Loss: 0.007382154802326113\n",
      "Validation Loss: 0.005210267978961046\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008001971404301002\n",
      "Training Loss: 0.0077581492636818435\n",
      "Training Loss: 0.00733527572476305\n",
      "Validation Loss: 0.005146607016265559\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007958510705502703\n",
      "Training Loss: 0.00771059377817437\n",
      "Training Loss: 0.007291785810375586\n",
      "Validation Loss: 0.005086734187213725\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007918317686999217\n",
      "Training Loss: 0.007666558843338862\n",
      "Training Loss: 0.007251397441141307\n",
      "Validation Loss: 0.005030396731905221\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007881111614406109\n",
      "Training Loss: 0.007625743210082874\n",
      "Training Loss: 0.007213843328645453\n",
      "Validation Loss: 0.004977351457424713\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007846628653351218\n",
      "Training Loss: 0.007587862624786794\n",
      "Training Loss: 0.007178875809768215\n",
      "Validation Loss: 0.004927379649467348\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007814628654159606\n",
      "Training Loss: 0.007552654723403975\n",
      "Training Loss: 0.0071462695323862135\n",
      "Validation Loss: 0.004880277612612823\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007784886307781562\n",
      "Training Loss: 0.007519876385340467\n",
      "Training Loss: 0.007115812272531912\n",
      "Validation Loss: 0.004835844665158833\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007757191816344857\n",
      "Training Loss: 0.007489299572771415\n",
      "Training Loss: 0.007087311089271679\n",
      "Validation Loss: 0.004793907125909509\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007731354070128873\n",
      "Training Loss: 0.007460715628694743\n",
      "Training Loss: 0.007060584191931412\n",
      "Validation Loss: 0.004754291023284699\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007707190298242494\n",
      "Training Loss: 0.007433928034733981\n",
      "Training Loss: 0.007035464141517877\n",
      "Validation Loss: 0.004716841844067479\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007684537385357544\n",
      "Training Loss: 0.0074087605299428105\n",
      "Training Loss: 0.007011799541069195\n",
      "Validation Loss: 0.004681410257484806\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007663240289548412\n",
      "Training Loss: 0.007385045541450381\n",
      "Training Loss: 0.006989443855127319\n",
      "Validation Loss: 0.004647857135454758\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00764315748703666\n",
      "Training Loss: 0.007362631192663684\n",
      "Training Loss: 0.006968268735799938\n",
      "Validation Loss: 0.00461605290903218\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007624157520476729\n",
      "Training Loss: 0.007341378417331726\n",
      "Training Loss: 0.006948150903917849\n",
      "Validation Loss: 0.004585875967370995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007606120865093544\n",
      "Training Loss: 0.0073211595183238385\n",
      "Training Loss: 0.006928980567026883\n",
      "Validation Loss: 0.0045572108343201745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007588937514228746\n",
      "Training Loss: 0.007301857604179531\n",
      "Training Loss: 0.006910655605606735\n",
      "Validation Loss: 0.004529956924425585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007572508008452133\n",
      "Training Loss: 0.007283369296928867\n",
      "Training Loss: 0.006893084581242874\n",
      "Validation Loss: 0.004504010907970787\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007556739075807855\n",
      "Training Loss: 0.007265596593497321\n",
      "Training Loss: 0.00687617868417874\n",
      "Validation Loss: 0.004479281565048889\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007541547605069354\n",
      "Training Loss: 0.0072484550043009225\n",
      "Training Loss: 0.006859866401646287\n",
      "Validation Loss: 0.0044556896966160015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007526860770303756\n",
      "Training Loss: 0.007231868917588144\n",
      "Training Loss: 0.006844076254637912\n",
      "Validation Loss: 0.004433154981332213\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007512610534904525\n",
      "Training Loss: 0.007215771302580834\n",
      "Training Loss: 0.006828746346291155\n",
      "Validation Loss: 0.004411602793052123\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007498735189437866\n",
      "Training Loss: 0.007200097910827026\n",
      "Training Loss: 0.006813820919487626\n",
      "Validation Loss: 0.004390970701770334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007485183348180726\n",
      "Training Loss: 0.007184798951493576\n",
      "Training Loss: 0.006799250196199864\n",
      "Validation Loss: 0.004371193457352981\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007471906240098179\n",
      "Training Loss: 0.007169826633762568\n",
      "Training Loss: 0.006784991351887584\n",
      "Validation Loss: 0.0043522178985471475\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007458863878855482\n",
      "Training Loss: 0.007155142963165417\n",
      "Training Loss: 0.0067710062442347405\n",
      "Validation Loss: 0.004333992572396659\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007446019229246303\n",
      "Training Loss: 0.007140714265406131\n",
      "Training Loss: 0.006757260704180226\n",
      "Validation Loss: 0.004316470590995603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007433341919677332\n",
      "Training Loss: 0.0071265112899709495\n",
      "Training Loss: 0.006743727617431432\n",
      "Validation Loss: 0.0042996059506713\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0074208062852267175\n",
      "Training Loss: 0.0071125107258558275\n",
      "Training Loss: 0.006730382653186098\n",
      "Validation Loss: 0.004283363847166635\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007408391869394109\n",
      "Training Loss: 0.007098694461164996\n",
      "Training Loss: 0.006717205229215324\n",
      "Validation Loss: 0.004267702964814693\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0073960793518926945\n",
      "Training Loss: 0.0070850471837911756\n",
      "Training Loss: 0.00670417896239087\n",
      "Validation Loss: 0.0042525911049687126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007383855069056153\n",
      "Training Loss: 0.007071556844748556\n",
      "Training Loss: 0.006691291314782574\n",
      "Validation Loss: 0.0042379960559015524\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007371710224542766\n",
      "Training Loss: 0.0070582172146532686\n",
      "Training Loss: 0.006678532598307356\n",
      "Validation Loss: 0.0042238948275492095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007359637123299762\n",
      "Training Loss: 0.007045022847596556\n",
      "Training Loss: 0.0066658958024345336\n",
      "Validation Loss: 0.0042102552160327685\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007347630109870806\n",
      "Training Loss: 0.007031971582910046\n",
      "Training Loss: 0.006653375293826685\n",
      "Validation Loss: 0.004197052520375406\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007335688427556306\n",
      "Training Loss: 0.007019062207546085\n",
      "Training Loss: 0.006640969881555065\n",
      "Validation Loss: 0.00418426545880986\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007323811500100419\n",
      "Training Loss: 0.007006297577172518\n",
      "Training Loss: 0.006628677705302835\n",
      "Validation Loss: 0.004171867970000492\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.0073120013158768414\n",
      "Training Loss: 0.006993679610313847\n",
      "Training Loss: 0.006616500336676836\n",
      "Validation Loss: 0.004159842884649386\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007300262027420104\n",
      "Training Loss: 0.006981213819235563\n",
      "Training Loss: 0.006604441629024222\n",
      "Validation Loss: 0.004148169132891331\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007288599213352427\n",
      "Training Loss: 0.006968905478715897\n",
      "Training Loss: 0.006592503573046997\n",
      "Validation Loss: 0.00413682717526478\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.00727701817639172\n",
      "Training Loss: 0.006956759564345703\n",
      "Training Loss: 0.006580690301489085\n",
      "Validation Loss: 0.004125794939947932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007265524133108556\n",
      "Training Loss: 0.006944781083147973\n",
      "Training Loss: 0.0065690069529227915\n",
      "Validation Loss: 0.00411505650027833\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007254125881008804\n",
      "Training Loss: 0.0069329779618419705\n",
      "Training Loss: 0.006557457691524177\n",
      "Validation Loss: 0.004104593535885215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007242828970775008\n",
      "Training Loss: 0.006921353675425053\n",
      "Training Loss: 0.006546046470757574\n",
      "Validation Loss: 0.00409438805863931\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007231642050901428\n",
      "Training Loss: 0.006909916474251076\n",
      "Training Loss: 0.006534781439695507\n",
      "Validation Loss: 0.0040844250888971805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007220571169164032\n",
      "Training Loss: 0.006898669980000704\n",
      "Training Loss: 0.0065236638253554705\n",
      "Validation Loss: 0.004074687836691737\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007209623832022771\n",
      "Training Loss: 0.006887617636239156\n",
      "Training Loss: 0.006512699066661298\n",
      "Validation Loss: 0.004065160289030062\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007198804814834148\n",
      "Training Loss: 0.006876764729386195\n",
      "Training Loss: 0.006501890704967082\n",
      "Validation Loss: 0.004055830954804263\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007188122686930001\n",
      "Training Loss: 0.006866113543510437\n",
      "Training Loss: 0.006491241158219054\n",
      "Validation Loss: 0.004046680064338228\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007177579272538423\n",
      "Training Loss: 0.006855665490729734\n",
      "Training Loss: 0.006480753161013126\n",
      "Validation Loss: 0.004037696751765907\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.0071671804483048615\n",
      "Training Loss: 0.006845422150800005\n",
      "Training Loss: 0.006470428706379607\n",
      "Validation Loss: 0.004028870569793194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0071569309348706155\n",
      "Training Loss: 0.006835384394507855\n",
      "Training Loss: 0.006460268388036638\n",
      "Validation Loss: 0.00402018571994529\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007146832416765392\n",
      "Training Loss: 0.006825550731737167\n",
      "Training Loss: 0.006450271774083376\n",
      "Validation Loss: 0.004011636632599271\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007136887629749253\n",
      "Training Loss: 0.006815921420929953\n",
      "Training Loss: 0.006440439784200862\n",
      "Validation Loss: 0.004003211230646526\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007127097835764289\n",
      "Training Loss: 0.006806493813637644\n",
      "Training Loss: 0.0064307711319997905\n",
      "Validation Loss: 0.003994898939913327\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007117465250194073\n",
      "Training Loss: 0.006797266139183193\n",
      "Training Loss: 0.006421265322715044\n",
      "Validation Loss: 0.00398669320844072\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007107989385258406\n",
      "Training Loss: 0.006788235632702708\n",
      "Training Loss: 0.006411919449456036\n",
      "Validation Loss: 0.003978585734115809\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007098670080304146\n",
      "Training Loss: 0.006779397201025859\n",
      "Training Loss: 0.006402730165282264\n",
      "Validation Loss: 0.003970567125230609\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007089506287593394\n",
      "Training Loss: 0.006770747456466779\n",
      "Training Loss: 0.006393696791492403\n",
      "Validation Loss: 0.003962635979569109\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007080496940761805\n",
      "Training Loss: 0.006762281495612115\n",
      "Training Loss: 0.0063848146074451505\n",
      "Validation Loss: 0.003954786239610462\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007071639767382294\n",
      "Training Loss: 0.006753996157785877\n",
      "Training Loss: 0.006376080056652427\n",
      "Validation Loss: 0.0039470085581199505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007062933912966401\n",
      "Training Loss: 0.006745884224073961\n",
      "Training Loss: 0.006367490239208564\n",
      "Validation Loss: 0.003939301267349988\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007054375184234232\n",
      "Training Loss: 0.006737940727034584\n",
      "Training Loss: 0.006359039989765733\n",
      "Validation Loss: 0.003931662228659549\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007045960434479639\n",
      "Training Loss: 0.006730159650323912\n",
      "Training Loss: 0.006350725865922868\n",
      "Validation Loss: 0.003924084544767824\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.00703768745996058\n",
      "Training Loss: 0.006722535120788961\n",
      "Training Loss: 0.006342541791964322\n",
      "Validation Loss: 0.0039165681005675305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007029552354943007\n",
      "Training Loss: 0.006715061656432226\n",
      "Training Loss: 0.006334484544349834\n",
      "Validation Loss: 0.003909109699215447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007021549713099376\n",
      "Training Loss: 0.006707732952199876\n",
      "Training Loss: 0.0063265499484259635\n",
      "Validation Loss: 0.003901705446684461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007013677193317563\n",
      "Training Loss: 0.006700542853213847\n",
      "Training Loss: 0.006318732221843675\n",
      "Validation Loss: 0.0038943545803888126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0070059303718153386\n",
      "Training Loss: 0.006693484798306599\n",
      "Training Loss: 0.0063110265566501764\n",
      "Validation Loss: 0.003887055388107645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.0069983053347095845\n",
      "Training Loss: 0.00668655275600031\n",
      "Training Loss: 0.006303429172839969\n",
      "Validation Loss: 0.0038798049542257624\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006990795548772439\n",
      "Training Loss: 0.006679740672698244\n",
      "Training Loss: 0.006295935036614537\n",
      "Validation Loss: 0.003872601771884169\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006983397621661424\n",
      "Training Loss: 0.006673042384209111\n",
      "Training Loss: 0.0062885373074095694\n",
      "Validation Loss: 0.0038654425461368447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0069761067884974185\n",
      "Training Loss: 0.0066664502263301985\n",
      "Training Loss: 0.006281233447371051\n",
      "Validation Loss: 0.0038583288165437205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006968918071361258\n",
      "Training Loss: 0.006659959164680913\n",
      "Training Loss: 0.006274017788236961\n",
      "Validation Loss: 0.0038512531194438257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006961826756596565\n",
      "Training Loss: 0.006653563104337081\n",
      "Training Loss: 0.00626688556978479\n",
      "Validation Loss: 0.003844220027735645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.0069548289233352985\n",
      "Training Loss: 0.006647256756550633\n",
      "Training Loss: 0.006259832319337875\n",
      "Validation Loss: 0.0038372252850657268\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006947919650701806\n",
      "Training Loss: 0.006641032971674577\n",
      "Training Loss: 0.0062528535502497104\n",
      "Validation Loss: 0.0038302671844453622\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006941093215718865\n",
      "Training Loss: 0.006634888216503896\n",
      "Training Loss: 0.006245945347473025\n",
      "Validation Loss: 0.0038233453334633556\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006934346022317186\n",
      "Training Loss: 0.006628817166201771\n",
      "Training Loss: 0.006239104439737275\n",
      "Validation Loss: 0.003816461766117744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.00692767389351502\n",
      "Training Loss: 0.006622813552967273\n",
      "Training Loss: 0.006232325924793258\n",
      "Validation Loss: 0.003809609763020796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006921072991099208\n",
      "Training Loss: 0.0066168727586045865\n",
      "Training Loss: 0.006225605773506686\n",
      "Validation Loss: 0.003802792458229939\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006914538271958009\n",
      "Training Loss: 0.006610990228946321\n",
      "Training Loss: 0.0062189408380072565\n",
      "Validation Loss: 0.003796002865517826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.00690806720755063\n",
      "Training Loss: 0.006605162605410442\n",
      "Training Loss: 0.006212327718967572\n",
      "Validation Loss: 0.003789246602904763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006901654180837795\n",
      "Training Loss: 0.006599383090506308\n",
      "Training Loss: 0.006205762861063704\n",
      "Validation Loss: 0.0037825213211557167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006895296847214922\n",
      "Training Loss: 0.0065936496085487305\n",
      "Training Loss: 0.0061992440582253035\n",
      "Validation Loss: 0.003775823849588214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006888990751467645\n",
      "Training Loss: 0.006587958588497713\n",
      "Training Loss: 0.006192768239416182\n",
      "Validation Loss: 0.0037691594792132297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006882732821395621\n",
      "Training Loss: 0.006582305107149296\n",
      "Training Loss: 0.0061863316921517255\n",
      "Validation Loss: 0.0037625209909727736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006876521281665191\n",
      "Training Loss: 0.006576687731430866\n",
      "Training Loss: 0.006179932170780376\n",
      "Validation Loss: 0.0037559091291400823\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.00687035296461545\n",
      "Training Loss: 0.006571101011359133\n",
      "Training Loss: 0.006173569858074188\n",
      "Validation Loss: 0.003749326758316896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.0068642245326191184\n",
      "Training Loss: 0.006565545121557079\n",
      "Training Loss: 0.0061672399891540405\n",
      "Validation Loss: 0.0037427715305762177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006858133383793756\n",
      "Training Loss: 0.006560015348368324\n",
      "Training Loss: 0.006160943318391219\n",
      "Validation Loss: 0.003736246892596396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006852078093215823\n",
      "Training Loss: 0.0065545107139041645\n",
      "Training Loss: 0.006154675730504096\n",
      "Validation Loss: 0.003729749183180962\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006846056059002876\n",
      "Training Loss: 0.006549027875880711\n",
      "Training Loss: 0.006148437064839527\n",
      "Validation Loss: 0.003723281519382857\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006840066153090447\n",
      "Training Loss: 0.006543565513566136\n",
      "Training Loss: 0.006142225351650268\n",
      "Validation Loss: 0.0037168391320979996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006834105952875689\n",
      "Training Loss: 0.006538122437195853\n",
      "Training Loss: 0.00613604074344039\n",
      "Validation Loss: 0.0037104288074240256\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006828173650428653\n",
      "Training Loss: 0.006532695828936994\n",
      "Training Loss: 0.0061298802483361215\n",
      "Validation Loss: 0.003704047631053777\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006822269958211109\n",
      "Training Loss: 0.006527286422206089\n",
      "Training Loss: 0.006123745612567291\n",
      "Validation Loss: 0.0036976963891176863\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006816391890170053\n",
      "Training Loss: 0.006521892482414842\n",
      "Training Loss: 0.00611763448221609\n",
      "Validation Loss: 0.0036913743765836352\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006810538936406374\n",
      "Training Loss: 0.006516512256930582\n",
      "Training Loss: 0.006111547623295337\n",
      "Validation Loss: 0.003685087813169099\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006804709823336452\n",
      "Training Loss: 0.006511145929107442\n",
      "Training Loss: 0.006105483782012016\n",
      "Validation Loss: 0.0036788323457258637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00679890449042432\n",
      "Training Loss: 0.006505792758543976\n",
      "Training Loss: 0.0060994432354345915\n",
      "Validation Loss: 0.0036726034994582447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.0067931224883068355\n",
      "Training Loss: 0.006500452234176919\n",
      "Training Loss: 0.006093425003346056\n",
      "Validation Loss: 0.0036664091722956034\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006787362484028563\n",
      "Training Loss: 0.006495124146458693\n",
      "Training Loss: 0.0060874308785423634\n",
      "Validation Loss: 0.0036602484276832153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006781625547446311\n",
      "Training Loss: 0.006489809969789349\n",
      "Training Loss: 0.006081459006527439\n",
      "Validation Loss: 0.0036541160975656147\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006775911619188264\n",
      "Training Loss: 0.006484507659333758\n",
      "Training Loss: 0.006075512127717957\n",
      "Validation Loss: 0.003648017298984812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006770220127655193\n",
      "Training Loss: 0.0064792190445587036\n",
      "Training Loss: 0.006069589144317433\n",
      "Validation Loss: 0.00364195036419322\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006764551658416167\n",
      "Training Loss: 0.006473943860037253\n",
      "Training Loss: 0.00606369105167687\n",
      "Validation Loss: 0.003635915739231565\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006758905145106837\n",
      "Training Loss: 0.006468684451538138\n",
      "Training Loss: 0.006057818577392027\n",
      "Validation Loss: 0.0036299121710000916\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006753282777499408\n",
      "Training Loss: 0.006463439689250663\n",
      "Training Loss: 0.006051972307031975\n",
      "Validation Loss: 0.0036239440348836477\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006747684166766703\n",
      "Training Loss: 0.006458211480057798\n",
      "Training Loss: 0.006046154014766217\n",
      "Validation Loss: 0.003618008120958641\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.00674210988683626\n",
      "Training Loss: 0.006453000777983106\n",
      "Training Loss: 0.006040364593500272\n",
      "Validation Loss: 0.0036121014874491296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.00673656000290066\n",
      "Training Loss: 0.006447808735538274\n",
      "Training Loss: 0.006034605230670422\n",
      "Validation Loss: 0.003606228258596796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006731036616256461\n",
      "Training Loss: 0.006442635295679793\n",
      "Training Loss: 0.006028876688797027\n",
      "Validation Loss: 0.003600388482799021\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006725539659382776\n",
      "Training Loss: 0.006437484169146046\n",
      "Training Loss: 0.006023180563934147\n",
      "Validation Loss: 0.003594580867714929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.00672006955370307\n",
      "Training Loss: 0.006432355456636288\n",
      "Training Loss: 0.006017517984146252\n",
      "Validation Loss: 0.0035888034643689064\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006714629223570228\n",
      "Training Loss: 0.00642724952136632\n",
      "Training Loss: 0.006011891100788489\n",
      "Validation Loss: 0.0035830526220287833\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006709216461749747\n",
      "Training Loss: 0.006422169456491247\n",
      "Training Loss: 0.006006300488952548\n",
      "Validation Loss: 0.0035773366101065213\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006703834095969796\n",
      "Training Loss: 0.006417116724187508\n",
      "Training Loss: 0.006000748651567847\n",
      "Validation Loss: 0.0035716495659788338\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006698483503423631\n",
      "Training Loss: 0.006412091502570547\n",
      "Training Loss: 0.005995236488524824\n",
      "Validation Loss: 0.0035659957250705763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006693164627067745\n",
      "Training Loss: 0.006407096774200909\n",
      "Training Loss: 0.005989765775157138\n",
      "Validation Loss: 0.003560370868961379\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006687878719530999\n",
      "Training Loss: 0.006402133222436533\n",
      "Training Loss: 0.005984337524278089\n",
      "Validation Loss: 0.0035547759080452198\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006682627074187621\n",
      "Training Loss: 0.006397201844374649\n",
      "Training Loss: 0.005978952362202108\n",
      "Validation Loss: 0.0035492126029403356\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006677409121766687\n",
      "Training Loss: 0.006392304256442003\n",
      "Training Loss: 0.005973613545065746\n",
      "Validation Loss: 0.0035436821034115352\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0066722283279523256\n",
      "Training Loss: 0.006387442286359147\n",
      "Training Loss: 0.005968320595566183\n",
      "Validation Loss: 0.003538182116469389\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.0066670827427878976\n",
      "Training Loss: 0.006382616874761879\n",
      "Training Loss: 0.005963075701147318\n",
      "Validation Loss: 0.003532716296770181\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006661974041489885\n",
      "Training Loss: 0.006377829851116985\n",
      "Training Loss: 0.005957878611516207\n",
      "Validation Loss: 0.0035272799458438424\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006656903971452266\n",
      "Training Loss: 0.006373080444172956\n",
      "Training Loss: 0.005952731630532071\n",
      "Validation Loss: 0.0035218755345728793\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006651871654903516\n",
      "Training Loss: 0.006368371658027172\n",
      "Training Loss: 0.005947634875774384\n",
      "Validation Loss: 0.003516507591382506\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006646877161692828\n",
      "Training Loss: 0.006363702109083534\n",
      "Training Loss: 0.00594258846133016\n",
      "Validation Loss: 0.0035111709835342645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006641922101844102\n",
      "Training Loss: 0.006359074941719882\n",
      "Training Loss: 0.0059375933452975\n",
      "Validation Loss: 0.003505866304877099\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006637007427634671\n",
      "Training Loss: 0.006354489023215138\n",
      "Training Loss: 0.005932649970054627\n",
      "Validation Loss: 0.0035005970583349634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006632130782818422\n",
      "Training Loss: 0.0063499448646325615\n",
      "Training Loss: 0.005927757991012186\n",
      "Validation Loss: 0.00349536344796168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.00662729347939603\n",
      "Training Loss: 0.006345444383914583\n",
      "Training Loss: 0.0059229178878013045\n",
      "Validation Loss: 0.0034901648498234456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.00662249657092616\n",
      "Training Loss: 0.006340985395945608\n",
      "Training Loss: 0.005918129732599482\n",
      "Validation Loss: 0.003484999314944647\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.00661773876810912\n",
      "Training Loss: 0.006336569716222584\n",
      "Training Loss: 0.0059133925824426115\n",
      "Validation Loss: 0.0034798673848527367\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006613019788055681\n",
      "Training Loss: 0.006332196892471984\n",
      "Training Loss: 0.005908705847105011\n",
      "Validation Loss: 0.003474772133435426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006608339060330763\n",
      "Training Loss: 0.0063278664677636695\n",
      "Training Loss: 0.005904070396791212\n",
      "Validation Loss: 0.0034697122866643613\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006603697150130756\n",
      "Training Loss: 0.0063235786638688295\n",
      "Training Loss: 0.00589948380657006\n",
      "Validation Loss: 0.0034646863913100757\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006599093325785361\n",
      "Training Loss: 0.006319332178100013\n",
      "Training Loss: 0.00589494685351383\n",
      "Validation Loss: 0.0034596946592746155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006594526849221438\n",
      "Training Loss: 0.0063151275814743715\n",
      "Training Loss: 0.005890458219801076\n",
      "Validation Loss: 0.0034547391572569528\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006589997950359247\n",
      "Training Loss: 0.006310964070726186\n",
      "Training Loss: 0.005886017726734281\n",
      "Validation Loss: 0.003449819042881051\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.0065855033189291135\n",
      "Training Loss: 0.006306840753532015\n",
      "Training Loss: 0.005881623052991927\n",
      "Validation Loss: 0.003444935433210784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006581045277416706\n",
      "Training Loss: 0.00630275699601043\n",
      "Training Loss: 0.005877274658996612\n",
      "Validation Loss: 0.0034400862262301732\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006576622290886007\n",
      "Training Loss: 0.006298712352290749\n",
      "Training Loss: 0.0058729702816344796\n",
      "Validation Loss: 0.003435268981141572\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.00657223243964836\n",
      "Training Loss: 0.006294705398613587\n",
      "Training Loss: 0.005868709880160168\n",
      "Validation Loss: 0.003430486770524654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006567875923356042\n",
      "Training Loss: 0.006290735903894529\n",
      "Training Loss: 0.005864491503452882\n",
      "Validation Loss: 0.0034257390436957056\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006563551779836416\n",
      "Training Loss: 0.006286802659742534\n",
      "Training Loss: 0.005860314464080148\n",
      "Validation Loss: 0.003421023860835376\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006559258924098686\n",
      "Training Loss: 0.006282904397812672\n",
      "Training Loss: 0.005856177103705704\n",
      "Validation Loss: 0.0034163428269054614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006554996092454531\n",
      "Training Loss: 0.00627904033055529\n",
      "Training Loss: 0.005852078960742801\n",
      "Validation Loss: 0.0034116950066592753\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0065507622837321835\n",
      "Training Loss: 0.006275210115127265\n",
      "Training Loss: 0.00584801847639028\n",
      "Validation Loss: 0.003407076763753057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006546558613190427\n",
      "Training Loss: 0.006271412132191472\n",
      "Training Loss: 0.005843994955066592\n",
      "Validation Loss: 0.003402493686122255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006542380971950479\n",
      "Training Loss: 0.006267645877087489\n",
      "Training Loss: 0.005840006682556123\n",
      "Validation Loss: 0.0033979388660217605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006538231527083553\n",
      "Training Loss: 0.0062639093218604105\n",
      "Training Loss: 0.005836051937076263\n",
      "Validation Loss: 0.003393412622612681\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006534107781480998\n",
      "Training Loss: 0.006260202801786363\n",
      "Training Loss: 0.005832129918271676\n",
      "Validation Loss: 0.0033889178623539512\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006530008481931872\n",
      "Training Loss: 0.006256523967022076\n",
      "Training Loss: 0.005828241014969535\n",
      "Validation Loss: 0.0033844544701382854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006525934161618352\n",
      "Training Loss: 0.00625287325354293\n",
      "Training Loss: 0.0058243825717363504\n",
      "Validation Loss: 0.0033800170372313494\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006521882781526074\n",
      "Training Loss: 0.0062492484453832734\n",
      "Training Loss: 0.005820553869125433\n",
      "Validation Loss: 0.003375607929872663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006517853358527646\n",
      "Training Loss: 0.006245649308548309\n",
      "Training Loss: 0.005816753639373928\n",
      "Validation Loss: 0.0033712261199448884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006513846718007699\n",
      "Training Loss: 0.006242074557230808\n",
      "Training Loss: 0.005812981058261358\n",
      "Validation Loss: 0.003366873967147359\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006509859504294582\n",
      "Training Loss: 0.006238524077343754\n",
      "Training Loss: 0.005809235294000245\n",
      "Validation Loss: 0.0033625461150672328\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0065058945241617035\n",
      "Training Loss: 0.006234996652929112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [20:06<05:01, 150.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.005805516363470815\n",
      "Validation Loss: 0.003358245386449055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.8077603486180306\n",
      "Training Loss: 0.6606598161160946\n",
      "Training Loss: 0.5247499960660934\n",
      "Validation Loss: 0.3596809474604853\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.3087680798768997\n",
      "Training Loss: 0.19584496472030877\n",
      "Training Loss: 0.12936291567981242\n",
      "Validation Loss: 0.08249010744222095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.08206522895023227\n",
      "Training Loss: 0.07456390470266343\n",
      "Training Loss: 0.07278195280581713\n",
      "Validation Loss: 0.064357882177227\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.06850168075412512\n",
      "Training Loss: 0.06920991517603398\n",
      "Training Loss: 0.06942361272871495\n",
      "Validation Loss: 0.062482595025153645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.06606551758944988\n",
      "Training Loss: 0.06658697225153447\n",
      "Training Loss: 0.06645962042734027\n",
      "Validation Loss: 0.05977380656626787\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.06268308920785785\n",
      "Training Loss: 0.0625712214037776\n",
      "Training Loss: 0.06162129782140255\n",
      "Validation Loss: 0.05500266073125132\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.05655200082808733\n",
      "Training Loss: 0.05504661919549107\n",
      "Training Loss: 0.05285888435319066\n",
      "Validation Loss: 0.04717388165214758\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.0473947980068624\n",
      "Training Loss: 0.04541774588637054\n",
      "Training Loss: 0.0433232347574085\n",
      "Validation Loss: 0.039063431427217604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.03891577620059252\n",
      "Training Loss: 0.0370318197645247\n",
      "Training Loss: 0.03531322154216468\n",
      "Validation Loss: 0.03209316238677234\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.03206784367095679\n",
      "Training Loss: 0.030479695815593004\n",
      "Training Loss: 0.029138606726191937\n",
      "Validation Loss: 0.026632774537533856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.026867176052182914\n",
      "Training Loss: 0.025574807054363193\n",
      "Training Loss: 0.024501751186326148\n",
      "Validation Loss: 0.0224297751955102\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.022936161872930825\n",
      "Training Loss: 0.02187950092367828\n",
      "Training Loss: 0.020973125994205474\n",
      "Validation Loss: 0.0191391675283065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.019899079110473394\n",
      "Training Loss: 0.019022693959996104\n",
      "Training Loss: 0.018217976843006908\n",
      "Validation Loss: 0.016497359202986353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.017492309098597617\n",
      "Training Loss: 0.016759059811010956\n",
      "Training Loss: 0.016023810447659343\n",
      "Validation Loss: 0.014329358731302318\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.015545233355369418\n",
      "Training Loss: 0.014916545937303453\n",
      "Training Loss: 0.014212878646794706\n",
      "Validation Loss: 0.012321047270356605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.013765695714391768\n",
      "Training Loss: 0.013193105710670353\n",
      "Training Loss: 0.01251442422857508\n",
      "Validation Loss: 0.01043337354896946\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.012377475638641045\n",
      "Training Loss: 0.012068963232450188\n",
      "Training Loss: 0.0115339459432289\n",
      "Validation Loss: 0.009411981172357382\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.011588869137922302\n",
      "Training Loss: 0.01134039007127285\n",
      "Training Loss: 0.010859931292943657\n",
      "Validation Loss: 0.008647703633675079\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.010994761204347014\n",
      "Training Loss: 0.010770193997304887\n",
      "Training Loss: 0.010330598592991009\n",
      "Validation Loss: 0.008036407354298267\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.010529849029844628\n",
      "Training Loss: 0.010317413596203551\n",
      "Training Loss: 0.00991032631136477\n",
      "Validation Loss: 0.007542338708855128\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.010164227093337103\n",
      "Training Loss: 0.009957490279339254\n",
      "Training Loss: 0.009575852598063648\n",
      "Validation Loss: 0.007140834729469726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.009875719882547856\n",
      "Training Loss: 0.009670876843156293\n",
      "Training Loss: 0.009308569066924975\n",
      "Validation Loss: 0.006813035239308571\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.00964673490030691\n",
      "Training Loss: 0.009441555672092363\n",
      "Training Loss: 0.009093337689992041\n",
      "Validation Loss: 0.006544109593469943\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.009463205711217598\n",
      "Training Loss: 0.009256427953951062\n",
      "Training Loss: 0.008917922812979668\n",
      "Validation Loss: 0.006322231842204928\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.009313953432720155\n",
      "Training Loss: 0.009104910801397637\n",
      "Training Loss: 0.008772552126320079\n",
      "Validation Loss: 0.006137918602424056\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.009190220147138462\n",
      "Training Loss: 0.008978609411278739\n",
      "Training Loss: 0.00864957989891991\n",
      "Validation Loss: 0.005983571192312441\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.009085280823055654\n",
      "Training Loss: 0.008871017652563751\n",
      "Training Loss: 0.008543155359802768\n",
      "Validation Loss: 0.005853110061058503\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008994110954226926\n",
      "Training Loss: 0.008777224377263337\n",
      "Training Loss: 0.00844893000787124\n",
      "Validation Loss: 0.005741686005605741\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008913059676997364\n",
      "Training Loss: 0.008693614802323281\n",
      "Training Loss: 0.008363779885694384\n",
      "Validation Loss: 0.005645464171833369\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.00883958738297224\n",
      "Training Loss: 0.008617638087598608\n",
      "Training Loss: 0.008285570772131904\n",
      "Validation Loss: 0.005561408492621411\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008772012087283655\n",
      "Training Loss: 0.008547553366515786\n",
      "Training Loss: 0.008212919848738238\n",
      "Validation Loss: 0.005487099077992058\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008709284467622638\n",
      "Training Loss: 0.008482236390700565\n",
      "Training Loss: 0.008144984691170976\n",
      "Validation Loss: 0.005420592160937324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008650767173385247\n",
      "Training Loss: 0.00842096947832033\n",
      "Training Loss: 0.008081254456192255\n",
      "Validation Loss: 0.005360296655320719\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008596054380759596\n",
      "Training Loss: 0.008363282565260305\n",
      "Training Loss: 0.008021379166748374\n",
      "Validation Loss: 0.0053049483299883226\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008544838884845377\n",
      "Training Loss: 0.008308828627923504\n",
      "Training Loss: 0.007965068998746574\n",
      "Validation Loss: 0.005253586167302192\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.00849684714106843\n",
      "Training Loss: 0.008257323533762247\n",
      "Training Loss: 0.007912051632301883\n",
      "Validation Loss: 0.005205543405700768\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.00845182363409549\n",
      "Training Loss: 0.008208518609171734\n",
      "Training Loss: 0.007862078344915062\n",
      "Validation Loss: 0.005160362839050005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008409530729986727\n",
      "Training Loss: 0.008162202114472166\n",
      "Training Loss: 0.00781491981470026\n",
      "Validation Loss: 0.005117771098013507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008369756048778073\n",
      "Training Loss: 0.008118201912147925\n",
      "Training Loss: 0.0077703929028939455\n",
      "Validation Loss: 0.005077585983456353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008332320993067696\n",
      "Training Loss: 0.008076385266613216\n",
      "Training Loss: 0.007728350028628483\n",
      "Validation Loss: 0.005039705811173059\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008297075751470403\n",
      "Training Loss: 0.008036648248089478\n",
      "Training Loss: 0.007688668675255031\n",
      "Validation Loss: 0.005004052975309196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008263890377711504\n",
      "Training Loss: 0.007998909500893206\n",
      "Training Loss: 0.007651250249473378\n",
      "Validation Loss: 0.004970565733887004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.008232646770775318\n",
      "Training Loss: 0.007963091911515221\n",
      "Training Loss: 0.007615990013582632\n",
      "Validation Loss: 0.004939161662242553\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.008203227687627077\n",
      "Training Loss: 0.007929120318731293\n",
      "Training Loss: 0.0075827843404840676\n",
      "Validation Loss: 0.004909756562013304\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.008175520052900538\n",
      "Training Loss: 0.00789691336802207\n",
      "Training Loss: 0.007551518864929676\n",
      "Validation Loss: 0.004882233622850159\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.008149402746930719\n",
      "Training Loss: 0.007866381687344983\n",
      "Training Loss: 0.0075220700446516275\n",
      "Validation Loss: 0.004856470917362008\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.008124755162280053\n",
      "Training Loss: 0.007837423918535933\n",
      "Training Loss: 0.007494300758698955\n",
      "Validation Loss: 0.0048323165693351725\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.008101447587832808\n",
      "Training Loss: 0.007809930533403531\n",
      "Training Loss: 0.007468068149173632\n",
      "Validation Loss: 0.004809610236938415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.008079346428858117\n",
      "Training Loss: 0.007783780265599489\n",
      "Training Loss: 0.007443220178829506\n",
      "Validation Loss: 0.004788184221938587\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.008058320106938481\n",
      "Training Loss: 0.007758850269019604\n",
      "Training Loss: 0.00741960386512801\n",
      "Validation Loss: 0.004767876877083203\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.008038232972612604\n",
      "Training Loss: 0.0077350125217344615\n",
      "Training Loss: 0.007397067949641496\n",
      "Validation Loss: 0.004748513532300176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.008018954736180604\n",
      "Training Loss: 0.007712139100767672\n",
      "Training Loss: 0.007375461561605334\n",
      "Validation Loss: 0.00472993493142925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.008000356720294803\n",
      "Training Loss: 0.007690104242647067\n",
      "Training Loss: 0.007354643682483584\n",
      "Validation Loss: 0.004712000476154551\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007982318546855821\n",
      "Training Loss: 0.007668789312010631\n",
      "Training Loss: 0.00733448049868457\n",
      "Validation Loss: 0.004694569262079476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007964729096274823\n",
      "Training Loss: 0.007648080361541361\n",
      "Training Loss: 0.007314846797380596\n",
      "Validation Loss: 0.004677526163606999\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007947483322350308\n",
      "Training Loss: 0.007627871967852116\n",
      "Training Loss: 0.007295631542801857\n",
      "Validation Loss: 0.0046607578445351525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007930483642267063\n",
      "Training Loss: 0.007608066649409011\n",
      "Training Loss: 0.007276729593286291\n",
      "Validation Loss: 0.0046441766872918335\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007913647828390822\n",
      "Training Loss: 0.00758857715758495\n",
      "Training Loss: 0.007258050261298194\n",
      "Validation Loss: 0.004627702179563682\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00789689913392067\n",
      "Training Loss: 0.007569323201896623\n",
      "Training Loss: 0.007239513585809618\n",
      "Validation Loss: 0.004611265759266327\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007880168156698347\n",
      "Training Loss: 0.007550233993679285\n",
      "Training Loss: 0.007221047903876752\n",
      "Validation Loss: 0.00459482126053046\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007863394434098154\n",
      "Training Loss: 0.007531246923608705\n",
      "Training Loss: 0.0072025898529682305\n",
      "Validation Loss: 0.00457831866139274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007846527163637802\n",
      "Training Loss: 0.007512307902798057\n",
      "Training Loss: 0.007184086532797665\n",
      "Validation Loss: 0.0045617232753194115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007829521438106895\n",
      "Training Loss: 0.007493368674768135\n",
      "Training Loss: 0.00716549142729491\n",
      "Validation Loss: 0.004544998608069138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007812337062787265\n",
      "Training Loss: 0.007474387304391712\n",
      "Training Loss: 0.007146767240483314\n",
      "Validation Loss: 0.004528136547194438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007794943219050765\n",
      "Training Loss: 0.007455332028912381\n",
      "Training Loss: 0.007127881912747398\n",
      "Validation Loss: 0.0045111196094684385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007777311485260725\n",
      "Training Loss: 0.007436175395268947\n",
      "Training Loss: 0.007108808982884511\n",
      "Validation Loss: 0.004493935617063655\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007759422338567674\n",
      "Training Loss: 0.007416895686183125\n",
      "Training Loss: 0.007089529504301026\n",
      "Validation Loss: 0.004476579816572452\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007741257889429107\n",
      "Training Loss: 0.0073974780167918655\n",
      "Training Loss: 0.007070029639871791\n",
      "Validation Loss: 0.004459056584687715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007722808687249199\n",
      "Training Loss: 0.007377913922537119\n",
      "Training Loss: 0.0070503018517047164\n",
      "Validation Loss: 0.004441366196097283\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007704066247679293\n",
      "Training Loss: 0.007358197940047831\n",
      "Training Loss: 0.007030342847574502\n",
      "Validation Loss: 0.004423523975766442\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007685028440319002\n",
      "Training Loss: 0.007338334418600425\n",
      "Training Loss: 0.007010157352779061\n",
      "Validation Loss: 0.0044055401237702435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007665701657533646\n",
      "Training Loss: 0.007318330479320139\n",
      "Training Loss: 0.006989752687513828\n",
      "Validation Loss: 0.0043874327930506695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.0076460923720151185\n",
      "Training Loss: 0.007298201358644292\n",
      "Training Loss: 0.006969145373441279\n",
      "Validation Loss: 0.004369225724485148\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007626215555937961\n",
      "Training Loss: 0.007277966000838205\n",
      "Training Loss: 0.006948355170898139\n",
      "Validation Loss: 0.0043509397420278765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007606087458552793\n",
      "Training Loss: 0.007257648945087567\n",
      "Training Loss: 0.006927406711038202\n",
      "Validation Loss: 0.004332609281069442\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007585731305880472\n",
      "Training Loss: 0.007237278089160099\n",
      "Training Loss: 0.006906331365462393\n",
      "Validation Loss: 0.004314261329035913\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007565175553318113\n",
      "Training Loss: 0.007216888798866421\n",
      "Training Loss: 0.00688516347319819\n",
      "Validation Loss: 0.004295930049246114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007544448809931055\n",
      "Training Loss: 0.00719651434221305\n",
      "Training Loss: 0.006863939490867779\n",
      "Validation Loss: 0.00427764393075296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007523585488088429\n",
      "Training Loss: 0.007176194016356021\n",
      "Training Loss: 0.006842702114954591\n",
      "Validation Loss: 0.004259441149933787\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007502621333114803\n",
      "Training Loss: 0.007155968038132414\n",
      "Training Loss: 0.006821492622839287\n",
      "Validation Loss: 0.004241356856367561\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0074815931322518734\n",
      "Training Loss: 0.007135874099330977\n",
      "Training Loss: 0.006800353970611468\n",
      "Validation Loss: 0.004223419465857108\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0074605384177993984\n",
      "Training Loss: 0.0071159527043346315\n",
      "Training Loss: 0.006779330178396777\n",
      "Validation Loss: 0.0042056591940646095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007439496309962124\n",
      "Training Loss: 0.0070962391374632715\n",
      "Training Loss: 0.00675845933961682\n",
      "Validation Loss: 0.004188098923795009\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007418498285114765\n",
      "Training Loss: 0.0070767663954757155\n",
      "Training Loss: 0.006737778449896723\n",
      "Validation Loss: 0.004170760593450304\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007397578549571336\n",
      "Training Loss: 0.007057560386601836\n",
      "Training Loss: 0.006717319916933775\n",
      "Validation Loss: 0.0041536543664805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007376763841602951\n",
      "Training Loss: 0.007038646363653243\n",
      "Training Loss: 0.0066971105069387705\n",
      "Validation Loss: 0.004136790900773714\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007356080055469647\n",
      "Training Loss: 0.007020039116032422\n",
      "Training Loss: 0.006677170502953232\n",
      "Validation Loss: 0.00412016704134392\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007335542911896482\n",
      "Training Loss: 0.007001749975606799\n",
      "Training Loss: 0.006657514498801902\n",
      "Validation Loss: 0.004103777998158436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007315166254993528\n",
      "Training Loss: 0.006983783566392958\n",
      "Training Loss: 0.006638152083614841\n",
      "Validation Loss: 0.004087614614955997\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007294959023129195\n",
      "Training Loss: 0.006966138925636187\n",
      "Training Loss: 0.006619083122350275\n",
      "Validation Loss: 0.0040716512201960835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0072749208507593725\n",
      "Training Loss: 0.00694880839320831\n",
      "Training Loss: 0.0066003047698177395\n",
      "Validation Loss: 0.004055872208993422\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007255049763480201\n",
      "Training Loss: 0.00693178050336428\n",
      "Training Loss: 0.006581807661568746\n",
      "Validation Loss: 0.004040237592030945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.0072353381186258045\n",
      "Training Loss: 0.006915039643645287\n",
      "Training Loss: 0.00656358024221845\n",
      "Validation Loss: 0.004024727871704302\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007215775626245886\n",
      "Training Loss: 0.0068985675368458035\n",
      "Training Loss: 0.006545605207793415\n",
      "Validation Loss: 0.0040092989409949336\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.00719634645152837\n",
      "Training Loss: 0.00688234219676815\n",
      "Training Loss: 0.006527865811949596\n",
      "Validation Loss: 0.003993920450820849\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007177036316134036\n",
      "Training Loss: 0.006866342730354517\n",
      "Training Loss: 0.006510344843845815\n",
      "Validation Loss: 0.003978560955964782\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007157830042997375\n",
      "Training Loss: 0.006850549269001931\n",
      "Training Loss: 0.006493026391835883\n",
      "Validation Loss: 0.003963190295702118\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007138714763568714\n",
      "Training Loss: 0.006834943534340709\n",
      "Training Loss: 0.0064758953708223994\n",
      "Validation Loss: 0.003947780865213174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007119676813017577\n",
      "Training Loss: 0.006819507718319073\n",
      "Training Loss: 0.006458942031022161\n",
      "Validation Loss: 0.0039323176756982554\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007100709148216993\n",
      "Training Loss: 0.0068042290123412385\n",
      "Training Loss: 0.006442158134886995\n",
      "Validation Loss: 0.003916786298197642\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0070818074105773125\n",
      "Training Loss: 0.006789100302266888\n",
      "Training Loss: 0.0064255419815890495\n",
      "Validation Loss: 0.0039011850021779537\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007062972893472761\n",
      "Training Loss: 0.006774116763845086\n",
      "Training Loss: 0.006409097940195352\n",
      "Validation Loss: 0.003885521253916236\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007044213952030987\n",
      "Training Loss: 0.00675928034237586\n",
      "Training Loss: 0.0063928331865463406\n",
      "Validation Loss: 0.0038698109552222355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0070255467598326505\n",
      "Training Loss: 0.006744595742784441\n",
      "Training Loss: 0.0063767597067635505\n",
      "Validation Loss: 0.0038540731550435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0070069870003499094\n",
      "Training Loss: 0.006730073185171932\n",
      "Training Loss: 0.006360893702367321\n",
      "Validation Loss: 0.0038383371965813166\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006988564039347694\n",
      "Training Loss: 0.006715725018293597\n",
      "Training Loss: 0.006345254078041762\n",
      "Validation Loss: 0.0038226350404196576\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006970306107541546\n",
      "Training Loss: 0.006701567512354813\n",
      "Training Loss: 0.006329863503342494\n",
      "Validation Loss: 0.0038070023358077482\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006952248491579667\n",
      "Training Loss: 0.006687616780982353\n",
      "Training Loss: 0.006314742109971121\n",
      "Validation Loss: 0.0037914780020881235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.00693442728719674\n",
      "Training Loss: 0.0066738914872985335\n",
      "Training Loss: 0.006299911626847461\n",
      "Validation Loss: 0.0037760938088713066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.00691687997430563\n",
      "Training Loss: 0.00666040932177566\n",
      "Training Loss: 0.006285392196150497\n",
      "Validation Loss: 0.0037608827644791663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.0068996421166229996\n",
      "Training Loss: 0.0066471850243397055\n",
      "Training Loss: 0.006271199596812949\n",
      "Validation Loss: 0.0037458739164965543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006882749239448458\n",
      "Training Loss: 0.006634233984514139\n",
      "Training Loss: 0.006257348745130002\n",
      "Validation Loss: 0.003731093115665102\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006866231999592856\n",
      "Training Loss: 0.006621567023685202\n",
      "Training Loss: 0.006243849829770625\n",
      "Validation Loss: 0.0037165601839824172\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0068501176626887175\n",
      "Training Loss: 0.006609195346827619\n",
      "Training Loss: 0.00623070900910534\n",
      "Validation Loss: 0.003702286732895823\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006834427791181952\n",
      "Training Loss: 0.006597122380626388\n",
      "Training Loss: 0.006217929628910497\n",
      "Validation Loss: 0.0036882848695987897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006819178825244307\n",
      "Training Loss: 0.006585352664114907\n",
      "Training Loss: 0.006205508934799582\n",
      "Validation Loss: 0.0036745604213975955\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006804381570545957\n",
      "Training Loss: 0.006573885509278625\n",
      "Training Loss: 0.006193444232922047\n",
      "Validation Loss: 0.0036611147133340494\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006790041321655735\n",
      "Training Loss: 0.006562719867215492\n",
      "Training Loss: 0.006181726184440777\n",
      "Validation Loss: 0.003647948514534098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006776158228749409\n",
      "Training Loss: 0.0065518497646553445\n",
      "Training Loss: 0.006170346894068643\n",
      "Validation Loss: 0.0036350601441697793\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006762726801680401\n",
      "Training Loss: 0.006541268938453868\n",
      "Training Loss: 0.006159294389653951\n",
      "Validation Loss: 0.0036224409901233537\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006749741673702374\n",
      "Training Loss: 0.006530970559688285\n",
      "Training Loss: 0.006148558389395475\n",
      "Validation Loss: 0.003610092393133078\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006737189304549247\n",
      "Training Loss: 0.006520946194650605\n",
      "Training Loss: 0.006138123164419085\n",
      "Validation Loss: 0.003598006791435182\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006725055880378932\n",
      "Training Loss: 0.006511184068513103\n",
      "Training Loss: 0.006127977872965857\n",
      "Validation Loss: 0.003586174826022614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006713327325414866\n",
      "Training Loss: 0.006501675731851719\n",
      "Training Loss: 0.00611810612725094\n",
      "Validation Loss: 0.003574594642098449\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006701984223909676\n",
      "Training Loss: 0.006492410213686526\n",
      "Training Loss: 0.00610849923803471\n",
      "Validation Loss: 0.003563258263251085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0066910120844841\n",
      "Training Loss: 0.006483377087861299\n",
      "Training Loss: 0.006099142333259806\n",
      "Validation Loss: 0.0035521607032422435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006680389908142388\n",
      "Training Loss: 0.006474567059194669\n",
      "Training Loss: 0.006090024001896382\n",
      "Validation Loss: 0.003541293130203914\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006670102459611371\n",
      "Training Loss: 0.006465968839474954\n",
      "Training Loss: 0.006081133000552654\n",
      "Validation Loss: 0.003530653439504042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006660130810923874\n",
      "Training Loss: 0.006457574295345694\n",
      "Training Loss: 0.006072459467686713\n",
      "Validation Loss: 0.0035202372819185257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006650458865333348\n",
      "Training Loss: 0.006449375872034579\n",
      "Training Loss: 0.006063993542920798\n",
      "Validation Loss: 0.0035100376699119806\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006641068939352408\n",
      "Training Loss: 0.006441361502511427\n",
      "Training Loss: 0.006055725557962433\n",
      "Validation Loss: 0.0035000482032577812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006631946673151106\n",
      "Training Loss: 0.006433523687301204\n",
      "Training Loss: 0.006047646928345784\n",
      "Validation Loss: 0.0034902621887289405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006623077780823224\n",
      "Training Loss: 0.006425856983987614\n",
      "Training Loss: 0.006039751352509484\n",
      "Validation Loss: 0.0034806792273740757\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006614447252941318\n",
      "Training Loss: 0.006418353288900107\n",
      "Training Loss: 0.006032029463676735\n",
      "Validation Loss: 0.0034712901145345374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006606043162173591\n",
      "Training Loss: 0.006411006955895573\n",
      "Training Loss: 0.006024477070895955\n",
      "Validation Loss: 0.0034620927704309815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.0065978537371847775\n",
      "Training Loss: 0.006403809682815335\n",
      "Training Loss: 0.006017086710780859\n",
      "Validation Loss: 0.0034530805606981007\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006589868137962185\n",
      "Training Loss: 0.00639675774727948\n",
      "Training Loss: 0.006009852304123342\n",
      "Validation Loss: 0.00344424895691068\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.0065820742433425035\n",
      "Training Loss: 0.006389843428623862\n",
      "Training Loss: 0.006002768389880658\n",
      "Validation Loss: 0.0034355943109526227\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006574463059660047\n",
      "Training Loss: 0.006383063574903644\n",
      "Training Loss: 0.005995829197345302\n",
      "Validation Loss: 0.003427109068908383\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006567026030970737\n",
      "Training Loss: 0.006376412505633197\n",
      "Training Loss: 0.005989031651988625\n",
      "Validation Loss: 0.0034187883661871547\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.00655975570785813\n",
      "Training Loss: 0.0063698863360332324\n",
      "Training Loss: 0.005982370503479615\n",
      "Validation Loss: 0.0034106324774767745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006552643228205852\n",
      "Training Loss: 0.0063634806004120035\n",
      "Training Loss: 0.005975841311737895\n",
      "Validation Loss: 0.0034026339678930933\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.0065456817985977975\n",
      "Training Loss: 0.006357191623537801\n",
      "Training Loss: 0.0059694396401755515\n",
      "Validation Loss: 0.003394788847922274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.0065388639701996\n",
      "Training Loss: 0.006351015218533576\n",
      "Training Loss: 0.005963162591215223\n",
      "Validation Loss: 0.003387094179712487\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006532183450763114\n",
      "Training Loss: 0.006344948871410452\n",
      "Training Loss: 0.005957006582757458\n",
      "Validation Loss: 0.0033795435211799118\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0065256359800696375\n",
      "Training Loss: 0.006338987065246329\n",
      "Training Loss: 0.005950967153767124\n",
      "Validation Loss: 0.0033721330999449063\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006519214487634599\n",
      "Training Loss: 0.006333128910046071\n",
      "Training Loss: 0.005945040720980614\n",
      "Validation Loss: 0.003364860384955249\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006512915417551994\n",
      "Training Loss: 0.006327369878999889\n",
      "Training Loss: 0.005939226511400193\n",
      "Validation Loss: 0.0033577239164412857\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006506733968271874\n",
      "Training Loss: 0.00632170827826485\n",
      "Training Loss: 0.005933518880046904\n",
      "Validation Loss: 0.003350717308564802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.00650066356814932\n",
      "Training Loss: 0.006316140395938419\n",
      "Training Loss: 0.005927915109787137\n",
      "Validation Loss: 0.00334383779875097\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006494702028576284\n",
      "Training Loss: 0.006310663388576359\n",
      "Training Loss: 0.0059224126278422775\n",
      "Validation Loss: 0.003337080889967469\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0064888438460184265\n",
      "Training Loss: 0.006305274432525039\n",
      "Training Loss: 0.0059170080372132365\n",
      "Validation Loss: 0.003330446421325709\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.0064830853568855675\n",
      "Training Loss: 0.006299972555134445\n",
      "Training Loss: 0.005911699396092445\n",
      "Validation Loss: 0.0033239278042416893\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006477423468604684\n",
      "Training Loss: 0.006294752389076166\n",
      "Training Loss: 0.005906482966383919\n",
      "Validation Loss: 0.0033175229955611104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.0064718540653120726\n",
      "Training Loss: 0.0062896133790491145\n",
      "Training Loss: 0.005901356306858361\n",
      "Validation Loss: 0.003311231190841017\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006466373353032395\n",
      "Training Loss: 0.006284552798024379\n",
      "Training Loss: 0.005896316632861272\n",
      "Validation Loss: 0.003305047029744457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.00646097992779687\n",
      "Training Loss: 0.006279568399768323\n",
      "Training Loss: 0.005891360745299608\n",
      "Validation Loss: 0.0032989663461725533\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.0064556692447513345\n",
      "Training Loss: 0.006274657472386025\n",
      "Training Loss: 0.0058864878385793415\n",
      "Validation Loss: 0.0032929893951925836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006450438806205057\n",
      "Training Loss: 0.0062698169075883926\n",
      "Training Loss: 0.005881692742696032\n",
      "Validation Loss: 0.0032871093122246726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006445284595247358\n",
      "Training Loss: 0.006265046528424137\n",
      "Training Loss: 0.005876974291168153\n",
      "Validation Loss: 0.003281328454352078\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006440204675891436\n",
      "Training Loss: 0.006260341303423047\n",
      "Training Loss: 0.00587233068421483\n",
      "Validation Loss: 0.0032756402408390234\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006435196549282409\n",
      "Training Loss: 0.00625570097763557\n",
      "Training Loss: 0.005867757158121094\n",
      "Validation Loss: 0.003270043765215643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006430257576867006\n",
      "Training Loss: 0.006251123193069361\n",
      "Training Loss: 0.0058632530539762225\n",
      "Validation Loss: 0.0032645341864358008\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.0064253839105367664\n",
      "Training Loss: 0.006246605471242219\n",
      "Training Loss: 0.00585881598177366\n",
      "Validation Loss: 0.00325911257839814\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006420574507210404\n",
      "Training Loss: 0.006242145943688228\n",
      "Training Loss: 0.005854441532865167\n",
      "Validation Loss: 0.0032537732027429207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006415825738804415\n",
      "Training Loss: 0.006237741795484908\n",
      "Training Loss: 0.005850129436003044\n",
      "Validation Loss: 0.00324851307452897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006411135125090368\n",
      "Training Loss: 0.006233392144786194\n",
      "Training Loss: 0.005845876754028723\n",
      "Validation Loss: 0.003243330829473359\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006406502642785199\n",
      "Training Loss: 0.006229093742440454\n",
      "Training Loss: 0.005841680817538873\n",
      "Validation Loss: 0.00323822397838106\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006401922867516987\n",
      "Training Loss: 0.006224845632677898\n",
      "Training Loss: 0.005837539940839634\n",
      "Validation Loss: 0.003233192213863302\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006397395550156944\n",
      "Training Loss: 0.006220645567518659\n",
      "Training Loss: 0.00583345354301855\n",
      "Validation Loss: 0.0032282312861068195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006392918741912581\n",
      "Training Loss: 0.006216491798404604\n",
      "Training Loss: 0.0058294151548761875\n",
      "Validation Loss: 0.003223338567264629\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006388488851953298\n",
      "Training Loss: 0.006212382939993404\n",
      "Training Loss: 0.005825426743831486\n",
      "Validation Loss: 0.0032185133536126505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006384105067118071\n",
      "Training Loss: 0.006208315786207095\n",
      "Training Loss: 0.005821484563639388\n",
      "Validation Loss: 0.003213751773360405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006379766020108946\n",
      "Training Loss: 0.006204289612942376\n",
      "Training Loss: 0.005817586968187243\n",
      "Validation Loss: 0.003209048442626267\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006375467838370241\n",
      "Training Loss: 0.00620030261517968\n",
      "Training Loss: 0.005813731796806678\n",
      "Validation Loss: 0.0032044067701555035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.0063712099444819615\n",
      "Training Loss: 0.006196352776023559\n",
      "Training Loss: 0.005809918110026046\n",
      "Validation Loss: 0.00319982117586089\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006366990705137141\n",
      "Training Loss: 0.006192439234000631\n",
      "Training Loss: 0.005806142468936741\n",
      "Validation Loss: 0.0031952919043454057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006362807977129705\n",
      "Training Loss: 0.0061885594681371\n",
      "Training Loss: 0.005802404223941266\n",
      "Validation Loss: 0.0031908164467934646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.0063586603797739375\n",
      "Training Loss: 0.006184713715338148\n",
      "Training Loss: 0.005798701095627621\n",
      "Validation Loss: 0.003186391348678493\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006354544764617458\n",
      "Training Loss: 0.0061808987951371816\n",
      "Training Loss: 0.0057950326602440325\n",
      "Validation Loss: 0.0031820160606248157\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006350462572881952\n",
      "Training Loss: 0.006177113031153567\n",
      "Training Loss: 0.005791396589484066\n",
      "Validation Loss: 0.003177689228813802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006346409263205715\n",
      "Training Loss: 0.006173356816289015\n",
      "Training Loss: 0.005787791558541358\n",
      "Validation Loss: 0.0031734060030430555\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006342385647003539\n",
      "Training Loss: 0.006169627493945882\n",
      "Training Loss: 0.005784215708263219\n",
      "Validation Loss: 0.003169166341455381\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.00633838955545798\n",
      "Training Loss: 0.006165924551314674\n",
      "Training Loss: 0.0057806675019674006\n",
      "Validation Loss: 0.0031649659994696633\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.00633441932441201\n",
      "Training Loss: 0.006162245902232826\n",
      "Training Loss: 0.005777145936153829\n",
      "Validation Loss: 0.003160808372750795\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006330473297857679\n",
      "Training Loss: 0.00615859134006314\n",
      "Training Loss: 0.005773650321643799\n",
      "Validation Loss: 0.003156688529998064\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006326551800593733\n",
      "Training Loss: 0.006154959848499857\n",
      "Training Loss: 0.005770177117083222\n",
      "Validation Loss: 0.0031526061834432602\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006322651815135032\n",
      "Training Loss: 0.006151349319261499\n",
      "Training Loss: 0.0057667281187605115\n",
      "Validation Loss: 0.0031485581283949397\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006318773935199715\n",
      "Training Loss: 0.0061477596452459695\n",
      "Training Loss: 0.005763300316175446\n",
      "Validation Loss: 0.0031445432713339\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006314916163682937\n",
      "Training Loss: 0.00614418928977102\n",
      "Training Loss: 0.0057598934054840355\n",
      "Validation Loss: 0.0031405624716406627\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006311076877173036\n",
      "Training Loss: 0.006140637572971172\n",
      "Training Loss: 0.005756505726603791\n",
      "Validation Loss: 0.0031366100785939883\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006307256688014604\n",
      "Training Loss: 0.006137103306828067\n",
      "Training Loss: 0.005753137324936688\n",
      "Validation Loss: 0.0031326887723623536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006303453255095519\n",
      "Training Loss: 0.006133586363284848\n",
      "Training Loss: 0.005749786186497658\n",
      "Validation Loss: 0.003128794580436406\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.0062996662879595535\n",
      "Training Loss: 0.006130086001940071\n",
      "Training Loss: 0.005746450903825462\n",
      "Validation Loss: 0.003124925998573223\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006295895332004875\n",
      "Training Loss: 0.006126600302523002\n",
      "Training Loss: 0.005743132292991504\n",
      "Validation Loss: 0.003121086223616024\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.0062921378447208555\n",
      "Training Loss: 0.006123129943734967\n",
      "Training Loss: 0.0057398291467688975\n",
      "Validation Loss: 0.0031172674871311427\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006288395089795813\n",
      "Training Loss: 0.006119672999484464\n",
      "Training Loss: 0.005736539015779272\n",
      "Validation Loss: 0.003113473199171883\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006284665495040826\n",
      "Training Loss: 0.0061162299459101635\n",
      "Training Loss: 0.005733262582216412\n",
      "Validation Loss: 0.0031097010183514335\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006280948526691646\n",
      "Training Loss: 0.006112799200927839\n",
      "Training Loss: 0.005730000026524067\n",
      "Validation Loss: 0.0031059511120985735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006277243892545812\n",
      "Training Loss: 0.006109380612033419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [22:36<02:30, 150.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.005726748197339475\n",
      "Validation Loss: 0.0031022206982179138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.4474074551463127\n",
      "Training Loss: 0.34777912192046645\n",
      "Training Loss: 0.2447179251909256\n",
      "Validation Loss: 0.13602800474742824\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.11016623761504889\n",
      "Training Loss: 0.08046676073223352\n",
      "Training Loss: 0.07412771591916681\n",
      "Validation Loss: 0.0668835434434789\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06915545275434852\n",
      "Training Loss: 0.06931339971721172\n",
      "Training Loss: 0.06928157145157457\n",
      "Validation Loss: 0.06388099010238486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.06558118689805269\n",
      "Training Loss: 0.06547983398661017\n",
      "Training Loss: 0.06514015294611454\n",
      "Validation Loss: 0.06022429960162452\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.06132585975341499\n",
      "Training Loss: 0.060887612653896216\n",
      "Training Loss: 0.06018752463161945\n",
      "Validation Loss: 0.05586764873580986\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.056305437218397855\n",
      "Training Loss: 0.05552084263414145\n",
      "Training Loss: 0.05450047958642244\n",
      "Validation Loss: 0.05082751237107127\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.050750587126240136\n",
      "Training Loss: 0.04970546768978238\n",
      "Training Loss: 0.0485031318012625\n",
      "Validation Loss: 0.04533983063831758\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.04502997472882271\n",
      "Training Loss: 0.043814122900366785\n",
      "Training Loss: 0.042559344740584495\n",
      "Validation Loss: 0.03976844554620512\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.039456024933606386\n",
      "Training Loss: 0.03817790163680911\n",
      "Training Loss: 0.036994040785357354\n",
      "Validation Loss: 0.03454981957761089\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.034383605956099926\n",
      "Training Loss: 0.03316277871839702\n",
      "Training Loss: 0.03214366973377764\n",
      "Validation Loss: 0.030050425884428987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.030106446272693575\n",
      "Training Loss: 0.02901106247678399\n",
      "Training Loss: 0.02817392464261502\n",
      "Validation Loss: 0.02637076269039947\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.02666850624140352\n",
      "Training Loss: 0.025701495232060553\n",
      "Training Loss: 0.025006317584775388\n",
      "Validation Loss: 0.02339030529131715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.02392075021751225\n",
      "Training Loss: 0.023062027650885285\n",
      "Training Loss: 0.02246082551777363\n",
      "Validation Loss: 0.02094060234892904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.021684937849640845\n",
      "Training Loss: 0.02091565295588225\n",
      "Training Loss: 0.020373369930312038\n",
      "Validation Loss: 0.018884334252791457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.019824151801876723\n",
      "Training Loss: 0.019130303363781423\n",
      "Training Loss: 0.01862559187458828\n",
      "Validation Loss: 0.01712440491622586\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.018246279787272213\n",
      "Training Loss: 0.01761736602988094\n",
      "Training Loss: 0.017138392385095357\n",
      "Validation Loss: 0.01559499009732115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.01689143184805289\n",
      "Training Loss: 0.01631909771589562\n",
      "Training Loss: 0.015859639539849014\n",
      "Validation Loss: 0.014251564399161365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.01572026717476547\n",
      "Training Loss: 0.015197557490319013\n",
      "Training Loss: 0.014754333614837379\n",
      "Validation Loss: 0.013063347783316387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.014705874375067651\n",
      "Training Loss: 0.014226719492580742\n",
      "Training Loss: 0.013797796613071114\n",
      "Validation Loss: 0.012008119364049328\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.013828485824633389\n",
      "Training Loss: 0.013387275985442102\n",
      "Training Loss: 0.012971181671600789\n",
      "Validation Loss: 0.011068648920300302\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.013071866515092552\n",
      "Training Loss: 0.012663017780287191\n",
      "Training Loss: 0.012258248310536147\n",
      "Validation Loss: 0.010230237434963497\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.01242078497307375\n",
      "Training Loss: 0.012038309290073813\n",
      "Training Loss: 0.011643089696299284\n",
      "Validation Loss: 0.009479211062569631\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.011859311014413834\n",
      "Training Loss: 0.011496527367271483\n",
      "Training Loss: 0.011108779851347207\n",
      "Validation Loss: 0.00880233292321392\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.011370065577793867\n",
      "Training Loss: 0.011019588713534176\n",
      "Training Loss: 0.010637093465775252\n",
      "Validation Loss: 0.008187053670185837\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.01093455146998167\n",
      "Training Loss: 0.010588695737533271\n",
      "Training Loss: 0.010209148863796145\n",
      "Validation Loss: 0.00762165357159932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.010533755728974938\n",
      "Training Loss: 0.010185253599192947\n",
      "Training Loss: 0.009805605318397283\n",
      "Validation Loss: 0.007093420401320196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.010147263144608587\n",
      "Training Loss: 0.009790623966837301\n",
      "Training Loss: 0.009406043648486956\n",
      "Validation Loss: 0.006586579747205035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.009754328678827733\n",
      "Training Loss: 0.009390969652449711\n",
      "Training Loss: 0.008998554934514687\n",
      "Validation Loss: 0.006098046673859438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.009355152189964428\n",
      "Training Loss: 0.00900125362444669\n",
      "Training Loss: 0.008608014743076637\n",
      "Validation Loss: 0.005669391311160969\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.00899417107924819\n",
      "Training Loss: 0.008669118870748206\n",
      "Training Loss: 0.00828434289433062\n",
      "Validation Loss: 0.0053465283983334735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008714571469463408\n",
      "Training Loss: 0.008419375127414242\n",
      "Training Loss: 0.008043550804723054\n",
      "Validation Loss: 0.0051192695986521375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.00851303848787211\n",
      "Training Loss: 0.008236304494785145\n",
      "Training Loss: 0.007866398693295196\n",
      "Validation Loss: 0.0049539684851685264\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008365684021264315\n",
      "Training Loss: 0.008097472109366208\n",
      "Training Loss: 0.0077315886714495715\n",
      "Validation Loss: 0.004826979266919112\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008253639454487711\n",
      "Training Loss: 0.007988173016346991\n",
      "Training Loss: 0.007625433360226452\n",
      "Validation Loss: 0.0047254703358192455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008165494113927706\n",
      "Training Loss: 0.007899826031643898\n",
      "Training Loss: 0.0075397579150740055\n",
      "Validation Loss: 0.004642490692583196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008094476313563064\n",
      "Training Loss: 0.007827173545956611\n",
      "Training Loss: 0.007469421084970236\n",
      "Validation Loss: 0.004573897429836098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008036348368041218\n",
      "Training Loss: 0.00776670259423554\n",
      "Training Loss: 0.007410922166891396\n",
      "Validation Loss: 0.004516855013044028\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007988216595258564\n",
      "Training Loss: 0.007715862006880343\n",
      "Training Loss: 0.007361713605932891\n",
      "Validation Loss: 0.004469205745694677\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007947953742695973\n",
      "Training Loss: 0.00767269755131565\n",
      "Training Loss: 0.007319861740106717\n",
      "Validation Loss: 0.004429193532743146\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.00791391646838747\n",
      "Training Loss: 0.007635669637238607\n",
      "Training Loss: 0.007283871722174809\n",
      "Validation Loss: 0.0043953664475277565\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007884819547180087\n",
      "Training Loss: 0.007603561540599912\n",
      "Training Loss: 0.007252581462962553\n",
      "Validation Loss: 0.004366544164256768\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.00785964765236713\n",
      "Training Loss: 0.007575410680146888\n",
      "Training Loss: 0.007225079106865451\n",
      "Validation Loss: 0.004341755274243736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007837612248258664\n",
      "Training Loss: 0.007550463369116187\n",
      "Training Loss: 0.007200656823115424\n",
      "Validation Loss: 0.004320230491961656\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007818095874972642\n",
      "Training Loss: 0.007528128379490227\n",
      "Training Loss: 0.00717875431291759\n",
      "Validation Loss: 0.004301341250538826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.0078006184927653524\n",
      "Training Loss: 0.007507944183889777\n",
      "Training Loss: 0.007158936759224161\n",
      "Validation Loss: 0.004284604026130244\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.0077848088426981125\n",
      "Training Loss: 0.0074895513441879304\n",
      "Training Loss: 0.007140859371284023\n",
      "Validation Loss: 0.004269628403603696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007770378075074405\n",
      "Training Loss: 0.007472665124805644\n",
      "Training Loss: 0.007124247000319883\n",
      "Validation Loss: 0.004256103190415529\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007757095461711288\n",
      "Training Loss: 0.007457059193402529\n",
      "Training Loss: 0.007108882962493226\n",
      "Validation Loss: 0.0042437844493771705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007744782797526568\n",
      "Training Loss: 0.007442555649904534\n",
      "Training Loss: 0.007094588998006657\n",
      "Validation Loss: 0.004232472339854314\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0077332958998158575\n",
      "Training Loss: 0.007429004953010008\n",
      "Training Loss: 0.007081224494613707\n",
      "Validation Loss: 0.004222011780941838\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.0077225194231141355\n",
      "Training Loss: 0.007416291300905868\n",
      "Training Loss: 0.007068670673761517\n",
      "Validation Loss: 0.004212275372420469\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007712358572753146\n",
      "Training Loss: 0.007404312946600839\n",
      "Training Loss: 0.007056830765213818\n",
      "Validation Loss: 0.004203155342479017\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007702735682250932\n",
      "Training Loss: 0.007392985949991271\n",
      "Training Loss: 0.007045622220030055\n",
      "Validation Loss: 0.004194576539141074\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007693587994435802\n",
      "Training Loss: 0.00738224241998978\n",
      "Training Loss: 0.007034976324066519\n",
      "Validation Loss: 0.004186463185188392\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007684861421585083\n",
      "Training Loss: 0.007372019910253585\n",
      "Training Loss: 0.007024834231706336\n",
      "Validation Loss: 0.004178762080126934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0076765102462377395\n",
      "Training Loss: 0.007362267408752814\n",
      "Training Loss: 0.007015145675977692\n",
      "Validation Loss: 0.004171425607473998\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007668497387785464\n",
      "Training Loss: 0.00735293954028748\n",
      "Training Loss: 0.007005867657717317\n",
      "Validation Loss: 0.004164414731471726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007660788076464087\n",
      "Training Loss: 0.007343996383715421\n",
      "Training Loss: 0.006996959361713379\n",
      "Validation Loss: 0.00415769304682532\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007653353927889839\n",
      "Training Loss: 0.007335402907337993\n",
      "Training Loss: 0.006988388991449029\n",
      "Validation Loss: 0.004151234361395407\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.00764617053209804\n",
      "Training Loss: 0.0073271285824012015\n",
      "Training Loss: 0.006980126884300262\n",
      "Validation Loss: 0.0041450161651676795\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007639215695671737\n",
      "Training Loss: 0.007319144995417446\n",
      "Training Loss: 0.006972143992315978\n",
      "Validation Loss: 0.004139016061927077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.00763246878865175\n",
      "Training Loss: 0.007311428018147126\n",
      "Training Loss: 0.006964418614516034\n",
      "Validation Loss: 0.004133215914428067\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007625912139192223\n",
      "Training Loss: 0.007303956378018483\n",
      "Training Loss: 0.006956928746076301\n",
      "Validation Loss: 0.004127604913050204\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007619530296651646\n",
      "Training Loss: 0.007296709476504475\n",
      "Training Loss: 0.0069496540131513025\n",
      "Validation Loss: 0.0041221609271100065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.00761330834357068\n",
      "Training Loss: 0.007289668213343248\n",
      "Training Loss: 0.006942575881257653\n",
      "Validation Loss: 0.0041168768905898495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007607232570881024\n",
      "Training Loss: 0.007282815320650116\n",
      "Training Loss: 0.00693568033282645\n",
      "Validation Loss: 0.004111737576627246\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007601290157763288\n",
      "Training Loss: 0.007276136773871258\n",
      "Training Loss: 0.0069289506727363915\n",
      "Validation Loss: 0.004106736851835184\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0075954717793501915\n",
      "Training Loss: 0.007269619000144303\n",
      "Training Loss: 0.006922372514382004\n",
      "Validation Loss: 0.004101862534462066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007589764815056696\n",
      "Training Loss: 0.007263246931834146\n",
      "Training Loss: 0.006915934429271147\n",
      "Validation Loss: 0.0040971094747649485\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007584159802645445\n",
      "Training Loss: 0.007257010425673798\n",
      "Training Loss: 0.006909623392857611\n",
      "Validation Loss: 0.004092466694386571\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007578647854970768\n",
      "Training Loss: 0.007250897663179785\n",
      "Training Loss: 0.006903430117527023\n",
      "Validation Loss: 0.004087926706121293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007573220955673605\n",
      "Training Loss: 0.007244897604687139\n",
      "Training Loss: 0.006897342138690874\n",
      "Validation Loss: 0.004083480623163534\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007567869112826884\n",
      "Training Loss: 0.007239002216374501\n",
      "Training Loss: 0.006891350374789908\n",
      "Validation Loss: 0.00407912657109581\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007562586887506768\n",
      "Training Loss: 0.0072332013631239535\n",
      "Training Loss: 0.006885447609238327\n",
      "Validation Loss: 0.004074856021514769\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.0075573663332033905\n",
      "Training Loss: 0.007227485483745113\n",
      "Training Loss: 0.006879623072454706\n",
      "Validation Loss: 0.004070661173285812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007552199125057086\n",
      "Training Loss: 0.007221848896006122\n",
      "Training Loss: 0.006873870705021545\n",
      "Validation Loss: 0.004066536188638361\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007547079608775676\n",
      "Training Loss: 0.007216281681321561\n",
      "Training Loss: 0.006868180568562821\n",
      "Validation Loss: 0.004062477546335941\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007542001708643511\n",
      "Training Loss: 0.007210778358858079\n",
      "Training Loss: 0.006862547118216753\n",
      "Validation Loss: 0.004058478294160175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007536959225544706\n",
      "Training Loss: 0.007205330724827945\n",
      "Training Loss: 0.006856962884776294\n",
      "Validation Loss: 0.004054532589108338\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007531944973161444\n",
      "Training Loss: 0.007199934138916433\n",
      "Training Loss: 0.006851421589963138\n",
      "Validation Loss: 0.004050637008046753\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007526956527726725\n",
      "Training Loss: 0.0071945807908196\n",
      "Training Loss: 0.006845917957834899\n",
      "Validation Loss: 0.004046783399286816\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0075219850870780645\n",
      "Training Loss: 0.007189265844644978\n",
      "Training Loss: 0.006840443608816713\n",
      "Validation Loss: 0.004042970810577357\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007517027341527864\n",
      "Training Loss: 0.0071839820977766065\n",
      "Training Loss: 0.00683499445556663\n",
      "Validation Loss: 0.00403919254345924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007512078910367564\n",
      "Training Loss: 0.007178726232377812\n",
      "Training Loss: 0.006829564671497792\n",
      "Validation Loss: 0.004035443915158845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007507133884355426\n",
      "Training Loss: 0.007173491836292669\n",
      "Training Loss: 0.006824148970190435\n",
      "Validation Loss: 0.00403172119253765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007502187552163377\n",
      "Training Loss: 0.0071682753786444665\n",
      "Training Loss: 0.006818741646129638\n",
      "Validation Loss: 0.00402801880597273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007497235359624028\n",
      "Training Loss: 0.007163070016540587\n",
      "Training Loss: 0.006813337872736156\n",
      "Validation Loss: 0.004024332738481462\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007492272879462689\n",
      "Training Loss: 0.007157872411189601\n",
      "Training Loss: 0.006807933987583965\n",
      "Validation Loss: 0.004020660823954039\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.0074872950452845545\n",
      "Training Loss: 0.0071526781329885126\n",
      "Training Loss: 0.006802523265359923\n",
      "Validation Loss: 0.004016996704021029\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007482298832619563\n",
      "Training Loss: 0.007147483003791422\n",
      "Training Loss: 0.006797102339332923\n",
      "Validation Loss: 0.004013336102708421\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0074772792845033105\n",
      "Training Loss: 0.00714228181168437\n",
      "Training Loss: 0.006791666118660941\n",
      "Validation Loss: 0.004009672177672972\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0074722324789036065\n",
      "Training Loss: 0.007137070881435647\n",
      "Training Loss: 0.00678621158702299\n",
      "Validation Loss: 0.004006008884419551\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007467154623009264\n",
      "Training Loss: 0.007131847098935396\n",
      "Training Loss: 0.006780732703628018\n",
      "Validation Loss: 0.004002337412008744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007462041252292693\n",
      "Training Loss: 0.007126605663215741\n",
      "Training Loss: 0.0067752262309659275\n",
      "Validation Loss: 0.003998650920713383\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007456889214226976\n",
      "Training Loss: 0.00712134305969812\n",
      "Training Loss: 0.006769688044441864\n",
      "Validation Loss: 0.003994951125370402\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007451692541362718\n",
      "Training Loss: 0.007116055564256385\n",
      "Training Loss: 0.0067641140497289596\n",
      "Validation Loss: 0.003991231525034299\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007446450459538028\n",
      "Training Loss: 0.007110740753123537\n",
      "Training Loss: 0.006758500267751515\n",
      "Validation Loss: 0.003987490326385903\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007441157815046609\n",
      "Training Loss: 0.007105393903329969\n",
      "Training Loss: 0.006752842512214557\n",
      "Validation Loss: 0.003983723262607465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0074358112388290465\n",
      "Training Loss: 0.007100012524751946\n",
      "Training Loss: 0.006747138982173055\n",
      "Validation Loss: 0.0039799261623679584\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007430407830979675\n",
      "Training Loss: 0.007094593264628202\n",
      "Training Loss: 0.006741384490160271\n",
      "Validation Loss: 0.003976098015862569\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007424942735815421\n",
      "Training Loss: 0.007089133269619197\n",
      "Training Loss: 0.00673557584406808\n",
      "Validation Loss: 0.003972232481726351\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.00741941281943582\n",
      "Training Loss: 0.007083628352265805\n",
      "Training Loss: 0.006729709430364892\n",
      "Validation Loss: 0.003968326830085409\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007413817308843136\n",
      "Training Loss: 0.007078077178448439\n",
      "Training Loss: 0.006723783739143983\n",
      "Validation Loss: 0.003964379386651968\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0074081514531280845\n",
      "Training Loss: 0.007072475899476558\n",
      "Training Loss: 0.006717793927527964\n",
      "Validation Loss: 0.003960387960463511\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007402412398951128\n",
      "Training Loss: 0.007066823281347751\n",
      "Training Loss: 0.006711739563615993\n",
      "Validation Loss: 0.00395635012118669\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007396597195183858\n",
      "Training Loss: 0.007061117512639612\n",
      "Training Loss: 0.0067056161456275735\n",
      "Validation Loss: 0.003952259817840845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007390705378493294\n",
      "Training Loss: 0.007055354373296723\n",
      "Training Loss: 0.0066994218912441285\n",
      "Validation Loss: 0.003948116800590848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.007384732767241076\n",
      "Training Loss: 0.007049534380203113\n",
      "Training Loss: 0.006693155550165102\n",
      "Validation Loss: 0.003943920643016529\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007378677412634716\n",
      "Training Loss: 0.007043653281871229\n",
      "Training Loss: 0.006686815362190828\n",
      "Validation Loss: 0.00393966428171634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.0073725395649671555\n",
      "Training Loss: 0.007037711631273851\n",
      "Training Loss: 0.006680398618336767\n",
      "Validation Loss: 0.003935351398815349\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.007366315321996808\n",
      "Training Loss: 0.007031708321301266\n",
      "Training Loss: 0.0066739040671382095\n",
      "Validation Loss: 0.003930975403113479\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.007360004282090813\n",
      "Training Loss: 0.007025640578940511\n",
      "Training Loss: 0.006667332327924669\n",
      "Validation Loss: 0.003926541180130136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007353607369586825\n",
      "Training Loss: 0.00701950910850428\n",
      "Training Loss: 0.006660682681249455\n",
      "Validation Loss: 0.003922040752145682\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007347122204955667\n",
      "Training Loss: 0.007013311787741259\n",
      "Training Loss: 0.006653953808126971\n",
      "Validation Loss: 0.003917477527905381\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007340546767227352\n",
      "Training Loss: 0.0070070500823203475\n",
      "Training Loss: 0.006647146726027131\n",
      "Validation Loss: 0.003912851465552041\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007333884502295405\n",
      "Training Loss: 0.007000724244862795\n",
      "Training Loss: 0.006640261152060702\n",
      "Validation Loss: 0.003908161434941412\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.0073271339782513675\n",
      "Training Loss: 0.006994334035553038\n",
      "Training Loss: 0.006633298808010295\n",
      "Validation Loss: 0.003903403825890482\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.0073202969250269236\n",
      "Training Loss: 0.00698788057314232\n",
      "Training Loss: 0.006626260536722839\n",
      "Validation Loss: 0.0038985829209134486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.007313372603384778\n",
      "Training Loss: 0.006981363941449672\n",
      "Training Loss: 0.006619148382451385\n",
      "Validation Loss: 0.0038936996251721393\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.007306365023832768\n",
      "Training Loss: 0.00697478600544855\n",
      "Training Loss: 0.006611965127522126\n",
      "Validation Loss: 0.0038887560315262735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0072992736636661\n",
      "Training Loss: 0.006968148603336885\n",
      "Training Loss: 0.006604712143307552\n",
      "Validation Loss: 0.0038837509522779605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007292101798811928\n",
      "Training Loss: 0.006961453394033015\n",
      "Training Loss: 0.006597391330869868\n",
      "Validation Loss: 0.0038786885116688825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.007284851520089433\n",
      "Training Loss: 0.006954702615039423\n",
      "Training Loss: 0.006590007492341101\n",
      "Validation Loss: 0.0038735704153797096\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.00727752475766465\n",
      "Training Loss: 0.006947897552745417\n",
      "Training Loss: 0.006582561578834429\n",
      "Validation Loss: 0.0038683965012138144\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.0072701257152948525\n",
      "Training Loss: 0.0069410424982197585\n",
      "Training Loss: 0.006575060627656057\n",
      "Validation Loss: 0.003863169654701533\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.007262655567610636\n",
      "Training Loss: 0.0069341389939654614\n",
      "Training Loss: 0.006567506418796256\n",
      "Validation Loss: 0.0038578943637189236\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.007255119952606037\n",
      "Training Loss: 0.006927190021378919\n",
      "Training Loss: 0.0065599033073522155\n",
      "Validation Loss: 0.0038525741050291933\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.007247520365053788\n",
      "Training Loss: 0.0069202006806153805\n",
      "Training Loss: 0.006552256769500673\n",
      "Validation Loss: 0.00384721288253459\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.0072398615395650266\n",
      "Training Loss: 0.006913171671330928\n",
      "Training Loss: 0.006544569442048669\n",
      "Validation Loss: 0.0038418127537778253\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007232146455207839\n",
      "Training Loss: 0.006906107815448195\n",
      "Training Loss: 0.006536847265670076\n",
      "Validation Loss: 0.003836376927374454\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007224379240069538\n",
      "Training Loss: 0.0068990126659628\n",
      "Training Loss: 0.006529094494180754\n",
      "Validation Loss: 0.00383091189249764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.007216562712565064\n",
      "Training Loss: 0.006891889828257263\n",
      "Training Loss: 0.006521317523438483\n",
      "Validation Loss: 0.0038254177394019587\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.007208702170755714\n",
      "Training Loss: 0.006884742904221639\n",
      "Training Loss: 0.006513517965795472\n",
      "Validation Loss: 0.003819900347715181\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.007200799054699019\n",
      "Training Loss: 0.006877573657548055\n",
      "Training Loss: 0.006505702985450625\n",
      "Validation Loss: 0.0038143616049351654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.007192859299248084\n",
      "Training Loss: 0.006870388401439414\n",
      "Training Loss: 0.006497875971253961\n",
      "Validation Loss: 0.0038088069668855895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.007184884620364755\n",
      "Training Loss: 0.006863189747091383\n",
      "Training Loss: 0.006490044213132933\n",
      "Validation Loss: 0.003803242056818909\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.007176879732869565\n",
      "Training Loss: 0.006855981583939865\n",
      "Training Loss: 0.0064822087355423715\n",
      "Validation Loss: 0.0037976654581307026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.007168846289860084\n",
      "Training Loss: 0.006848766017938033\n",
      "Training Loss: 0.006474375239340588\n",
      "Validation Loss: 0.003792081958237575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.007160788261098787\n",
      "Training Loss: 0.006841547312214971\n",
      "Training Loss: 0.006466547838645056\n",
      "Validation Loss: 0.003786497135918713\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00715270952670835\n",
      "Training Loss: 0.006834326053503901\n",
      "Training Loss: 0.006458730010781438\n",
      "Validation Loss: 0.003780909566721471\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.007144610421964899\n",
      "Training Loss: 0.006827109380392358\n",
      "Training Loss: 0.00645092630176805\n",
      "Validation Loss: 0.0037753242774332843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.007136495985323563\n",
      "Training Loss: 0.0068198971636593345\n",
      "Training Loss: 0.0064431391609832646\n",
      "Validation Loss: 0.0037697438488117933\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.007128366550896317\n",
      "Training Loss: 0.006812692314852029\n",
      "Training Loss: 0.006435371349798515\n",
      "Validation Loss: 0.0037641714933966653\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.007120224093087018\n",
      "Training Loss: 0.006805497300811112\n",
      "Training Loss: 0.006427625819342211\n",
      "Validation Loss: 0.0037586036886433872\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.007112072013551369\n",
      "Training Loss: 0.0067983133345842365\n",
      "Training Loss: 0.00641990595497191\n",
      "Validation Loss: 0.0037530438537615235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.007103911059675738\n",
      "Training Loss: 0.006791142992442474\n",
      "Training Loss: 0.006412212902214378\n",
      "Validation Loss: 0.0037474937611416486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.007095744139514863\n",
      "Training Loss: 0.0067839861218817535\n",
      "Training Loss: 0.006404549173312262\n",
      "Validation Loss: 0.0037419510563165784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.007087570877047256\n",
      "Training Loss: 0.006776847344590351\n",
      "Training Loss: 0.006396915890509263\n",
      "Validation Loss: 0.00373642232525424\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.007079393221065402\n",
      "Training Loss: 0.0067697273660451175\n",
      "Training Loss: 0.006389313996769488\n",
      "Validation Loss: 0.0037309047138171917\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.007071212670998648\n",
      "Training Loss: 0.006762626019772142\n",
      "Training Loss: 0.006381746197585017\n",
      "Validation Loss: 0.003725397526129578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.007063030555145815\n",
      "Training Loss: 0.006755541906459257\n",
      "Training Loss: 0.006374210334615782\n",
      "Validation Loss: 0.0037199003972489845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.007054847136605531\n",
      "Training Loss: 0.006748480120440945\n",
      "Training Loss: 0.0063667094358243045\n",
      "Validation Loss: 0.003714412571129839\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.007046664212830365\n",
      "Training Loss: 0.006741438555764034\n",
      "Training Loss: 0.0063592448853887614\n",
      "Validation Loss: 0.003708936595828848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.007038482184289024\n",
      "Training Loss: 0.006734418835258112\n",
      "Training Loss: 0.006351813558721915\n",
      "Validation Loss: 0.003703470153503873\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.007030301695922389\n",
      "Training Loss: 0.006727421175455674\n",
      "Training Loss: 0.006344417394138872\n",
      "Validation Loss: 0.003698009257257152\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.007022123639471829\n",
      "Training Loss: 0.006720443456433713\n",
      "Training Loss: 0.0063370562402997165\n",
      "Validation Loss: 0.0036925570058291045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.007013950551627204\n",
      "Training Loss: 0.006713488857494667\n",
      "Training Loss: 0.0063297289586626\n",
      "Validation Loss: 0.0036871108498549863\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.007005780740873888\n",
      "Training Loss: 0.0067065558140166105\n",
      "Training Loss: 0.006322435975307599\n",
      "Validation Loss: 0.00368167201496577\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006997617660090327\n",
      "Training Loss: 0.006699644275940954\n",
      "Training Loss: 0.006315176263451576\n",
      "Validation Loss: 0.003676234523430885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006989461166085675\n",
      "Training Loss: 0.006692755373660475\n",
      "Training Loss: 0.006307949166512117\n",
      "Validation Loss: 0.0036708024903357533\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006981312216958031\n",
      "Training Loss: 0.006685888902284205\n",
      "Training Loss: 0.006300755492411554\n",
      "Validation Loss: 0.0036653721603052167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006973171447170899\n",
      "Training Loss: 0.006679042181931436\n",
      "Training Loss: 0.006293591936118901\n",
      "Validation Loss: 0.0036599418708238374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006965041427174583\n",
      "Training Loss: 0.0066722184303216634\n",
      "Training Loss: 0.006286460318369791\n",
      "Validation Loss: 0.0036545118939633786\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.0069569220195990055\n",
      "Training Loss: 0.006665415446041152\n",
      "Training Loss: 0.0062793587520718576\n",
      "Validation Loss: 0.003649079838631612\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0069488149194512515\n",
      "Training Loss: 0.006658634446794167\n",
      "Training Loss: 0.006272286358289421\n",
      "Validation Loss: 0.003643646146945153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.0069407225260511045\n",
      "Training Loss: 0.006651874775998295\n",
      "Training Loss: 0.006265243543311954\n",
      "Validation Loss: 0.003638211213931274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006932644752087071\n",
      "Training Loss: 0.006645137282903306\n",
      "Training Loss: 0.006258229376981035\n",
      "Validation Loss: 0.003632769760351335\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.00692458339734003\n",
      "Training Loss: 0.006638420363888144\n",
      "Training Loss: 0.006251242042053491\n",
      "Validation Loss: 0.0036273258188844063\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.0069165412662550805\n",
      "Training Loss: 0.006631724705221132\n",
      "Training Loss: 0.006244284682907164\n",
      "Validation Loss: 0.0036218763574999706\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.0069085186708252875\n",
      "Training Loss: 0.006625052120070905\n",
      "Training Loss: 0.0062373530492186545\n",
      "Validation Loss: 0.0036164249641444076\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006900516739115119\n",
      "Training Loss: 0.006618402164895088\n",
      "Training Loss: 0.006230449402937666\n",
      "Validation Loss: 0.003610970181664138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006892537800595165\n",
      "Training Loss: 0.006611774633056484\n",
      "Training Loss: 0.006223572691669688\n",
      "Validation Loss: 0.00360550950385965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006884584296494722\n",
      "Training Loss: 0.006605168588575907\n",
      "Training Loss: 0.00621672272332944\n",
      "Validation Loss: 0.0036000449660370188\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006876655798405409\n",
      "Training Loss: 0.006598586540203542\n",
      "Training Loss: 0.006209898964734748\n",
      "Validation Loss: 0.003594575196065092\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.0068687561748083685\n",
      "Training Loss: 0.006592028593877331\n",
      "Training Loss: 0.006203102623112499\n",
      "Validation Loss: 0.003589101469280261\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006860885970527306\n",
      "Training Loss: 0.006585493768216111\n",
      "Training Loss: 0.0061963342712260784\n",
      "Validation Loss: 0.0035836258353770115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.00685304653365165\n",
      "Training Loss: 0.006578984542284161\n",
      "Training Loss: 0.006189592694863677\n",
      "Validation Loss: 0.0035781480000469457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006845239949179813\n",
      "Training Loss: 0.006572501535993069\n",
      "Training Loss: 0.006182878037216142\n",
      "Validation Loss: 0.0035726714662140174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006837467607110739\n",
      "Training Loss: 0.006566042900667526\n",
      "Training Loss: 0.0061761915008537475\n",
      "Validation Loss: 0.00356719096771996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006829730133758858\n",
      "Training Loss: 0.00655961069103796\n",
      "Training Loss: 0.0061695317958947275\n",
      "Validation Loss: 0.003561709379630812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006822031025076285\n",
      "Training Loss: 0.0065532053582137454\n",
      "Training Loss: 0.0061629012308549135\n",
      "Validation Loss: 0.0035562247281182516\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006814369076164439\n",
      "Training Loss: 0.0065468262694776055\n",
      "Training Loss: 0.006156299362191931\n",
      "Validation Loss: 0.0035507426377427713\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006806745913345367\n",
      "Training Loss: 0.006540476437658071\n",
      "Training Loss: 0.006149726043222472\n",
      "Validation Loss: 0.003545261898569846\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.0067991635738871995\n",
      "Training Loss: 0.0065341552923200655\n",
      "Training Loss: 0.006143183548701927\n",
      "Validation Loss: 0.003539784383708925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006791623421013355\n",
      "Training Loss: 0.006527862687362358\n",
      "Training Loss: 0.006136671599233523\n",
      "Validation Loss: 0.003534311699429841\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006784125124104321\n",
      "Training Loss: 0.006521599694970064\n",
      "Training Loss: 0.006130188647657633\n",
      "Validation Loss: 0.003528838276109669\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.0067766706086695196\n",
      "Training Loss: 0.0065153672895394265\n",
      "Training Loss: 0.006123737881425768\n",
      "Validation Loss: 0.003523372149021689\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006769260194851086\n",
      "Training Loss: 0.006509164297021926\n",
      "Training Loss: 0.006117317661410198\n",
      "Validation Loss: 0.0035179114084314097\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0067618950339965525\n",
      "Training Loss: 0.006502992574241944\n",
      "Training Loss: 0.00611092968727462\n",
      "Validation Loss: 0.0035124602871476097\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006754575551021844\n",
      "Training Loss: 0.0064968533185310665\n",
      "Training Loss: 0.006104574560886249\n",
      "Validation Loss: 0.0035070156040235183\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006747300964780152\n",
      "Training Loss: 0.006490744941984304\n",
      "Training Loss: 0.006098252962110564\n",
      "Validation Loss: 0.0035015795513296897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.00674007300985977\n",
      "Training Loss: 0.006484669161727652\n",
      "Training Loss: 0.006091964752413332\n",
      "Validation Loss: 0.0034961506640643217\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006732892765430733\n",
      "Training Loss: 0.00647862512501888\n",
      "Training Loss: 0.00608570946729742\n",
      "Validation Loss: 0.003490732577263137\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006725759323453531\n",
      "Training Loss: 0.006472615638049319\n",
      "Training Loss: 0.006079490646952763\n",
      "Validation Loss: 0.0034853291548683904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006718672720016912\n",
      "Training Loss: 0.006466637153062038\n",
      "Training Loss: 0.006073304329765961\n",
      "Validation Loss: 0.0034799325316516535\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006711633106460795\n",
      "Training Loss: 0.00646069263399113\n",
      "Training Loss: 0.006067152769537643\n",
      "Validation Loss: 0.003474549887429797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006704641532851383\n",
      "Training Loss: 0.0064547825144836675\n",
      "Training Loss: 0.006061037000035867\n",
      "Validation Loss: 0.003469179213057492\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006697696775663644\n",
      "Training Loss: 0.006448904353892431\n",
      "Training Loss: 0.006054956226143986\n",
      "Validation Loss: 0.0034638225961623067\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006690799455391243\n",
      "Training Loss: 0.006443060074234381\n",
      "Training Loss: 0.006048910811077803\n",
      "Validation Loss: 0.0034584803650616093\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006683948063291609\n",
      "Training Loss: 0.0064372507372172545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [25:08<00:00, 150.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.006042901771143079\n",
      "Validation Loss: 0.0034531505668556756\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Shape of the data after splitting into sequences: (22797, 12, 5)\n",
      "Shape of the data after splitting into sequences: (5692, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (28500, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Loss: 0.527780042886734\n",
      "Training Loss: 0.4406342294812202\n",
      "Training Loss: 0.3525332354754209\n",
      "Validation Loss: 0.25925375625825997\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.2082717040926218\n",
      "Training Loss: 0.1294027565047145\n",
      "Training Loss: 0.09041277589276434\n",
      "Validation Loss: 0.07708330986121398\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.07318253699690104\n",
      "Training Loss: 0.06918315690010786\n",
      "Training Loss: 0.06848396083340048\n",
      "Validation Loss: 0.06879869921739852\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.06815459558740258\n",
      "Training Loss: 0.06585992977023125\n",
      "Training Loss: 0.06499405948445201\n",
      "Validation Loss: 0.0649461592181345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.06418027767911554\n",
      "Training Loss: 0.06164193958044052\n",
      "Training Loss: 0.06034828143194318\n",
      "Validation Loss: 0.05986137531195464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.058927707467228176\n",
      "Training Loss: 0.05612669687718153\n",
      "Training Loss: 0.05432710995897651\n",
      "Validation Loss: 0.053416144898098504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.05244567723013461\n",
      "Training Loss: 0.04961766701191664\n",
      "Training Loss: 0.047498051896691325\n",
      "Validation Loss: 0.04633891817935732\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.04555113758891821\n",
      "Training Loss: 0.042842952450737354\n",
      "Training Loss: 0.040575985806062816\n",
      "Validation Loss: 0.039325133489256495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.03882552701979876\n",
      "Training Loss: 0.036328153358772396\n",
      "Training Loss: 0.03408582115545869\n",
      "Validation Loss: 0.03294777236041728\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.03280123486183584\n",
      "Training Loss: 0.030630675833672284\n",
      "Training Loss: 0.02857784896157682\n",
      "Validation Loss: 0.027690823414896645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.0279039914207533\n",
      "Training Loss: 0.026092958147637547\n",
      "Training Loss: 0.024310984946787358\n",
      "Validation Loss: 0.023666716567920836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.024193268390372395\n",
      "Training Loss: 0.02267987743485719\n",
      "Training Loss: 0.021163923111744226\n",
      "Validation Loss: 0.02066723234804996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.021447856235317884\n",
      "Training Loss: 0.0201445219758898\n",
      "Training Loss: 0.01885463326703757\n",
      "Validation Loss: 0.01840909409828568\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.01939480388071388\n",
      "Training Loss: 0.018231395091861488\n",
      "Training Loss: 0.017120561343617738\n",
      "Validation Loss: 0.016657292497543136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.01781007563928142\n",
      "Training Loss: 0.016737717562355103\n",
      "Training Loss: 0.015759891180787234\n",
      "Validation Loss: 0.01523650842823423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.01652791724074632\n",
      "Training Loss: 0.015515882729087025\n",
      "Training Loss: 0.014634519198443741\n",
      "Validation Loss: 0.014030715283215716\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.015441769254393875\n",
      "Training Loss: 0.0144743652199395\n",
      "Training Loss: 0.013666209187358619\n",
      "Validation Loss: 0.012976695979641897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.014495437047444285\n",
      "Training Loss: 0.013565692650154233\n",
      "Training Loss: 0.012817213793750852\n",
      "Validation Loss: 0.012043634076403935\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.01366178086725995\n",
      "Training Loss: 0.012765627002809197\n",
      "Training Loss: 0.012068713509943336\n",
      "Validation Loss: 0.011214406127528696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.012925517074763774\n",
      "Training Loss: 0.012059245985001325\n",
      "Training Loss: 0.011408830834552646\n",
      "Validation Loss: 0.010476823414811928\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.012275619902648032\n",
      "Training Loss: 0.011435383886564524\n",
      "Training Loss: 0.010828162105754017\n",
      "Validation Loss: 0.009820961696880588\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.011703041475266218\n",
      "Training Loss: 0.010885081791784615\n",
      "Training Loss: 0.010318652444984764\n",
      "Validation Loss: 0.00923859249287693\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.011200184168992565\n",
      "Training Loss: 0.010401153357233852\n",
      "Training Loss: 0.009873256380669772\n",
      "Validation Loss: 0.008722840627822815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.01076056255842559\n",
      "Training Loss: 0.009977722237817944\n",
      "Training Loss: 0.009485605336958542\n",
      "Validation Loss: 0.008267617215444282\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.010378227510955185\n",
      "Training Loss: 0.009609502949751913\n",
      "Training Loss: 0.009149446247611194\n",
      "Validation Loss: 0.00786698291083442\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.010047235382953658\n",
      "Training Loss: 0.009291122537106275\n",
      "Training Loss: 0.008858367230277508\n",
      "Validation Loss: 0.0075147959058371825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.009761417475529015\n",
      "Training Loss: 0.009016855363734066\n",
      "Training Loss: 0.008605961018474773\n",
      "Validation Loss: 0.007204935757778167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.00951470404630527\n",
      "Training Loss: 0.008780911457724869\n",
      "Training Loss: 0.008386371245142072\n",
      "Validation Loss: 0.006931842723338122\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.009301633207360282\n",
      "Training Loss: 0.008577922885306179\n",
      "Training Loss: 0.008194759358884767\n",
      "Validation Loss: 0.0066909169074503725\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.009117684888187795\n",
      "Training Loss: 0.008403297703480347\n",
      "Training Loss: 0.008027415621327236\n",
      "Validation Loss: 0.00647854734489464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.00895923713222146\n",
      "Training Loss: 0.008253217558376491\n",
      "Training Loss: 0.007881488860584795\n",
      "Validation Loss: 0.006291805986273154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.00882325370097533\n",
      "Training Loss: 0.008124432378681377\n",
      "Training Loss: 0.007754612826975062\n",
      "Validation Loss: 0.006128125011743036\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008706994673702865\n",
      "Training Loss: 0.008014047686010599\n",
      "Training Loss: 0.007644612112781033\n",
      "Validation Loss: 0.0059850474170754465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008607854435686022\n",
      "Training Loss: 0.007919420797843486\n",
      "Training Loss: 0.007549385901074857\n",
      "Validation Loss: 0.005860166522161512\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008523341357940807\n",
      "Training Loss: 0.007838132894830778\n",
      "Training Loss: 0.007466914826072753\n",
      "Validation Loss: 0.005751149444396116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.00845113723888062\n",
      "Training Loss: 0.0077680060628335925\n",
      "Training Loss: 0.00739530410268344\n",
      "Validation Loss: 0.005655800911932765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008389152935706079\n",
      "Training Loss: 0.007707132481737062\n",
      "Training Loss: 0.007332844362827018\n",
      "Validation Loss: 0.005572134136415893\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008335570569615812\n",
      "Training Loss: 0.0076538760890252884\n",
      "Training Loss: 0.0072780318045988675\n",
      "Validation Loss: 0.005498393053539474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008288848899537698\n",
      "Training Loss: 0.007606865876587108\n",
      "Training Loss: 0.0072295847395434975\n",
      "Validation Loss: 0.005433052920166068\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008247707588598132\n",
      "Training Loss: 0.007564969101222232\n",
      "Training Loss: 0.007186428123386577\n",
      "Validation Loss: 0.00537482520507837\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.00821110587916337\n",
      "Training Loss: 0.007527265693061054\n",
      "Training Loss: 0.007147672431310639\n",
      "Validation Loss: 0.0053226283151039\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008178202242124826\n",
      "Training Loss: 0.007493012109771371\n",
      "Training Loss: 0.007112589792814106\n",
      "Validation Loss: 0.005275565449389096\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.008148324533831328\n",
      "Training Loss: 0.0074616093037184325\n",
      "Training Loss: 0.007080586066003889\n",
      "Validation Loss: 0.005232890415151886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.008120935934130102\n",
      "Training Loss: 0.00743257834459655\n",
      "Training Loss: 0.007051175402011722\n",
      "Validation Loss: 0.005193977245244752\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.008095606353599578\n",
      "Training Loss: 0.007405532078118995\n",
      "Training Loss: 0.007023961383383721\n",
      "Validation Loss: 0.005158314692625629\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.008071993505582213\n",
      "Training Loss: 0.007380161897744983\n",
      "Training Loss: 0.006998619691003114\n",
      "Validation Loss: 0.005125471214198748\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.008049821561435237\n",
      "Training Loss: 0.007356213525636122\n",
      "Training Loss: 0.006974881934002042\n",
      "Validation Loss: 0.005095085412331888\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.00802886660792865\n",
      "Training Loss: 0.007333480921806768\n",
      "Training Loss: 0.006952524908119813\n",
      "Validation Loss: 0.005066851460704517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.008008946005720646\n",
      "Training Loss: 0.007311792046530172\n",
      "Training Loss: 0.006931360992603004\n",
      "Validation Loss: 0.005040499598201197\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007989908312447369\n",
      "Training Loss: 0.007291006541345268\n",
      "Training Loss: 0.006911234236322343\n",
      "Validation Loss: 0.005015809831910589\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007971629910171031\n",
      "Training Loss: 0.007271006541559473\n",
      "Training Loss: 0.006892011294839903\n",
      "Validation Loss: 0.004992595143234252\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007954005799256265\n",
      "Training Loss: 0.0072516912152059375\n",
      "Training Loss: 0.006873578730737791\n",
      "Validation Loss: 0.004970680342809287\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00793694933061488\n",
      "Training Loss: 0.007232975328806787\n",
      "Training Loss: 0.0068558402464259415\n",
      "Validation Loss: 0.004949927252188869\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.00792038596351631\n",
      "Training Loss: 0.007214785931864753\n",
      "Training Loss: 0.006838712035678327\n",
      "Validation Loss: 0.004930200955825283\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007904251320287586\n",
      "Training Loss: 0.0071970611729193475\n",
      "Training Loss: 0.006822120504220947\n",
      "Validation Loss: 0.004911393630239861\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.00788849129108712\n",
      "Training Loss: 0.007179745112080127\n",
      "Training Loss: 0.006806002677185461\n",
      "Validation Loss: 0.004893403095480976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007873057279502972\n",
      "Training Loss: 0.007162789229769259\n",
      "Training Loss: 0.006790304359747097\n",
      "Validation Loss: 0.004876141991659874\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.00785790903493762\n",
      "Training Loss: 0.007146153530338779\n",
      "Training Loss: 0.006774974897271022\n",
      "Validation Loss: 0.00485953372403498\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00784300928702578\n",
      "Training Loss: 0.007129800368566066\n",
      "Training Loss: 0.006759972347645089\n",
      "Validation Loss: 0.004843507477427634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007828326042508707\n",
      "Training Loss: 0.007113694980507717\n",
      "Training Loss: 0.006745256574358791\n",
      "Validation Loss: 0.004827998674689174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007813829930964857\n",
      "Training Loss: 0.007097808844409883\n",
      "Training Loss: 0.00673079595901072\n",
      "Validation Loss: 0.004812957410271583\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.0077994968765415255\n",
      "Training Loss: 0.0070821161253843455\n",
      "Training Loss: 0.006716559772612527\n",
      "Validation Loss: 0.004798324521646699\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007785303291166201\n",
      "Training Loss: 0.007066592554328963\n",
      "Training Loss: 0.006702518640086055\n",
      "Validation Loss: 0.004784060133296703\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007771228818455711\n",
      "Training Loss: 0.0070512158703058955\n",
      "Training Loss: 0.006688649661373347\n",
      "Validation Loss: 0.004770117267715127\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007757254109019413\n",
      "Training Loss: 0.007035967756528407\n",
      "Training Loss: 0.006674928757129237\n",
      "Validation Loss: 0.004756466312899014\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007743361809989437\n",
      "Training Loss: 0.007020829438697546\n",
      "Training Loss: 0.006661337794503197\n",
      "Validation Loss: 0.004743068379590686\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007729537381092087\n",
      "Training Loss: 0.007005785076180473\n",
      "Training Loss: 0.0066478556534275415\n",
      "Validation Loss: 0.0047298927053683595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007715764635941013\n",
      "Training Loss: 0.0069908187165856365\n",
      "Training Loss: 0.006634466213872656\n",
      "Validation Loss: 0.0047169112877690055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007702030912041664\n",
      "Training Loss: 0.006975915661314502\n",
      "Training Loss: 0.006621152879670262\n",
      "Validation Loss: 0.004704096310070894\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0076883224956691264\n",
      "Training Loss: 0.006961065059294924\n",
      "Training Loss: 0.006607901266543195\n",
      "Validation Loss: 0.00469142850161415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.00767462803167291\n",
      "Training Loss: 0.006946252768393606\n",
      "Training Loss: 0.006594697552500293\n",
      "Validation Loss: 0.004678881007738495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007660936107859015\n",
      "Training Loss: 0.006931468114489689\n",
      "Training Loss: 0.006581528601236642\n",
      "Validation Loss: 0.004666437009046001\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007647235231706873\n",
      "Training Loss: 0.006916700672591105\n",
      "Training Loss: 0.00656838089111261\n",
      "Validation Loss: 0.004654076176674597\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007633514904882759\n",
      "Training Loss: 0.006901940174866467\n",
      "Training Loss: 0.00655524542205967\n",
      "Validation Loss: 0.004641781706922803\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.0076197661343030635\n",
      "Training Loss: 0.006887176471063867\n",
      "Training Loss: 0.00654210833599791\n",
      "Validation Loss: 0.004629533567199002\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007605977705679834\n",
      "Training Loss: 0.006872400270076469\n",
      "Training Loss: 0.006528959419811144\n",
      "Validation Loss: 0.00461732230693353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007592139907646924\n",
      "Training Loss: 0.006857601708034054\n",
      "Training Loss: 0.006515789715340361\n",
      "Validation Loss: 0.004605130646953338\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007578243615571409\n",
      "Training Loss: 0.0068427732202690095\n",
      "Training Loss: 0.006502587958239019\n",
      "Validation Loss: 0.004592949833218636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007564278929494322\n",
      "Training Loss: 0.006827906066318974\n",
      "Training Loss: 0.006489344817819074\n",
      "Validation Loss: 0.004580762503496005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007550235856324434\n",
      "Training Loss: 0.006812990813050419\n",
      "Training Loss: 0.006476050653727725\n",
      "Validation Loss: 0.004568554290303479\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007536106180632487\n",
      "Training Loss: 0.006798020922578871\n",
      "Training Loss: 0.006462696234229952\n",
      "Validation Loss: 0.004556320518715663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0075218798161949966\n",
      "Training Loss: 0.0067829881084617225\n",
      "Training Loss: 0.0064492722030263395\n",
      "Validation Loss: 0.0045440462300177205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.0075075472134631125\n",
      "Training Loss: 0.006767882566200569\n",
      "Training Loss: 0.006435769774252548\n",
      "Validation Loss: 0.0045317221992764245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007493098311824724\n",
      "Training Loss: 0.006752698553027585\n",
      "Training Loss: 0.006422179639339447\n",
      "Validation Loss: 0.004519336467106523\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007478523848112673\n",
      "Training Loss: 0.006737428263295442\n",
      "Training Loss: 0.006408492698101327\n",
      "Validation Loss: 0.004506883185273141\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007463815598748624\n",
      "Training Loss: 0.0067220638773869724\n",
      "Training Loss: 0.006394702652469278\n",
      "Validation Loss: 0.004494353630475365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007448962886119261\n",
      "Training Loss: 0.006706597534939646\n",
      "Training Loss: 0.006380799434846267\n",
      "Validation Loss: 0.004481733739349908\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007433957459870726\n",
      "Training Loss: 0.006691024617757648\n",
      "Training Loss: 0.006366776643553749\n",
      "Validation Loss: 0.004469024481152425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007418790004448965\n",
      "Training Loss: 0.006675337398191914\n",
      "Training Loss: 0.00635262701427564\n",
      "Validation Loss: 0.004456212275839421\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007403453243896365\n",
      "Training Loss: 0.00665953109622933\n",
      "Training Loss: 0.006338345044059679\n",
      "Validation Loss: 0.00444329630523355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0073879398335702715\n",
      "Training Loss: 0.006643600035458803\n",
      "Training Loss: 0.00632392329745926\n",
      "Validation Loss: 0.0044302651403586935\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.00737224145559594\n",
      "Training Loss: 0.006627541175112129\n",
      "Training Loss: 0.006309357472928241\n",
      "Validation Loss: 0.004417117336487628\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007356353760696948\n",
      "Training Loss: 0.006611352213658392\n",
      "Training Loss: 0.006294646273599938\n",
      "Validation Loss: 0.004403850928294274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007340272465953603\n",
      "Training Loss: 0.006595031464239582\n",
      "Training Loss: 0.0062797857308760285\n",
      "Validation Loss: 0.004390462322600102\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0073239936446771025\n",
      "Training Loss: 0.006578577806940302\n",
      "Training Loss: 0.006264775150921196\n",
      "Validation Loss: 0.004376951284612414\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.00730751772003714\n",
      "Training Loss: 0.006561994351213798\n",
      "Training Loss: 0.006249616381246596\n",
      "Validation Loss: 0.004363316757437051\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007290844636736437\n",
      "Training Loss: 0.00654528479790315\n",
      "Training Loss: 0.006234312107553705\n",
      "Validation Loss: 0.004349564417955999\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007273978041484952\n",
      "Training Loss: 0.0065284551889635625\n",
      "Training Loss: 0.00621886907494627\n",
      "Validation Loss: 0.004335695504880605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007256923828972504\n",
      "Training Loss: 0.006511514210142195\n",
      "Training Loss: 0.006203292795689776\n",
      "Validation Loss: 0.0043217203052704\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007239690609276294\n",
      "Training Loss: 0.006494473080383614\n",
      "Training Loss: 0.006187594834482297\n",
      "Validation Loss: 0.004307642399223543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007222290984354913\n",
      "Training Loss: 0.006477343674050644\n",
      "Training Loss: 0.006171787298517302\n",
      "Validation Loss: 0.004293479620520905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007204738493310287\n",
      "Training Loss: 0.0064601421623956415\n",
      "Training Loss: 0.006155884778127074\n",
      "Validation Loss: 0.004279236929240019\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.0071870508621213955\n",
      "Training Loss: 0.0064428871171548965\n",
      "Training Loss: 0.006139903662260622\n",
      "Validation Loss: 0.004264932210829234\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007169248078134842\n",
      "Training Loss: 0.006425596472108736\n",
      "Training Loss: 0.006123862166423351\n",
      "Validation Loss: 0.0042505816934536094\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007151352088549175\n",
      "Training Loss: 0.006408292681444436\n",
      "Training Loss: 0.006107781030004844\n",
      "Validation Loss: 0.00423620214942173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007133387285284698\n",
      "Training Loss: 0.006390996898990125\n",
      "Training Loss: 0.006091681631514803\n",
      "Validation Loss: 0.004221809834367439\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007115379141177982\n",
      "Training Loss: 0.006373731965431943\n",
      "Training Loss: 0.006075585365761071\n",
      "Validation Loss: 0.004207429325658033\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.007097356194281019\n",
      "Training Loss: 0.00635652283555828\n",
      "Training Loss: 0.006059516526293009\n",
      "Validation Loss: 0.004193076116341596\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007079344358062372\n",
      "Training Loss: 0.006339390635257586\n",
      "Training Loss: 0.006043495056801475\n",
      "Validation Loss: 0.004178768596114863\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.0070613709377357736\n",
      "Training Loss: 0.006322356922319159\n",
      "Training Loss: 0.006027542527299375\n",
      "Validation Loss: 0.00416452479431029\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.007043463662848808\n",
      "Training Loss: 0.006305443017045036\n",
      "Training Loss: 0.006011681779637001\n",
      "Validation Loss: 0.00415036548190656\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.007025647453847341\n",
      "Training Loss: 0.006288669019704684\n",
      "Training Loss: 0.00599593052524142\n",
      "Validation Loss: 0.004136307552205712\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007007945759687573\n",
      "Training Loss: 0.006272051469422877\n",
      "Training Loss: 0.005980307877180167\n",
      "Validation Loss: 0.004122361998607436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006990382760413923\n",
      "Training Loss: 0.006255605777259916\n",
      "Training Loss: 0.005964828009018675\n",
      "Validation Loss: 0.004108542150666079\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006972977021941915\n",
      "Training Loss: 0.006239345122594386\n",
      "Training Loss: 0.005949506104225293\n",
      "Validation Loss: 0.004094865362766837\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006955747407046147\n",
      "Training Loss: 0.006223279807018116\n",
      "Training Loss: 0.00593435451388359\n",
      "Validation Loss: 0.004081338607402665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006938707025838085\n",
      "Training Loss: 0.006207420192658901\n",
      "Training Loss: 0.005919382737483829\n",
      "Validation Loss: 0.004067969298529114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006921870619989932\n",
      "Training Loss: 0.0061917717370670285\n",
      "Training Loss: 0.005904600784997455\n",
      "Validation Loss: 0.004054770012925054\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006905249223927967\n",
      "Training Loss: 0.006176341896643862\n",
      "Training Loss: 0.005890014088363387\n",
      "Validation Loss: 0.004041741970989309\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006888850122340955\n",
      "Training Loss: 0.00616113381809555\n",
      "Training Loss: 0.005875628938665614\n",
      "Validation Loss: 0.004028892811659849\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0068726802337914705\n",
      "Training Loss: 0.006146148528205231\n",
      "Training Loss: 0.005861449803342111\n",
      "Validation Loss: 0.004016219793290444\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006856745083932764\n",
      "Training Loss: 0.006131388315698132\n",
      "Training Loss: 0.005847477291245014\n",
      "Validation Loss: 0.004003730116626561\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0068410472245886925\n",
      "Training Loss: 0.006116852610139176\n",
      "Training Loss: 0.005833714211476036\n",
      "Validation Loss: 0.003991424406256025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006825588323408738\n",
      "Training Loss: 0.006102541037835181\n",
      "Training Loss: 0.005820161673473195\n",
      "Validation Loss: 0.003979299777302514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006810369355953298\n",
      "Training Loss: 0.006088452717522159\n",
      "Training Loss: 0.00580681896826718\n",
      "Validation Loss: 0.003967361227122627\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0067953902535373345\n",
      "Training Loss: 0.006074585085734725\n",
      "Training Loss: 0.0057936848001554605\n",
      "Validation Loss: 0.003955600016064984\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.00678064874082338\n",
      "Training Loss: 0.006060936165740713\n",
      "Training Loss: 0.005780758777400479\n",
      "Validation Loss: 0.003944021093088799\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.0067661449330626055\n",
      "Training Loss: 0.0060475031996611505\n",
      "Training Loss: 0.005768038631067611\n",
      "Validation Loss: 0.003932618880068905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006751874045003206\n",
      "Training Loss: 0.006034282415639609\n",
      "Training Loss: 0.0057555209088604896\n",
      "Validation Loss: 0.003921390730191775\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006737833886872977\n",
      "Training Loss: 0.006021271450445056\n",
      "Training Loss: 0.005743204017053358\n",
      "Validation Loss: 0.003910335548630173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006724021486588754\n",
      "Training Loss: 0.0060084667080082\n",
      "Training Loss: 0.005731085374718532\n",
      "Validation Loss: 0.0038994517657393066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006710433309199289\n",
      "Training Loss: 0.00599586488911882\n",
      "Training Loss: 0.005719161246088333\n",
      "Validation Loss: 0.0038887313318574862\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006697065570624545\n",
      "Training Loss: 0.005983462545555085\n",
      "Training Loss: 0.005707428870955482\n",
      "Validation Loss: 0.003878175705446328\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006683914543245919\n",
      "Training Loss: 0.0059712546877563\n",
      "Training Loss: 0.0056958850612863895\n",
      "Validation Loss: 0.003867781466642248\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006670975461020134\n",
      "Training Loss: 0.00595923928776756\n",
      "Training Loss: 0.005684524704120122\n",
      "Validation Loss: 0.0038575418470061145\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.0066582438902696595\n",
      "Training Loss: 0.005947411985835061\n",
      "Training Loss: 0.005673346129478887\n",
      "Validation Loss: 0.003847456975379603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.0066457171086221935\n",
      "Training Loss: 0.005935769324423745\n",
      "Training Loss: 0.005662345334421844\n",
      "Validation Loss: 0.0038375222579356324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006633389522903599\n",
      "Training Loss: 0.005924308317480609\n",
      "Training Loss: 0.005651518781669438\n",
      "Validation Loss: 0.0038277366253536907\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006621256625512615\n",
      "Training Loss: 0.0059130234236363325\n",
      "Training Loss: 0.005640861972933635\n",
      "Validation Loss: 0.0038180945884551476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.0066093159490264956\n",
      "Training Loss: 0.005901912536937743\n",
      "Training Loss: 0.0056303728558123115\n",
      "Validation Loss: 0.0038085917476564646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006597561894450337\n",
      "Training Loss: 0.005890972372144461\n",
      "Training Loss: 0.0056200472108321265\n",
      "Validation Loss: 0.003799228129118388\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006585991100873798\n",
      "Training Loss: 0.0058801990083884445\n",
      "Training Loss: 0.0056098808831302445\n",
      "Validation Loss: 0.0037899985497430217\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006574598402366973\n",
      "Training Loss: 0.005869587563211098\n",
      "Training Loss: 0.005599871387821622\n",
      "Validation Loss: 0.003780902628891505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006563380709267221\n",
      "Training Loss: 0.0058591352985240515\n",
      "Training Loss: 0.005590015173074789\n",
      "Validation Loss: 0.003771935502627048\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006552333331783302\n",
      "Training Loss: 0.005848840068792925\n",
      "Training Loss: 0.0055803089495748285\n",
      "Validation Loss: 0.003763096669971357\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006541452806559391\n",
      "Training Loss: 0.005838697332656011\n",
      "Training Loss: 0.0055707493785303085\n",
      "Validation Loss: 0.003754382336308154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006530736443819478\n",
      "Training Loss: 0.005828704739687964\n",
      "Training Loss: 0.005561334216617979\n",
      "Validation Loss: 0.003745789435597953\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006520179933868348\n",
      "Training Loss: 0.005818857580889017\n",
      "Training Loss: 0.005552059132605791\n",
      "Validation Loss: 0.003737318014276078\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006509778476902284\n",
      "Training Loss: 0.005809154800372199\n",
      "Training Loss: 0.005542921497835778\n",
      "Validation Loss: 0.003728967086736406\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006499530338915065\n",
      "Training Loss: 0.005799591465620324\n",
      "Training Loss: 0.005533919146400877\n",
      "Validation Loss: 0.0037207314214836597\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006489430771907791\n",
      "Training Loss: 0.005790165510261431\n",
      "Training Loss: 0.005525048939161934\n",
      "Validation Loss: 0.003712607927625643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006479476661770605\n",
      "Training Loss: 0.005780873129842803\n",
      "Training Loss: 0.0055163082043873145\n",
      "Validation Loss: 0.0037045962578784466\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.0064696658495813604\n",
      "Training Loss: 0.005771712809801102\n",
      "Training Loss: 0.005507693677791394\n",
      "Validation Loss: 0.0036966990943726025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006459992908057757\n",
      "Training Loss: 0.005762681106571108\n",
      "Training Loss: 0.005499204071238637\n",
      "Validation Loss: 0.0036889101572209195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.00645045721961651\n",
      "Training Loss: 0.005753775347257033\n",
      "Training Loss: 0.005490836063981988\n",
      "Validation Loss: 0.0036812259232248653\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006441054961760529\n",
      "Training Loss: 0.005744993303669617\n",
      "Training Loss: 0.005482587764272466\n",
      "Validation Loss: 0.0036736474139615893\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.00643178241502028\n",
      "Training Loss: 0.005736331287771464\n",
      "Training Loss: 0.0054744566127192225\n",
      "Validation Loss: 0.0036661718570459843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006422637692885473\n",
      "Training Loss: 0.005727787965442985\n",
      "Training Loss: 0.005466441258904524\n",
      "Validation Loss: 0.0036588029699201247\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006413617774378508\n",
      "Training Loss: 0.005719360762741417\n",
      "Training Loss: 0.005458537528757006\n",
      "Validation Loss: 0.0036515313451795767\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006404719625134021\n",
      "Training Loss: 0.005711046752985567\n",
      "Training Loss: 0.005450745018315502\n",
      "Validation Loss: 0.0036443610724030333\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006395941327791661\n",
      "Training Loss: 0.005702844582265243\n",
      "Training Loss: 0.005443062428967096\n",
      "Validation Loss: 0.0036372868815081174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006387280956842005\n",
      "Training Loss: 0.005694751823320985\n",
      "Training Loss: 0.005435486487112939\n",
      "Validation Loss: 0.003630310593282783\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006378733947058208\n",
      "Training Loss: 0.005686765661230311\n",
      "Training Loss: 0.0054280144983204085\n",
      "Validation Loss: 0.003623428197938614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006370299145928584\n",
      "Training Loss: 0.005678884268272668\n",
      "Training Loss: 0.005420645587728359\n",
      "Validation Loss: 0.0036166395103883293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006361974441097118\n",
      "Training Loss: 0.005671106851659715\n",
      "Training Loss: 0.005413379142410122\n",
      "Validation Loss: 0.0036099443815156736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006353757690521889\n",
      "Training Loss: 0.005663429609267041\n",
      "Training Loss: 0.005406211674562655\n",
      "Validation Loss: 0.0036033355837604138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006345646250993014\n",
      "Training Loss: 0.005655852423515171\n",
      "Training Loss: 0.005399142038659193\n",
      "Validation Loss: 0.003596819058228075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006337637734250165\n",
      "Training Loss: 0.005648372183786705\n",
      "Training Loss: 0.005392168713151477\n",
      "Validation Loss: 0.0035903911483003183\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006329731058212928\n",
      "Training Loss: 0.005640987823717296\n",
      "Training Loss: 0.005385289710829966\n",
      "Validation Loss: 0.0035840506433886003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006321922715287655\n",
      "Training Loss: 0.0056336965505033735\n",
      "Training Loss: 0.005378504049149342\n",
      "Validation Loss: 0.0035777940595355094\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0063142114313086495\n",
      "Training Loss: 0.005626497893827036\n",
      "Training Loss: 0.005371809347998351\n",
      "Validation Loss: 0.003571618885309383\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006306595127098262\n",
      "Training Loss: 0.0056193902890663595\n",
      "Training Loss: 0.005365204852423631\n",
      "Validation Loss: 0.003565526408472967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.00629907195456326\n",
      "Training Loss: 0.00561237072921358\n",
      "Training Loss: 0.005358688185806386\n",
      "Validation Loss: 0.0035595167729103668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006291639705887064\n",
      "Training Loss: 0.005605439252685755\n",
      "Training Loss: 0.005352258756174706\n",
      "Validation Loss: 0.003553585730116354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.0062842966680182145\n",
      "Training Loss: 0.005598593379836529\n",
      "Training Loss: 0.0053459137491881845\n",
      "Validation Loss: 0.0035477336777342753\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006277041130815632\n",
      "Training Loss: 0.005591831762576476\n",
      "Training Loss: 0.005339652817347087\n",
      "Validation Loss: 0.003541955798916686\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006269871517433785\n",
      "Training Loss: 0.005585152419516817\n",
      "Training Loss: 0.005333474244107492\n",
      "Validation Loss: 0.0035362550445957883\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.00626278537674807\n",
      "Training Loss: 0.005578554839594289\n",
      "Training Loss: 0.005327375854831189\n",
      "Validation Loss: 0.003530625232333171\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.00625578154285904\n",
      "Training Loss: 0.005572037083329633\n",
      "Training Loss: 0.005321357271750458\n",
      "Validation Loss: 0.0035250645982405023\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006248857074533589\n",
      "Training Loss: 0.005565597816603258\n",
      "Training Loss: 0.005315415620571002\n",
      "Validation Loss: 0.0035195776321558974\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006242011825088411\n",
      "Training Loss: 0.00555923463194631\n",
      "Training Loss: 0.005309550756937825\n",
      "Validation Loss: 0.003514156774277642\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.0062352425081189725\n",
      "Training Loss: 0.00555294738849625\n",
      "Training Loss: 0.005303759829839692\n",
      "Validation Loss: 0.003508805217524844\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006228548365761526\n",
      "Training Loss: 0.005546733607770875\n",
      "Training Loss: 0.005298042326467111\n",
      "Validation Loss: 0.003503514552525548\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006221926797297783\n",
      "Training Loss: 0.005540593181503937\n",
      "Training Loss: 0.005292395865544677\n",
      "Validation Loss: 0.003498288707969857\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006215376881300472\n",
      "Training Loss: 0.005534524315735325\n",
      "Training Loss: 0.005286820069304667\n",
      "Validation Loss: 0.0034931234497409523\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.0062088965810835365\n",
      "Training Loss: 0.005528524621622637\n",
      "Training Loss: 0.005281312020379118\n",
      "Validation Loss: 0.0034880137700179403\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.0062024840584490445\n",
      "Training Loss: 0.005522593889618292\n",
      "Training Loss: 0.005275871779303998\n",
      "Validation Loss: 0.0034829664503203267\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.0061961377272382375\n",
      "Training Loss: 0.00551672956906259\n",
      "Training Loss: 0.005270496153389104\n",
      "Validation Loss: 0.0034779729387000875\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0061898566095624115\n",
      "Training Loss: 0.00551093139569275\n",
      "Training Loss: 0.0052651851833797995\n",
      "Validation Loss: 0.0034730340422162516\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006183638112270273\n",
      "Training Loss: 0.005505197219317779\n",
      "Training Loss: 0.005259936343063601\n",
      "Validation Loss: 0.003468146758269142\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.0061774802411673595\n",
      "Training Loss: 0.005499526368221268\n",
      "Training Loss: 0.005254748259321786\n",
      "Validation Loss: 0.0034633088442287668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006171382346656174\n",
      "Training Loss: 0.005493916861014441\n",
      "Training Loss: 0.0052496195497224105\n",
      "Validation Loss: 0.0034585169090083643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006165342590538785\n",
      "Training Loss: 0.005488366924691945\n",
      "Training Loss: 0.005244548780610785\n",
      "Validation Loss: 0.0034537759087613544\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006159358702716417\n",
      "Training Loss: 0.005482876546448096\n",
      "Training Loss: 0.005239534624270164\n",
      "Validation Loss: 0.0034490770436666402\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006153430410777218\n",
      "Training Loss: 0.005477442934643477\n",
      "Training Loss: 0.00523457505449187\n",
      "Validation Loss: 0.003444423827766493\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006147554679191671\n",
      "Training Loss: 0.005472066086949781\n",
      "Training Loss: 0.005229668089305051\n",
      "Validation Loss: 0.00343980914075974\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006141730245435611\n",
      "Training Loss: 0.005466743431752547\n",
      "Training Loss: 0.005224813664681278\n",
      "Validation Loss: 0.00343523393620559\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006135955594945699\n",
      "Training Loss: 0.005461474476614967\n",
      "Training Loss: 0.005220009075128473\n",
      "Validation Loss: 0.003430695709212568\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006130229837726802\n",
      "Training Loss: 0.0054562574101146314\n",
      "Training Loss: 0.005215253183851018\n",
      "Validation Loss: 0.0034261944002649757\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0061245502822566775\n",
      "Training Loss: 0.005451090524438769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:35<23:22, 155.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.005210543618304655\n",
      "Validation Loss: 0.0034217253749865744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.4111085778474808\n",
      "Training Loss: 0.33355422407388685\n",
      "Training Loss: 0.24585677452385427\n",
      "Validation Loss: 0.14990679775312377\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.11490037458017469\n",
      "Training Loss: 0.07371145309880375\n",
      "Training Loss: 0.06451186621561647\n",
      "Validation Loss: 0.06387939620135205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06251219097524881\n",
      "Training Loss: 0.05949104825034737\n",
      "Training Loss: 0.05802638996392488\n",
      "Validation Loss: 0.05785227876700712\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.05636348186060786\n",
      "Training Loss: 0.053189888997003436\n",
      "Training Loss: 0.051199394762516025\n",
      "Validation Loss: 0.050856741930075576\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.04930808899924159\n",
      "Training Loss: 0.04623171767219901\n",
      "Training Loss: 0.04396319627761841\n",
      "Validation Loss: 0.043655310056350205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.04225302911363542\n",
      "Training Loss: 0.0394477660022676\n",
      "Training Loss: 0.0370931720174849\n",
      "Validation Loss: 0.036866941353243386\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.035776134990155695\n",
      "Training Loss: 0.033313988400623204\n",
      "Training Loss: 0.031005647853016852\n",
      "Validation Loss: 0.030845539466467467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.030149089228361844\n",
      "Training Loss: 0.028005649549886585\n",
      "Training Loss: 0.025759024834260343\n",
      "Validation Loss: 0.025528260668886176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.025179460388608278\n",
      "Training Loss: 0.02314635869115591\n",
      "Training Loss: 0.020866873799823226\n",
      "Validation Loss: 0.02033180839204219\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.02040761961834505\n",
      "Training Loss: 0.018552167932502927\n",
      "Training Loss: 0.016593729932792484\n",
      "Validation Loss: 0.015916294546463015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.016598887073341756\n",
      "Training Loss: 0.015124801509082318\n",
      "Training Loss: 0.013654409488663078\n",
      "Validation Loss: 0.012962630405770928\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.014142115276772529\n",
      "Training Loss: 0.012984759921673686\n",
      "Training Loss: 0.011883488136809318\n",
      "Validation Loss: 0.0111877219672995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.012664019260555506\n",
      "Training Loss: 0.011658672254998236\n",
      "Training Loss: 0.010753547095227987\n",
      "Validation Loss: 0.010031545993851058\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.011679005617043003\n",
      "Training Loss: 0.010748999963980169\n",
      "Training Loss: 0.009959688008530065\n",
      "Validation Loss: 0.009196749910597135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.010963107753777877\n",
      "Training Loss: 0.010079597213771194\n",
      "Training Loss: 0.009368739515775815\n",
      "Validation Loss: 0.008554479293002005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.010415676452685147\n",
      "Training Loss: 0.0095627785124816\n",
      "Training Loss: 0.008909609731053933\n",
      "Validation Loss: 0.008038374580955656\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.009981022068532184\n",
      "Training Loss: 0.009148841977585108\n",
      "Training Loss: 0.008540869056014344\n",
      "Validation Loss: 0.0076101217353507205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.009625620041042567\n",
      "Training Loss: 0.008808037212584168\n",
      "Training Loss: 0.008237337211612612\n",
      "Validation Loss: 0.007246227742675064\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.009328460750402882\n",
      "Training Loss: 0.008521822760812939\n",
      "Training Loss: 0.007983071567723527\n",
      "Validation Loss: 0.006931686240216989\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.00907599832979031\n",
      "Training Loss: 0.008278216671897098\n",
      "Training Loss: 0.007767576588084921\n",
      "Validation Loss: 0.006656638542818052\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.00885930573916994\n",
      "Training Loss: 0.008069230308756233\n",
      "Training Loss: 0.007583683260018006\n",
      "Validation Loss: 0.006414450593868249\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.00867234755307436\n",
      "Training Loss: 0.007889308006269857\n",
      "Training Loss: 0.007426274954341352\n",
      "Validation Loss: 0.006200498061547621\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.00851082881912589\n",
      "Training Loss: 0.007734362435294315\n",
      "Training Loss: 0.007291509581264109\n",
      "Validation Loss: 0.006011383938917032\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008371480960631743\n",
      "Training Loss: 0.007601179924095049\n",
      "Training Loss: 0.00717633533407934\n",
      "Validation Loss: 0.0058444357500673175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.008251611195737496\n",
      "Training Loss: 0.0074870541470590975\n",
      "Training Loss: 0.00707819037605077\n",
      "Validation Loss: 0.00569737197967309\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008148854225873947\n",
      "Training Loss: 0.00738958461326547\n",
      "Training Loss: 0.00699482855387032\n",
      "Validation Loss: 0.005568155437430597\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008061066137161106\n",
      "Training Loss: 0.007306599994190037\n",
      "Training Loss: 0.006924237712519243\n",
      "Validation Loss: 0.005454894862912093\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007986279169563204\n",
      "Training Loss: 0.007236120594898239\n",
      "Training Loss: 0.006864603601861745\n",
      "Validation Loss: 0.005355826607157131\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.00792269193334505\n",
      "Training Loss: 0.007176346107153222\n",
      "Training Loss: 0.0068142880348023025\n",
      "Validation Loss: 0.005269282871564285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.00786866903421469\n",
      "Training Loss: 0.007125659964513034\n",
      "Training Loss: 0.006771832016529516\n",
      "Validation Loss: 0.005193737094170215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007822754975641145\n",
      "Training Loss: 0.007082638522842899\n",
      "Training Loss: 0.006735957036726177\n",
      "Validation Loss: 0.0051277755724339425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007783668158808723\n",
      "Training Loss: 0.0070460376958362755\n",
      "Training Loss: 0.006705554695799947\n",
      "Validation Loss: 0.005070139048359451\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007750298066530377\n",
      "Training Loss: 0.007014790212269873\n",
      "Training Loss: 0.0066796810738742355\n",
      "Validation Loss: 0.005019699768137103\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007721697349334136\n",
      "Training Loss: 0.006987989726476371\n",
      "Training Loss: 0.006657538830768317\n",
      "Validation Loss: 0.004975465660668868\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007697060626232996\n",
      "Training Loss: 0.006964869663352147\n",
      "Training Loss: 0.0066384626238141205\n",
      "Validation Loss: 0.004936567901403465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007675713587086647\n",
      "Training Loss: 0.006944796352181584\n",
      "Training Loss: 0.006621902883052826\n",
      "Validation Loss: 0.004902258134065091\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007657093030866235\n",
      "Training Loss: 0.006927238224307075\n",
      "Training Loss: 0.006607407901901752\n",
      "Validation Loss: 0.004871891091415512\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007640735385939479\n",
      "Training Loss: 0.006911760913208127\n",
      "Training Loss: 0.006594608044251799\n",
      "Validation Loss: 0.004844907076056167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007626253612106666\n",
      "Training Loss: 0.0068980034382548185\n",
      "Training Loss: 0.006583199847955257\n",
      "Validation Loss: 0.004820835432613247\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007613331221509725\n",
      "Training Loss: 0.006885673610959202\n",
      "Training Loss: 0.006572940457845107\n",
      "Validation Loss: 0.004799269740809843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.0076017072796821595\n",
      "Training Loss: 0.0068745254713576285\n",
      "Training Loss: 0.006563626825227402\n",
      "Validation Loss: 0.0047798655316327815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0075911673239897936\n",
      "Training Loss: 0.006864364594221115\n",
      "Training Loss: 0.0065550987614551555\n",
      "Validation Loss: 0.004762326246205969\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0075815339596010745\n",
      "Training Loss: 0.006855025482363999\n",
      "Training Loss: 0.006547223409288563\n",
      "Validation Loss: 0.004746402497403324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007572661113226786\n",
      "Training Loss: 0.00684637688449584\n",
      "Training Loss: 0.006539892160799354\n",
      "Validation Loss: 0.004731876977060115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007564428332261741\n",
      "Training Loss: 0.006838305741548538\n",
      "Training Loss: 0.00653301541809924\n",
      "Validation Loss: 0.004718569234887326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007556735165417194\n",
      "Training Loss: 0.006830720903817564\n",
      "Training Loss: 0.00652652054966893\n",
      "Validation Loss: 0.0047063215274698615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007549497775034979\n",
      "Training Loss: 0.00682354949763976\n",
      "Training Loss: 0.006520348658668808\n",
      "Validation Loss: 0.004695001147881025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007542647591326386\n",
      "Training Loss: 0.0068167256005108355\n",
      "Training Loss: 0.00651444801071193\n",
      "Validation Loss: 0.004684494152335513\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007536126307677478\n",
      "Training Loss: 0.006810198106104508\n",
      "Training Loss: 0.0065087768709054215\n",
      "Validation Loss: 0.0046746958416018095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007529883132083342\n",
      "Training Loss: 0.006803922188701108\n",
      "Training Loss: 0.006503299744217657\n",
      "Validation Loss: 0.004665526677854359\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007523877079365775\n",
      "Training Loss: 0.006797859538346529\n",
      "Training Loss: 0.006497986884787679\n",
      "Validation Loss: 0.00465690497220023\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007518070938531309\n",
      "Training Loss: 0.006791978707769886\n",
      "Training Loss: 0.006492811645148322\n",
      "Validation Loss: 0.00464877109317988\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007512434697709978\n",
      "Training Loss: 0.0067862504534423355\n",
      "Training Loss: 0.006487752731190994\n",
      "Validation Loss: 0.004641065334121623\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.0075069407385308295\n",
      "Training Loss: 0.0067806524143088605\n",
      "Training Loss: 0.006482788780704141\n",
      "Validation Loss: 0.00463373831334639\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007501566162100062\n",
      "Training Loss: 0.006775163151323795\n",
      "Training Loss: 0.006477904943167232\n",
      "Validation Loss: 0.004626747873465248\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007496289195260033\n",
      "Training Loss: 0.006769762460608036\n",
      "Training Loss: 0.006473083454184234\n",
      "Validation Loss: 0.004620054933593054\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007491091064875946\n",
      "Training Loss: 0.006764436246594414\n",
      "Training Loss: 0.006468311200733296\n",
      "Validation Loss: 0.004613626187829447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007485954392468557\n",
      "Training Loss: 0.006759164918912574\n",
      "Training Loss: 0.00646357394522056\n",
      "Validation Loss: 0.004607426906475441\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007480862437514588\n",
      "Training Loss: 0.006753936486784368\n",
      "Training Loss: 0.006458860001293943\n",
      "Validation Loss: 0.0046014306894137285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007475801852997392\n",
      "Training Loss: 0.006748738163150847\n",
      "Training Loss: 0.006454157534171827\n",
      "Validation Loss: 0.004595614177059759\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.0074707576632499694\n",
      "Training Loss: 0.006743555327411741\n",
      "Training Loss: 0.006449454446556047\n",
      "Validation Loss: 0.004589952321152799\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007465715029975399\n",
      "Training Loss: 0.006738376151770353\n",
      "Training Loss: 0.006444739992148243\n",
      "Validation Loss: 0.004584423571777938\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007460662150988355\n",
      "Training Loss: 0.006733188189100474\n",
      "Training Loss: 0.006440003366442397\n",
      "Validation Loss: 0.00457900566744796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007455584917915985\n",
      "Training Loss: 0.006727980386931449\n",
      "Training Loss: 0.00643523208971601\n",
      "Validation Loss: 0.004573681100083285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007450470805633813\n",
      "Training Loss: 0.006722739060642198\n",
      "Training Loss: 0.00643041631730739\n",
      "Validation Loss: 0.004568429930616965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.0074453062552493066\n",
      "Training Loss: 0.006717452643206343\n",
      "Training Loss: 0.006425543315126561\n",
      "Validation Loss: 0.004563234477000458\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0074400774843525145\n",
      "Training Loss: 0.006712108644424006\n",
      "Training Loss: 0.006420602008001879\n",
      "Validation Loss: 0.004558074075514137\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0074347725999541585\n",
      "Training Loss: 0.006706693825544789\n",
      "Training Loss: 0.006415580868488178\n",
      "Validation Loss: 0.004552935706608499\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007429377004737035\n",
      "Training Loss: 0.006701198193477467\n",
      "Training Loss: 0.006410469052498229\n",
      "Validation Loss: 0.004547797954442461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007423879287671298\n",
      "Training Loss: 0.006695608438458294\n",
      "Training Loss: 0.006405254007549956\n",
      "Validation Loss: 0.0045426438276111826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007418266289168969\n",
      "Training Loss: 0.006689911546418443\n",
      "Training Loss: 0.006399925136356614\n",
      "Validation Loss: 0.004537457061324562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.00741252516512759\n",
      "Training Loss: 0.006684096930548548\n",
      "Training Loss: 0.006394471096573397\n",
      "Validation Loss: 0.004532220382949735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.0074066453496925535\n",
      "Training Loss: 0.006678151861997321\n",
      "Training Loss: 0.006388881204766221\n",
      "Validation Loss: 0.004526913456430429\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007400615514488891\n",
      "Training Loss: 0.006672067637555301\n",
      "Training Loss: 0.006383148510358296\n",
      "Validation Loss: 0.004521517980362425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007394426296232268\n",
      "Training Loss: 0.00666583321755752\n",
      "Training Loss: 0.00637726228334941\n",
      "Validation Loss: 0.004516018517943246\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.0073880710685625675\n",
      "Training Loss: 0.006659442067611963\n",
      "Training Loss: 0.006371217370033264\n",
      "Validation Loss: 0.004510398999280348\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007381544309901074\n",
      "Training Loss: 0.006652888893149793\n",
      "Training Loss: 0.00636500988388434\n",
      "Validation Loss: 0.004504648279241716\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007374843560392037\n",
      "Training Loss: 0.006646167821018025\n",
      "Training Loss: 0.006358636533841491\n",
      "Validation Loss: 0.004498743028981674\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007367966307792812\n",
      "Training Loss: 0.006639277066569776\n",
      "Training Loss: 0.006352096300688572\n",
      "Validation Loss: 0.004492678866575189\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007360917441546917\n",
      "Training Loss: 0.00663221747148782\n",
      "Training Loss: 0.0063453933969140055\n",
      "Validation Loss: 0.004486439978505035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007353700859239325\n",
      "Training Loss: 0.006624992850702256\n",
      "Training Loss: 0.006338530236971565\n",
      "Validation Loss: 0.004480015565615064\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007346322680823505\n",
      "Training Loss: 0.006617608406813815\n",
      "Training Loss: 0.006331516105565243\n",
      "Validation Loss: 0.004473404708355037\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007338796524563805\n",
      "Training Loss: 0.0066100712202023714\n",
      "Training Loss: 0.0063243585225427525\n",
      "Validation Loss: 0.004466600623243394\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.00733113327936735\n",
      "Training Loss: 0.006602392033673823\n",
      "Training Loss: 0.006317069402430206\n",
      "Validation Loss: 0.004459609640526763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007323349406360649\n",
      "Training Loss: 0.0065945825830567625\n",
      "Training Loss: 0.006309662617859431\n",
      "Validation Loss: 0.004452432282112037\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007315460945828817\n",
      "Training Loss: 0.006586656082654372\n",
      "Training Loss: 0.006302150869742036\n",
      "Validation Loss: 0.004445070874343595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.00730748581700027\n",
      "Training Loss: 0.006578627396374941\n",
      "Training Loss: 0.00629454995621927\n",
      "Validation Loss: 0.004437536738284477\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007299441803479567\n",
      "Training Loss: 0.006570511787431315\n",
      "Training Loss: 0.006286873503122479\n",
      "Validation Loss: 0.004429844287851972\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007291345820995047\n",
      "Training Loss: 0.006562323346734047\n",
      "Training Loss: 0.006279137198580429\n",
      "Validation Loss: 0.004422007588371425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.0072832173504866655\n",
      "Training Loss: 0.006554078382905572\n",
      "Training Loss: 0.006271356143406593\n",
      "Validation Loss: 0.0044140474010112415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007275073124910705\n",
      "Training Loss: 0.006545791524695232\n",
      "Training Loss: 0.006263544778339565\n",
      "Validation Loss: 0.004405975800263957\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.00726692870433908\n",
      "Training Loss: 0.00653747605974786\n",
      "Training Loss: 0.006255714422441088\n",
      "Validation Loss: 0.0043978135963493765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007258798662805929\n",
      "Training Loss: 0.006529145331587643\n",
      "Training Loss: 0.0062478785193525255\n",
      "Validation Loss: 0.004389581621723941\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007250696059199982\n",
      "Training Loss: 0.006520811779191718\n",
      "Training Loss: 0.0062400483747478575\n",
      "Validation Loss: 0.004381302568219142\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007242633191635832\n",
      "Training Loss: 0.006512487080181017\n",
      "Training Loss: 0.006232234151102602\n",
      "Validation Loss: 0.004372997601840938\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007234621003153734\n",
      "Training Loss: 0.006504179934272543\n",
      "Training Loss: 0.006224444238469005\n",
      "Validation Loss: 0.004364681460899876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.00722666802816093\n",
      "Training Loss: 0.006495902021415531\n",
      "Training Loss: 0.0062166880478616805\n",
      "Validation Loss: 0.004356377621395827\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007218783192220144\n",
      "Training Loss: 0.0064876598294358705\n",
      "Training Loss: 0.00620897259679623\n",
      "Validation Loss: 0.004348102811818126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0072109729121439155\n",
      "Training Loss: 0.006479461202397943\n",
      "Training Loss: 0.006201304087880999\n",
      "Validation Loss: 0.00433986879403839\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007203241978422739\n",
      "Training Loss: 0.006471311780624092\n",
      "Training Loss: 0.006193688489147462\n",
      "Validation Loss: 0.004331694740613692\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007195597285171971\n",
      "Training Loss: 0.006463218986755237\n",
      "Training Loss: 0.006186129476991482\n",
      "Validation Loss: 0.004323592153116224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007188040839973837\n",
      "Training Loss: 0.006455186872044578\n",
      "Training Loss: 0.00617863338033203\n",
      "Validation Loss: 0.0043155733813803766\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007180577319231816\n",
      "Training Loss: 0.006447220222325995\n",
      "Training Loss: 0.00617120236507617\n",
      "Validation Loss: 0.00430764609573702\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007173208854510449\n",
      "Training Loss: 0.0064393228793051096\n",
      "Training Loss: 0.006163841668167152\n",
      "Validation Loss: 0.004299818532243161\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007165937661193311\n",
      "Training Loss: 0.006431498420424759\n",
      "Training Loss: 0.00615655260218773\n",
      "Validation Loss: 0.004292100817069746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007158765059430152\n",
      "Training Loss: 0.00642375041381456\n",
      "Training Loss: 0.00614933849137742\n",
      "Validation Loss: 0.004284492644136039\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007151692577172071\n",
      "Training Loss: 0.006416081895586103\n",
      "Training Loss: 0.006142201647162437\n",
      "Validation Loss: 0.004277003756358048\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0071447209012694655\n",
      "Training Loss: 0.00640849425108172\n",
      "Training Loss: 0.00613514308759477\n",
      "Validation Loss: 0.004269636631812482\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007137851352454163\n",
      "Training Loss: 0.006400991436094045\n",
      "Training Loss: 0.00612816697161179\n",
      "Validation Loss: 0.004262393057932344\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.007131084179272875\n",
      "Training Loss: 0.006393574746325612\n",
      "Training Loss: 0.00612127372995019\n",
      "Validation Loss: 0.004255273909140588\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.007124419531901367\n",
      "Training Loss: 0.006386244972236455\n",
      "Training Loss: 0.006114463878329843\n",
      "Validation Loss: 0.004248281320808142\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.007117857284029015\n",
      "Training Loss: 0.006379005488706752\n",
      "Training Loss: 0.006107740286970511\n",
      "Validation Loss: 0.0042414143276034615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007111397209810093\n",
      "Training Loss: 0.006371857145568356\n",
      "Training Loss: 0.006101102672982961\n",
      "Validation Loss: 0.004234674109376214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007105039398884401\n",
      "Training Loss: 0.006364801439922303\n",
      "Training Loss: 0.006094553136499599\n",
      "Validation Loss: 0.004228060405172952\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007098783969413489\n",
      "Training Loss: 0.00635783901438117\n",
      "Training Loss: 0.006088092395220884\n",
      "Validation Loss: 0.004221569038430501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007092629533726722\n",
      "Training Loss: 0.006350970439380035\n",
      "Training Loss: 0.0060817200597375635\n",
      "Validation Loss: 0.004215200161535209\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.007086575847351923\n",
      "Training Loss: 0.006344197775470093\n",
      "Training Loss: 0.006075437298277393\n",
      "Validation Loss: 0.0042089522349271465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.007080621286295354\n",
      "Training Loss: 0.006337520985398442\n",
      "Training Loss: 0.00606924565450754\n",
      "Validation Loss: 0.0042028239734596415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.00707476710609626\n",
      "Training Loss: 0.006330941120395437\n",
      "Training Loss: 0.006063143977080472\n",
      "Validation Loss: 0.0041968144915914265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.007069011021521874\n",
      "Training Loss: 0.006324457859154791\n",
      "Training Loss: 0.00605713308614213\n",
      "Validation Loss: 0.004190921897900531\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.007063353544799611\n",
      "Training Loss: 0.0063180734065826985\n",
      "Training Loss: 0.006051214545150288\n",
      "Validation Loss: 0.004185138163653862\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007057793393032625\n",
      "Training Loss: 0.0063117859151680025\n",
      "Training Loss: 0.006045386427431367\n",
      "Validation Loss: 0.004179465489624215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.007052328165154904\n",
      "Training Loss: 0.006305596330203116\n",
      "Training Loss: 0.006039648347650655\n",
      "Validation Loss: 0.00417390230224269\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.007046957763377577\n",
      "Training Loss: 0.0062995033152401445\n",
      "Training Loss: 0.006034000175422989\n",
      "Validation Loss: 0.004168443216319625\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.007041680929251015\n",
      "Training Loss: 0.006293508021626621\n",
      "Training Loss: 0.006028442995157093\n",
      "Validation Loss: 0.004163087231206467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.007036497233202681\n",
      "Training Loss: 0.006287609335267916\n",
      "Training Loss: 0.0060229761846130716\n",
      "Validation Loss: 0.004157832293284617\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.007031405601883307\n",
      "Training Loss: 0.006281808551866561\n",
      "Training Loss: 0.006017599377664737\n",
      "Validation Loss: 0.0041526782351252985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.007026404843782075\n",
      "Training Loss: 0.00627610435243696\n",
      "Training Loss: 0.00601231224660296\n",
      "Validation Loss: 0.0041476193321031635\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.007021492613130249\n",
      "Training Loss: 0.006270494867349044\n",
      "Training Loss: 0.006007112711085938\n",
      "Validation Loss: 0.00414265503222646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007016669715521857\n",
      "Training Loss: 0.006264980853302404\n",
      "Training Loss: 0.006002001616870985\n",
      "Validation Loss: 0.004137780211258973\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007011932260356843\n",
      "Training Loss: 0.006259561180486344\n",
      "Training Loss: 0.005996977158356458\n",
      "Validation Loss: 0.004132998030072894\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.007007281135884114\n",
      "Training Loss: 0.006254235968226567\n",
      "Training Loss: 0.005992039937991649\n",
      "Validation Loss: 0.004128302826828668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.007002714403206482\n",
      "Training Loss: 0.006249000953976065\n",
      "Training Loss: 0.005987187333521433\n",
      "Validation Loss: 0.0041236912660311195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006998231229954399\n",
      "Training Loss: 0.0062438601691974325\n",
      "Training Loss: 0.005982420300133526\n",
      "Validation Loss: 0.004119163868279102\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006993829138809815\n",
      "Training Loss: 0.006238808691850863\n",
      "Training Loss: 0.005977736972272396\n",
      "Validation Loss: 0.004114717991337222\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006989507636753842\n",
      "Training Loss: 0.006233847861876711\n",
      "Training Loss: 0.005973136326065287\n",
      "Validation Loss: 0.004110352669873934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006985264917602763\n",
      "Training Loss: 0.00622897541266866\n",
      "Training Loss: 0.005968617729959078\n",
      "Validation Loss: 0.00410605979405783\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006981100811390206\n",
      "Training Loss: 0.006224190192879178\n",
      "Training Loss: 0.005964179742732085\n",
      "Validation Loss: 0.004101845924350108\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006977012865245342\n",
      "Training Loss: 0.006219492040690966\n",
      "Training Loss: 0.005959822527947836\n",
      "Validation Loss: 0.004097707770382881\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006973001033766195\n",
      "Training Loss: 0.00621487878030166\n",
      "Training Loss: 0.005955543411546387\n",
      "Validation Loss: 0.00409364013009171\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006969061958952807\n",
      "Training Loss: 0.006210349684697576\n",
      "Training Loss: 0.005951341997133568\n",
      "Validation Loss: 0.004089641890336833\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006965195210650563\n",
      "Training Loss: 0.00620590299135074\n",
      "Training Loss: 0.005947216908098198\n",
      "Validation Loss: 0.004085715894791392\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006961400113068521\n",
      "Training Loss: 0.0062015378836076706\n",
      "Training Loss: 0.005943167046643794\n",
      "Validation Loss: 0.004081854921127303\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006957674485165626\n",
      "Training Loss: 0.0061972527182661\n",
      "Training Loss: 0.0059391924564260986\n",
      "Validation Loss: 0.0040780596900731325\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006954017264652066\n",
      "Training Loss: 0.0061930460669100285\n",
      "Training Loss: 0.00593528981669806\n",
      "Validation Loss: 0.004074328407001671\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006950426486437209\n",
      "Training Loss: 0.006188917571562342\n",
      "Training Loss: 0.005931458782870322\n",
      "Validation Loss: 0.004070666011715873\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006946903194184415\n",
      "Training Loss: 0.006184865300892853\n",
      "Training Loss: 0.005927699594176375\n",
      "Validation Loss: 0.0040670632563918575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006943443833151832\n",
      "Training Loss: 0.0061808865750208495\n",
      "Training Loss: 0.005924008945585229\n",
      "Validation Loss: 0.004063518687800159\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.0069400481798220425\n",
      "Training Loss: 0.0061769827338866885\n",
      "Training Loss: 0.005920386720681563\n",
      "Validation Loss: 0.004060033068526536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006936713789473288\n",
      "Training Loss: 0.006173149985261262\n",
      "Training Loss: 0.005916830924688838\n",
      "Validation Loss: 0.004056605490139068\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.00693343929713592\n",
      "Training Loss: 0.006169388610869646\n",
      "Training Loss: 0.005913342123967595\n",
      "Validation Loss: 0.004053236812018276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006930225199903361\n",
      "Training Loss: 0.006165696054813452\n",
      "Training Loss: 0.005909916586242616\n",
      "Validation Loss: 0.0040499194455856265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006927068535005674\n",
      "Training Loss: 0.006162072374718264\n",
      "Training Loss: 0.005906555073452182\n",
      "Validation Loss: 0.00404665754516766\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006923969131312333\n",
      "Training Loss: 0.006158514462294988\n",
      "Training Loss: 0.0059032550314441325\n",
      "Validation Loss: 0.004043447824320599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006920924240839668\n",
      "Training Loss: 0.006155022208113223\n",
      "Training Loss: 0.005900016778032296\n",
      "Validation Loss: 0.0040402880796555725\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006917935055680573\n",
      "Training Loss: 0.006151593533577398\n",
      "Training Loss: 0.0058968377730343495\n",
      "Validation Loss: 0.004037178487103599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006914998855791055\n",
      "Training Loss: 0.00614822911506053\n",
      "Training Loss: 0.005893717273138464\n",
      "Validation Loss: 0.004034122114012206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006912114950246177\n",
      "Training Loss: 0.006144925557309762\n",
      "Training Loss: 0.005890654967515729\n",
      "Validation Loss: 0.0040311130702893215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006909281467087567\n",
      "Training Loss: 0.006141681583831087\n",
      "Training Loss: 0.005887648885836825\n",
      "Validation Loss: 0.004028150691451986\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006906497973250225\n",
      "Training Loss: 0.0061384966829791665\n",
      "Training Loss: 0.005884697352303192\n",
      "Validation Loss: 0.004025233863706418\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.00690376315265894\n",
      "Training Loss: 0.006135369604453445\n",
      "Training Loss: 0.005881799835478887\n",
      "Validation Loss: 0.0040223599762101185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.00690107473754324\n",
      "Training Loss: 0.006132297968142666\n",
      "Training Loss: 0.005878953571664169\n",
      "Validation Loss: 0.004019529291224571\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.00689843257307075\n",
      "Training Loss: 0.006129281232715584\n",
      "Training Loss: 0.005876159739564173\n",
      "Validation Loss: 0.004016741692334455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006895835930481553\n",
      "Training Loss: 0.00612631831667386\n",
      "Training Loss: 0.005873415922396816\n",
      "Validation Loss: 0.004013996823099599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006893281937809661\n",
      "Training Loss: 0.0061234077345579865\n",
      "Training Loss: 0.005870720116654411\n",
      "Validation Loss: 0.004011289151830159\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006890771026955917\n",
      "Training Loss: 0.0061205481103388595\n",
      "Training Loss: 0.0058680732920765874\n",
      "Validation Loss: 0.004008624086606453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.0068883028056006875\n",
      "Training Loss: 0.00611773831944447\n",
      "Training Loss: 0.005865471829893067\n",
      "Validation Loss: 0.0040059965653240345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.00688587405718863\n",
      "Training Loss: 0.006114977690740489\n",
      "Training Loss: 0.005862916682381183\n",
      "Validation Loss: 0.004003405132137364\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006883485555881634\n",
      "Training Loss: 0.006112263587419875\n",
      "Training Loss: 0.005860407428699545\n",
      "Validation Loss: 0.0040008509531617165\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006881136379670352\n",
      "Training Loss: 0.006109596791211516\n",
      "Training Loss: 0.00585793994134292\n",
      "Validation Loss: 0.00399833221545736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006878822994185612\n",
      "Training Loss: 0.006106974462163635\n",
      "Training Loss: 0.005855515152215957\n",
      "Validation Loss: 0.003995850461200298\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006876546854618937\n",
      "Training Loss: 0.006104396194568835\n",
      "Training Loss: 0.005853131428011693\n",
      "Validation Loss: 0.00399339997295939\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006874307278776541\n",
      "Training Loss: 0.006101860256167129\n",
      "Training Loss: 0.0058507884660502895\n",
      "Validation Loss: 0.0039909805996563245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006872100839391351\n",
      "Training Loss: 0.006099366853013635\n",
      "Training Loss: 0.0058484840771416205\n",
      "Validation Loss: 0.003988597132631818\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006869928850792348\n",
      "Training Loss: 0.006096913077053614\n",
      "Training Loss: 0.005846218013903126\n",
      "Validation Loss: 0.003986239174951203\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006867788395611569\n",
      "Training Loss: 0.0060944996640319\n",
      "Training Loss: 0.005843988762935623\n",
      "Validation Loss: 0.003983914967249619\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.0068656818487215785\n",
      "Training Loss: 0.006092124183196574\n",
      "Training Loss: 0.00584179564844817\n",
      "Validation Loss: 0.0039816213767776745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006863604112295434\n",
      "Training Loss: 0.006089785764343106\n",
      "Training Loss: 0.005839637468452566\n",
      "Validation Loss: 0.003979352149154907\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.0068615573609713465\n",
      "Training Loss: 0.006087484288727864\n",
      "Training Loss: 0.005837514055310748\n",
      "Validation Loss: 0.003977113788596909\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006859539815923199\n",
      "Training Loss: 0.0060852185392286625\n",
      "Training Loss: 0.005835423209937289\n",
      "Validation Loss: 0.003974902232339752\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006857550271088257\n",
      "Training Loss: 0.006082986281253398\n",
      "Training Loss: 0.005833364186692051\n",
      "Validation Loss: 0.0039727165013175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006855588089674711\n",
      "Training Loss: 0.006080787081154995\n",
      "Training Loss: 0.0058313365071080625\n",
      "Validation Loss: 0.003970557125939286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006853652183199301\n",
      "Training Loss: 0.006078620334155857\n",
      "Training Loss: 0.0058293392695486545\n",
      "Validation Loss: 0.003968419619637092\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006851742325816304\n",
      "Training Loss: 0.006076484622317366\n",
      "Training Loss: 0.0058273716771509495\n",
      "Validation Loss: 0.003966308484021365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006849857808556408\n",
      "Training Loss: 0.0060743797873146835\n",
      "Training Loss: 0.0058254319382831455\n",
      "Validation Loss: 0.003964218482364681\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006847998071461916\n",
      "Training Loss: 0.0060723047953797505\n",
      "Training Loss: 0.0058235212427098304\n",
      "Validation Loss: 0.003962154742173349\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006846161630237475\n",
      "Training Loss: 0.006070258556283079\n",
      "Training Loss: 0.00582163744373247\n",
      "Validation Loss: 0.003960113383808665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006844348276499659\n",
      "Training Loss: 0.006068239661981351\n",
      "Training Loss: 0.00581977941095829\n",
      "Validation Loss: 0.003958090588455664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006842556521296501\n",
      "Training Loss: 0.006066247507114895\n",
      "Training Loss: 0.005817945842863992\n",
      "Validation Loss: 0.003956088474480791\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006840786386746913\n",
      "Training Loss: 0.006064281762810424\n",
      "Training Loss: 0.005816137032816186\n",
      "Validation Loss: 0.003954108435597815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006839036848396063\n",
      "Training Loss: 0.0060623415559530255\n",
      "Training Loss: 0.005814352452289313\n",
      "Validation Loss: 0.003952149210858755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006837306842207909\n",
      "Training Loss: 0.006060424854513258\n",
      "Training Loss: 0.005812589512206614\n",
      "Validation Loss: 0.003950205432732453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.00683559708413668\n",
      "Training Loss: 0.006058531975140795\n",
      "Training Loss: 0.005810849633417092\n",
      "Validation Loss: 0.003948280052151113\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.0068339058151468636\n",
      "Training Loss: 0.006056662166374735\n",
      "Training Loss: 0.005809130301931873\n",
      "Validation Loss: 0.003946374475908874\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006832232639426366\n",
      "Training Loss: 0.006054815164534375\n",
      "Training Loss: 0.005807431970606558\n",
      "Validation Loss: 0.003944487184066367\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006830577624496073\n",
      "Training Loss: 0.006052988246665336\n",
      "Training Loss: 0.0058057543897302825\n",
      "Validation Loss: 0.00394261801181089\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0068289398774504665\n",
      "Training Loss: 0.006051182309747673\n",
      "Training Loss: 0.005804096008650958\n",
      "Validation Loss: 0.003940763818544827\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006827317815041169\n",
      "Training Loss: 0.006049397278693505\n",
      "Training Loss: 0.0058024559495970605\n",
      "Validation Loss: 0.003938926899219665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006825711841229349\n",
      "Training Loss: 0.006047631027176976\n",
      "Training Loss: 0.0058008342242101205\n",
      "Validation Loss: 0.003937101603114161\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006824121510144323\n",
      "Training Loss: 0.006045882970793173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [05:11<20:44, 155.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.005799228476826101\n",
      "Validation Loss: 0.003935294964199028\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.8737330201268196\n",
      "Training Loss: 0.6483163394033908\n",
      "Training Loss: 0.4337647503614426\n",
      "Validation Loss: 0.23632426245996122\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.17144425116479398\n",
      "Training Loss: 0.09039131388068199\n",
      "Training Loss: 0.06908112406730652\n",
      "Validation Loss: 0.06549013745081558\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06408564979210496\n",
      "Training Loss: 0.06102075358852744\n",
      "Training Loss: 0.05947609059512615\n",
      "Validation Loss: 0.059029754753528015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.0574801230430603\n",
      "Training Loss: 0.054453493393957615\n",
      "Training Loss: 0.0523752335831523\n",
      "Validation Loss: 0.051870023130617116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.050051821898669004\n",
      "Training Loss: 0.04707591372542083\n",
      "Training Loss: 0.044691402995958925\n",
      "Validation Loss: 0.04432244363418791\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.04255350415594876\n",
      "Training Loss: 0.03993027849122882\n",
      "Training Loss: 0.0376213033683598\n",
      "Validation Loss: 0.03746824408096544\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.03608317694626748\n",
      "Training Loss: 0.0339098687376827\n",
      "Training Loss: 0.03183365811593831\n",
      "Validation Loss: 0.031763499696854124\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.030892332289367913\n",
      "Training Loss: 0.029060604851692914\n",
      "Training Loss: 0.02719303287565708\n",
      "Validation Loss: 0.027087073495841763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.02671996780205518\n",
      "Training Loss: 0.025117762852460146\n",
      "Training Loss: 0.023418767373077572\n",
      "Validation Loss: 0.02322889401922735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.023305180897004903\n",
      "Training Loss: 0.021858433266170322\n",
      "Training Loss: 0.020300309471786022\n",
      "Validation Loss: 0.020002624966999453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.020446421285159885\n",
      "Training Loss: 0.019113105442374944\n",
      "Training Loss: 0.017677634449210018\n",
      "Validation Loss: 0.017252734961678808\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.018004989626351744\n",
      "Training Loss: 0.016770987678319214\n",
      "Training Loss: 0.015452829468995333\n",
      "Validation Loss: 0.01488775719386222\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.01591954842209816\n",
      "Training Loss: 0.014786211396567524\n",
      "Training Loss: 0.013594360877759755\n",
      "Validation Loss: 0.012885792625628495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.014188945351634175\n",
      "Training Loss: 0.013162288591265678\n",
      "Training Loss: 0.012113273113500326\n",
      "Validation Loss: 0.011265937254628103\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.012837794232182205\n",
      "Training Loss: 0.01191517526516691\n",
      "Training Loss: 0.011012313237879425\n",
      "Validation Loss: 0.010028022688060079\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.011853164832573385\n",
      "Training Loss: 0.011009794692508876\n",
      "Training Loss: 0.010228122596163302\n",
      "Validation Loss: 0.009104275071386541\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.011148542219307273\n",
      "Training Loss: 0.010349974473938347\n",
      "Training Loss: 0.009653535559773446\n",
      "Validation Loss: 0.008397707714322494\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.010621294181328267\n",
      "Training Loss: 0.00984487049281597\n",
      "Training Loss: 0.009206782958935946\n",
      "Validation Loss: 0.00783476678215051\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.010204051353503019\n",
      "Training Loss: 0.009438999318517745\n",
      "Training Loss: 0.008843176180962474\n",
      "Validation Loss: 0.007370266056190548\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.009859403227455914\n",
      "Training Loss: 0.009100761903682723\n",
      "Training Loss: 0.00853849588893354\n",
      "Validation Loss: 0.006978012172745939\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.009566484600072726\n",
      "Training Loss: 0.008812343458412215\n",
      "Training Loss: 0.008279347348725423\n",
      "Validation Loss: 0.006643114223019377\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.009314175063045695\n",
      "Training Loss: 0.00856428749160841\n",
      "Training Loss: 0.00805842960253358\n",
      "Validation Loss: 0.006356941878774695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.009096855353564024\n",
      "Training Loss: 0.008351584230549634\n",
      "Training Loss: 0.007871328928740696\n",
      "Validation Loss: 0.006113730159096336\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008911279543535783\n",
      "Training Loss: 0.008170910935150459\n",
      "Training Loss: 0.0077144819661043584\n",
      "Validation Loss: 0.005908668585372775\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.008754706997424365\n",
      "Training Loss: 0.008019161631818861\n",
      "Training Loss: 0.007584316956344992\n",
      "Validation Loss: 0.005737127921952123\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008624163056956603\n",
      "Training Loss: 0.007892998494207859\n",
      "Training Loss: 0.007477137894602493\n",
      "Validation Loss: 0.005594542265733641\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.00851636503241025\n",
      "Training Loss: 0.007788903028704226\n",
      "Training Loss: 0.007389305575052276\n",
      "Validation Loss: 0.005476521733583191\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008427923108683899\n",
      "Training Loss: 0.00770338288275525\n",
      "Training Loss: 0.007317418019520119\n",
      "Validation Loss: 0.005378999328763967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.00835555413388647\n",
      "Training Loss: 0.007633151187328622\n",
      "Training Loss: 0.007258426062762737\n",
      "Validation Loss: 0.005298307543526289\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008296234421432019\n",
      "Training Loss: 0.007575241678860038\n",
      "Training Loss: 0.007209689294686541\n",
      "Validation Loss: 0.0052312340344082605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008247298636706545\n",
      "Training Loss: 0.007527076333062723\n",
      "Training Loss: 0.00716898595332168\n",
      "Validation Loss: 0.00517505252396353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008206494147889315\n",
      "Training Loss: 0.007486498787766322\n",
      "Training Loss: 0.0071345120668411255\n",
      "Validation Loss: 0.005127500007556898\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.00817197497584857\n",
      "Training Loss: 0.007451762488344684\n",
      "Training Loss: 0.0071048367198091\n",
      "Validation Loss: 0.005086775150364578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008142278641462325\n",
      "Training Loss: 0.007421497919131071\n",
      "Training Loss: 0.007078858939930797\n",
      "Validation Loss: 0.005051452705108174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008116276051150634\n",
      "Training Loss: 0.007394652599468827\n",
      "Training Loss: 0.007055743720848114\n",
      "Validation Loss: 0.00502042594206195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008093110369518399\n",
      "Training Loss: 0.0073704355861991646\n",
      "Training Loss: 0.007034866408212111\n",
      "Validation Loss: 0.004992841644175979\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008072142937453463\n",
      "Training Loss: 0.007348260055296123\n",
      "Training Loss: 0.007015762283699587\n",
      "Validation Loss: 0.004968051153125263\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008052899300819262\n",
      "Training Loss: 0.007327693378319964\n",
      "Training Loss: 0.006998087237589061\n",
      "Validation Loss: 0.0049455529641297255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008035030514001847\n",
      "Training Loss: 0.007308418316533789\n",
      "Training Loss: 0.006981584216700867\n",
      "Validation Loss: 0.004924958656160056\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008018278812523932\n",
      "Training Loss: 0.007290202589938417\n",
      "Training Loss: 0.006966060809791088\n",
      "Validation Loss: 0.004905969051649438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008002453878289089\n",
      "Training Loss: 0.007272873859619722\n",
      "Training Loss: 0.006951370935421437\n",
      "Validation Loss: 0.004888344127710992\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007987412832444534\n",
      "Training Loss: 0.007256306164199486\n",
      "Training Loss: 0.006937402179464698\n",
      "Validation Loss: 0.004871901886647546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007973048363346606\n",
      "Training Loss: 0.007240404313197359\n",
      "Training Loss: 0.0069240677484776825\n",
      "Validation Loss: 0.004856493034311099\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007959279551869258\n",
      "Training Loss: 0.007225093969609588\n",
      "Training Loss: 0.0069112983881495895\n",
      "Validation Loss: 0.004841990701545532\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.00794604267575778\n",
      "Training Loss: 0.007210321475286037\n",
      "Training Loss: 0.0068990383425261825\n",
      "Validation Loss: 0.004828292998129481\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.00793328757979907\n",
      "Training Loss: 0.007196041774004697\n",
      "Training Loss: 0.006887241906952113\n",
      "Validation Loss: 0.004815324780588781\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007920975252054631\n",
      "Training Loss: 0.007182220402173698\n",
      "Training Loss: 0.006875873081153259\n",
      "Validation Loss: 0.004803010024486131\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007909072939073667\n",
      "Training Loss: 0.007168829080183059\n",
      "Training Loss: 0.006864899207139388\n",
      "Validation Loss: 0.004791291853088592\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007897553513757885\n",
      "Training Loss: 0.007155842476058751\n",
      "Training Loss: 0.006854292830685154\n",
      "Validation Loss: 0.004780116871004568\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007886393959634006\n",
      "Training Loss: 0.0071432406350504605\n",
      "Training Loss: 0.006844030106440187\n",
      "Validation Loss: 0.004769442181150021\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007875574788777158\n",
      "Training Loss: 0.007131005305564031\n",
      "Training Loss: 0.006834091218188405\n",
      "Validation Loss: 0.004759226513639344\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.00786507828393951\n",
      "Training Loss: 0.007119120102142915\n",
      "Training Loss: 0.006824456895701587\n",
      "Validation Loss: 0.00474944158085737\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00785488828434609\n",
      "Training Loss: 0.007107570790685714\n",
      "Training Loss: 0.0068151110992766915\n",
      "Validation Loss: 0.004740048766926224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007844991718884557\n",
      "Training Loss: 0.007096343361772597\n",
      "Training Loss: 0.00680603896966204\n",
      "Validation Loss: 0.004731029629173657\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007835374692222103\n",
      "Training Loss: 0.007085424902616069\n",
      "Training Loss: 0.006797225943300873\n",
      "Validation Loss: 0.004722359222673884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007826027066912503\n",
      "Training Loss: 0.007074805719312281\n",
      "Training Loss: 0.006788662567269057\n",
      "Validation Loss: 0.004714013297722958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007816936601884664\n",
      "Training Loss: 0.007064472921192646\n",
      "Training Loss: 0.006780335219809785\n",
      "Validation Loss: 0.004705971392693019\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007808093765052035\n",
      "Training Loss: 0.007054417460458353\n",
      "Training Loss: 0.006772235002135858\n",
      "Validation Loss: 0.0046982229772295925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00779948934679851\n",
      "Training Loss: 0.007044628905132413\n",
      "Training Loss: 0.006764351729070767\n",
      "Validation Loss: 0.004690743219552122\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.0077911144576501105\n",
      "Training Loss: 0.007035097897751257\n",
      "Training Loss: 0.0067566756554879245\n",
      "Validation Loss: 0.004683520961448215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007782960928743705\n",
      "Training Loss: 0.00702581561054103\n",
      "Training Loss: 0.006749199186451733\n",
      "Validation Loss: 0.004676542964926136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007775020634289831\n",
      "Training Loss: 0.007016773615032434\n",
      "Training Loss: 0.006741914190351963\n",
      "Validation Loss: 0.004669798204539281\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007767285669688135\n",
      "Training Loss: 0.007007963808719069\n",
      "Training Loss: 0.006734813989605754\n",
      "Validation Loss: 0.004663273759494965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007759750408586115\n",
      "Training Loss: 0.006999378587352112\n",
      "Training Loss: 0.006727891734335571\n",
      "Validation Loss: 0.004656963468937392\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007752407119842247\n",
      "Training Loss: 0.006991010250058025\n",
      "Training Loss: 0.006721140645677224\n",
      "Validation Loss: 0.004650848217863129\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007745250423904508\n",
      "Training Loss: 0.0069828513241373\n",
      "Training Loss: 0.006714555277721956\n",
      "Validation Loss: 0.00464492608868946\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007738272632705048\n",
      "Training Loss: 0.0069748948991764335\n",
      "Training Loss: 0.006708128114696592\n",
      "Validation Loss: 0.004639186696254052\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007731468153651804\n",
      "Training Loss: 0.006967135309241712\n",
      "Training Loss: 0.006701855999417603\n",
      "Validation Loss: 0.004633623931368583\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007724833280080929\n",
      "Training Loss: 0.006959565659053623\n",
      "Training Loss: 0.006695732137886807\n",
      "Validation Loss: 0.00462822709101765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007718361417064443\n",
      "Training Loss: 0.006952179075451568\n",
      "Training Loss: 0.006689753000391647\n",
      "Validation Loss: 0.004622990670326176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007712046335218475\n",
      "Training Loss: 0.00694496926967986\n",
      "Training Loss: 0.006683912182925269\n",
      "Validation Loss: 0.0046179091173297404\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.00770588377257809\n",
      "Training Loss: 0.006937932178843766\n",
      "Training Loss: 0.006678205450298264\n",
      "Validation Loss: 0.004612971826461719\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007699869619682431\n",
      "Training Loss: 0.006931060350034386\n",
      "Training Loss: 0.006672627818770707\n",
      "Validation Loss: 0.00460817509009032\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007693997801980003\n",
      "Training Loss: 0.00692434920114465\n",
      "Training Loss: 0.006667176109040156\n",
      "Validation Loss: 0.004603514850683762\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.0076882650330662725\n",
      "Training Loss: 0.006917792687891051\n",
      "Training Loss: 0.006661845768103376\n",
      "Validation Loss: 0.004598979862391154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007682665988104418\n",
      "Training Loss: 0.0069113876274786885\n",
      "Training Loss: 0.006656633339589462\n",
      "Validation Loss: 0.004594573643832897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007677196840522811\n",
      "Training Loss: 0.006905126785859465\n",
      "Training Loss: 0.0066515333810821175\n",
      "Validation Loss: 0.004590284351385042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007671853170031682\n",
      "Training Loss: 0.006899006703170016\n",
      "Training Loss: 0.006646543444367126\n",
      "Validation Loss: 0.004586113138082489\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007666631530737504\n",
      "Training Loss: 0.006893022154690698\n",
      "Training Loss: 0.006641660121968016\n",
      "Validation Loss: 0.004582051717508794\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007661527446471155\n",
      "Training Loss: 0.006887169812107459\n",
      "Training Loss: 0.006636880004080013\n",
      "Validation Loss: 0.004578095940569562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007656537628499791\n",
      "Training Loss: 0.006881443966412917\n",
      "Training Loss: 0.0066321987425908445\n",
      "Validation Loss: 0.004574243274904536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007651658087270335\n",
      "Training Loss: 0.006875840745633468\n",
      "Training Loss: 0.006627613324671984\n",
      "Validation Loss: 0.004570484033189296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.00764688448398374\n",
      "Training Loss: 0.006870357011212036\n",
      "Training Loss: 0.006623120936565101\n",
      "Validation Loss: 0.0045668217634750805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007642213978106156\n",
      "Training Loss: 0.006864986561704427\n",
      "Training Loss: 0.006618718026438728\n",
      "Validation Loss: 0.0045632496521954705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007637643683701754\n",
      "Training Loss: 0.00685972829349339\n",
      "Training Loss: 0.0066144019621424376\n",
      "Validation Loss: 0.004559767309555344\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007633169458713383\n",
      "Training Loss: 0.006854576185578481\n",
      "Training Loss: 0.0066101692442316565\n",
      "Validation Loss: 0.004556366773531511\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007628788738511503\n",
      "Training Loss: 0.006849527363665402\n",
      "Training Loss: 0.0066060177714098245\n",
      "Validation Loss: 0.004553047401877643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007624497637152672\n",
      "Training Loss: 0.006844578358577564\n",
      "Training Loss: 0.006601943565765395\n",
      "Validation Loss: 0.004549803082789346\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.0076202941569499675\n",
      "Training Loss: 0.00683972480124794\n",
      "Training Loss: 0.006597945182584226\n",
      "Validation Loss: 0.004546635687414012\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.0076161744026467205\n",
      "Training Loss: 0.0068349660083185885\n",
      "Training Loss: 0.006594018955947831\n",
      "Validation Loss: 0.004543541243242288\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007612135643139482\n",
      "Training Loss: 0.006830295742256567\n",
      "Training Loss: 0.006590163157088682\n",
      "Validation Loss: 0.004540511751625938\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0076081745862029496\n",
      "Training Loss: 0.006825712413992733\n",
      "Training Loss: 0.0065863744344096635\n",
      "Validation Loss: 0.004537551335498607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.0076042900211177765\n",
      "Training Loss: 0.006821212894283235\n",
      "Training Loss: 0.00658265205915086\n",
      "Validation Loss: 0.004534656202860093\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007600478478707373\n",
      "Training Loss: 0.006816793396137655\n",
      "Training Loss: 0.006578991569112986\n",
      "Validation Loss: 0.004531818500254303\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007596736976411194\n",
      "Training Loss: 0.0068124525644816455\n",
      "Training Loss: 0.006575391833903268\n",
      "Validation Loss: 0.004529043405546985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007593063145177439\n",
      "Training Loss: 0.006808186549460515\n",
      "Training Loss: 0.006571851009503007\n",
      "Validation Loss: 0.004526322827219252\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007589455329580232\n",
      "Training Loss: 0.006803992333589122\n",
      "Training Loss: 0.006568365351995453\n",
      "Validation Loss: 0.004523658745639612\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007585909420158714\n",
      "Training Loss: 0.006799868680536747\n",
      "Training Loss: 0.006564934465568512\n",
      "Validation Loss: 0.004521048272661655\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007582426054868847\n",
      "Training Loss: 0.006795811839401722\n",
      "Training Loss: 0.0065615552244707945\n",
      "Validation Loss: 0.004518488467163458\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007579000907717273\n",
      "Training Loss: 0.0067918201885186135\n",
      "Training Loss: 0.006558226482011378\n",
      "Validation Loss: 0.004515975729426306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007575632254593075\n",
      "Training Loss: 0.00678789061261341\n",
      "Training Loss: 0.0065549449692480265\n",
      "Validation Loss: 0.004513509130743699\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.0075723168987315145\n",
      "Training Loss: 0.006784020942868665\n",
      "Training Loss: 0.006551710197236389\n",
      "Validation Loss: 0.00451108931401598\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007569054301129654\n",
      "Training Loss: 0.00678020971128717\n",
      "Training Loss: 0.006548520082142204\n",
      "Validation Loss: 0.004508711430348791\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007565842658514157\n",
      "Training Loss: 0.006776453925995156\n",
      "Training Loss: 0.006545371914980933\n",
      "Validation Loss: 0.004506376095827711\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0075626795925199984\n",
      "Training Loss: 0.006772751840762794\n",
      "Training Loss: 0.006542265876196325\n",
      "Validation Loss: 0.004504080054093822\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007559563334798441\n",
      "Training Loss: 0.006769101042300463\n",
      "Training Loss: 0.006539198238169774\n",
      "Validation Loss: 0.004501818554355564\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007556490926072001\n",
      "Training Loss: 0.006765500170877204\n",
      "Training Loss: 0.0065361681091599165\n",
      "Validation Loss: 0.004499599158376707\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.007553461805218831\n",
      "Training Loss: 0.006761947629274801\n",
      "Training Loss: 0.006533174433861859\n",
      "Validation Loss: 0.004497414881891958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007550474746385589\n",
      "Training Loss: 0.006758441916899755\n",
      "Training Loss: 0.006530215197708458\n",
      "Validation Loss: 0.0044952627039891195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.007547527265269309\n",
      "Training Loss: 0.006754979748511687\n",
      "Training Loss: 0.006527289684745483\n",
      "Validation Loss: 0.004493146323797743\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.007544619240798056\n",
      "Training Loss: 0.0067515608272515236\n",
      "Training Loss: 0.006524396523600444\n",
      "Validation Loss: 0.004491058841420944\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.007541746725328266\n",
      "Training Loss: 0.006748183245072141\n",
      "Training Loss: 0.006521533023333177\n",
      "Validation Loss: 0.0044890042653391036\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007538910254370421\n",
      "Training Loss: 0.006744845368666574\n",
      "Training Loss: 0.00651869916706346\n",
      "Validation Loss: 0.004486979569407871\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007536107996711508\n",
      "Training Loss: 0.006741544101387263\n",
      "Training Loss: 0.006515893713221885\n",
      "Validation Loss: 0.0044849829426495715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007533338138600811\n",
      "Training Loss: 0.006738280627178028\n",
      "Training Loss: 0.006513114903355017\n",
      "Validation Loss: 0.004483010904376887\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007530599725432694\n",
      "Training Loss: 0.006735053095035255\n",
      "Training Loss: 0.006510361486580223\n",
      "Validation Loss: 0.004481065361054216\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.0075278918421827256\n",
      "Training Loss: 0.0067318583303131166\n",
      "Training Loss: 0.006507631640415639\n",
      "Validation Loss: 0.004479146186455959\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.007525212689070031\n",
      "Training Loss: 0.0067286968592088665\n",
      "Training Loss: 0.006504926760098897\n",
      "Validation Loss: 0.0044772490771393175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.007522561682853848\n",
      "Training Loss: 0.006725565994856879\n",
      "Training Loss: 0.006502243152935989\n",
      "Validation Loss: 0.004475371439920299\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.0075199368922039865\n",
      "Training Loss: 0.006722466228529811\n",
      "Training Loss: 0.006499581222888082\n",
      "Validation Loss: 0.004473519089678826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.007517337517347187\n",
      "Training Loss: 0.006719394188839942\n",
      "Training Loss: 0.006496939713833853\n",
      "Validation Loss: 0.004471688548343654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007514762762002647\n",
      "Training Loss: 0.006716350474162028\n",
      "Training Loss: 0.006494317039032467\n",
      "Validation Loss: 0.004469875431373674\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.00751221107551828\n",
      "Training Loss: 0.006713334486121312\n",
      "Training Loss: 0.006491713199648075\n",
      "Validation Loss: 0.004468084747897817\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.007509683024836704\n",
      "Training Loss: 0.006710343854501843\n",
      "Training Loss: 0.0064891269349027425\n",
      "Validation Loss: 0.004466309518228923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.0075071758858393876\n",
      "Training Loss: 0.006707377559505403\n",
      "Training Loss: 0.006486557095777243\n",
      "Validation Loss: 0.004464553016384415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.007504689591005444\n",
      "Training Loss: 0.006704434382263571\n",
      "Training Loss: 0.006484003669465892\n",
      "Validation Loss: 0.004462811891826686\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0075022236781660465\n",
      "Training Loss: 0.006701515467138961\n",
      "Training Loss: 0.006481465238612145\n",
      "Validation Loss: 0.004461089086985697\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.007499775959877298\n",
      "Training Loss: 0.006698617783840745\n",
      "Training Loss: 0.00647894182940945\n",
      "Validation Loss: 0.0044593804423027576\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.007497346861055121\n",
      "Training Loss: 0.00669574159081094\n",
      "Training Loss: 0.0064764310838654636\n",
      "Validation Loss: 0.004457688651680737\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007494934998685494\n",
      "Training Loss: 0.0066928859555628155\n",
      "Training Loss: 0.006473934855894186\n",
      "Validation Loss: 0.004456008129146243\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007492540628882125\n",
      "Training Loss: 0.006690049722092226\n",
      "Training Loss: 0.0064714499085675925\n",
      "Validation Loss: 0.004454346462623791\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.007490162366302684\n",
      "Training Loss: 0.006687232389813289\n",
      "Training Loss: 0.006468977055628784\n",
      "Validation Loss: 0.004452696455292027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.007487799290101975\n",
      "Training Loss: 0.006684433141490444\n",
      "Training Loss: 0.0064665156241972\n",
      "Validation Loss: 0.004451056033257772\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.007485450310632587\n",
      "Training Loss: 0.006681650711689144\n",
      "Training Loss: 0.006464063862804324\n",
      "Validation Loss: 0.004449426963025432\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.007483115621143952\n",
      "Training Loss: 0.00667888549156487\n",
      "Training Loss: 0.006461623175418936\n",
      "Validation Loss: 0.004447809728568818\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.007480794308939949\n",
      "Training Loss: 0.006676135964225977\n",
      "Training Loss: 0.006459190908353776\n",
      "Validation Loss: 0.004446205231125645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.007478486769832671\n",
      "Training Loss: 0.006673401972511784\n",
      "Training Loss: 0.0064567683410132306\n",
      "Validation Loss: 0.004444607587798049\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.00747619028785266\n",
      "Training Loss: 0.0066706828121095895\n",
      "Training Loss: 0.006454353433800861\n",
      "Validation Loss: 0.004443021229562465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.007473905618535354\n",
      "Training Loss: 0.006667976906755939\n",
      "Training Loss: 0.006451946282759309\n",
      "Validation Loss: 0.004441441397779108\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.007471632176311686\n",
      "Training Loss: 0.0066652846708893775\n",
      "Training Loss: 0.0064495462161721666\n",
      "Validation Loss: 0.004439873792874637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.007469369188183919\n",
      "Training Loss: 0.00666260571568273\n",
      "Training Loss: 0.006447153481421992\n",
      "Validation Loss: 0.0044383123063147486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.007467116701882333\n",
      "Training Loss: 0.00665993966627866\n",
      "Training Loss: 0.00644476740853861\n",
      "Validation Loss: 0.00443675932003541\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.0074648738361429426\n",
      "Training Loss: 0.006657285389956087\n",
      "Training Loss: 0.006442386676790193\n",
      "Validation Loss: 0.004435214802643724\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00746264003450051\n",
      "Training Loss: 0.0066546424268744886\n",
      "Training Loss: 0.006440012448583729\n",
      "Validation Loss: 0.004433676121060558\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.007460415787063539\n",
      "Training Loss: 0.006652010452235117\n",
      "Training Loss: 0.006437642669770867\n",
      "Validation Loss: 0.004432142039190643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.007458198831882328\n",
      "Training Loss: 0.006649389086524024\n",
      "Training Loss: 0.006435276790289208\n",
      "Validation Loss: 0.004430612681951541\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.007455989926820621\n",
      "Training Loss: 0.006646777063142508\n",
      "Training Loss: 0.006432916344492696\n",
      "Validation Loss: 0.004429092726884723\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.007453788265120238\n",
      "Training Loss: 0.006644175517139956\n",
      "Training Loss: 0.006430559450527653\n",
      "Validation Loss: 0.004427574822166411\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.007451594291487709\n",
      "Training Loss: 0.006641582471784205\n",
      "Training Loss: 0.006428206323180348\n",
      "Validation Loss: 0.004426064113615437\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.007449407373787836\n",
      "Training Loss: 0.006638998518465087\n",
      "Training Loss: 0.006425857483409345\n",
      "Validation Loss: 0.004424555796810709\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.007447226624935866\n",
      "Training Loss: 0.006636423616437241\n",
      "Training Loss: 0.006423511411412619\n",
      "Validation Loss: 0.004423052630402943\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.007445051660761237\n",
      "Training Loss: 0.006633856535190716\n",
      "Training Loss: 0.006421167837688699\n",
      "Validation Loss: 0.004421556193847209\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.007442882742034271\n",
      "Training Loss: 0.006631297307321802\n",
      "Training Loss: 0.006418826885637827\n",
      "Validation Loss: 0.004420061927979414\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.007440719924634322\n",
      "Training Loss: 0.0066287446964997795\n",
      "Training Loss: 0.006416488739778288\n",
      "Validation Loss: 0.004418569609779195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.007438562237657606\n",
      "Training Loss: 0.006626200405880809\n",
      "Training Loss: 0.0064141517510870475\n",
      "Validation Loss: 0.004417081969120445\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.007436409560032189\n",
      "Training Loss: 0.00662366284406744\n",
      "Training Loss: 0.006411816589534282\n",
      "Validation Loss: 0.004415593195047355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.007434261471498758\n",
      "Training Loss: 0.006621130065759644\n",
      "Training Loss: 0.00640948269632645\n",
      "Validation Loss: 0.004414108137762321\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.007432116640266031\n",
      "Training Loss: 0.006618604101240635\n",
      "Training Loss: 0.006407149644801393\n",
      "Validation Loss: 0.004412623037966073\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.007429976314306259\n",
      "Training Loss: 0.006616083144908771\n",
      "Training Loss: 0.006404817379661836\n",
      "Validation Loss: 0.004411142407079342\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.007427839377196505\n",
      "Training Loss: 0.006613567888271063\n",
      "Training Loss: 0.0064024867326952515\n",
      "Validation Loss: 0.004409660651280513\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.0074257058755029\n",
      "Training Loss: 0.006611057969275862\n",
      "Training Loss: 0.006400155366281979\n",
      "Validation Loss: 0.004408180200248915\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.0074235758022405205\n",
      "Training Loss: 0.006608553339028731\n",
      "Training Loss: 0.006397824864252471\n",
      "Validation Loss: 0.004406702478437193\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0074214490421582015\n",
      "Training Loss: 0.00660605315468274\n",
      "Training Loss: 0.006395494096213952\n",
      "Validation Loss: 0.0044052257219570155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.007419324785005301\n",
      "Training Loss: 0.006603557426715269\n",
      "Training Loss: 0.0063931639870861545\n",
      "Validation Loss: 0.004403748093015944\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0074172036908566955\n",
      "Training Loss: 0.0066010653937701134\n",
      "Training Loss: 0.006390832418110222\n",
      "Validation Loss: 0.004402269597500144\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.007415084943640977\n",
      "Training Loss: 0.006598576889373362\n",
      "Training Loss: 0.006388500468456186\n",
      "Validation Loss: 0.004400792958089307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.007412967495620251\n",
      "Training Loss: 0.006596092215040698\n",
      "Training Loss: 0.006386167618911713\n",
      "Validation Loss: 0.004399312482205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.007410851612221449\n",
      "Training Loss: 0.006593610410345718\n",
      "Training Loss: 0.006383833212312311\n",
      "Validation Loss: 0.00439783218813788\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.007408738058293238\n",
      "Training Loss: 0.006591131687164306\n",
      "Training Loss: 0.00638149831269402\n",
      "Validation Loss: 0.004396347279315082\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.007406625046860427\n",
      "Training Loss: 0.006588655693922192\n",
      "Training Loss: 0.006379160683136434\n",
      "Validation Loss: 0.0043948646798321785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.007404514189111069\n",
      "Training Loss: 0.006586181774036959\n",
      "Training Loss: 0.006376822334714234\n",
      "Validation Loss: 0.00439337956564752\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.007402403205633163\n",
      "Training Loss: 0.006583710577106103\n",
      "Training Loss: 0.006374481888487935\n",
      "Validation Loss: 0.004391891811843543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.00740029399166815\n",
      "Training Loss: 0.0065812415594700725\n",
      "Training Loss: 0.006372138927108608\n",
      "Validation Loss: 0.004390404136521805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.007398184352787211\n",
      "Training Loss: 0.006578773916698992\n",
      "Training Loss: 0.00636979476956185\n",
      "Validation Loss: 0.004388909790209714\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.007396076563745737\n",
      "Training Loss: 0.006576308861840516\n",
      "Training Loss: 0.0063674473890569065\n",
      "Validation Loss: 0.0043874132758257595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.007393968089018017\n",
      "Training Loss: 0.006573844326194376\n",
      "Training Loss: 0.006365097776870244\n",
      "Validation Loss: 0.004385915936070254\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.007391860007774084\n",
      "Training Loss: 0.006571381314424798\n",
      "Training Loss: 0.006362745774677023\n",
      "Validation Loss: 0.004384413983521125\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.007389752350281924\n",
      "Training Loss: 0.006568919132696464\n",
      "Training Loss: 0.006360390583286062\n",
      "Validation Loss: 0.004382907448917251\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.007387642906978726\n",
      "Training Loss: 0.0065664578368887306\n",
      "Training Loss: 0.006358032402349636\n",
      "Validation Loss: 0.004381398277310131\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.007385533978231251\n",
      "Training Loss: 0.00656399640138261\n",
      "Training Loss: 0.00635567105084192\n",
      "Validation Loss: 0.0043798846466037746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.00738342386786826\n",
      "Training Loss: 0.006561535397777334\n",
      "Training Loss: 0.006353305742377416\n",
      "Validation Loss: 0.004378366047317644\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.0073813128273468465\n",
      "Training Loss: 0.006559074923861772\n",
      "Training Loss: 0.006350937696406617\n",
      "Validation Loss: 0.004376845741696823\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.007379200590075925\n",
      "Training Loss: 0.006556615832960233\n",
      "Training Loss: 0.006348566446104087\n",
      "Validation Loss: 0.0043753210816197515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.007377089777728543\n",
      "Training Loss: 0.006554156288038939\n",
      "Training Loss: 0.00634619212592952\n",
      "Validation Loss: 0.004373790857151904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.007374975674320012\n",
      "Training Loss: 0.006551695027155802\n",
      "Training Loss: 0.0063438128388952466\n",
      "Validation Loss: 0.004372251875374173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.007372859913157299\n",
      "Training Loss: 0.006549232809338719\n",
      "Training Loss: 0.00634142971248366\n",
      "Validation Loss: 0.004370711146058578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.007370742961065844\n",
      "Training Loss: 0.006546770754503086\n",
      "Training Loss: 0.006339041472529061\n",
      "Validation Loss: 0.004369161577026747\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.007368623458314687\n",
      "Training Loss: 0.006544307437725366\n",
      "Training Loss: 0.006336650144075975\n",
      "Validation Loss: 0.004367610299698172\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.007366502636577934\n",
      "Training Loss: 0.006541843774029985\n",
      "Training Loss: 0.006334254960529506\n",
      "Validation Loss: 0.004366050793506791\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.007364379750797525\n",
      "Training Loss: 0.006539378277957439\n",
      "Training Loss: 0.00633185364655219\n",
      "Validation Loss: 0.004364486309579375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.007362253987230361\n",
      "Training Loss: 0.006536911547882483\n",
      "Training Loss: 0.006329449125914834\n",
      "Validation Loss: 0.004362915673298405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.007360126692801714\n",
      "Training Loss: 0.0065344429994001986\n",
      "Training Loss: 0.006327039035968482\n",
      "Validation Loss: 0.004361338035093642\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.007357995304046199\n",
      "Training Loss: 0.006531972613884136\n",
      "Training Loss: 0.006324624381377362\n",
      "Validation Loss: 0.00435975380176469\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.007355863179545849\n",
      "Training Loss: 0.0065295005217194555\n",
      "Training Loss: 0.006322204477037303\n",
      "Validation Loss: 0.0043581631440104225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.007353727190056816\n",
      "Training Loss: 0.0065270257834345105\n",
      "Training Loss: 0.0063197794504230844\n",
      "Validation Loss: 0.004356562280948954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.007351587250595913\n",
      "Training Loss: 0.006524548964807764\n",
      "Training Loss: 0.006317348787561059\n",
      "Validation Loss: 0.004354956748194239\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0073494446731638165\n",
      "Training Loss: 0.006522070257924497\n",
      "Training Loss: 0.006314913617679849\n",
      "Validation Loss: 0.00435334409121424\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.007347299607936293\n",
      "Training Loss: 0.006519588925875723\n",
      "Training Loss: 0.0063124725938541815\n",
      "Validation Loss: 0.0043517234349319854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.007345150581095368\n",
      "Training Loss: 0.006517105017555877\n",
      "Training Loss: 0.0063100264855893325\n",
      "Validation Loss: 0.004350094130560965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.00734299816773273\n",
      "Training Loss: 0.006514617995126173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [07:47<18:10, 155.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.006307574636884965\n",
      "Validation Loss: 0.004348457826228206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.45002232044935225\n",
      "Training Loss: 0.36459931716322896\n",
      "Training Loss: 0.291213800534606\n",
      "Validation Loss: 0.23297072109881412\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.20058846700936556\n",
      "Training Loss: 0.14581985648721457\n",
      "Training Loss: 0.1094549199193716\n",
      "Validation Loss: 0.08937574322387744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.08073346503078938\n",
      "Training Loss: 0.06986771734431386\n",
      "Training Loss: 0.06668979920446873\n",
      "Validation Loss: 0.06670313820326595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.06593587826937437\n",
      "Training Loss: 0.06374122178182005\n",
      "Training Loss: 0.06318891543895006\n",
      "Validation Loss: 0.06326365529486302\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.06260514348745345\n",
      "Training Loss: 0.06004183236509562\n",
      "Training Loss: 0.05856691472232342\n",
      "Validation Loss: 0.05738602584918563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.055980203356593844\n",
      "Training Loss: 0.05193657041527331\n",
      "Training Loss: 0.048705796590074894\n",
      "Validation Loss: 0.04639419545995051\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.04472147916443646\n",
      "Training Loss: 0.04092102905735374\n",
      "Training Loss: 0.037828211355954405\n",
      "Validation Loss: 0.035924697441331456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.0349811293464154\n",
      "Training Loss: 0.03194715400226414\n",
      "Training Loss: 0.028783730315044522\n",
      "Validation Loss: 0.026636168125263426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.026195759042166172\n",
      "Training Loss: 0.023570091705769302\n",
      "Training Loss: 0.02095029308926314\n",
      "Validation Loss: 0.01972440251913131\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.019999229805544017\n",
      "Training Loss: 0.018116247816942634\n",
      "Training Loss: 0.01633362025488168\n",
      "Validation Loss: 0.015775614972697215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.016630010050721466\n",
      "Training Loss: 0.015187706782016903\n",
      "Training Loss: 0.014069409759249538\n",
      "Validation Loss: 0.013692626669437866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.014894892270676792\n",
      "Training Loss: 0.013564401641488075\n",
      "Training Loss: 0.012756555397063494\n",
      "Validation Loss: 0.012276107625868381\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.013688651216216385\n",
      "Training Loss: 0.012395459357649088\n",
      "Training Loss: 0.011757348617538809\n",
      "Validation Loss: 0.011139596880444984\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.012702435373794287\n",
      "Training Loss: 0.011451581339351834\n",
      "Training Loss: 0.010921378838829696\n",
      "Validation Loss: 0.010175085346622582\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.0118595728604123\n",
      "Training Loss: 0.010659828877542167\n",
      "Training Loss: 0.010205452420050278\n",
      "Validation Loss: 0.009344217349157741\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.011137070676777511\n",
      "Training Loss: 0.009992030374705791\n",
      "Training Loss: 0.009594729610253126\n",
      "Validation Loss: 0.00863098593433963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.01052544462494552\n",
      "Training Loss: 0.009433153131976724\n",
      "Training Loss: 0.009079878749325872\n",
      "Validation Loss: 0.008023671353277698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.010015164595097303\n",
      "Training Loss: 0.008970196432201191\n",
      "Training Loss: 0.0086508087196853\n",
      "Validation Loss: 0.007509921823910783\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.009594287833897397\n",
      "Training Loss: 0.00859003035351634\n",
      "Training Loss: 0.008296425555599854\n",
      "Validation Loss: 0.007076922040223406\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.009249817987438291\n",
      "Training Loss: 0.0082799213193357\n",
      "Training Loss: 0.008005678941262886\n",
      "Validation Loss: 0.006712489736559434\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008969307914376258\n",
      "Training Loss: 0.00802826405619271\n",
      "Training Loss: 0.00776838262565434\n",
      "Validation Loss: 0.006405803067557347\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.008741715488722547\n",
      "Training Loss: 0.007824927606852725\n",
      "Training Loss: 0.007575559711549431\n",
      "Validation Loss: 0.006147623391759195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008557609748095274\n",
      "Training Loss: 0.00766125614522025\n",
      "Training Loss: 0.00741946829133667\n",
      "Validation Loss: 0.005930176589460102\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008409040051046758\n",
      "Training Loss: 0.0075299221219029275\n",
      "Training Loss: 0.007293497693608515\n",
      "Validation Loss: 0.005746975494025463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.008289340031333269\n",
      "Training Loss: 0.007424771847436205\n",
      "Training Loss: 0.007192040340160019\n",
      "Validation Loss: 0.0055925893071473816\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008192935580154881\n",
      "Training Loss: 0.007340653627179563\n",
      "Training Loss: 0.007110368157154881\n",
      "Validation Loss: 0.0054624548425186385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.00811518331291154\n",
      "Training Loss: 0.007273287728894502\n",
      "Training Loss: 0.007044527587713674\n",
      "Validation Loss: 0.005352709278872425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.00805224780109711\n",
      "Training Loss: 0.007219157135114074\n",
      "Training Loss: 0.006991254794411361\n",
      "Validation Loss: 0.005260088568647507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008000999065116048\n",
      "Training Loss: 0.007175402659922839\n",
      "Training Loss: 0.006947884621913545\n",
      "Validation Loss: 0.0051818227021316636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.007958913248730824\n",
      "Training Loss: 0.007139734050724655\n",
      "Training Loss: 0.006912275361828506\n",
      "Validation Loss: 0.00511557529052573\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007923986550886185\n",
      "Training Loss: 0.007110343892127276\n",
      "Training Loss: 0.006882729866774752\n",
      "Validation Loss: 0.005059373572866401\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007894644372863696\n",
      "Training Loss: 0.007085817004553974\n",
      "Training Loss: 0.006857914772699587\n",
      "Validation Loss: 0.0050115513097327416\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007869661012664437\n",
      "Training Loss: 0.007065057726576924\n",
      "Training Loss: 0.006836794358096085\n",
      "Validation Loss: 0.004970719178437433\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007848093138309196\n",
      "Training Loss: 0.007047227561706677\n",
      "Training Loss: 0.006818565895082429\n",
      "Validation Loss: 0.004935712469846345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007829215087695048\n",
      "Training Loss: 0.007031683064997196\n",
      "Training Loss: 0.006802616926142946\n",
      "Validation Loss: 0.004905560272327216\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007812472068471834\n",
      "Training Loss: 0.007017932761227712\n",
      "Training Loss: 0.006788472144398839\n",
      "Validation Loss: 0.004879450366643959\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007797435577958822\n",
      "Training Loss: 0.007005603724392131\n",
      "Training Loss: 0.006775768952211365\n",
      "Validation Loss: 0.004856712853253474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007783778642769903\n",
      "Training Loss: 0.0069944056391250346\n",
      "Training Loss: 0.006764223233330995\n",
      "Validation Loss: 0.0048367882832842934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007771243925672025\n",
      "Training Loss: 0.00698411853518337\n",
      "Training Loss: 0.0067536171310348435\n",
      "Validation Loss: 0.004819212368888299\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007759634566027671\n",
      "Training Loss: 0.006974567711586133\n",
      "Training Loss: 0.0067437773529673\n",
      "Validation Loss: 0.004803589962454241\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007748789773322642\n",
      "Training Loss: 0.006965616252273321\n",
      "Training Loss: 0.006734568886458874\n",
      "Validation Loss: 0.004789608925727479\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007738586121704429\n",
      "Training Loss: 0.006957159896846861\n",
      "Training Loss: 0.006725882691680454\n",
      "Validation Loss: 0.00477699515425464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007728921890957281\n",
      "Training Loss: 0.006949110098648816\n",
      "Training Loss: 0.006717633918742649\n",
      "Validation Loss: 0.004765530515807482\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0077197144133970145\n",
      "Training Loss: 0.0069413984171114865\n",
      "Training Loss: 0.006709749908186496\n",
      "Validation Loss: 0.004755029932260932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007710896944627166\n",
      "Training Loss: 0.006933968708617613\n",
      "Training Loss: 0.006702173377852887\n",
      "Validation Loss: 0.004745340962329188\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007702413293300197\n",
      "Training Loss: 0.006926775580504909\n",
      "Training Loss: 0.0066948584886267785\n",
      "Validation Loss: 0.004736336638753334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007694217694224789\n",
      "Training Loss: 0.006919779125601053\n",
      "Training Loss: 0.006687763263471425\n",
      "Validation Loss: 0.004727900476885562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007686268894467503\n",
      "Training Loss: 0.006912946358788759\n",
      "Training Loss: 0.006680854692822322\n",
      "Validation Loss: 0.0047199441881306215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.0076785341883078215\n",
      "Training Loss: 0.006906252106418833\n",
      "Training Loss: 0.006674104671692476\n",
      "Validation Loss: 0.004712393805593922\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0076709849038161335\n",
      "Training Loss: 0.006899671765277162\n",
      "Training Loss: 0.006667488337843679\n",
      "Validation Loss: 0.004705189728554798\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007663596775382757\n",
      "Training Loss: 0.0068931880465243015\n",
      "Training Loss: 0.006660988372168504\n",
      "Validation Loss: 0.004698277860513648\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.0076563494664151225\n",
      "Training Loss: 0.006886783078080043\n",
      "Training Loss: 0.006654584658099339\n",
      "Validation Loss: 0.004691613987192846\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007649225034983828\n",
      "Training Loss: 0.0068804424116387965\n",
      "Training Loss: 0.006648264190880581\n",
      "Validation Loss: 0.004685159426004616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007642208196921274\n",
      "Training Loss: 0.006874155492987484\n",
      "Training Loss: 0.006642011384246871\n",
      "Validation Loss: 0.004678882937093548\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007635284055722878\n",
      "Training Loss: 0.006867910098517314\n",
      "Training Loss: 0.006635817344649695\n",
      "Validation Loss: 0.004672758392414099\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0076284430676605555\n",
      "Training Loss: 0.006861698373686522\n",
      "Training Loss: 0.0066296710696769874\n",
      "Validation Loss: 0.004666762015771832\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007621673321118579\n",
      "Training Loss: 0.006855509825982153\n",
      "Training Loss: 0.006623563162283972\n",
      "Validation Loss: 0.00466087359014187\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.0076149660581722855\n",
      "Training Loss: 0.0068493399734143165\n",
      "Training Loss: 0.006617485859314911\n",
      "Validation Loss: 0.004655076732356729\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007608313626842573\n",
      "Training Loss: 0.006843181371223182\n",
      "Training Loss: 0.006611431908095255\n",
      "Validation Loss: 0.004649361830958155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007601709652226419\n",
      "Training Loss: 0.006837029503658414\n",
      "Training Loss: 0.00660539660428185\n",
      "Validation Loss: 0.004643715202567701\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007595148141263053\n",
      "Training Loss: 0.006830877067986876\n",
      "Training Loss: 0.006599372774944641\n",
      "Validation Loss: 0.00463812103432216\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.0075886226969305425\n",
      "Training Loss: 0.0068247203435748815\n",
      "Training Loss: 0.006593354876968078\n",
      "Validation Loss: 0.004632575876927108\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007582127320347354\n",
      "Training Loss: 0.006818556060316041\n",
      "Training Loss: 0.00658733872754965\n",
      "Validation Loss: 0.004627073538133853\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.0075756607146468015\n",
      "Training Loss: 0.006812380267074331\n",
      "Training Loss: 0.006581319808610715\n",
      "Validation Loss: 0.004621603892425473\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007569217088166625\n",
      "Training Loss: 0.006806187571492046\n",
      "Training Loss: 0.0065752932202303785\n",
      "Validation Loss: 0.004616160322441144\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007562792064854875\n",
      "Training Loss: 0.006799974654568359\n",
      "Training Loss: 0.00656925457646139\n",
      "Validation Loss: 0.004610738579675639\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007556382979964837\n",
      "Training Loss: 0.006793740019202232\n",
      "Training Loss: 0.006563201344106346\n",
      "Validation Loss: 0.004605335554924239\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0075499851559288796\n",
      "Training Loss: 0.00678747859899886\n",
      "Training Loss: 0.006557128294953145\n",
      "Validation Loss: 0.004599943430047859\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007543598042102531\n",
      "Training Loss: 0.006781187419546768\n",
      "Training Loss: 0.0065510327741503714\n",
      "Validation Loss: 0.004594559906824921\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007537217468488962\n",
      "Training Loss: 0.006774863512255251\n",
      "Training Loss: 0.006544910318916663\n",
      "Validation Loss: 0.004589180039566303\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007530839766841382\n",
      "Training Loss: 0.006768503272905946\n",
      "Training Loss: 0.00653875656076707\n",
      "Validation Loss: 0.004583795891101524\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007524461057037115\n",
      "Training Loss: 0.006762103197397664\n",
      "Training Loss: 0.006532569264527411\n",
      "Validation Loss: 0.004578413554268439\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007518081828020513\n",
      "Training Loss: 0.006755660087801516\n",
      "Training Loss: 0.006526344439480454\n",
      "Validation Loss: 0.004573019666811765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007511695796856657\n",
      "Training Loss: 0.0067491702537518\n",
      "Training Loss: 0.006520076951710507\n",
      "Validation Loss: 0.00456761626861571\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.0075053020019549875\n",
      "Training Loss: 0.0067426290409639475\n",
      "Training Loss: 0.006513763470575213\n",
      "Validation Loss: 0.004562194696549075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007498895485186949\n",
      "Training Loss: 0.006736032980261371\n",
      "Training Loss: 0.006507400893606245\n",
      "Validation Loss: 0.004556751579145661\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.0074924737657420335\n",
      "Training Loss: 0.006729375359136611\n",
      "Training Loss: 0.006500982269062661\n",
      "Validation Loss: 0.004551282027615967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007486033522291109\n",
      "Training Loss: 0.006722650909796357\n",
      "Training Loss: 0.006494505374575965\n",
      "Validation Loss: 0.004545778262408011\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007479569464921951\n",
      "Training Loss: 0.006715851193293929\n",
      "Training Loss: 0.006487963240360841\n",
      "Validation Loss: 0.004540234403240003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007473074707668274\n",
      "Training Loss: 0.006708958133822307\n",
      "Training Loss: 0.006481347159133293\n",
      "Validation Loss: 0.004534632055837075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007466532377293333\n",
      "Training Loss: 0.006701943860389292\n",
      "Training Loss: 0.00647464334324468\n",
      "Validation Loss: 0.004528944338070166\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007459902580594644\n",
      "Training Loss: 0.006694747138535604\n",
      "Training Loss: 0.006467830928158946\n",
      "Validation Loss: 0.0045231190697143505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.00745308036217466\n",
      "Training Loss: 0.0066872301895637065\n",
      "Training Loss: 0.006460866578272544\n",
      "Validation Loss: 0.004517033309649593\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.00744577277568169\n",
      "Training Loss: 0.0066790600994136185\n",
      "Training Loss: 0.006453641693806276\n",
      "Validation Loss: 0.004510362184569868\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007437246181070805\n",
      "Training Loss: 0.006669477972900495\n",
      "Training Loss: 0.006445933919167146\n",
      "Validation Loss: 0.004502363364981341\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007426184710348025\n",
      "Training Loss: 0.006657368271844461\n",
      "Training Loss: 0.0064375868521165105\n",
      "Validation Loss: 0.004492116504704601\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.0074119613599032165\n",
      "Training Loss: 0.006642724490957335\n",
      "Training Loss: 0.006428817394189537\n",
      "Validation Loss: 0.0044796835451931015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007396144268568605\n",
      "Training Loss: 0.00662711282260716\n",
      "Training Loss: 0.0064195050828857346\n",
      "Validation Loss: 0.00446594021351108\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007380163496127352\n",
      "Training Loss: 0.006611250812420621\n",
      "Training Loss: 0.006409151277621277\n",
      "Validation Loss: 0.0044513254974487375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007364058683160693\n",
      "Training Loss: 0.006594948049169034\n",
      "Training Loss: 0.006397587734390982\n",
      "Validation Loss: 0.004435992258825873\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007347629969008267\n",
      "Training Loss: 0.006578165499959141\n",
      "Training Loss: 0.006384937982074916\n",
      "Validation Loss: 0.00442032204166557\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007331008312758058\n",
      "Training Loss: 0.006561248059151694\n",
      "Training Loss: 0.006371491168392822\n",
      "Validation Loss: 0.004404918543779909\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.00731459581060335\n",
      "Training Loss: 0.006544709139270708\n",
      "Training Loss: 0.0063576229143654924\n",
      "Validation Loss: 0.0043903336712431285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007298825258621946\n",
      "Training Loss: 0.006528956970432773\n",
      "Training Loss: 0.00634369806793984\n",
      "Validation Loss: 0.004376839325119731\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0072839657543227075\n",
      "Training Loss: 0.0065141657332424075\n",
      "Training Loss: 0.006329980899463408\n",
      "Validation Loss: 0.004364419634338845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.0072700777574209495\n",
      "Training Loss: 0.006500305141089484\n",
      "Training Loss: 0.0063166060944786296\n",
      "Validation Loss: 0.004352910693321544\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.0072570691932924095\n",
      "Training Loss: 0.006487226923927665\n",
      "Training Loss: 0.006303603125852533\n",
      "Validation Loss: 0.004342080346371434\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007244773809215985\n",
      "Training Loss: 0.006474735789233819\n",
      "Training Loss: 0.006290922734769992\n",
      "Validation Loss: 0.004331709389977702\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0072329948242986575\n",
      "Training Loss: 0.006462627566652373\n",
      "Training Loss: 0.006278474543360062\n",
      "Validation Loss: 0.004321591273274566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007221539656748064\n",
      "Training Loss: 0.00645071581355296\n",
      "Training Loss: 0.0062661457428475845\n",
      "Validation Loss: 0.004311557678019188\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00721022614336107\n",
      "Training Loss: 0.006438821132760495\n",
      "Training Loss: 0.006253808939945884\n",
      "Validation Loss: 0.004301442090406219\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007198883931851014\n",
      "Training Loss: 0.006426778883906081\n",
      "Training Loss: 0.006241325808223337\n",
      "Validation Loss: 0.004291097759112214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007187346097780391\n",
      "Training Loss: 0.006414421870140359\n",
      "Training Loss: 0.0062285431427881125\n",
      "Validation Loss: 0.00428037582621962\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007175445937318728\n",
      "Training Loss: 0.006401577095966786\n",
      "Training Loss: 0.006215283343335614\n",
      "Validation Loss: 0.004269120391337934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007163000844884664\n",
      "Training Loss: 0.006388049757806585\n",
      "Training Loss: 0.006201337829115801\n",
      "Validation Loss: 0.0042571518145072575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007149809001712129\n",
      "Training Loss: 0.00637361467233859\n",
      "Training Loss: 0.006186452611000277\n",
      "Validation Loss: 0.004244269991523764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007135645050439053\n",
      "Training Loss: 0.0063580221845768396\n",
      "Training Loss: 0.006170332159381359\n",
      "Validation Loss: 0.0042302639621278545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.007120272271567956\n",
      "Training Loss: 0.006341006502043456\n",
      "Training Loss: 0.006152655046898872\n",
      "Validation Loss: 0.004214933331731414\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007103482132661157\n",
      "Training Loss: 0.006322345731314271\n",
      "Training Loss: 0.006133136834250763\n",
      "Validation Loss: 0.004198172877233977\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.007085187782649882\n",
      "Training Loss: 0.006301970918430016\n",
      "Training Loss: 0.006111670458922163\n",
      "Validation Loss: 0.004180064672745471\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.0070655531313968825\n",
      "Training Loss: 0.006280115224653855\n",
      "Training Loss: 0.006088504220824689\n",
      "Validation Loss: 0.004160946098370791\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0070450779056409375\n",
      "Training Loss: 0.006257389906095341\n",
      "Training Loss: 0.006064346031052992\n",
      "Validation Loss: 0.004141353212657981\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.0070244895509677\n",
      "Training Loss: 0.006234621581388638\n",
      "Training Loss: 0.006040176509413868\n",
      "Validation Loss: 0.004121746902558116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007004407250205986\n",
      "Training Loss: 0.0062124934268649665\n",
      "Training Loss: 0.006016831401502713\n",
      "Validation Loss: 0.004102348649529007\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006985051955562085\n",
      "Training Loss: 0.006191271004499867\n",
      "Training Loss: 0.005994686645572074\n",
      "Validation Loss: 0.004083148977494265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006966275678714737\n",
      "Training Loss: 0.006170861352002248\n",
      "Training Loss: 0.005973700975882821\n",
      "Validation Loss: 0.00406404487328248\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006947789989062585\n",
      "Training Loss: 0.006151023640995845\n",
      "Training Loss: 0.005953617292107083\n",
      "Validation Loss: 0.004044921340792325\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.0069293428357923405\n",
      "Training Loss: 0.006131519264308736\n",
      "Training Loss: 0.005934143181075342\n",
      "Validation Loss: 0.004025680177813668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006910765130887739\n",
      "Training Loss: 0.006112159454496577\n",
      "Training Loss: 0.005915023685083724\n",
      "Validation Loss: 0.004006208751868624\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006891956541803666\n",
      "Training Loss: 0.006092813170980662\n",
      "Training Loss: 0.005896058994112536\n",
      "Validation Loss: 0.003986407280637977\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006872857330017723\n",
      "Training Loss: 0.0060733844677452\n",
      "Training Loss: 0.005877103679813445\n",
      "Validation Loss: 0.003966179390547776\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006853429823531769\n",
      "Training Loss: 0.00605380728491582\n",
      "Training Loss: 0.005858054546406493\n",
      "Validation Loss: 0.003945424189409136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0068336515838745985\n",
      "Training Loss: 0.006034039313672111\n",
      "Training Loss: 0.0058388434414519\n",
      "Validation Loss: 0.0039240970090554834\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006813518639537506\n",
      "Training Loss: 0.006014071424724534\n",
      "Training Loss: 0.005819439521874301\n",
      "Validation Loss: 0.0039021612506024957\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006793045761878602\n",
      "Training Loss: 0.005993918731110171\n",
      "Training Loss: 0.005799844031571411\n",
      "Validation Loss: 0.0038796386588626445\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006772279751603491\n",
      "Training Loss: 0.005973630305379629\n",
      "Training Loss: 0.005780095053196419\n",
      "Validation Loss: 0.003856596302141676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.00675129588576965\n",
      "Training Loss: 0.005953289936878718\n",
      "Training Loss: 0.005760265295102727\n",
      "Validation Loss: 0.0038331353342359402\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.0067302032263251025\n",
      "Training Loss: 0.00593301463406533\n",
      "Training Loss: 0.005740460507804528\n",
      "Validation Loss: 0.0038094281603508954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006709147241781466\n",
      "Training Loss: 0.005912952985381708\n",
      "Training Loss: 0.005720819279667921\n",
      "Validation Loss: 0.003785677874863692\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006688294763443991\n",
      "Training Loss: 0.005893273565452546\n",
      "Training Loss: 0.005701496207038872\n",
      "Validation Loss: 0.003762130862954753\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006667835385305807\n",
      "Training Loss: 0.0058741574117448184\n",
      "Training Loss: 0.005682655895652715\n",
      "Validation Loss: 0.003739039261898633\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006647952878265642\n",
      "Training Loss: 0.005855776314856485\n",
      "Training Loss: 0.005664457881939597\n",
      "Validation Loss: 0.0037166559538971424\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.0066288183757569645\n",
      "Training Loss: 0.005838278231094591\n",
      "Training Loss: 0.005647032567940187\n",
      "Validation Loss: 0.00369520860088445\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006610568634350784\n",
      "Training Loss: 0.0058217757090460506\n",
      "Training Loss: 0.005630479944811668\n",
      "Validation Loss: 0.0036748745851480306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006593293877085671\n",
      "Training Loss: 0.005806332046049647\n",
      "Training Loss: 0.005614855555759277\n",
      "Validation Loss: 0.003655773844554225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.00657704146229662\n",
      "Training Loss: 0.005791963262599893\n",
      "Training Loss: 0.00560017245617928\n",
      "Validation Loss: 0.0036379719627864155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00656180981954094\n",
      "Training Loss: 0.005778644125093706\n",
      "Training Loss: 0.005586406300426461\n",
      "Validation Loss: 0.0036214711448685215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006547559862956405\n",
      "Training Loss: 0.005766312952619046\n",
      "Training Loss: 0.005573503978375811\n",
      "Validation Loss: 0.0036062300761146577\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006534224464558065\n",
      "Training Loss: 0.005754888764931821\n",
      "Training Loss: 0.005561393415846396\n",
      "Validation Loss: 0.0035921818442025293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006521721777389757\n",
      "Training Loss: 0.005744275962933898\n",
      "Training Loss: 0.005549995429755654\n",
      "Validation Loss: 0.0035792259728991208\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.00650996180716902\n",
      "Training Loss: 0.005734379213536158\n",
      "Training Loss: 0.0055392264123656785\n",
      "Validation Loss: 0.0035672633424061193\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006498856103280559\n",
      "Training Loss: 0.005725102992728353\n",
      "Training Loss: 0.005529005351709202\n",
      "Validation Loss: 0.003556182100162894\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006488321109209209\n",
      "Training Loss: 0.005716364525142126\n",
      "Training Loss: 0.005519263752212282\n",
      "Validation Loss: 0.0035458760740879\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006478282164316625\n",
      "Training Loss: 0.005708086612867191\n",
      "Training Loss: 0.005509935669833794\n",
      "Validation Loss: 0.0035362507725458887\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.00646867455157917\n",
      "Training Loss: 0.005700204987078905\n",
      "Training Loss: 0.005500966747640632\n",
      "Validation Loss: 0.0035272159543688923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006459440804901533\n",
      "Training Loss: 0.005692664075177163\n",
      "Training Loss: 0.005492311095586047\n",
      "Validation Loss: 0.0035186921772180817\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006450534808100201\n",
      "Training Loss: 0.0056854180514346805\n",
      "Training Loss: 0.0054839291513781065\n",
      "Validation Loss: 0.003510605023907016\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006441913049202413\n",
      "Training Loss: 0.005678425463847816\n",
      "Training Loss: 0.005475785132439341\n",
      "Validation Loss: 0.0035028946242593438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006433542133891024\n",
      "Training Loss: 0.005671654412290081\n",
      "Training Loss: 0.0054678538223379295\n",
      "Validation Loss: 0.003495507622778604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.0064253938500769435\n",
      "Training Loss: 0.005665077621815726\n",
      "Training Loss: 0.005460108391707763\n",
      "Validation Loss: 0.003488394552436291\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006417443850077689\n",
      "Training Loss: 0.0056586725881788875\n",
      "Training Loss: 0.005452530844777357\n",
      "Validation Loss: 0.0034815178879306474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0064096706110285595\n",
      "Training Loss: 0.005652418573154136\n",
      "Training Loss: 0.005445104002137669\n",
      "Validation Loss: 0.003474843164113235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.0064020580012584104\n",
      "Training Loss: 0.005646300266962498\n",
      "Training Loss: 0.00543781333748484\n",
      "Validation Loss: 0.003468343542347233\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.0063945900730323045\n",
      "Training Loss: 0.00564030326786451\n",
      "Training Loss: 0.005430646444146987\n",
      "Validation Loss: 0.00346199000905676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.0063872527430066835\n",
      "Training Loss: 0.005634415830718354\n",
      "Training Loss: 0.005423591939907055\n",
      "Validation Loss: 0.0034557660365362097\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.00638003706233576\n",
      "Training Loss: 0.005628626244142652\n",
      "Training Loss: 0.005416641943738796\n",
      "Validation Loss: 0.0034496487138578353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006372933042584918\n",
      "Training Loss: 0.005622927759541199\n",
      "Training Loss: 0.005409787919779774\n",
      "Validation Loss: 0.003443628487444033\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.00636593303992413\n",
      "Training Loss: 0.005617311822716147\n",
      "Training Loss: 0.005403022782411426\n",
      "Validation Loss: 0.003437692875675648\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006359028713195585\n",
      "Training Loss: 0.005611772416159511\n",
      "Training Loss: 0.0053963418144849125\n",
      "Validation Loss: 0.003431826621932344\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006352214468643069\n",
      "Training Loss: 0.005606302232481539\n",
      "Training Loss: 0.005389738860249054\n",
      "Validation Loss: 0.003426025289351518\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006345485575147905\n",
      "Training Loss: 0.0056008979305624965\n",
      "Training Loss: 0.005383209652791265\n",
      "Validation Loss: 0.0034202768472598845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006338836205541157\n",
      "Training Loss: 0.005595553942257538\n",
      "Training Loss: 0.005376749304123223\n",
      "Validation Loss: 0.003414582683485043\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006332263190415688\n",
      "Training Loss: 0.0055902647017501295\n",
      "Training Loss: 0.005370355128252413\n",
      "Validation Loss: 0.0034089307810880997\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006325760658364743\n",
      "Training Loss: 0.005585029559442773\n",
      "Training Loss: 0.0053640239193919114\n",
      "Validation Loss: 0.003403319467089317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006319328323588707\n",
      "Training Loss: 0.005579844660824165\n",
      "Training Loss: 0.005357751786941662\n",
      "Validation Loss: 0.0033977463137714213\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006312961363000795\n",
      "Training Loss: 0.005574706569314003\n",
      "Training Loss: 0.00535153823235305\n",
      "Validation Loss: 0.003392207020961711\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006306656068773009\n",
      "Training Loss: 0.005569612281396985\n",
      "Training Loss: 0.005345379537320696\n",
      "Validation Loss: 0.0033866972224315985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006300413958961144\n",
      "Training Loss: 0.0055645602918229994\n",
      "Training Loss: 0.005339273960271384\n",
      "Validation Loss: 0.003381214640888103\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006294228839688003\n",
      "Training Loss: 0.005559549042955041\n",
      "Training Loss: 0.00533321993163554\n",
      "Validation Loss: 0.003375765341046277\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006288102157996036\n",
      "Training Loss: 0.005554577128496021\n",
      "Training Loss: 0.00532721607160056\n",
      "Validation Loss: 0.0033703413445324627\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006282029640278779\n",
      "Training Loss: 0.005549641348188743\n",
      "Training Loss: 0.005321260092023295\n",
      "Validation Loss: 0.003364945964605202\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.0062760108435759325\n",
      "Training Loss: 0.005544741979101673\n",
      "Training Loss: 0.005315351581957657\n",
      "Validation Loss: 0.0033595751712015005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006270045206183568\n",
      "Training Loss: 0.005539876490365714\n",
      "Training Loss: 0.0053094905082252805\n",
      "Validation Loss: 0.003354230024485608\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006264131696661934\n",
      "Training Loss: 0.005535044525749981\n",
      "Training Loss: 0.005303674495953601\n",
      "Validation Loss: 0.0033489092759359\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.00625826847506687\n",
      "Training Loss: 0.005530244617257267\n",
      "Training Loss: 0.005297902898455505\n",
      "Validation Loss: 0.0033436151786466663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006252454018685967\n",
      "Training Loss: 0.005525477275950834\n",
      "Training Loss: 0.0052921753580449146\n",
      "Validation Loss: 0.003338346015164901\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006246688869432546\n",
      "Training Loss: 0.005520739834755659\n",
      "Training Loss: 0.005286490587168373\n",
      "Validation Loss: 0.0033330992204294112\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.0062409710139036175\n",
      "Training Loss: 0.005516032279701904\n",
      "Training Loss: 0.00528084893070627\n",
      "Validation Loss: 0.00332788072704348\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006235300782136619\n",
      "Training Loss: 0.005511354353511706\n",
      "Training Loss: 0.005275249460537452\n",
      "Validation Loss: 0.0033226846743458775\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006229676379007287\n",
      "Training Loss: 0.005506705265725031\n",
      "Training Loss: 0.005269690964196343\n",
      "Validation Loss: 0.0033175183847295436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006224099027458578\n",
      "Training Loss: 0.005502085760235786\n",
      "Training Loss: 0.0052641751177725385\n",
      "Validation Loss: 0.0033123759210130638\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006218566911993548\n",
      "Training Loss: 0.0054974936845246705\n",
      "Training Loss: 0.005258700021659024\n",
      "Validation Loss: 0.0033072610915471077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006213079657754861\n",
      "Training Loss: 0.005492930208565667\n",
      "Training Loss: 0.005253265796927736\n",
      "Validation Loss: 0.0033021757249678536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.0062076374335447325\n",
      "Training Loss: 0.005488393362611532\n",
      "Training Loss: 0.005247872697655111\n",
      "Validation Loss: 0.003297114554463017\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006202238949481398\n",
      "Training Loss: 0.005483884526183829\n",
      "Training Loss: 0.005242519264866133\n",
      "Validation Loss: 0.003292083540758683\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006196884892415255\n",
      "Training Loss: 0.005479402566561475\n",
      "Training Loss: 0.005237207403115463\n",
      "Validation Loss: 0.0032870845602343927\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006191573585965671\n",
      "Training Loss: 0.005474947727052495\n",
      "Training Loss: 0.0052319353367784064\n",
      "Validation Loss: 0.0032821139516936763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006186306470190175\n",
      "Training Loss: 0.0054705194698181\n",
      "Training Loss: 0.005226702747459058\n",
      "Validation Loss: 0.003277170961707035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.00618108116905205\n",
      "Training Loss: 0.005466117538744584\n",
      "Training Loss: 0.005221510374103673\n",
      "Validation Loss: 0.0032722585274722804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006175898652290925\n",
      "Training Loss: 0.005461742531042546\n",
      "Training Loss: 0.0052163572536665015\n",
      "Validation Loss: 0.003267379527979562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.00617075904738158\n",
      "Training Loss: 0.005457394124241546\n",
      "Training Loss: 0.005211245435639285\n",
      "Validation Loss: 0.0032625324655261435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006165659723919816\n",
      "Training Loss: 0.005453072247328237\n",
      "Training Loss: 0.005206173031474463\n",
      "Validation Loss: 0.0032577186573056107\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.0061606040259357545\n",
      "Training Loss: 0.005448777171550318\n",
      "Training Loss: 0.0052011411587591285\n",
      "Validation Loss: 0.003252940965303628\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006155589746194892\n",
      "Training Loss: 0.005444508671062067\n",
      "Training Loss: 0.005196149614057503\n",
      "Validation Loss: 0.0032481958375448422\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.00615061660471838\n",
      "Training Loss: 0.005440266110235825\n",
      "Training Loss: 0.0051911969669163225\n",
      "Validation Loss: 0.003243484622615735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.0061456843523774295\n",
      "Training Loss: 0.005436049766140059\n",
      "Training Loss: 0.005186285837262403\n",
      "Validation Loss: 0.0032388074748647094\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0061407914530718696\n",
      "Training Loss: 0.005431859529344365\n",
      "Training Loss: 0.005181412408419419\n",
      "Validation Loss: 0.003234163952175151\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006135938435909339\n",
      "Training Loss: 0.005427695086691529\n",
      "Training Loss: 0.005176580589031801\n",
      "Validation Loss: 0.0032295557281808154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006131127736298367\n",
      "Training Loss: 0.005423557736212388\n",
      "Training Loss: 0.0051717870807624425\n",
      "Validation Loss: 0.0032249868669536677\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006126354582374915\n",
      "Training Loss: 0.0054194468318019065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [10:21<15:30, 155.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.005167034551559482\n",
      "Validation Loss: 0.003220455083352503\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.6048874482512474\n",
      "Training Loss: 0.5269002915918827\n",
      "Training Loss: 0.45123921558260915\n",
      "Validation Loss: 0.37488507764058165\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.3346602816879749\n",
      "Training Loss: 0.2508139880001545\n",
      "Training Loss: 0.1853283302485943\n",
      "Validation Loss: 0.14091514170253544\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.1236425793543458\n",
      "Training Loss: 0.09548830352723599\n",
      "Training Loss: 0.07995226563885809\n",
      "Validation Loss: 0.07336226509611928\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.06981799507513642\n",
      "Training Loss: 0.0641472278535366\n",
      "Training Loss: 0.061883536148816344\n",
      "Validation Loss: 0.06210045518583796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.06077550640329719\n",
      "Training Loss: 0.058260938767343756\n",
      "Training Loss: 0.056630344558507205\n",
      "Validation Loss: 0.05594439333660549\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.0541702021099627\n",
      "Training Loss: 0.05053418515250087\n",
      "Training Loss: 0.04720926994457841\n",
      "Validation Loss: 0.04460607190731536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.042360792737454175\n",
      "Training Loss: 0.038003216618672016\n",
      "Training Loss: 0.03425702431239188\n",
      "Validation Loss: 0.03217860413819886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.031080336230807006\n",
      "Training Loss: 0.028055928228423\n",
      "Training Loss: 0.02528746801894158\n",
      "Validation Loss: 0.024109801120553793\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.023881725883111358\n",
      "Training Loss: 0.021513114874251185\n",
      "Training Loss: 0.01948124460875988\n",
      "Validation Loss: 0.018701986226598532\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.019116362622007728\n",
      "Training Loss: 0.017271845834329724\n",
      "Training Loss: 0.015716291298158467\n",
      "Validation Loss: 0.014967266302765085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.015873344659339636\n",
      "Training Loss: 0.014441728065721691\n",
      "Training Loss: 0.013341304739005863\n",
      "Validation Loss: 0.01265649149309467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.013938695848919451\n",
      "Training Loss: 0.012790408991277218\n",
      "Training Loss: 0.011950455924961715\n",
      "Validation Loss: 0.011205024122563976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.012722693998366594\n",
      "Training Loss: 0.011689058204647154\n",
      "Training Loss: 0.010952223411295562\n",
      "Validation Loss: 0.010088958566893269\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.011786550660617649\n",
      "Training Loss: 0.010811576317064465\n",
      "Training Loss: 0.01013939016032964\n",
      "Validation Loss: 0.009154273075407391\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.011014261904638261\n",
      "Training Loss: 0.01008021884597838\n",
      "Training Loss: 0.00946696771774441\n",
      "Validation Loss: 0.008364220029475648\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.01037526993197389\n",
      "Training Loss: 0.009473124688956886\n",
      "Training Loss: 0.008916873447597028\n",
      "Validation Loss: 0.007702989158419411\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.009852969700004906\n",
      "Training Loss: 0.008977077915333212\n",
      "Training Loss: 0.008473688265075907\n",
      "Validation Loss: 0.007156653353274705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.009432165543548763\n",
      "Training Loss: 0.00857855619629845\n",
      "Training Loss: 0.00812145671574399\n",
      "Validation Loss: 0.006710109115170127\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.009097372654359788\n",
      "Training Loss: 0.008262895561056211\n",
      "Training Loss: 0.007844311129301786\n",
      "Validation Loss: 0.006347736683831121\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.00883344847126864\n",
      "Training Loss: 0.008015365758910774\n",
      "Training Loss: 0.007627532806945964\n",
      "Validation Loss: 0.006054638336893966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008626474401680753\n",
      "Training Loss: 0.007822340208804235\n",
      "Training Loss: 0.007458287136396393\n",
      "Validation Loss: 0.005817600285236755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.008464355914620683\n",
      "Training Loss: 0.007671995324781165\n",
      "Training Loss: 0.007325871490174904\n",
      "Validation Loss: 0.00562545766640622\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008337018601596356\n",
      "Training Loss: 0.007554534799419343\n",
      "Training Loss: 0.007221652996959165\n",
      "Validation Loss: 0.005469083411912067\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008236326852347702\n",
      "Training Loss: 0.007462091721827165\n",
      "Training Loss: 0.007138816203223541\n",
      "Validation Loss: 0.005341135159009293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0081558533385396\n",
      "Training Loss: 0.007388483867980539\n",
      "Training Loss: 0.007072069920832291\n",
      "Validation Loss: 0.00523574074097187\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.0080906024586875\n",
      "Training Loss: 0.007328932745149359\n",
      "Training Loss: 0.0070173628290649505\n",
      "Validation Loss: 0.00514822263987421\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008036751650506631\n",
      "Training Loss: 0.007279804851859808\n",
      "Training Loss: 0.006971636349335313\n",
      "Validation Loss: 0.005074878199695704\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007991420170292259\n",
      "Training Loss: 0.0072383845364674924\n",
      "Training Loss: 0.006932616699486971\n",
      "Validation Loss: 0.00501278376081184\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.00795246803550981\n",
      "Training Loss: 0.007202670986298471\n",
      "Training Loss: 0.006898636254481972\n",
      "Validation Loss: 0.0049596542356473\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.00791832646355033\n",
      "Training Loss: 0.007171214198460802\n",
      "Training Loss: 0.0068684863077942285\n",
      "Validation Loss: 0.004913711911224331\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.00788785947370343\n",
      "Training Loss: 0.007142973820446059\n",
      "Training Loss: 0.0068412936711683865\n",
      "Validation Loss: 0.00487357050735043\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007860243876930326\n",
      "Training Loss: 0.00711720640421845\n",
      "Training Loss: 0.006816431302577257\n",
      "Validation Loss: 0.004838160342495009\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007834888444049283\n",
      "Training Loss: 0.00709338468965143\n",
      "Training Loss: 0.006793447746895253\n",
      "Validation Loss: 0.004806643659301269\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007811363494256512\n",
      "Training Loss: 0.007071132314158604\n",
      "Training Loss: 0.006772014064481482\n",
      "Validation Loss: 0.004778368203649611\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.0077893575001507995\n",
      "Training Loss: 0.007050179557409137\n",
      "Training Loss: 0.00675188923603855\n",
      "Validation Loss: 0.004752820918174314\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007768636805703864\n",
      "Training Loss: 0.00703032745514065\n",
      "Training Loss: 0.006732892015716061\n",
      "Validation Loss: 0.004729584067421599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007749029938131571\n",
      "Training Loss: 0.007011433553416282\n",
      "Training Loss: 0.0067148861521855\n",
      "Validation Loss: 0.004708335712667178\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007730404871981591\n",
      "Training Loss: 0.0069933892192784695\n",
      "Training Loss: 0.006697762307012454\n",
      "Validation Loss: 0.004688805500254705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007712655569193884\n",
      "Training Loss: 0.006976108668604866\n",
      "Training Loss: 0.006681435216451064\n",
      "Validation Loss: 0.004670778563471113\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007695700587937608\n",
      "Training Loss: 0.0069595279218629\n",
      "Training Loss: 0.006665834534214809\n",
      "Validation Loss: 0.0046540659909879555\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.0076794711372349415\n",
      "Training Loss: 0.006943590731825679\n",
      "Training Loss: 0.006650897694053129\n",
      "Validation Loss: 0.004638520990932632\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007663908859249204\n",
      "Training Loss: 0.006928250486962498\n",
      "Training Loss: 0.006636573121650145\n",
      "Validation Loss: 0.004624013425363751\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007648963776882738\n",
      "Training Loss: 0.0069134675234090535\n",
      "Training Loss: 0.006622813803842291\n",
      "Validation Loss: 0.00461042187113859\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007634589477675036\n",
      "Training Loss: 0.00689920453238301\n",
      "Training Loss: 0.00660957645624876\n",
      "Validation Loss: 0.0045976574516991215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007620746467728168\n",
      "Training Loss: 0.006885429542744532\n",
      "Training Loss: 0.006596822979627177\n",
      "Validation Loss: 0.004585632551435298\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007607398560503497\n",
      "Training Loss: 0.006872111961711198\n",
      "Training Loss: 0.006584516509319655\n",
      "Validation Loss: 0.004574272145904349\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007594510695198551\n",
      "Training Loss: 0.006859221637714654\n",
      "Training Loss: 0.006572624739492312\n",
      "Validation Loss: 0.004563512409425058\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007582052107900381\n",
      "Training Loss: 0.006846731816185638\n",
      "Training Loss: 0.006561114723444917\n",
      "Validation Loss: 0.004553287697490305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007569991769269108\n",
      "Training Loss: 0.0068346162349917\n",
      "Training Loss: 0.006549958104151301\n",
      "Validation Loss: 0.004543550672229338\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0075583020027261225\n",
      "Training Loss: 0.006822851018514484\n",
      "Training Loss: 0.006539126408752054\n",
      "Validation Loss: 0.004534252997084908\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007546956049045548\n",
      "Training Loss: 0.006811411374947056\n",
      "Training Loss: 0.006528594910050743\n",
      "Validation Loss: 0.004525351200090575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007535929841687903\n",
      "Training Loss: 0.00680027462891303\n",
      "Training Loss: 0.006518338707392104\n",
      "Validation Loss: 0.004516804985848538\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00752519940957427\n",
      "Training Loss: 0.006789420165587217\n",
      "Training Loss: 0.0065083355794195085\n",
      "Validation Loss: 0.004508584427772864\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007514742963248864\n",
      "Training Loss: 0.0067788261454552416\n",
      "Training Loss: 0.006498564116773195\n",
      "Validation Loss: 0.004500647805359089\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007504538900684565\n",
      "Training Loss: 0.006768476291326806\n",
      "Training Loss: 0.006489006478805095\n",
      "Validation Loss: 0.004492980955833088\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007494570986600593\n",
      "Training Loss: 0.006758349608862773\n",
      "Training Loss: 0.006479644933133386\n",
      "Validation Loss: 0.004485550594877117\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007484819147503003\n",
      "Training Loss: 0.00674843109678477\n",
      "Training Loss: 0.006470462864963338\n",
      "Validation Loss: 0.004478335030654215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.00747526794904843\n",
      "Training Loss: 0.0067387050006072965\n",
      "Training Loss: 0.006461445658351295\n",
      "Validation Loss: 0.004471316207671182\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.0074659019720274954\n",
      "Training Loss: 0.006729155796347186\n",
      "Training Loss: 0.006452579131582752\n",
      "Validation Loss: 0.0044644714471768964\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.0074567063699942085\n",
      "Training Loss: 0.006719770720228553\n",
      "Training Loss: 0.006443851914955303\n",
      "Validation Loss: 0.004457788916011707\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.0074476698611397295\n",
      "Training Loss: 0.0067105368815828115\n",
      "Training Loss: 0.006435251778457314\n",
      "Validation Loss: 0.004451246887675665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007438779352232814\n",
      "Training Loss: 0.006701442103367299\n",
      "Training Loss: 0.006426769311656244\n",
      "Validation Loss: 0.00444483623076021\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007430024339118973\n",
      "Training Loss: 0.006692477710312232\n",
      "Training Loss: 0.0064183954254258425\n",
      "Validation Loss: 0.004438548581674695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007421396251302213\n",
      "Training Loss: 0.0066836338548455386\n",
      "Training Loss: 0.0064101219904841855\n",
      "Validation Loss: 0.004432368926766715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007412885612575337\n",
      "Training Loss: 0.006674900645157323\n",
      "Training Loss: 0.0064019415498478335\n",
      "Validation Loss: 0.004426286226201258\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007404484021244571\n",
      "Training Loss: 0.006666270560817793\n",
      "Training Loss: 0.006393847433500923\n",
      "Validation Loss: 0.004420293179822018\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007396184757817536\n",
      "Training Loss: 0.006657736951019615\n",
      "Training Loss: 0.006385833643143997\n",
      "Validation Loss: 0.004414377983900185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007387981462525204\n",
      "Training Loss: 0.006649293837836012\n",
      "Training Loss: 0.006377895753830671\n",
      "Validation Loss: 0.004408544448748482\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007379868066636846\n",
      "Training Loss: 0.006640935288742184\n",
      "Training Loss: 0.006370028117671609\n",
      "Validation Loss: 0.004402778696744923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007371838978724554\n",
      "Training Loss: 0.0066326551174279305\n",
      "Training Loss: 0.006362227138597518\n",
      "Validation Loss: 0.004397072303475121\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0073638906190171835\n",
      "Training Loss: 0.006624449284281582\n",
      "Training Loss: 0.0063544892554637045\n",
      "Validation Loss: 0.004391425230351978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007356017593992874\n",
      "Training Loss: 0.00661631470778957\n",
      "Training Loss: 0.006346810709801503\n",
      "Validation Loss: 0.004385828416600949\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.0073482163739390675\n",
      "Training Loss: 0.006608246851246804\n",
      "Training Loss: 0.006339189158170484\n",
      "Validation Loss: 0.004380281501203734\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007340484608430415\n",
      "Training Loss: 0.006600241577252745\n",
      "Training Loss: 0.006331622165744193\n",
      "Validation Loss: 0.004374775387414763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007332818408031017\n",
      "Training Loss: 0.006592296893941239\n",
      "Training Loss: 0.0063241067610215395\n",
      "Validation Loss: 0.0043693076566730235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.0073252151510678236\n",
      "Training Loss: 0.006584410096984357\n",
      "Training Loss: 0.0063166413432918485\n",
      "Validation Loss: 0.004363877407740802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007317671154160052\n",
      "Training Loss: 0.006576579314423725\n",
      "Training Loss: 0.006309224062715657\n",
      "Validation Loss: 0.004358480146201851\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007310185957467183\n",
      "Training Loss: 0.006568801193498075\n",
      "Training Loss: 0.006301853175391443\n",
      "Validation Loss: 0.004353112216091851\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007302756188437343\n",
      "Training Loss: 0.006561074471101165\n",
      "Training Loss: 0.0062945269083138555\n",
      "Validation Loss: 0.004347772328994038\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007295380613068119\n",
      "Training Loss: 0.006553397080861032\n",
      "Training Loss: 0.00628724355250597\n",
      "Validation Loss: 0.004342457845943112\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007288057170808315\n",
      "Training Loss: 0.006545767694478855\n",
      "Training Loss: 0.006280003153369762\n",
      "Validation Loss: 0.004337169078251954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007280784016475081\n",
      "Training Loss: 0.006538183832308277\n",
      "Training Loss: 0.006272803764441051\n",
      "Validation Loss: 0.004331897656443749\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.0072735584888141604\n",
      "Training Loss: 0.006530645722523332\n",
      "Training Loss: 0.006265643806545995\n",
      "Validation Loss: 0.004326644793723113\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007266382046509534\n",
      "Training Loss: 0.006523151679430157\n",
      "Training Loss: 0.00625852302997373\n",
      "Validation Loss: 0.004321409628747471\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007259250800125301\n",
      "Training Loss: 0.006515699188457802\n",
      "Training Loss: 0.006251440226915293\n",
      "Validation Loss: 0.004316192248501302\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007252164209494367\n",
      "Training Loss: 0.006508288899203763\n",
      "Training Loss: 0.006244394521927461\n",
      "Validation Loss: 0.004310985653676888\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007245120826410129\n",
      "Training Loss: 0.006500918788369745\n",
      "Training Loss: 0.006237384986015968\n",
      "Validation Loss: 0.00430579045447364\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007238120613619685\n",
      "Training Loss: 0.0064935885928571226\n",
      "Training Loss: 0.006230411721626297\n",
      "Validation Loss: 0.004300615461176952\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007231161497766152\n",
      "Training Loss: 0.006486295895883814\n",
      "Training Loss: 0.0062234727659961206\n",
      "Validation Loss: 0.00429544481514231\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007224241679068655\n",
      "Training Loss: 0.006479040884878487\n",
      "Training Loss: 0.006216567419469357\n",
      "Validation Loss: 0.004290287949280876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007217360386275687\n",
      "Training Loss: 0.006471822935855016\n",
      "Training Loss: 0.006209695435827598\n",
      "Validation Loss: 0.004285138695542649\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007210517046041786\n",
      "Training Loss: 0.006464639886980876\n",
      "Training Loss: 0.006202855858718976\n",
      "Validation Loss: 0.004279997390093219\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007203710728790611\n",
      "Training Loss: 0.006457491979235783\n",
      "Training Loss: 0.006196047689882107\n",
      "Validation Loss: 0.0042748636176307385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007196939499117434\n",
      "Training Loss: 0.006450378055451438\n",
      "Training Loss: 0.006189270678441971\n",
      "Validation Loss: 0.004269739507639969\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007190203652135097\n",
      "Training Loss: 0.006443296949146316\n",
      "Training Loss: 0.006182524457690306\n",
      "Validation Loss: 0.004264616625896247\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.0071835014293901624\n",
      "Training Loss: 0.006436248748796061\n",
      "Training Loss: 0.0061758072947850455\n",
      "Validation Loss: 0.0042595016950573985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007176831762189977\n",
      "Training Loss: 0.006429232098162175\n",
      "Training Loss: 0.0061691187386168165\n",
      "Validation Loss: 0.004254394139587\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007170194877544418\n",
      "Training Loss: 0.00642224705312401\n",
      "Training Loss: 0.006162459662300534\n",
      "Validation Loss: 0.004249293337513306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007163589625852182\n",
      "Training Loss: 0.006415291285375133\n",
      "Training Loss: 0.006155827757320367\n",
      "Validation Loss: 0.004244197075637162\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007157013842370361\n",
      "Training Loss: 0.006408365920651704\n",
      "Training Loss: 0.006149223265238106\n",
      "Validation Loss: 0.004239105269590163\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007150468381005339\n",
      "Training Loss: 0.006401467467658222\n",
      "Training Loss: 0.00614264449221082\n",
      "Validation Loss: 0.004234013434112323\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007143951046746224\n",
      "Training Loss: 0.006394598909537308\n",
      "Training Loss: 0.006136092114029452\n",
      "Validation Loss: 0.00422893145927385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.00713746236404404\n",
      "Training Loss: 0.0063877575105288995\n",
      "Training Loss: 0.006129565289011225\n",
      "Validation Loss: 0.004223854167208997\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007131001243833452\n",
      "Training Loss: 0.006380943434778601\n",
      "Training Loss: 0.006123063393752091\n",
      "Validation Loss: 0.004218781487937765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007124566829297692\n",
      "Training Loss: 0.006374155136290938\n",
      "Training Loss: 0.006116585503332317\n",
      "Validation Loss: 0.004213712869468407\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0071181591006461535\n",
      "Training Loss: 0.006367393498076126\n",
      "Training Loss: 0.0061101318703731524\n",
      "Validation Loss: 0.004208648672165203\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007111776567180641\n",
      "Training Loss: 0.006360656406613998\n",
      "Training Loss: 0.006103700859821402\n",
      "Validation Loss: 0.004203585527178025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.007105418421560899\n",
      "Training Loss: 0.006353944219881669\n",
      "Training Loss: 0.00609729258925654\n",
      "Validation Loss: 0.004198525105524557\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007099084065994248\n",
      "Training Loss: 0.006347256294102408\n",
      "Training Loss: 0.006090906539466232\n",
      "Validation Loss: 0.004193473840132356\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.007092773971962742\n",
      "Training Loss: 0.006340591083862819\n",
      "Training Loss: 0.006084541766904294\n",
      "Validation Loss: 0.004188423391609463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.007086485523614101\n",
      "Training Loss: 0.006333949026884511\n",
      "Training Loss: 0.006078197992173955\n",
      "Validation Loss: 0.004183374534313975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.007080219947383739\n",
      "Training Loss: 0.006327329850755632\n",
      "Training Loss: 0.006071874446352012\n",
      "Validation Loss: 0.004178330772477882\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007073976057581604\n",
      "Training Loss: 0.006320732343010604\n",
      "Training Loss: 0.006065570801729337\n",
      "Validation Loss: 0.004173288831429744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007067752845468931\n",
      "Training Loss: 0.006314156654989347\n",
      "Training Loss: 0.006059287479729392\n",
      "Validation Loss: 0.0041682534606530845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007061551286606118\n",
      "Training Loss: 0.006307602236629464\n",
      "Training Loss: 0.006053022642154246\n",
      "Validation Loss: 0.004163219873385316\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007055368876317516\n",
      "Training Loss: 0.006301067915046588\n",
      "Training Loss: 0.006046776923467405\n",
      "Validation Loss: 0.004158189886490281\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.007049206198425964\n",
      "Training Loss: 0.006294553613988682\n",
      "Training Loss: 0.006040549819008447\n",
      "Validation Loss: 0.004153164598719332\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.007043062351294793\n",
      "Training Loss: 0.0062880601652432234\n",
      "Training Loss: 0.006034340552287176\n",
      "Validation Loss: 0.004148142834800934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.007036937240627594\n",
      "Training Loss: 0.006281585264950991\n",
      "Training Loss: 0.006028148398036138\n",
      "Validation Loss: 0.004143124960331518\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.007030829822761007\n",
      "Training Loss: 0.006275129850255325\n",
      "Training Loss: 0.0060219742485787715\n",
      "Validation Loss: 0.004138108923654543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0070247408380964774\n",
      "Training Loss: 0.006268693287856877\n",
      "Training Loss: 0.006015816039871424\n",
      "Validation Loss: 0.004133099581512591\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007018668495584279\n",
      "Training Loss: 0.006262275165063329\n",
      "Training Loss: 0.006009675022796728\n",
      "Validation Loss: 0.00412809182274982\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.00701261184294708\n",
      "Training Loss: 0.006255874646594748\n",
      "Training Loss: 0.006003548570442945\n",
      "Validation Loss: 0.004123089058073551\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.007006572044920176\n",
      "Training Loss: 0.006249491668422706\n",
      "Training Loss: 0.005997438229387626\n",
      "Validation Loss: 0.004118089241059392\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.0070005468098679555\n",
      "Training Loss: 0.006243124896427616\n",
      "Training Loss: 0.005991343181813135\n",
      "Validation Loss: 0.004113092340314447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006994536317652091\n",
      "Training Loss: 0.006236776008736342\n",
      "Training Loss: 0.005985262487083674\n",
      "Validation Loss: 0.004108099203446901\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.00698854036803823\n",
      "Training Loss: 0.006230443372623995\n",
      "Training Loss: 0.005979196364642121\n",
      "Validation Loss: 0.004103110432153924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006982558558229357\n",
      "Training Loss: 0.006224126850138418\n",
      "Training Loss: 0.005973144284216687\n",
      "Validation Loss: 0.0040981220460779375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006976588894613087\n",
      "Training Loss: 0.006217825642670505\n",
      "Training Loss: 0.005967104964074679\n",
      "Validation Loss: 0.004093138278027725\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006970632640295662\n",
      "Training Loss: 0.006211540574440733\n",
      "Training Loss: 0.005961079771514051\n",
      "Validation Loss: 0.004088159940294461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.0069646881392691285\n",
      "Training Loss: 0.006205269976053387\n",
      "Training Loss: 0.005955066995229572\n",
      "Validation Loss: 0.004083184306274346\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006958755612722598\n",
      "Training Loss: 0.0061990141158457844\n",
      "Training Loss: 0.005949067018227652\n",
      "Validation Loss: 0.004078213074453845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006952834414551035\n",
      "Training Loss: 0.006192772866925224\n",
      "Training Loss: 0.005943079279968515\n",
      "Validation Loss: 0.004073243915872502\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006946924066869542\n",
      "Training Loss: 0.006186545431846753\n",
      "Training Loss: 0.005937102764146402\n",
      "Validation Loss: 0.004068276004504747\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006941022577811964\n",
      "Training Loss: 0.006180332836229354\n",
      "Training Loss: 0.005931137559236958\n",
      "Validation Loss: 0.004063311543739453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006935131176142022\n",
      "Training Loss: 0.006174133532913402\n",
      "Training Loss: 0.005925184627994895\n",
      "Validation Loss: 0.004058352818063805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006929248695960268\n",
      "Training Loss: 0.006167947484645993\n",
      "Training Loss: 0.005919242053641938\n",
      "Validation Loss: 0.0040533980727457345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006923375395708717\n",
      "Training Loss: 0.006161774678621441\n",
      "Training Loss: 0.005913310609175823\n",
      "Validation Loss: 0.004048448423541078\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006917510384228081\n",
      "Training Loss: 0.006155615384923294\n",
      "Training Loss: 0.0059073893062304705\n",
      "Validation Loss: 0.004043504209231502\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006911652908311226\n",
      "Training Loss: 0.0061494679161114615\n",
      "Training Loss: 0.005901477919542231\n",
      "Validation Loss: 0.004038556887679262\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006905801401007921\n",
      "Training Loss: 0.0061433332238812\n",
      "Training Loss: 0.005895575667964294\n",
      "Validation Loss: 0.0040336180409008444\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006899957199930213\n",
      "Training Loss: 0.006137210392626003\n",
      "Training Loss: 0.005889683368150145\n",
      "Validation Loss: 0.004028680628385353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.0068941191158955915\n",
      "Training Loss: 0.0061311000969726595\n",
      "Training Loss: 0.0058837996772490445\n",
      "Validation Loss: 0.004023747834549652\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006888285935274325\n",
      "Training Loss: 0.0061250010028015825\n",
      "Training Loss: 0.005877925317035988\n",
      "Validation Loss: 0.00401881786607457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006882458058535121\n",
      "Training Loss: 0.006118913207319565\n",
      "Training Loss: 0.005872058720560744\n",
      "Validation Loss: 0.004013891608501376\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006876634254003875\n",
      "Training Loss: 0.006112837296677753\n",
      "Training Loss: 0.005866200826712884\n",
      "Validation Loss: 0.0040089739937848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006870814758585766\n",
      "Training Loss: 0.006106771939666942\n",
      "Training Loss: 0.005860350352595561\n",
      "Validation Loss: 0.00400405713838389\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006864998708479107\n",
      "Training Loss: 0.006100717787630856\n",
      "Training Loss: 0.005854508384363726\n",
      "Validation Loss: 0.00399914738562565\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006859185314970091\n",
      "Training Loss: 0.006094674654304981\n",
      "Training Loss: 0.005848674045410007\n",
      "Validation Loss: 0.003994240668822038\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.0068533751275390384\n",
      "Training Loss: 0.006088642119430006\n",
      "Training Loss: 0.005842846053419635\n",
      "Validation Loss: 0.003989337397388737\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006847565683419816\n",
      "Training Loss: 0.006082619739463553\n",
      "Training Loss: 0.005837025266955607\n",
      "Validation Loss: 0.003984436350926924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006841758673544973\n",
      "Training Loss: 0.006076607338036411\n",
      "Training Loss: 0.005831211145268753\n",
      "Validation Loss: 0.003979544661510108\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006835953279514797\n",
      "Training Loss: 0.00607060587964952\n",
      "Training Loss: 0.005825403696508147\n",
      "Validation Loss: 0.003974657662061116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006830147670116276\n",
      "Training Loss: 0.00606461358605884\n",
      "Training Loss: 0.005819601932307706\n",
      "Validation Loss: 0.003969774192350843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006824342734180391\n",
      "Training Loss: 0.006058631631312892\n",
      "Training Loss: 0.005813806701335125\n",
      "Validation Loss: 0.0039648946716052515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006818536954233423\n",
      "Training Loss: 0.006052658549742773\n",
      "Training Loss: 0.005808016717783176\n",
      "Validation Loss: 0.003960019895765051\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.00681273034773767\n",
      "Training Loss: 0.0060466945444932205\n",
      "Training Loss: 0.005802231904817745\n",
      "Validation Loss: 0.003955151377575409\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006806922160903923\n",
      "Training Loss: 0.006040740512544289\n",
      "Training Loss: 0.005796452127979137\n",
      "Validation Loss: 0.00395028766773097\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006801113275578245\n",
      "Training Loss: 0.006034795553423464\n",
      "Training Loss: 0.005790678464691155\n",
      "Validation Loss: 0.003945433327357881\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00679530217021238\n",
      "Training Loss: 0.006028859431389719\n",
      "Training Loss: 0.005784909452195279\n",
      "Validation Loss: 0.003940582251845954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.0067894883430562915\n",
      "Training Loss: 0.006022932442720048\n",
      "Training Loss: 0.005779144384432584\n",
      "Validation Loss: 0.003935732245000519\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.0067836711660493165\n",
      "Training Loss: 0.006017013309756294\n",
      "Training Loss: 0.005773382852203213\n",
      "Validation Loss: 0.003930892076557816\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006777850316721015\n",
      "Training Loss: 0.00601110229792539\n",
      "Training Loss: 0.005767625630251132\n",
      "Validation Loss: 0.003926053426746446\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006772025067475624\n",
      "Training Loss: 0.00600519961095415\n",
      "Training Loss: 0.005761871858267114\n",
      "Validation Loss: 0.003921223293566093\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006766195656964556\n",
      "Training Loss: 0.005999305063742213\n",
      "Training Loss: 0.005756122017046437\n",
      "Validation Loss: 0.003916395639770486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006760360347689129\n",
      "Training Loss: 0.005993417339632288\n",
      "Training Loss: 0.005750374057679437\n",
      "Validation Loss: 0.003911574354808611\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006754519845126197\n",
      "Training Loss: 0.005987537453765981\n",
      "Training Loss: 0.005744628963293507\n",
      "Validation Loss: 0.003906756376565089\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.0067486734437989075\n",
      "Training Loss: 0.005981663980055601\n",
      "Training Loss: 0.005738885544124059\n",
      "Validation Loss: 0.0039019426232821237\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006742820194922387\n",
      "Training Loss: 0.00597579845925793\n",
      "Training Loss: 0.005733145129634068\n",
      "Validation Loss: 0.003897136858183095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006736960168927908\n",
      "Training Loss: 0.005969938905909657\n",
      "Training Loss: 0.005727406016085297\n",
      "Validation Loss: 0.003892335034200524\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006731092932168395\n",
      "Training Loss: 0.005964085842715576\n",
      "Training Loss: 0.005721668413025327\n",
      "Validation Loss: 0.00388753904275638\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006725217453204095\n",
      "Training Loss: 0.005958238735911436\n",
      "Training Loss: 0.0057159308396512645\n",
      "Validation Loss: 0.00388274629001265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006719332535285502\n",
      "Training Loss: 0.005952395947533659\n",
      "Training Loss: 0.005710193548584357\n",
      "Validation Loss: 0.003877956595460183\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.00671343891415745\n",
      "Training Loss: 0.00594655898574274\n",
      "Training Loss: 0.0057044565369142215\n",
      "Validation Loss: 0.003873171604609933\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006707535922760144\n",
      "Training Loss: 0.005940726761473343\n",
      "Training Loss: 0.00569871888961643\n",
      "Validation Loss: 0.003868387991122985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006701622412074357\n",
      "Training Loss: 0.005934899286366999\n",
      "Training Loss: 0.005692979450104758\n",
      "Validation Loss: 0.0038636081886027802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006695697451941669\n",
      "Training Loss: 0.005929076034808531\n",
      "Training Loss: 0.005687239694525488\n",
      "Validation Loss: 0.003858833348122164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.0066897618619259445\n",
      "Training Loss: 0.005923256583628245\n",
      "Training Loss: 0.005681498119956814\n",
      "Validation Loss: 0.0038540594029930954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006683814318384975\n",
      "Training Loss: 0.005917439711629413\n",
      "Training Loss: 0.00567575289402157\n",
      "Validation Loss: 0.003849286804488452\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006677854649024084\n",
      "Training Loss: 0.005911625241278671\n",
      "Training Loss: 0.005670005553402007\n",
      "Validation Loss: 0.003844515369483008\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.0066718814813066275\n",
      "Training Loss: 0.005905813894351013\n",
      "Training Loss: 0.005664254177245312\n",
      "Validation Loss: 0.003839746368735024\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006665894582401961\n",
      "Training Loss: 0.0059000040555838495\n",
      "Training Loss: 0.005658499281271361\n",
      "Validation Loss: 0.0038349764968341822\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006659893570467829\n",
      "Training Loss: 0.005894195390865207\n",
      "Training Loss: 0.0056527390528935935\n",
      "Validation Loss: 0.0038302085634446545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006653877611970529\n",
      "Training Loss: 0.005888387901359237\n",
      "Training Loss: 0.005646973318071105\n",
      "Validation Loss: 0.003825438494030177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0066478463960811494\n",
      "Training Loss: 0.005882580627221614\n",
      "Training Loss: 0.005641202653059736\n",
      "Validation Loss: 0.003820666432474855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006641799703938887\n",
      "Training Loss: 0.005876773761119693\n",
      "Training Loss: 0.005635424549691379\n",
      "Validation Loss: 0.003815892977859783\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006635736131574959\n",
      "Training Loss: 0.005870965593494475\n",
      "Training Loss: 0.0056296401628060265\n",
      "Validation Loss: 0.0038111168541945517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006629655739525333\n",
      "Training Loss: 0.005865156682557426\n",
      "Training Loss: 0.00562384775315877\n",
      "Validation Loss: 0.0038063377769810432\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006623558031860739\n",
      "Training Loss: 0.0058593468728940935\n",
      "Training Loss: 0.0056180467287776995\n",
      "Validation Loss: 0.0038015556677370175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.0066174421948380765\n",
      "Training Loss: 0.005853535066125915\n",
      "Training Loss: 0.005612237799796276\n",
      "Validation Loss: 0.0037967698116200778\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.0066113081527873875\n",
      "Training Loss: 0.005847721897880547\n",
      "Training Loss: 0.005606420079711825\n",
      "Validation Loss: 0.0037919801353801337\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006605155849829316\n",
      "Training Loss: 0.005841905052657239\n",
      "Training Loss: 0.005600592285627499\n",
      "Validation Loss: 0.0037871812570976155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.00659898383077234\n",
      "Training Loss: 0.005836085802875459\n",
      "Training Loss: 0.005594754912308418\n",
      "Validation Loss: 0.003782376659421899\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006592793025774881\n",
      "Training Loss: 0.00583026371197775\n",
      "Training Loss: 0.005588907516212203\n",
      "Validation Loss: 0.0037775657563522626\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006586582553572953\n",
      "Training Loss: 0.005824439668795094\n",
      "Training Loss: 0.005583049645647407\n",
      "Validation Loss: 0.0037727496871892154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006580352243036032\n",
      "Training Loss: 0.00581861152080819\n",
      "Training Loss: 0.0055771807249402625\n",
      "Validation Loss: 0.003767925589293074\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006574102264130488\n",
      "Training Loss: 0.0058127799542853605\n",
      "Training Loss: 0.0055713005026336755\n",
      "Validation Loss: 0.003763091078111797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006567832278087735\n",
      "Training Loss: 0.0058069442736450585\n",
      "Training Loss: 0.005565408857655711\n",
      "Validation Loss: 0.003758246807664047\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006561542002018541\n",
      "Training Loss: 0.00580110504059121\n",
      "Training Loss: 0.005559506496647373\n",
      "Validation Loss: 0.003753396407099378\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.00655523197259754\n",
      "Training Loss: 0.005795262162573636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [12:49<12:42, 152.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.005553593265940435\n",
      "Validation Loss: 0.003748532579531579\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.40479973927140234\n",
      "Training Loss: 0.34911960534751413\n",
      "Training Loss: 0.2974344905465841\n",
      "Validation Loss: 0.23522366784261853\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.19791626647114755\n",
      "Training Loss: 0.13177838638424874\n",
      "Training Loss: 0.09662434292957187\n",
      "Validation Loss: 0.08256278969766048\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.07701819146052002\n",
      "Training Loss: 0.06956214524805546\n",
      "Training Loss: 0.06706185832619667\n",
      "Validation Loss: 0.0678766333613168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.06640594029799103\n",
      "Training Loss: 0.0640949211269617\n",
      "Training Loss: 0.06303202982991934\n",
      "Validation Loss: 0.06379024148657081\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.062270024754107\n",
      "Training Loss: 0.059780487129464745\n",
      "Training Loss: 0.05816021496430039\n",
      "Validation Loss: 0.058376432532507384\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.056633453797549006\n",
      "Training Loss: 0.05375363750383258\n",
      "Training Loss: 0.051364980647340415\n",
      "Validation Loss: 0.050719159602951466\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.04875903599895537\n",
      "Training Loss: 0.04538389915600419\n",
      "Training Loss: 0.04231014930643141\n",
      "Validation Loss: 0.04124269408456395\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.03938892036676407\n",
      "Training Loss: 0.035808567591011524\n",
      "Training Loss: 0.03217046590521932\n",
      "Validation Loss: 0.03065598930828692\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.02897921164985746\n",
      "Training Loss: 0.025610802699811756\n",
      "Training Loss: 0.022695871787145733\n",
      "Validation Loss: 0.022325238385604004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.022034435220994054\n",
      "Training Loss: 0.020240408410318197\n",
      "Training Loss: 0.01865303875412792\n",
      "Validation Loss: 0.01885831274427055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.01925703006563708\n",
      "Training Loss: 0.0178737178677693\n",
      "Training Loss: 0.016669485410675408\n",
      "Validation Loss: 0.01681605708952784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.017481827810406685\n",
      "Training Loss: 0.016194662456400694\n",
      "Training Loss: 0.015223001753911376\n",
      "Validation Loss: 0.015238280756022321\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.016100435229018332\n",
      "Training Loss: 0.014857703435700387\n",
      "Training Loss: 0.014079304540064185\n",
      "Validation Loss: 0.013961866597404306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.014994664839468897\n",
      "Training Loss: 0.013785935419145972\n",
      "Training Loss: 0.013171040292363614\n",
      "Validation Loss: 0.012935414206592387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.014114000962581485\n",
      "Training Loss: 0.012940353765152394\n",
      "Training Loss: 0.012456820888910442\n",
      "Validation Loss: 0.012120196771065003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.013417451817076653\n",
      "Training Loss: 0.01228073865873739\n",
      "Training Loss: 0.01189443283714354\n",
      "Validation Loss: 0.011472404861215795\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.012862589973956347\n",
      "Training Loss: 0.011761677283793688\n",
      "Training Loss: 0.011441426037345081\n",
      "Validation Loss: 0.010947014430140177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.012409257526742294\n",
      "Training Loss: 0.011340360769536347\n",
      "Training Loss: 0.011061848003882914\n",
      "Validation Loss: 0.010505214890758141\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.012024662976618855\n",
      "Training Loss: 0.010982969645410776\n",
      "Training Loss: 0.010729512111283838\n",
      "Validation Loss: 0.010118085527374001\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.01168524921988137\n",
      "Training Loss: 0.010666258954443038\n",
      "Training Loss: 0.010427470894064754\n",
      "Validation Loss: 0.009766416216080778\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.011375823165290058\n",
      "Training Loss: 0.010375971898902207\n",
      "Training Loss: 0.010145847366657109\n",
      "Validation Loss: 0.009438702643043204\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.011087545244954526\n",
      "Training Loss: 0.010104285252746195\n",
      "Training Loss: 0.00987953961128369\n",
      "Validation Loss: 0.009128746317615836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.010815858396235854\n",
      "Training Loss: 0.009847475355491042\n",
      "Training Loss: 0.009626350325997918\n",
      "Validation Loss: 0.008833692242977408\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.010558793153613806\n",
      "Training Loss: 0.009604140081210062\n",
      "Training Loss: 0.009385644784197211\n",
      "Validation Loss: 0.008552471184946094\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0103157414868474\n",
      "Training Loss: 0.009374025117140264\n",
      "Training Loss: 0.009157504488248378\n",
      "Validation Loss: 0.008284916837040461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.010086717291269452\n",
      "Training Loss: 0.00915732103982009\n",
      "Training Loss: 0.008942229787353427\n",
      "Validation Loss: 0.008031141044299923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.009871868977788835\n",
      "Training Loss: 0.008954255732242018\n",
      "Training Loss: 0.008740066743921488\n",
      "Validation Loss: 0.007791257176935421\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.009671277957968414\n",
      "Training Loss: 0.008764926251024008\n",
      "Training Loss: 0.008551094121066853\n",
      "Validation Loss: 0.0075652531172891845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.009484845321858303\n",
      "Training Loss: 0.008589226506883279\n",
      "Training Loss: 0.008375194214750081\n",
      "Validation Loss: 0.007352935371324002\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.009312278756406158\n",
      "Training Loss: 0.00842685338575393\n",
      "Training Loss: 0.008212069312576205\n",
      "Validation Loss: 0.007153961916056493\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.009153123438591138\n",
      "Training Loss: 0.008277337928302587\n",
      "Training Loss: 0.00806128022260964\n",
      "Validation Loss: 0.00696786921706804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.009006796028697863\n",
      "Training Loss: 0.00814009363646619\n",
      "Training Loss: 0.007922288780100643\n",
      "Validation Loss: 0.00679412100129248\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008872630931437015\n",
      "Training Loss: 0.008014458011602983\n",
      "Training Loss: 0.0077944910153746605\n",
      "Validation Loss: 0.00663213441383847\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.00874991503311321\n",
      "Training Loss: 0.007899724843446166\n",
      "Training Loss: 0.007677251767599955\n",
      "Validation Loss: 0.006481307017187891\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008637912741396575\n",
      "Training Loss: 0.007795171519974247\n",
      "Training Loss: 0.007569915455533192\n",
      "Validation Loss: 0.006341035341781177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.0085358857922256\n",
      "Training Loss: 0.00770007555722259\n",
      "Training Loss: 0.007471830425783992\n",
      "Validation Loss: 0.006210733585718894\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008443107423372566\n",
      "Training Loss: 0.007613728241994977\n",
      "Training Loss: 0.007382352479035035\n",
      "Validation Loss: 0.006089812165565705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008358861218439415\n",
      "Training Loss: 0.007535428070696071\n",
      "Training Loss: 0.007300835002679378\n",
      "Validation Loss: 0.005977696990280339\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008282455109292642\n",
      "Training Loss: 0.007464515464380384\n",
      "Training Loss: 0.007226668851217255\n",
      "Validation Loss: 0.005873841313341779\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008213231761474163\n",
      "Training Loss: 0.0074003486533183605\n",
      "Training Loss: 0.0071592528990004215\n",
      "Validation Loss: 0.005777694287531999\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.00815055247861892\n",
      "Training Loss: 0.007342316810972988\n",
      "Training Loss: 0.007098013964714482\n",
      "Validation Loss: 0.005688744733172856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008093824327224865\n",
      "Training Loss: 0.00728985043358989\n",
      "Training Loss: 0.007042406174587085\n",
      "Validation Loss: 0.005606476340463825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.00804247880121693\n",
      "Training Loss: 0.007242409859318286\n",
      "Training Loss: 0.0069919141579885035\n",
      "Validation Loss: 0.005530414337637636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.00799599354970269\n",
      "Training Loss: 0.007199492320651189\n",
      "Training Loss: 0.006946048066020012\n",
      "Validation Loss: 0.005460089105286039\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007953875288367272\n",
      "Training Loss: 0.007160634150495753\n",
      "Training Loss: 0.006904359498294071\n",
      "Validation Loss: 0.005395063522188098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.00791567663429305\n",
      "Training Loss: 0.0071254090603906665\n",
      "Training Loss: 0.0068664260266814384\n",
      "Validation Loss: 0.005334921368251272\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007880982308415696\n",
      "Training Loss: 0.007093423864571378\n",
      "Training Loss: 0.006831863229162991\n",
      "Validation Loss: 0.005279264212690629\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007849414909724146\n",
      "Training Loss: 0.007064321903744713\n",
      "Training Loss: 0.006800312144914642\n",
      "Validation Loss: 0.005227724841031968\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007820629888447\n",
      "Training Loss: 0.007037779792444781\n",
      "Training Loss: 0.0067714530404191465\n",
      "Validation Loss: 0.005179965506265923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007794321168912575\n",
      "Training Loss: 0.0070135077380109575\n",
      "Training Loss: 0.0067449934571050104\n",
      "Validation Loss: 0.005135658495337441\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007770208921283483\n",
      "Training Loss: 0.006991241328651085\n",
      "Training Loss: 0.006720667751505971\n",
      "Validation Loss: 0.0050945127150043845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007748044166946784\n",
      "Training Loss: 0.006970747353043407\n",
      "Training Loss: 0.006698237504460849\n",
      "Validation Loss: 0.005056251927404508\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007727601259248331\n",
      "Training Loss: 0.00695181499235332\n",
      "Training Loss: 0.006677488821442239\n",
      "Validation Loss: 0.005020626030569331\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007708685018587858\n",
      "Training Loss: 0.006934259716654196\n",
      "Training Loss: 0.006658233790658414\n",
      "Validation Loss: 0.004987405242890203\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007691118767252192\n",
      "Training Loss: 0.0069179176480975\n",
      "Training Loss: 0.006640301673905924\n",
      "Validation Loss: 0.004956384270525213\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007674746596021578\n",
      "Training Loss: 0.006902640042826533\n",
      "Training Loss: 0.006623541529406794\n",
      "Validation Loss: 0.004927363036739315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007659429869381711\n",
      "Training Loss: 0.006888297495897859\n",
      "Training Loss: 0.006607819246710278\n",
      "Validation Loss: 0.004900161774692994\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007645045997342095\n",
      "Training Loss: 0.0068747777037788185\n",
      "Training Loss: 0.006593014029785991\n",
      "Validation Loss: 0.004874629626479628\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00763148654717952\n",
      "Training Loss: 0.006861977769294754\n",
      "Training Loss: 0.006579021968063898\n",
      "Validation Loss: 0.004850617551710457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007618657731218264\n",
      "Training Loss: 0.006849811053834856\n",
      "Training Loss: 0.006565748573630117\n",
      "Validation Loss: 0.004827991926524621\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007606474293861538\n",
      "Training Loss: 0.006838197177276015\n",
      "Training Loss: 0.00655311162408907\n",
      "Validation Loss: 0.004806631709726297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007594860853860155\n",
      "Training Loss: 0.006827068164711818\n",
      "Training Loss: 0.006541037174174562\n",
      "Validation Loss: 0.004786429892744074\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.0075837543385569\n",
      "Training Loss: 0.006816364807309583\n",
      "Training Loss: 0.006529461581376381\n",
      "Validation Loss: 0.004767278718809189\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007573095543775708\n",
      "Training Loss: 0.006806031009182334\n",
      "Training Loss: 0.006518325589713641\n",
      "Validation Loss: 0.004749098137345458\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007562834223499522\n",
      "Training Loss: 0.006796023555798456\n",
      "Training Loss: 0.006507580978213809\n",
      "Validation Loss: 0.004731799099133925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007552925574127584\n",
      "Training Loss: 0.006786299537634477\n",
      "Training Loss: 0.00649718186352402\n",
      "Validation Loss: 0.004715314092444193\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007543330336920917\n",
      "Training Loss: 0.006776823959080502\n",
      "Training Loss: 0.00648708947468549\n",
      "Validation Loss: 0.004699569897925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007534012875985354\n",
      "Training Loss: 0.006767564931651577\n",
      "Training Loss: 0.00647726867464371\n",
      "Validation Loss: 0.004684508909612601\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007524943478638306\n",
      "Training Loss: 0.006758495097747072\n",
      "Training Loss: 0.0064676890312694016\n",
      "Validation Loss: 0.004670072541169278\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007516095724422484\n",
      "Training Loss: 0.006749590351246297\n",
      "Training Loss: 0.0064583227806724606\n",
      "Validation Loss: 0.004656207657502859\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0075074441684409975\n",
      "Training Loss: 0.006740830919006839\n",
      "Training Loss: 0.006449146990198642\n",
      "Validation Loss: 0.004642874797089339\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007498968768632039\n",
      "Training Loss: 0.0067321971268393095\n",
      "Training Loss: 0.00644014103570953\n",
      "Validation Loss: 0.004630027586081473\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007490652073174715\n",
      "Training Loss: 0.0067236747778952125\n",
      "Training Loss: 0.006431285286089406\n",
      "Validation Loss: 0.004617628488386196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007482476359000429\n",
      "Training Loss: 0.006715250289998948\n",
      "Training Loss: 0.00642256599094253\n",
      "Validation Loss: 0.004605646001886618\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007474429360590875\n",
      "Training Loss: 0.006706911030923948\n",
      "Training Loss: 0.0064139672578312455\n",
      "Validation Loss: 0.004594045697078318\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00746649757726118\n",
      "Training Loss: 0.0066986483114305885\n",
      "Training Loss: 0.006405478107044473\n",
      "Validation Loss: 0.004582801714717421\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007458670979831368\n",
      "Training Loss: 0.006690452529583126\n",
      "Training Loss: 0.006397087303339504\n",
      "Validation Loss: 0.004571881437585218\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007450940172420814\n",
      "Training Loss: 0.006682317804079503\n",
      "Training Loss: 0.0063887858320958914\n",
      "Validation Loss: 0.004561268313123395\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007443296746350825\n",
      "Training Loss: 0.0066742386168334635\n",
      "Training Loss: 0.006380567451124079\n",
      "Validation Loss: 0.0045509393780827105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.00743573499028571\n",
      "Training Loss: 0.0066662098933011294\n",
      "Training Loss: 0.006372424337896519\n",
      "Validation Loss: 0.004540875925567378\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007428249182412401\n",
      "Training Loss: 0.006658227846492082\n",
      "Training Loss: 0.006364350934745744\n",
      "Validation Loss: 0.004531058836649852\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007420833984506317\n",
      "Training Loss: 0.006650290172547102\n",
      "Training Loss: 0.006356344172963872\n",
      "Validation Loss: 0.004521470853739701\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.00741348440235015\n",
      "Training Loss: 0.006642394348746166\n",
      "Training Loss: 0.006348398989648558\n",
      "Validation Loss: 0.004512101417051607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.0074061987013556065\n",
      "Training Loss: 0.006634538880316541\n",
      "Training Loss: 0.00634051285975147\n",
      "Validation Loss: 0.004502936824894605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007398973725503311\n",
      "Training Loss: 0.006626723340013995\n",
      "Training Loss: 0.006332684266380966\n",
      "Validation Loss: 0.004493961147990161\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007391806709347293\n",
      "Training Loss: 0.006618947512470186\n",
      "Training Loss: 0.006324910106486641\n",
      "Validation Loss: 0.004485166158783511\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.0073846977745415645\n",
      "Training Loss: 0.006611211684066802\n",
      "Training Loss: 0.006317190256668255\n",
      "Validation Loss: 0.004476539154924201\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007377644272637553\n",
      "Training Loss: 0.006603516770992428\n",
      "Training Loss: 0.006309524565003812\n",
      "Validation Loss: 0.004468075964427187\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007370645958581008\n",
      "Training Loss: 0.006595862827962265\n",
      "Training Loss: 0.006301911175251007\n",
      "Validation Loss: 0.004459765273423635\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007363702898146585\n",
      "Training Loss: 0.00658825158374384\n",
      "Training Loss: 0.006294351217220538\n",
      "Validation Loss: 0.004451602290572829\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007356814339291304\n",
      "Training Loss: 0.006580685671651736\n",
      "Training Loss: 0.006286846229340881\n",
      "Validation Loss: 0.004443579010032327\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007349981470615603\n",
      "Training Loss: 0.006573164976434782\n",
      "Training Loss: 0.006279394450248219\n",
      "Validation Loss: 0.004435690021105739\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007343203417840414\n",
      "Training Loss: 0.006565691970754415\n",
      "Training Loss: 0.006271997384028509\n",
      "Validation Loss: 0.004427924462505145\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007336481439415366\n",
      "Training Loss: 0.006558269393863157\n",
      "Training Loss: 0.006264656315906904\n",
      "Validation Loss: 0.004420285639640865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007329815998673439\n",
      "Training Loss: 0.006550898287678138\n",
      "Training Loss: 0.006257373144617304\n",
      "Validation Loss: 0.004412767508726442\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007323208461748436\n",
      "Training Loss: 0.00654358207131736\n",
      "Training Loss: 0.006250148342805914\n",
      "Validation Loss: 0.004405362744098843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007316660066717304\n",
      "Training Loss: 0.006536322000902146\n",
      "Training Loss: 0.006242983671836555\n",
      "Validation Loss: 0.004398071313711155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.0073101715237135064\n",
      "Training Loss: 0.006529120880877599\n",
      "Training Loss: 0.00623588033602573\n",
      "Validation Loss: 0.004390887127995616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007303743523661979\n",
      "Training Loss: 0.006521980818361044\n",
      "Training Loss: 0.006228840238763951\n",
      "Validation Loss: 0.0043838080182263435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007297378221992403\n",
      "Training Loss: 0.006514904138166458\n",
      "Training Loss: 0.006221864256658592\n",
      "Validation Loss: 0.004376837779673633\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00729107617517002\n",
      "Training Loss: 0.006507892103400081\n",
      "Training Loss: 0.00621495412720833\n",
      "Validation Loss: 0.004369962309733075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007284837866900489\n",
      "Training Loss: 0.0065009484993061055\n",
      "Training Loss: 0.0062081111804582175\n",
      "Validation Loss: 0.004363190317971192\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007278666218044236\n",
      "Training Loss: 0.006494073909707367\n",
      "Training Loss: 0.0062013366067549215\n",
      "Validation Loss: 0.004356515323717064\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007272559499251656\n",
      "Training Loss: 0.006487269379431382\n",
      "Training Loss: 0.0061946315295062955\n",
      "Validation Loss: 0.004349933744256458\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007266519940458238\n",
      "Training Loss: 0.006480538547621108\n",
      "Training Loss: 0.006187998266541399\n",
      "Validation Loss: 0.004343446015165805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007260549707571045\n",
      "Training Loss: 0.006473883038852364\n",
      "Training Loss: 0.00618143689527642\n",
      "Validation Loss: 0.004337052398052568\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.0072546476940624416\n",
      "Training Loss: 0.006467302240198478\n",
      "Training Loss: 0.006174947813269683\n",
      "Validation Loss: 0.004330747482220359\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.007248814957565628\n",
      "Training Loss: 0.006460799060296268\n",
      "Training Loss: 0.006168533104355447\n",
      "Validation Loss: 0.004324533461901789\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007243052861304023\n",
      "Training Loss: 0.006454373938031495\n",
      "Training Loss: 0.006162192742340267\n",
      "Validation Loss: 0.004318406298531617\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.0072373608167981725\n",
      "Training Loss: 0.006448029612656682\n",
      "Training Loss: 0.0061559286218835045\n",
      "Validation Loss: 0.0043123648344968145\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.0072317395015852525\n",
      "Training Loss: 0.006441763193579391\n",
      "Training Loss: 0.006149739013053477\n",
      "Validation Loss: 0.0043064109389827155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.007226188781787641\n",
      "Training Loss: 0.00643557891366072\n",
      "Training Loss: 0.006143625969416462\n",
      "Validation Loss: 0.004300541887347576\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007220710649853572\n",
      "Training Loss: 0.00642947529326193\n",
      "Training Loss: 0.006137589895515703\n",
      "Validation Loss: 0.00429475538006178\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007215302084805444\n",
      "Training Loss: 0.006423452761955559\n",
      "Training Loss: 0.006131629724404774\n",
      "Validation Loss: 0.004289050538778263\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007209964903304353\n",
      "Training Loss: 0.006417512148618698\n",
      "Training Loss: 0.006125745813478716\n",
      "Validation Loss: 0.004283425622499349\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007204697250272147\n",
      "Training Loss: 0.006411653822287917\n",
      "Training Loss: 0.006119938165065833\n",
      "Validation Loss: 0.00427788333297613\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.007199500467977487\n",
      "Training Loss: 0.006405876966309734\n",
      "Training Loss: 0.006114207391510718\n",
      "Validation Loss: 0.004272418736291819\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.0071943737455876546\n",
      "Training Loss: 0.006400180468335748\n",
      "Training Loss: 0.006108552248915658\n",
      "Validation Loss: 0.004267033003793841\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.007189315062132664\n",
      "Training Loss: 0.006394565704395064\n",
      "Training Loss: 0.006102972086519003\n",
      "Validation Loss: 0.004261722141390227\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.007184325637645088\n",
      "Training Loss: 0.006389030074933544\n",
      "Training Loss: 0.006097466450300999\n",
      "Validation Loss: 0.004256486630438712\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.007179402758483775\n",
      "Training Loss: 0.006383574248175137\n",
      "Training Loss: 0.006092035283800214\n",
      "Validation Loss: 0.004251325248578417\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007174546679016203\n",
      "Training Loss: 0.006378197647863999\n",
      "Training Loss: 0.006086677186540328\n",
      "Validation Loss: 0.004246239994490373\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.007169755921931937\n",
      "Training Loss: 0.006372897835099138\n",
      "Training Loss: 0.0060813918843632565\n",
      "Validation Loss: 0.0042412246641535525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.00716503011295572\n",
      "Training Loss: 0.0063676751271123065\n",
      "Training Loss: 0.006076178097282536\n",
      "Validation Loss: 0.004236279931207177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.007160367941833101\n",
      "Training Loss: 0.006362527777673677\n",
      "Training Loss: 0.006071035076165572\n",
      "Validation Loss: 0.004231406405850659\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0071557678119279445\n",
      "Training Loss: 0.006357455383986235\n",
      "Training Loss: 0.006065962189459242\n",
      "Validation Loss: 0.0042266014170717825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.007151227975846268\n",
      "Training Loss: 0.006352456230670214\n",
      "Training Loss: 0.006060957167064771\n",
      "Validation Loss: 0.004221865660092385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.00714674897142686\n",
      "Training Loss: 0.0063475282664876434\n",
      "Training Loss: 0.0060560197045560924\n",
      "Validation Loss: 0.004217190655560516\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.007142327445326373\n",
      "Training Loss: 0.006342670809244737\n",
      "Training Loss: 0.0060511482454603535\n",
      "Validation Loss: 0.004212584497611133\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007137962998240255\n",
      "Training Loss: 0.006337882142397575\n",
      "Training Loss: 0.006046341571491211\n",
      "Validation Loss: 0.004208037808924662\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007133654574281536\n",
      "Training Loss: 0.006333161301445216\n",
      "Training Loss: 0.006041598879965022\n",
      "Validation Loss: 0.004203555336368553\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00712940032070037\n",
      "Training Loss: 0.006328506353311241\n",
      "Training Loss: 0.0060369185905437914\n",
      "Validation Loss: 0.004199136800676836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.007125199547153897\n",
      "Training Loss: 0.006323916143737733\n",
      "Training Loss: 0.00603230151173193\n",
      "Validation Loss: 0.004194775244398984\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.0071210512740071865\n",
      "Training Loss: 0.006319388564443216\n",
      "Training Loss: 0.006027743206941522\n",
      "Validation Loss: 0.004190472464124264\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.007116952324286103\n",
      "Training Loss: 0.006314923490281217\n",
      "Training Loss: 0.006023244459647685\n",
      "Validation Loss: 0.0041862272989695495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.007112903200904839\n",
      "Training Loss: 0.006310518365353346\n",
      "Training Loss: 0.006018802806502208\n",
      "Validation Loss: 0.004182039947756514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.007108901779865846\n",
      "Training Loss: 0.006306171517935582\n",
      "Training Loss: 0.006014418322010897\n",
      "Validation Loss: 0.004177910214933577\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.007104946388863027\n",
      "Training Loss: 0.006301882039406337\n",
      "Training Loss: 0.006010089066112414\n",
      "Validation Loss: 0.004173830791842192\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.007101036306121387\n",
      "Training Loss: 0.0062976482335943725\n",
      "Training Loss: 0.006005814085365273\n",
      "Validation Loss: 0.004169807911934226\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.007097170311026275\n",
      "Training Loss: 0.006293468760559335\n",
      "Training Loss: 0.006001591960084624\n",
      "Validation Loss: 0.004165835584398736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.007093346783658489\n",
      "Training Loss: 0.006289342000382021\n",
      "Training Loss: 0.005997421524953097\n",
      "Validation Loss: 0.004161913639190868\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.007089564867201261\n",
      "Training Loss: 0.006285265854094177\n",
      "Training Loss: 0.005993301872513257\n",
      "Validation Loss: 0.004158042531507613\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.007085822616936639\n",
      "Training Loss: 0.00628124100912828\n",
      "Training Loss: 0.005989232329302468\n",
      "Validation Loss: 0.004154221559586945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.007082120272098109\n",
      "Training Loss: 0.0062772639037575574\n",
      "Training Loss: 0.005985210160142742\n",
      "Validation Loss: 0.004150445938628334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.007078455746523105\n",
      "Training Loss: 0.0062733339617261664\n",
      "Training Loss: 0.005981236087391153\n",
      "Validation Loss: 0.004146719870701684\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0070748275832738725\n",
      "Training Loss: 0.0062694507144624365\n",
      "Training Loss: 0.005977308055735193\n",
      "Validation Loss: 0.0041430388025290674\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.007071236012270674\n",
      "Training Loss: 0.0062656109360978\n",
      "Training Loss: 0.005973425101255998\n",
      "Validation Loss: 0.004139402318808637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.00706767933035735\n",
      "Training Loss: 0.006261815646430477\n",
      "Training Loss: 0.0059695872833253815\n",
      "Validation Loss: 0.004135810223988812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.007064156233682297\n",
      "Training Loss: 0.006258062393171713\n",
      "Training Loss: 0.0059657922375481575\n",
      "Validation Loss: 0.00413226135064628\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.007060666480101645\n",
      "Training Loss: 0.0062543508817907425\n",
      "Training Loss: 0.005962040046579204\n",
      "Validation Loss: 0.0041287539832900915\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.007057209177874029\n",
      "Training Loss: 0.006250679562799633\n",
      "Training Loss: 0.005958329427521676\n",
      "Validation Loss: 0.004125290270371551\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.007053782205330208\n",
      "Training Loss: 0.006247046208009124\n",
      "Training Loss: 0.005954658871633001\n",
      "Validation Loss: 0.004121865993470288\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.007050386510090902\n",
      "Training Loss: 0.0062434516352368515\n",
      "Training Loss: 0.0059510287886951115\n",
      "Validation Loss: 0.004118483293189384\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.007047020072350279\n",
      "Training Loss: 0.006239894239697606\n",
      "Training Loss: 0.0059474375104764474\n",
      "Validation Loss: 0.004115135820969688\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.007043682393850758\n",
      "Training Loss: 0.006236371578415856\n",
      "Training Loss: 0.005943884637672454\n",
      "Validation Loss: 0.0041118309587198365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.00704037283489015\n",
      "Training Loss: 0.006232884816126898\n",
      "Training Loss: 0.005940368760493584\n",
      "Validation Loss: 0.0041085592754907235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.007037090581725351\n",
      "Training Loss: 0.006229432681575418\n",
      "Training Loss: 0.005936889048316516\n",
      "Validation Loss: 0.004105325485448866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.007033833921886981\n",
      "Training Loss: 0.006226012011757121\n",
      "Training Loss: 0.005933445514529012\n",
      "Validation Loss: 0.004102123503604632\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.007030604226747528\n",
      "Training Loss: 0.006222625321242958\n",
      "Training Loss: 0.005930037238867953\n",
      "Validation Loss: 0.004098964743237691\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.007027399883954785\n",
      "Training Loss: 0.006219269662396982\n",
      "Training Loss: 0.00592666351760272\n",
      "Validation Loss: 0.004095835293371105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.0070242203952511776\n",
      "Training Loss: 0.006215945269213989\n",
      "Training Loss: 0.005923323206952773\n",
      "Validation Loss: 0.0040927370016075925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.00702106504701078\n",
      "Training Loss: 0.006212650021188893\n",
      "Training Loss: 0.005920016286545433\n",
      "Validation Loss: 0.004089673249877654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.007017933984170668\n",
      "Training Loss: 0.006209385592956096\n",
      "Training Loss: 0.005916742322151549\n",
      "Validation Loss: 0.0040866427301439676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.007014825401711278\n",
      "Training Loss: 0.0062061488488689065\n",
      "Training Loss: 0.0059134990384336564\n",
      "Validation Loss: 0.004083643224629249\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.007011738765868358\n",
      "Training Loss: 0.0062029398325830695\n",
      "Training Loss: 0.005910287678707391\n",
      "Validation Loss: 0.00408067612573923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.007008675721590407\n",
      "Training Loss: 0.006199758525472134\n",
      "Training Loss: 0.005907107196981088\n",
      "Validation Loss: 0.004077736161429477\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.007005633315420709\n",
      "Training Loss: 0.006196603737189435\n",
      "Training Loss: 0.0059039561753161255\n",
      "Validation Loss: 0.0040748274023121415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.007002611756906845\n",
      "Training Loss: 0.006193475244799629\n",
      "Training Loss: 0.005900835320353508\n",
      "Validation Loss: 0.004071945999487398\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006999611110077239\n",
      "Training Loss: 0.006190371942939237\n",
      "Training Loss: 0.00589774323743768\n",
      "Validation Loss: 0.004069096733831664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006996631072834134\n",
      "Training Loss: 0.006187293939292431\n",
      "Training Loss: 0.0058946787356399\n",
      "Validation Loss: 0.00406627134116513\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006993671110249125\n",
      "Training Loss: 0.006184239989379421\n",
      "Training Loss: 0.005891642973874695\n",
      "Validation Loss: 0.004063472243972918\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006990730530815199\n",
      "Training Loss: 0.006181210238719359\n",
      "Training Loss: 0.005888634743751026\n",
      "Validation Loss: 0.004060701335639055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.0069878089061239735\n",
      "Training Loss: 0.0061782043939456344\n",
      "Training Loss: 0.005885653242585249\n",
      "Validation Loss: 0.004057958743697179\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006984905693680048\n",
      "Training Loss: 0.006175220769946464\n",
      "Training Loss: 0.005882697842898779\n",
      "Validation Loss: 0.004055237955429455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006982020984287374\n",
      "Training Loss: 0.006172259438317269\n",
      "Training Loss: 0.005879769003367983\n",
      "Validation Loss: 0.0040525441630900405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006979154656874016\n",
      "Training Loss: 0.006169320266926661\n",
      "Training Loss: 0.005876865114551037\n",
      "Validation Loss: 0.0040498737094065785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006976305675925687\n",
      "Training Loss: 0.006166402519447729\n",
      "Training Loss: 0.005873986248043366\n",
      "Validation Loss: 0.004047225356975759\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006973474845872261\n",
      "Training Loss: 0.006163505901349708\n",
      "Training Loss: 0.00587113133398816\n",
      "Validation Loss: 0.0040445997578541885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006970659836078994\n",
      "Training Loss: 0.006160630100639537\n",
      "Training Loss: 0.005868300600559451\n",
      "Validation Loss: 0.004041997978092286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006967862030724064\n",
      "Training Loss: 0.006157774170860648\n",
      "Training Loss: 0.005865493764285929\n",
      "Validation Loss: 0.004039415922225191\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006965081172529608\n",
      "Training Loss: 0.006154937734245323\n",
      "Training Loss: 0.005862710782093927\n",
      "Validation Loss: 0.004036857718418697\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006962316843564622\n",
      "Training Loss: 0.006152121715713292\n",
      "Training Loss: 0.005859950752346776\n",
      "Validation Loss: 0.004034322133193609\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006959567650337704\n",
      "Training Loss: 0.006149325217120349\n",
      "Training Loss: 0.005857213002163917\n",
      "Validation Loss: 0.0040318064641448135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006956834801239892\n",
      "Training Loss: 0.006146547077805735\n",
      "Training Loss: 0.0058544977218844\n",
      "Validation Loss: 0.004029308241697845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006954117270652205\n",
      "Training Loss: 0.006143787392647937\n",
      "Training Loss: 0.005851803291589022\n",
      "Validation Loss: 0.00402683097597086\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.00695141424657777\n",
      "Training Loss: 0.0061410454980796204\n",
      "Training Loss: 0.005849130695569329\n",
      "Validation Loss: 0.004024374052840337\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006948726288974285\n",
      "Training Loss: 0.006138321873149835\n",
      "Training Loss: 0.005846478683524765\n",
      "Validation Loss: 0.004021933094959371\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006946053350693546\n",
      "Training Loss: 0.006135615267558023\n",
      "Training Loss: 0.005843847775831818\n",
      "Validation Loss: 0.004019511721667237\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006943394838017412\n",
      "Training Loss: 0.0061329258733894675\n",
      "Training Loss: 0.00584123662323691\n",
      "Validation Loss: 0.004017108173653735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006940750386565924\n",
      "Training Loss: 0.006130254266317934\n",
      "Training Loss: 0.005838645616895519\n",
      "Validation Loss: 0.004014726544421668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006938120019040071\n",
      "Training Loss: 0.006127598850871436\n",
      "Training Loss: 0.005836074468097649\n",
      "Validation Loss: 0.004012357202238205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006935502483393066\n",
      "Training Loss: 0.006124959157896228\n",
      "Training Loss: 0.005833521935273893\n",
      "Validation Loss: 0.004010006350470351\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006932899282546714\n",
      "Training Loss: 0.00612233552034013\n",
      "Training Loss: 0.005830988328671083\n",
      "Validation Loss: 0.004007672439747898\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006930308798910118\n",
      "Training Loss: 0.006119728093617596\n",
      "Training Loss: 0.0058284735662164164\n",
      "Validation Loss: 0.004005352623781629\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006927731526084244\n",
      "Training Loss: 0.006117135418462567\n",
      "Training Loss: 0.00582597610540688\n",
      "Validation Loss: 0.004003048860703417\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006925167414010502\n",
      "Training Loss: 0.006114557691616938\n",
      "Training Loss: 0.005823497314122505\n",
      "Validation Loss: 0.004000761094921677\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0069226154492935165\n",
      "Training Loss: 0.0061119964573299514\n",
      "Training Loss: 0.005821035634726286\n",
      "Validation Loss: 0.003998488432392896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.00692007590434514\n",
      "Training Loss: 0.0061094490758841855\n",
      "Training Loss: 0.005818591344868764\n",
      "Validation Loss: 0.003996230506212607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.00691754852887243\n",
      "Training Loss: 0.006106915237614885\n",
      "Training Loss: 0.005816163154668175\n",
      "Validation Loss: 0.003993984767016065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.00691503283684142\n",
      "Training Loss: 0.006104396701557562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [15:16<10:03, 150.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.005813752188696526\n",
      "Validation Loss: 0.0039917528786267455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.0819719933718443\n",
      "Training Loss: 0.07437907328829169\n",
      "Training Loss: 0.07108328295871616\n",
      "Validation Loss: 0.07018689557993679\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06846898131072521\n",
      "Training Loss: 0.06563308909535408\n",
      "Training Loss: 0.06359197864308953\n",
      "Validation Loss: 0.06276511084916217\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.061454639211297034\n",
      "Training Loss: 0.058650835445150734\n",
      "Training Loss: 0.05612019138410687\n",
      "Validation Loss: 0.054273559387480276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.05317695923149586\n",
      "Training Loss: 0.050103281382471325\n",
      "Training Loss: 0.04685492817312479\n",
      "Validation Loss: 0.04412878986992193\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.04352199252694845\n",
      "Training Loss: 0.04055939368903637\n",
      "Training Loss: 0.037129006078466774\n",
      "Validation Loss: 0.03458052752225586\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.034450773913413286\n",
      "Training Loss: 0.03198176725767553\n",
      "Training Loss: 0.029001508690416813\n",
      "Validation Loss: 0.027394116887550675\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.027643126952461898\n",
      "Training Loss: 0.025737187736667694\n",
      "Training Loss: 0.023411343302577735\n",
      "Validation Loss: 0.022538194725855012\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.023085192069411276\n",
      "Training Loss: 0.02151807939168066\n",
      "Training Loss: 0.01966471303254366\n",
      "Validation Loss: 0.01899991839276522\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.019817301989533007\n",
      "Training Loss: 0.018342341100797056\n",
      "Training Loss: 0.016755464524030685\n",
      "Validation Loss: 0.01601130915334804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.017115606497973203\n",
      "Training Loss: 0.01572445602621883\n",
      "Training Loss: 0.014477884124498814\n",
      "Validation Loss: 0.013709758099670826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.015092446412891149\n",
      "Training Loss: 0.013878861996345223\n",
      "Training Loss: 0.012969625091645866\n",
      "Validation Loss: 0.012193537712243668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.013768606474623085\n",
      "Training Loss: 0.01268527785083279\n",
      "Training Loss: 0.011992013913113624\n",
      "Validation Loss: 0.011180296937903662\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.012876844150014222\n",
      "Training Loss: 0.01186642438871786\n",
      "Training Loss: 0.011296418248675763\n",
      "Validation Loss: 0.010431676409782821\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.012216559471562504\n",
      "Training Loss: 0.011249117255210876\n",
      "Training Loss: 0.01075273938011378\n",
      "Validation Loss: 0.009829837870731783\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.011689232019707561\n",
      "Training Loss: 0.010750640609767288\n",
      "Training Loss: 0.010301998434588313\n",
      "Validation Loss: 0.00932064310783583\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.011248104514088482\n",
      "Training Loss: 0.010330965525936335\n",
      "Training Loss: 0.00991571144433692\n",
      "Validation Loss: 0.008877109547846773\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.010868797940202058\n",
      "Training Loss: 0.009968701570760458\n",
      "Training Loss: 0.009578372398391367\n",
      "Validation Loss: 0.008484080402452625\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.010537089952267707\n",
      "Training Loss: 0.009651187842246145\n",
      "Training Loss: 0.009280507148941979\n",
      "Validation Loss: 0.008132025671421728\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.010243898854823782\n",
      "Training Loss: 0.009370307677891106\n",
      "Training Loss: 0.009015760605689139\n",
      "Validation Loss: 0.007814386741812812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.009983007966075092\n",
      "Training Loss: 0.009120488532353192\n",
      "Training Loss: 0.008779562264680862\n",
      "Validation Loss: 0.00752635191330749\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.00974993652314879\n",
      "Training Loss: 0.00889768582303077\n",
      "Training Loss: 0.008568454866763204\n",
      "Validation Loss: 0.007264258028688223\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.009541325131431222\n",
      "Training Loss: 0.008698805847670884\n",
      "Training Loss: 0.008379710188601166\n",
      "Validation Loss: 0.007025224967660864\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.009354543902445585\n",
      "Training Loss: 0.008521352056413889\n",
      "Training Loss: 0.008211091273697093\n",
      "Validation Loss: 0.006806967184837028\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.009187436798820272\n",
      "Training Loss: 0.008363205047789962\n",
      "Training Loss: 0.008060678519541398\n",
      "Validation Loss: 0.0066076159213010344\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.009038154633017256\n",
      "Training Loss: 0.008222478805109859\n",
      "Training Loss: 0.007926762014394627\n",
      "Validation Loss: 0.006425616714653423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008905027938308194\n",
      "Training Loss: 0.008097436785465106\n",
      "Training Loss: 0.007807754571549594\n",
      "Validation Loss: 0.006259630545398242\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008786498681874946\n",
      "Training Loss: 0.00798644099268131\n",
      "Training Loss: 0.007702156197046861\n",
      "Validation Loss: 0.0061084702951059245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008681086014257744\n",
      "Training Loss: 0.007887942215893418\n",
      "Training Loss: 0.007608531472506001\n",
      "Validation Loss: 0.005971053201872646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008587374448543414\n",
      "Training Loss: 0.007800475284457206\n",
      "Training Loss: 0.007525516304885968\n",
      "Validation Loss: 0.0058463627414966236\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008504023138666526\n",
      "Training Loss: 0.007722672772360966\n",
      "Training Loss: 0.007451822615694255\n",
      "Validation Loss: 0.005733425442516552\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008429773835232482\n",
      "Training Loss: 0.007653279969235882\n",
      "Training Loss: 0.007386264925589785\n",
      "Validation Loss: 0.005631320234754363\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008363480898551642\n",
      "Training Loss: 0.007591169102815911\n",
      "Training Loss: 0.007327766951639205\n",
      "Validation Loss: 0.005539154208767531\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008304108155425639\n",
      "Training Loss: 0.007535346762742847\n",
      "Training Loss: 0.0072753769473638385\n",
      "Validation Loss: 0.005456081068331606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008250748600112275\n",
      "Training Loss: 0.007484947615303099\n",
      "Training Loss: 0.007228260518750176\n",
      "Validation Loss: 0.005381292392096869\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008202608331339433\n",
      "Training Loss: 0.00743923535104841\n",
      "Training Loss: 0.007185704519506544\n",
      "Validation Loss: 0.005314023759781143\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008159010795643552\n",
      "Training Loss: 0.007397585373837501\n",
      "Training Loss: 0.007147099283756688\n",
      "Validation Loss: 0.005253555797291606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008119379394920544\n",
      "Training Loss: 0.007359473319957033\n",
      "Training Loss: 0.00711193204857409\n",
      "Validation Loss: 0.005199206704561588\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008083222309360281\n",
      "Training Loss: 0.007324462067335844\n",
      "Training Loss: 0.007079768697731197\n",
      "Validation Loss: 0.005150345046847557\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.00805012583383359\n",
      "Training Loss: 0.007292180308140815\n",
      "Training Loss: 0.007050242510158569\n",
      "Validation Loss: 0.005106386523876925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008019734803820028\n",
      "Training Loss: 0.007262317197164521\n",
      "Training Loss: 0.00702304259990342\n",
      "Validation Loss: 0.005066783595150023\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007991744074970483\n",
      "Training Loss: 0.007234607064165175\n",
      "Training Loss: 0.006997898371191695\n",
      "Validation Loss: 0.005031042570101746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007965887266909704\n",
      "Training Loss: 0.007208818283397704\n",
      "Training Loss: 0.006974581086542458\n",
      "Validation Loss: 0.004998710591168133\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007941935181152075\n",
      "Training Loss: 0.0071847514156252144\n",
      "Training Loss: 0.00695288633229211\n",
      "Validation Loss: 0.004969381795296174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007919679795159028\n",
      "Training Loss: 0.007162227937951684\n",
      "Training Loss: 0.006932636597193778\n",
      "Validation Loss: 0.004942684502372246\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007898940159939229\n",
      "Training Loss: 0.007141091002849862\n",
      "Training Loss: 0.006913673813687638\n",
      "Validation Loss: 0.004918296453856936\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007879552411613986\n",
      "Training Loss: 0.007121198951499537\n",
      "Training Loss: 0.006895858201896771\n",
      "Validation Loss: 0.004895928241260183\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007861371331382544\n",
      "Training Loss: 0.0071024252648930995\n",
      "Training Loss: 0.006879064415115863\n",
      "Validation Loss: 0.004875324837852981\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007844264163868502\n",
      "Training Loss: 0.007084654899081215\n",
      "Training Loss: 0.006863181407097727\n",
      "Validation Loss: 0.004856259821066528\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007828113434370607\n",
      "Training Loss: 0.007067784374812618\n",
      "Training Loss: 0.006848108590347692\n",
      "Validation Loss: 0.004838542047406683\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007812813522759825\n",
      "Training Loss: 0.007051722186151892\n",
      "Training Loss: 0.006833758605644107\n",
      "Validation Loss: 0.004822006361464855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007798270980129018\n",
      "Training Loss: 0.007036384412785992\n",
      "Training Loss: 0.006820052386028692\n",
      "Validation Loss: 0.004806499049234926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007784401867538691\n",
      "Training Loss: 0.007021696713054553\n",
      "Training Loss: 0.006806919729569927\n",
      "Validation Loss: 0.00479189768365553\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007771129109896719\n",
      "Training Loss: 0.0070075933809857814\n",
      "Training Loss: 0.006794297315645963\n",
      "Validation Loss: 0.004778091998488297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007758388802176342\n",
      "Training Loss: 0.006994013303192333\n",
      "Training Loss: 0.006782130853971466\n",
      "Validation Loss: 0.004764983214678641\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007746121678501368\n",
      "Training Loss: 0.006980906022945419\n",
      "Training Loss: 0.0067703733663074675\n",
      "Validation Loss: 0.004752492398714249\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007734276126138866\n",
      "Training Loss: 0.00696822379832156\n",
      "Training Loss: 0.006758980272570625\n",
      "Validation Loss: 0.004740544197180968\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007722806511446833\n",
      "Training Loss: 0.006955928391544148\n",
      "Training Loss: 0.006747915230225772\n",
      "Validation Loss: 0.00472907153261679\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007711672188015654\n",
      "Training Loss: 0.006943981621880084\n",
      "Training Loss: 0.006737145435763523\n",
      "Validation Loss: 0.004718025256827307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007700841051992029\n",
      "Training Loss: 0.006932353845331818\n",
      "Training Loss: 0.006726643808651716\n",
      "Validation Loss: 0.004707353406721789\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007690281133400276\n",
      "Training Loss: 0.006921019093133509\n",
      "Training Loss: 0.0067163858353160326\n",
      "Validation Loss: 0.0046970157980856094\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007679967419244349\n",
      "Training Loss: 0.0069099532417021695\n",
      "Training Loss: 0.006706351381726563\n",
      "Validation Loss: 0.004686972134212932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.00766987684299238\n",
      "Training Loss: 0.006899138501612469\n",
      "Training Loss: 0.006696524048456922\n",
      "Validation Loss: 0.004677194769734915\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007659992526751012\n",
      "Training Loss: 0.0068885547947138545\n",
      "Training Loss: 0.006686888105468825\n",
      "Validation Loss: 0.004667651682589831\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007650294543709606\n",
      "Training Loss: 0.006878191163996235\n",
      "Training Loss: 0.006677432687720284\n",
      "Validation Loss: 0.004658320760406721\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007640772557351738\n",
      "Training Loss: 0.006868032852653414\n",
      "Training Loss: 0.006668147731106728\n",
      "Validation Loss: 0.004649181150454568\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007631412298651412\n",
      "Training Loss: 0.006858069588197395\n",
      "Training Loss: 0.006659024058608338\n",
      "Validation Loss: 0.004640215547399574\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007622203793143854\n",
      "Training Loss: 0.00684829156845808\n",
      "Training Loss: 0.0066500559810083355\n",
      "Validation Loss: 0.004631404627606356\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007613138033775613\n",
      "Training Loss: 0.006838690012227744\n",
      "Training Loss: 0.006641234938288107\n",
      "Validation Loss: 0.004622742629170501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007604206679388881\n",
      "Training Loss: 0.006829257485223934\n",
      "Training Loss: 0.006632557649863884\n",
      "Validation Loss: 0.004614218662681288\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0075954024365637455\n",
      "Training Loss: 0.006819986167829484\n",
      "Training Loss: 0.006624019157607108\n",
      "Validation Loss: 0.004605822577769083\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007586717519443482\n",
      "Training Loss: 0.006810867394087836\n",
      "Training Loss: 0.006615613671019673\n",
      "Validation Loss: 0.00459754775445699\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007578146607847884\n",
      "Training Loss: 0.006801895747194067\n",
      "Training Loss: 0.0066073384578339755\n",
      "Validation Loss: 0.004589389975632677\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007569682850735262\n",
      "Training Loss: 0.006793063153745607\n",
      "Training Loss: 0.006599187054671347\n",
      "Validation Loss: 0.004581344833210362\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.0075613186985719946\n",
      "Training Loss: 0.0067843624274246395\n",
      "Training Loss: 0.00659115472692065\n",
      "Validation Loss: 0.0045734112022779464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007553049575071782\n",
      "Training Loss: 0.00677578620845452\n",
      "Training Loss: 0.006583237504819408\n",
      "Validation Loss: 0.00456558483040609\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007544866742100567\n",
      "Training Loss: 0.0067673282511532306\n",
      "Training Loss: 0.006575429632794112\n",
      "Validation Loss: 0.004557864280061775\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007536767275305465\n",
      "Training Loss: 0.0067589791573118415\n",
      "Training Loss: 0.006567725273780525\n",
      "Validation Loss: 0.004550246897237271\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007528742376016453\n",
      "Training Loss: 0.006750733513617888\n",
      "Training Loss: 0.006560119150672108\n",
      "Validation Loss: 0.0045427285943159396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007520786356180907\n",
      "Training Loss: 0.0067425830999854955\n",
      "Training Loss: 0.006552606418263167\n",
      "Validation Loss: 0.004535314804884825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.0075128928886260835\n",
      "Training Loss: 0.006734520536847413\n",
      "Training Loss: 0.006545179596869275\n",
      "Validation Loss: 0.004528002080957542\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007505055465735495\n",
      "Training Loss: 0.006726538616931066\n",
      "Training Loss: 0.006537833890179172\n",
      "Validation Loss: 0.004520787393119646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007497268909355625\n",
      "Training Loss: 0.006718630529940128\n",
      "Training Loss: 0.006530562354018912\n",
      "Validation Loss: 0.0045136707858444095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.00748952584923245\n",
      "Training Loss: 0.006710790037177503\n",
      "Training Loss: 0.0065233605960384015\n",
      "Validation Loss: 0.004506651436376354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007481822724221274\n",
      "Training Loss: 0.006703008969780057\n",
      "Training Loss: 0.006516221391502768\n",
      "Validation Loss: 0.004499729024246335\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007474150761263445\n",
      "Training Loss: 0.006695282426662743\n",
      "Training Loss: 0.006509140386478976\n",
      "Validation Loss: 0.004492898075318152\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007466508175712079\n",
      "Training Loss: 0.006687603222671896\n",
      "Training Loss: 0.0065021110174711795\n",
      "Validation Loss: 0.00448616463403228\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007458886905806139\n",
      "Training Loss: 0.00667996586766094\n",
      "Training Loss: 0.006495128745445982\n",
      "Validation Loss: 0.004479519181801111\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.00745128320180811\n",
      "Training Loss: 0.006672364609548822\n",
      "Training Loss: 0.006488186694914475\n",
      "Validation Loss: 0.004472964728418528\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.0074436928483191875\n",
      "Training Loss: 0.006664792968658731\n",
      "Training Loss: 0.006481280637672171\n",
      "Validation Loss: 0.004466497297451067\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007436109815025702\n",
      "Training Loss: 0.006657247453695163\n",
      "Training Loss: 0.006474405599292367\n",
      "Validation Loss: 0.004460112924237599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007428531356854364\n",
      "Training Loss: 0.00664972162921913\n",
      "Training Loss: 0.0064675568731036035\n",
      "Validation Loss: 0.004453812192162771\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0074209517030976715\n",
      "Training Loss: 0.006642210715217516\n",
      "Training Loss: 0.006460729730315506\n",
      "Validation Loss: 0.004447593654537302\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007413368164561689\n",
      "Training Loss: 0.006634710383368656\n",
      "Training Loss: 0.00645391893805936\n",
      "Validation Loss: 0.004441448861440079\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007405775302322582\n",
      "Training Loss: 0.006627214477630332\n",
      "Training Loss: 0.0064471194567158815\n",
      "Validation Loss: 0.0044353806146870504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007398169609950855\n",
      "Training Loss: 0.00661971845664084\n",
      "Training Loss: 0.006440326623851434\n",
      "Validation Loss: 0.004429380111187027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007390547457616776\n",
      "Training Loss: 0.006612218269146979\n",
      "Training Loss: 0.006433536633849144\n",
      "Validation Loss: 0.004423447850610266\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007382905428530648\n",
      "Training Loss: 0.006604708711383864\n",
      "Training Loss: 0.0064267444075085224\n",
      "Validation Loss: 0.0044175778395297485\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007375238206004724\n",
      "Training Loss: 0.006597183612175286\n",
      "Training Loss: 0.0064199456945061685\n",
      "Validation Loss: 0.004411764110679205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007367542760912329\n",
      "Training Loss: 0.0065896396106109026\n",
      "Training Loss: 0.006413134703179822\n",
      "Validation Loss: 0.004406005485517004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007359814281808212\n",
      "Training Loss: 0.006582071479642764\n",
      "Training Loss: 0.006406307672150433\n",
      "Validation Loss: 0.0044002924807725495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007352046903688461\n",
      "Training Loss: 0.006574471709318459\n",
      "Training Loss: 0.0063994582486338915\n",
      "Validation Loss: 0.004394622277625407\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.00734423849848099\n",
      "Training Loss: 0.006566835794365034\n",
      "Training Loss: 0.0063925814069807525\n",
      "Validation Loss: 0.004388986661601184\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007336382932262495\n",
      "Training Loss: 0.006559158046729863\n",
      "Training Loss: 0.0063856719003524635\n",
      "Validation Loss: 0.004383380768109071\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007328474231762812\n",
      "Training Loss: 0.006551430088002234\n",
      "Training Loss: 0.006378723118687048\n",
      "Validation Loss: 0.004377798484690654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007320507941767573\n",
      "Training Loss: 0.006543646462960169\n",
      "Training Loss: 0.006371730632381513\n",
      "Validation Loss: 0.004372230445798696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0073124760226346555\n",
      "Training Loss: 0.0065357988770119845\n",
      "Training Loss: 0.00636468535172753\n",
      "Validation Loss: 0.004366667482091554\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007304371813079342\n",
      "Training Loss: 0.0065278782311361284\n",
      "Training Loss: 0.006357579780742526\n",
      "Validation Loss: 0.004361104477835254\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0072961876320187\n",
      "Training Loss: 0.006519875306403264\n",
      "Training Loss: 0.006350407531717792\n",
      "Validation Loss: 0.004355526207475348\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007287913945037872\n",
      "Training Loss: 0.0065117801027372475\n",
      "Training Loss: 0.006343157851370051\n",
      "Validation Loss: 0.004349925615458509\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.00727954187314026\n",
      "Training Loss: 0.006503581488505006\n",
      "Training Loss: 0.006335822460241616\n",
      "Validation Loss: 0.004344289196299368\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.007271059695631266\n",
      "Training Loss: 0.006495267522986978\n",
      "Training Loss: 0.006328389741829596\n",
      "Validation Loss: 0.004338605041626129\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.007262455113232136\n",
      "Training Loss: 0.006486823343439027\n",
      "Training Loss: 0.006320848425384611\n",
      "Validation Loss: 0.004332856625695242\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007253714913967996\n",
      "Training Loss: 0.00647823425475508\n",
      "Training Loss: 0.006313185438630171\n",
      "Validation Loss: 0.004327034164387523\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007244822584325448\n",
      "Training Loss: 0.006469482263782993\n",
      "Training Loss: 0.006305385746527463\n",
      "Validation Loss: 0.004321112485832713\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007235760842449963\n",
      "Training Loss: 0.006460549135226756\n",
      "Training Loss: 0.006297432610881515\n",
      "Validation Loss: 0.004315079098274366\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.0072265085077378895\n",
      "Training Loss: 0.0064514125033747404\n",
      "Training Loss: 0.006289308738778345\n",
      "Validation Loss: 0.004308908304011219\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.00721704417723231\n",
      "Training Loss: 0.006442048135213554\n",
      "Training Loss: 0.00628099157824181\n",
      "Validation Loss: 0.004302577090742631\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.007207340087043121\n",
      "Training Loss: 0.006432427666150034\n",
      "Training Loss: 0.006272459098836407\n",
      "Validation Loss: 0.0042960551721724066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.00719736555358395\n",
      "Training Loss: 0.0064225215336773545\n",
      "Training Loss: 0.006263683713041246\n",
      "Validation Loss: 0.004289313761015119\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.007187087701167911\n",
      "Training Loss: 0.006412291394080966\n",
      "Training Loss: 0.006254634404904209\n",
      "Validation Loss: 0.004282315273434342\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.007176465599332005\n",
      "Training Loss: 0.0064016998570878055\n",
      "Training Loss: 0.006245277099660598\n",
      "Validation Loss: 0.004275016506265304\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007165453406050801\n",
      "Training Loss: 0.006390698800096288\n",
      "Training Loss: 0.006235570111311972\n",
      "Validation Loss: 0.004267371699130267\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.007153998764697462\n",
      "Training Loss: 0.00637923544854857\n",
      "Training Loss: 0.006225469201453961\n",
      "Validation Loss: 0.004259320649611397\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.007142038301099092\n",
      "Training Loss: 0.006367248889291659\n",
      "Training Loss: 0.006214918703772127\n",
      "Validation Loss: 0.004250806149388297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.007129499794682487\n",
      "Training Loss: 0.0063546675827819855\n",
      "Training Loss: 0.006203857173677534\n",
      "Validation Loss: 0.004241744884331659\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0071162975707557054\n",
      "Training Loss: 0.006341410339809954\n",
      "Training Loss: 0.006192209832370281\n",
      "Validation Loss: 0.004232044469073331\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.007102330913767219\n",
      "Training Loss: 0.006327380251022987\n",
      "Training Loss: 0.00617989131482318\n",
      "Validation Loss: 0.00422160110300344\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.0070874813222326334\n",
      "Training Loss: 0.006312467348761857\n",
      "Training Loss: 0.006166799321654252\n",
      "Validation Loss: 0.004210278651483482\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.007071606176905334\n",
      "Training Loss: 0.006296538672177121\n",
      "Training Loss: 0.0061528102966258305\n",
      "Validation Loss: 0.004197924124505999\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007054536095820367\n",
      "Training Loss: 0.006279442270752043\n",
      "Training Loss: 0.006137781895231456\n",
      "Validation Loss: 0.004184344228163487\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007036066941218451\n",
      "Training Loss: 0.006260997444624081\n",
      "Training Loss: 0.006121538588777184\n",
      "Validation Loss: 0.004169316302456506\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.007015953101217747\n",
      "Training Loss: 0.006240991708473302\n",
      "Training Loss: 0.006103871824452653\n",
      "Validation Loss: 0.004152558144218592\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.0069938981381710615\n",
      "Training Loss: 0.006219174168072641\n",
      "Training Loss: 0.006084526706836187\n",
      "Validation Loss: 0.004133725193527977\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006969542012084276\n",
      "Training Loss: 0.006195250710006804\n",
      "Training Loss: 0.006063198880874552\n",
      "Validation Loss: 0.0041123929229470784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006942448538029567\n",
      "Training Loss: 0.0061688809865154326\n",
      "Training Loss: 0.00603951949684415\n",
      "Validation Loss: 0.004088047069099763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.0069120967295020815\n",
      "Training Loss: 0.006139670428819954\n",
      "Training Loss: 0.006013046011212282\n",
      "Validation Loss: 0.004060056604042105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006877861870452762\n",
      "Training Loss: 0.006107178219826892\n",
      "Training Loss: 0.005983263900852762\n",
      "Validation Loss: 0.004027667255245484\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006839028673712164\n",
      "Training Loss: 0.006070938003831543\n",
      "Training Loss: 0.005949590904638171\n",
      "Validation Loss: 0.00399000910790951\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006794824918033555\n",
      "Training Loss: 0.006030514669255353\n",
      "Training Loss: 0.00591143632249441\n",
      "Validation Loss: 0.003946167643815069\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.0067445391684304926\n",
      "Training Loss: 0.005985637975391001\n",
      "Training Loss: 0.005868324293987826\n",
      "Validation Loss: 0.003895391534171538\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006687789242714643\n",
      "Training Loss: 0.005936459520598874\n",
      "Training Loss: 0.005820164909237064\n",
      "Validation Loss: 0.003837535469112604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.00662499041063711\n",
      "Training Loss: 0.005883928528055549\n",
      "Training Loss: 0.0057676578080281616\n",
      "Validation Loss: 0.003773748921099632\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.0065579408244229856\n",
      "Training Loss: 0.005830105962813832\n",
      "Training Loss: 0.0057126817695097995\n",
      "Validation Loss: 0.0037071045070855217\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.0064900702238082884\n",
      "Training Loss: 0.00577793902659323\n",
      "Training Loss: 0.0056581661576638\n",
      "Validation Loss: 0.0036422552402246367\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006425652204779908\n",
      "Training Loss: 0.0057301560451742265\n",
      "Training Loss: 0.005607044353382662\n",
      "Validation Loss: 0.0035835998911107106\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0063679956475971265\n",
      "Training Loss: 0.005687952868756838\n",
      "Training Loss: 0.005560869847540744\n",
      "Validation Loss: 0.003533229010122085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006318042225320824\n",
      "Training Loss: 0.005650735282106325\n",
      "Training Loss: 0.005519402507925406\n",
      "Validation Loss: 0.0034905861895740704\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006274656702298671\n",
      "Training Loss: 0.005616994793526829\n",
      "Training Loss: 0.005481365472078324\n",
      "Validation Loss: 0.0034536478643336995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.00623591614421457\n",
      "Training Loss: 0.005585259359213524\n",
      "Training Loss: 0.005445357121061534\n",
      "Validation Loss: 0.003420108477349571\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006200065082521178\n",
      "Training Loss: 0.005554470375645906\n",
      "Training Loss: 0.005410261747892946\n",
      "Validation Loss: 0.003387878773200294\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006165804067859426\n",
      "Training Loss: 0.0055239583039656285\n",
      "Training Loss: 0.005375304931076244\n",
      "Validation Loss: 0.003355265950805016\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006132281481404789\n",
      "Training Loss: 0.005493352160556242\n",
      "Training Loss: 0.005340044935583137\n",
      "Validation Loss: 0.0033211407423008946\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006099048307514749\n",
      "Training Loss: 0.005462555033154785\n",
      "Training Loss: 0.005304387685609982\n",
      "Validation Loss: 0.003285139450526095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006066026782500558\n",
      "Training Loss: 0.005431741283973679\n",
      "Training Loss: 0.005268575837253593\n",
      "Validation Loss: 0.0032476576710946522\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006033436404541135\n",
      "Training Loss: 0.005401286609703675\n",
      "Training Loss: 0.0052330969896866005\n",
      "Validation Loss: 0.0032095642226716774\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006001658776658587\n",
      "Training Loss: 0.0053716404514852914\n",
      "Training Loss: 0.005198524393490516\n",
      "Validation Loss: 0.0031718629604634524\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005971100366441533\n",
      "Training Loss: 0.005343226291006431\n",
      "Training Loss: 0.005165396864176728\n",
      "Validation Loss: 0.003135442345277563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.0059420959069393575\n",
      "Training Loss: 0.005316378540592268\n",
      "Training Loss: 0.005134135808330029\n",
      "Validation Loss: 0.003100987897892849\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005914877345203422\n",
      "Training Loss: 0.005291321716504172\n",
      "Training Loss: 0.005105014774017036\n",
      "Validation Loss: 0.0030689454543289174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005889560247887857\n",
      "Training Loss: 0.005268163910368457\n",
      "Training Loss: 0.005078161424025893\n",
      "Validation Loss: 0.003039544909993668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005866161800804548\n",
      "Training Loss: 0.005246908965636976\n",
      "Training Loss: 0.0050535767461406065\n",
      "Validation Loss: 0.003012832747955461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005844621965079569\n",
      "Training Loss: 0.005227476833970286\n",
      "Training Loss: 0.005031161273363977\n",
      "Validation Loss: 0.00298871720743397\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005824821317510213\n",
      "Training Loss: 0.0052097264194162565\n",
      "Training Loss: 0.005010747237829492\n",
      "Validation Loss: 0.002967017341740011\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005806610805739183\n",
      "Training Loss: 0.0051934846851509064\n",
      "Training Loss: 0.004992128141457215\n",
      "Validation Loss: 0.002947494592988508\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005789825149113312\n",
      "Training Loss: 0.00517856553546153\n",
      "Training Loss: 0.004975088917999529\n",
      "Validation Loss: 0.002929902126688217\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005774301783530973\n",
      "Training Loss: 0.005164793836884201\n",
      "Training Loss: 0.004959419266087934\n",
      "Validation Loss: 0.002913986748016408\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005759886179293972\n",
      "Training Loss: 0.005152008819277398\n",
      "Training Loss: 0.004944927605683915\n",
      "Validation Loss: 0.0028995207348691946\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005746436750050634\n",
      "Training Loss: 0.005140068389009684\n",
      "Training Loss: 0.00493144347332418\n",
      "Validation Loss: 0.0028862944225979486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005733829056262039\n",
      "Training Loss: 0.005128856265218929\n",
      "Training Loss: 0.004918822150793858\n",
      "Validation Loss: 0.002874128702305927\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005721957642526832\n",
      "Training Loss: 0.005118272478575818\n",
      "Training Loss: 0.004906940195360221\n",
      "Validation Loss: 0.00286286675209033\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005710729329730384\n",
      "Training Loss: 0.005108236317755655\n",
      "Training Loss: 0.004895695955492556\n",
      "Validation Loss: 0.0028523821122107213\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005700068129517604\n",
      "Training Loss: 0.005098684439435602\n",
      "Training Loss: 0.004885005908436142\n",
      "Validation Loss: 0.002842562082825291\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005689908514905255\n",
      "Training Loss: 0.005089562793727964\n",
      "Training Loss: 0.004874801018740982\n",
      "Validation Loss: 0.0028333192517237967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005680196297180373\n",
      "Training Loss: 0.005080829790676944\n",
      "Training Loss: 0.004865024446626193\n",
      "Validation Loss: 0.0028245744846101977\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005670886058942415\n",
      "Training Loss: 0.005072449435247108\n",
      "Training Loss: 0.004855630251695402\n",
      "Validation Loss: 0.0028162713776070498\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005661940556601621\n",
      "Training Loss: 0.005064392706262879\n",
      "Training Loss: 0.004846582535537891\n",
      "Validation Loss: 0.0028083581016974503\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.0056533299305010585\n",
      "Training Loss: 0.0050566371547756716\n",
      "Training Loss: 0.00483785112912301\n",
      "Validation Loss: 0.0028007968408683462\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005645027713035234\n",
      "Training Loss: 0.0050491655885707585\n",
      "Training Loss: 0.004829412520048208\n",
      "Validation Loss: 0.002793553595959596\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.00563701152743306\n",
      "Training Loss: 0.005041959087830037\n",
      "Training Loss: 0.004821245280909352\n",
      "Validation Loss: 0.002786601720942875\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005629264601448085\n",
      "Training Loss: 0.005035005685640499\n",
      "Training Loss: 0.004813335690414533\n",
      "Validation Loss: 0.0027799194461530013\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.0056217724314774386\n",
      "Training Loss: 0.005028293511131778\n",
      "Training Loss: 0.00480566983751487\n",
      "Validation Loss: 0.0027734890025569482\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005614519410883077\n",
      "Training Loss: 0.005021811586339027\n",
      "Training Loss: 0.004798235613270663\n",
      "Validation Loss: 0.002767296504684504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.0056074958361568865\n",
      "Training Loss: 0.0050155517883831635\n",
      "Training Loss: 0.004791025372105651\n",
      "Validation Loss: 0.0027613317014471618\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005600692699663341\n",
      "Training Loss: 0.005009505123598501\n",
      "Training Loss: 0.004784030709997751\n",
      "Validation Loss: 0.002755583520308974\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.00559410154557554\n",
      "Training Loss: 0.005003664604155347\n",
      "Training Loss: 0.004777244472061284\n",
      "Validation Loss: 0.0027500405748050365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.00558771540352609\n",
      "Training Loss: 0.00499802089529112\n",
      "Training Loss: 0.004770659960922785\n",
      "Validation Loss: 0.0027446943548445203\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.00558152575569693\n",
      "Training Loss: 0.004992569628520869\n",
      "Training Loss: 0.0047642721474403515\n",
      "Validation Loss: 0.002739536570086866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005575526779866777\n",
      "Training Loss: 0.0049873004079563545\n",
      "Training Loss: 0.00475807465845719\n",
      "Validation Loss: 0.0027345616142841036\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005569712430005893\n",
      "Training Loss: 0.004982207185821608\n",
      "Training Loss: 0.004752061387407593\n",
      "Validation Loss: 0.0027297577641517174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005564075984293595\n",
      "Training Loss: 0.0049772825930267574\n",
      "Training Loss: 0.004746226013521664\n",
      "Validation Loss: 0.0027251239935344273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005558613106841221\n",
      "Training Loss: 0.004972519096336328\n",
      "Training Loss: 0.004740563364466652\n",
      "Validation Loss: 0.0027206476896823374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005553315939614549\n",
      "Training Loss: 0.004967909685801714\n",
      "Training Loss: 0.004735065328422934\n",
      "Validation Loss: 0.0027163213922045707\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005548178927856497\n",
      "Training Loss: 0.004963446218171157\n",
      "Training Loss: 0.004729728492675349\n",
      "Validation Loss: 0.0027121398866103356\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005543195430655032\n",
      "Training Loss: 0.004959122152649797\n",
      "Training Loss: 0.0047245438839308915\n",
      "Validation Loss: 0.0027080940303727567\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005538359907804988\n",
      "Training Loss: 0.004954930279054679\n",
      "Training Loss: 0.00471950599225238\n",
      "Validation Loss: 0.0027041752558473625\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005533666578121483\n",
      "Training Loss: 0.0049508627044269815\n",
      "Training Loss: 0.004714609051588923\n",
      "Validation Loss: 0.0027003770385439813\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005529107566107996\n",
      "Training Loss: 0.004946912595769391\n",
      "Training Loss: 0.004709845983888954\n",
      "Validation Loss: 0.0026966952738414905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005524679577210918\n",
      "Training Loss: 0.0049430753104388716\n",
      "Training Loss: 0.00470521068200469\n",
      "Validation Loss: 0.0026931218093972694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005520374723128043\n",
      "Training Loss: 0.00493934171332512\n",
      "Training Loss: 0.004700697333319113\n",
      "Validation Loss: 0.0026896489147107336\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005516188563778997\n",
      "Training Loss: 0.00493570800870657\n",
      "Training Loss: 0.0046962997096125034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [17:44<07:29, 149.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0026862728612214912\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.0756915850751102\n",
      "Training Loss: 0.07332164742052555\n",
      "Training Loss: 0.07228668110445141\n",
      "Validation Loss: 0.07254908895224668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.07173549761995673\n",
      "Training Loss: 0.06929223340004682\n",
      "Training Loss: 0.06792311847209931\n",
      "Validation Loss: 0.06752153257891703\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.0663650020584464\n",
      "Training Loss: 0.06294372208416461\n",
      "Training Loss: 0.06030748184770346\n",
      "Validation Loss: 0.05825424688250831\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.056510394420474765\n",
      "Training Loss: 0.05171666641719639\n",
      "Training Loss: 0.0475356507115066\n",
      "Validation Loss: 0.04395019669043884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.04212333985604346\n",
      "Training Loss: 0.03694539275020361\n",
      "Training Loss: 0.03198342415504157\n",
      "Validation Loss: 0.0277379352620311\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.026522644967772065\n",
      "Training Loss: 0.02201429313980043\n",
      "Training Loss: 0.018296820404939355\n",
      "Validation Loss: 0.01628539836285322\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.016829363380093127\n",
      "Training Loss: 0.015051082326099276\n",
      "Training Loss: 0.013890049131587147\n",
      "Validation Loss: 0.013224043153498448\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.014424550946569071\n",
      "Training Loss: 0.013263253942131997\n",
      "Training Loss: 0.01250147853512317\n",
      "Validation Loss: 0.011816670699568276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.01321298250113614\n",
      "Training Loss: 0.012160030286759138\n",
      "Training Loss: 0.011513918149285018\n",
      "Validation Loss: 0.01075641379503303\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.012283215939532965\n",
      "Training Loss: 0.011291075008921325\n",
      "Training Loss: 0.010717478394508363\n",
      "Validation Loss: 0.00988668492168523\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.011522102979943155\n",
      "Training Loss: 0.010576483090408146\n",
      "Training Loss: 0.01005598000017926\n",
      "Validation Loss: 0.009154769531817417\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.010887943371199072\n",
      "Training Loss: 0.009980439855717123\n",
      "Training Loss: 0.009501224404666574\n",
      "Validation Loss: 0.008533247066347787\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.010357086996082216\n",
      "Training Loss: 0.009480837471783162\n",
      "Training Loss: 0.009034936461830512\n",
      "Validation Loss: 0.008003499918732416\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.009912378878798336\n",
      "Training Loss: 0.009061215773690491\n",
      "Training Loss: 0.008642778099747374\n",
      "Validation Loss: 0.007550546295124661\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.009539591115899384\n",
      "Training Loss: 0.008708095614565536\n",
      "Training Loss: 0.008312526172958315\n",
      "Validation Loss: 0.007161587529051839\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.009226507494458929\n",
      "Training Loss: 0.008410124377114699\n",
      "Training Loss: 0.008033669868018479\n",
      "Validation Loss: 0.0068257233033772935\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.008962782346643508\n",
      "Training Loss: 0.00815786474966444\n",
      "Training Loss: 0.007797404294833541\n",
      "Validation Loss: 0.0065339126642479485\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.008739903131499886\n",
      "Training Loss: 0.007943642439786345\n",
      "Training Loss: 0.007596580244135112\n",
      "Validation Loss: 0.0062789075152446215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.008551066251238809\n",
      "Training Loss: 0.007761357688577846\n",
      "Training Loss: 0.007425534692592919\n",
      "Validation Loss: 0.00605502721246625\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.008390910280868412\n",
      "Training Loss: 0.0076062096003443\n",
      "Training Loss: 0.0072798285353928805\n",
      "Validation Loss: 0.005857905754924155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008255214447854087\n",
      "Training Loss: 0.007474385343957692\n",
      "Training Loss: 0.0071559293533209715\n",
      "Validation Loss: 0.005684151234408694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.008140568158123642\n",
      "Training Loss: 0.007362772966735065\n",
      "Training Loss: 0.0070509317074902355\n",
      "Validation Loss: 0.005531053834469214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008044094303622842\n",
      "Training Loss: 0.007268694301601499\n",
      "Training Loss: 0.0069623098254669455\n",
      "Validation Loss: 0.0053963365185001255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.007963259095558896\n",
      "Training Loss: 0.007189759789034724\n",
      "Training Loss: 0.006887791618937627\n",
      "Validation Loss: 0.005277996690514801\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.00789576152805239\n",
      "Training Loss: 0.007123771246988327\n",
      "Training Loss: 0.006825288895051926\n",
      "Validation Loss: 0.005174217404263994\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007839491532649845\n",
      "Training Loss: 0.007068702435353771\n",
      "Training Loss: 0.0067728854832239446\n",
      "Validation Loss: 0.005083312293556467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.007792531403247267\n",
      "Training Loss: 0.007022712305188179\n",
      "Training Loss: 0.006728854373795912\n",
      "Validation Loss: 0.00500372831782933\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007753168044146151\n",
      "Training Loss: 0.006984153150115162\n",
      "Training Loss: 0.006691670854343102\n",
      "Validation Loss: 0.004934044567909971\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.007719908491708338\n",
      "Training Loss: 0.006951591613469646\n",
      "Training Loss: 0.006660017124377191\n",
      "Validation Loss: 0.004872973245736002\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.007691475013270974\n",
      "Training Loss: 0.006923803684767336\n",
      "Training Loss: 0.006632781126536429\n",
      "Validation Loss: 0.004819357992683569\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.0076668040652293715\n",
      "Training Loss: 0.0068997694901190695\n",
      "Training Loss: 0.006609041761839763\n",
      "Validation Loss: 0.00477219064160135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007645023463992402\n",
      "Training Loss: 0.00687865145271644\n",
      "Training Loss: 0.0065880470420233905\n",
      "Validation Loss: 0.004730568428293624\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007625430806074291\n",
      "Training Loss: 0.006859775839839131\n",
      "Training Loss: 0.006569191287271678\n",
      "Validation Loss: 0.004693723328181364\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007607469639042392\n",
      "Training Loss: 0.006842605612473562\n",
      "Training Loss: 0.0065519941644743085\n",
      "Validation Loss: 0.004660964923848011\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007590702587040141\n",
      "Training Loss: 0.006826717611402273\n",
      "Training Loss: 0.0065360738150775435\n",
      "Validation Loss: 0.004631703642322525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007574793599778786\n",
      "Training Loss: 0.006811783229932189\n",
      "Training Loss: 0.006521134288050234\n",
      "Validation Loss: 0.004605432594289187\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0075594873924274\n",
      "Training Loss: 0.006797552476637065\n",
      "Training Loss: 0.006506945454748348\n",
      "Validation Loss: 0.004581710727874901\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007544591771438718\n",
      "Training Loss: 0.006783834644593299\n",
      "Training Loss: 0.006493331085657701\n",
      "Validation Loss: 0.004560156139346321\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007529967335285619\n",
      "Training Loss: 0.006770487260073423\n",
      "Training Loss: 0.006480155935278163\n",
      "Validation Loss: 0.004540443561678187\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007515512442914769\n",
      "Training Loss: 0.006757403921801597\n",
      "Training Loss: 0.006467318125069141\n",
      "Validation Loss: 0.004522292341074247\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007501157259102911\n",
      "Training Loss: 0.00674451029044576\n",
      "Training Loss: 0.00645474057411775\n",
      "Validation Loss: 0.00450546427347352\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007486853493610397\n",
      "Training Loss: 0.006731749675236642\n",
      "Training Loss: 0.006442365971161053\n",
      "Validation Loss: 0.004489748033673994\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007472572335973382\n",
      "Training Loss: 0.006719085285440087\n",
      "Training Loss: 0.006430152106331661\n",
      "Validation Loss: 0.004474972304103248\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007458296403056011\n",
      "Training Loss: 0.006706490906653925\n",
      "Training Loss: 0.0064180686417967085\n",
      "Validation Loss: 0.004460989679419174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.00744401773205027\n",
      "Training Loss: 0.006693951264023781\n",
      "Training Loss: 0.006406093830591999\n",
      "Validation Loss: 0.0044476829856383\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007429735951591283\n",
      "Training Loss: 0.006681457136292011\n",
      "Training Loss: 0.006394214173778892\n",
      "Validation Loss: 0.0044349441265561776\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007415454394649714\n",
      "Training Loss: 0.006669003234710544\n",
      "Training Loss: 0.00638241870328784\n",
      "Validation Loss: 0.004422686664450369\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007401180256856606\n",
      "Training Loss: 0.006656589850317687\n",
      "Training Loss: 0.006370702594867907\n",
      "Validation Loss: 0.004410846699081528\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007386922412551939\n",
      "Training Loss: 0.0066442191507667305\n",
      "Training Loss: 0.006359065226279199\n",
      "Validation Loss: 0.004399362611427401\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007372691802447662\n",
      "Training Loss: 0.0066318955342285334\n",
      "Training Loss: 0.0063475055224262176\n",
      "Validation Loss: 0.004388191051823035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007358500038972125\n",
      "Training Loss: 0.006619623396545649\n",
      "Training Loss: 0.006336027477518655\n",
      "Validation Loss: 0.004377294755593109\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.0073443595285061745\n",
      "Training Loss: 0.006607411364093423\n",
      "Training Loss: 0.006324635038035922\n",
      "Validation Loss: 0.00436664407607168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007330283300252632\n",
      "Training Loss: 0.006595264849020168\n",
      "Training Loss: 0.0063133342238143085\n",
      "Validation Loss: 0.004356218444371826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007316284087719396\n",
      "Training Loss: 0.006583193923579529\n",
      "Training Loss: 0.006302131903357804\n",
      "Validation Loss: 0.00434599958459606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0073023759247735145\n",
      "Training Loss: 0.006571206045337021\n",
      "Training Loss: 0.006291033803718165\n",
      "Validation Loss: 0.004335979934980611\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007288570405216887\n",
      "Training Loss: 0.006559309537988156\n",
      "Training Loss: 0.00628004792961292\n",
      "Validation Loss: 0.004326148679364849\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007274880524491891\n",
      "Training Loss: 0.006547512300312519\n",
      "Training Loss: 0.0062691822409396995\n",
      "Validation Loss: 0.0043164969662602025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007261319517856464\n",
      "Training Loss: 0.0065358224895317105\n",
      "Training Loss: 0.006258442617254331\n",
      "Validation Loss: 0.004307020914065913\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007247897220076993\n",
      "Training Loss: 0.006524247125489637\n",
      "Training Loss: 0.006247834618552588\n",
      "Validation Loss: 0.004297722186605456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007234624525299296\n",
      "Training Loss: 0.0065127927146386355\n",
      "Training Loss: 0.006237363811233081\n",
      "Validation Loss: 0.004288596241719226\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007221510433591902\n",
      "Training Loss: 0.0065014655468985435\n",
      "Training Loss: 0.00622703461383935\n",
      "Validation Loss: 0.004279637433264111\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007208562678424641\n",
      "Training Loss: 0.006490269917994737\n",
      "Training Loss: 0.006216850060736761\n",
      "Validation Loss: 0.00427085124257052\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007195788470562547\n",
      "Training Loss: 0.0064792104519438\n",
      "Training Loss: 0.006206813295721076\n",
      "Validation Loss: 0.004262234286399914\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007183194167446345\n",
      "Training Loss: 0.006468291006749496\n",
      "Training Loss: 0.0061969247442903\n",
      "Validation Loss: 0.004253786188037543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007170784781919792\n",
      "Training Loss: 0.006457513550994918\n",
      "Training Loss: 0.006187187161995098\n",
      "Validation Loss: 0.004245506302621006\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007158563721459359\n",
      "Training Loss: 0.006446883276803419\n",
      "Training Loss: 0.006177599324728362\n",
      "Validation Loss: 0.004237393822437257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007146536181680858\n",
      "Training Loss: 0.006436399454250932\n",
      "Training Loss: 0.006168161337845959\n",
      "Validation Loss: 0.004229444375371539\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0071347036317456515\n",
      "Training Loss: 0.006426064262632281\n",
      "Training Loss: 0.006158871329971589\n",
      "Validation Loss: 0.004221651397692563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0071230681438464675\n",
      "Training Loss: 0.006415879777632654\n",
      "Training Loss: 0.00614972896000836\n",
      "Validation Loss: 0.00421402125692602\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007111633343156427\n",
      "Training Loss: 0.006405848715221509\n",
      "Training Loss: 0.006140734363580123\n",
      "Validation Loss: 0.004206550930197654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0071003993437625465\n",
      "Training Loss: 0.006395969707518816\n",
      "Training Loss: 0.006131883940543048\n",
      "Validation Loss: 0.0041992244385235175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.0070893683284521105\n",
      "Training Loss: 0.006386246234178543\n",
      "Training Loss: 0.006123177966801449\n",
      "Validation Loss: 0.004192049067017486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007078541547525674\n",
      "Training Loss: 0.006376679715467617\n",
      "Training Loss: 0.006114614321268164\n",
      "Validation Loss: 0.0041850179713742615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007067921363050118\n",
      "Training Loss: 0.006367271230556071\n",
      "Training Loss: 0.006106192321749404\n",
      "Validation Loss: 0.004178127397526725\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007057508684229105\n",
      "Training Loss: 0.00635802308563143\n",
      "Training Loss: 0.006097912846598774\n",
      "Validation Loss: 0.004171375049215355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007047304168809205\n",
      "Training Loss: 0.006348938720766455\n",
      "Training Loss: 0.006089772877749056\n",
      "Validation Loss: 0.00416475814947691\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007037309451261535\n",
      "Training Loss: 0.0063400181708857415\n",
      "Training Loss: 0.0060817736189346765\n",
      "Validation Loss: 0.004158262669611094\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007027525687590241\n",
      "Training Loss: 0.006331264589680359\n",
      "Training Loss: 0.006073914385051466\n",
      "Validation Loss: 0.00415189804122103\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007017953959293663\n",
      "Training Loss: 0.0063226809573825446\n",
      "Training Loss: 0.006066193918813951\n",
      "Validation Loss: 0.004145650797408451\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007008594939252362\n",
      "Training Loss: 0.006314269902650267\n",
      "Training Loss: 0.0060586138919461515\n",
      "Validation Loss: 0.004139520811293734\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006999449174618349\n",
      "Training Loss: 0.006306033661821857\n",
      "Training Loss: 0.006051173228188418\n",
      "Validation Loss: 0.00413350112804243\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006990516716032289\n",
      "Training Loss: 0.0062979732954408976\n",
      "Training Loss: 0.006043872690061108\n",
      "Validation Loss: 0.004127590374869368\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006981797001790255\n",
      "Training Loss: 0.006290091599803418\n",
      "Training Loss: 0.006036710881744511\n",
      "Validation Loss: 0.004121785481156928\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006973290281603112\n",
      "Training Loss: 0.006282389184925705\n",
      "Training Loss: 0.006029688726412133\n",
      "Validation Loss: 0.004116081106188717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006964995102607645\n",
      "Training Loss: 0.00627486924175173\n",
      "Training Loss: 0.00602280515129678\n",
      "Validation Loss: 0.0041104752826766015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006956909583532251\n",
      "Training Loss: 0.006267530376790092\n",
      "Training Loss: 0.0060160598543006925\n",
      "Validation Loss: 0.004104965019139221\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006949032348929904\n",
      "Training Loss: 0.006260373794939369\n",
      "Training Loss: 0.006009452295256778\n",
      "Validation Loss: 0.0040995460572610665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006941359146148898\n",
      "Training Loss: 0.00625339942635037\n",
      "Training Loss: 0.006002981123165227\n",
      "Validation Loss: 0.00409421787840534\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006933888647472486\n",
      "Training Loss: 0.006246605121996254\n",
      "Training Loss: 0.0059966443665325645\n",
      "Validation Loss: 0.004088978445957928\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006926616274868138\n",
      "Training Loss: 0.006239989968016744\n",
      "Training Loss: 0.005990440736641176\n",
      "Validation Loss: 0.004083821603641165\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0069195378967560825\n",
      "Training Loss: 0.0062335515161976215\n",
      "Training Loss: 0.005984368682838977\n",
      "Validation Loss: 0.004078745775270077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006912648784345947\n",
      "Training Loss: 0.006227288334630429\n",
      "Training Loss: 0.0059784242691239345\n",
      "Validation Loss: 0.004073749531813887\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006905944750178606\n",
      "Training Loss: 0.006221196197438985\n",
      "Training Loss: 0.005972606171271763\n",
      "Validation Loss: 0.00406883099362296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006899419368128292\n",
      "Training Loss: 0.006215271520195529\n",
      "Training Loss: 0.005966911290888674\n",
      "Validation Loss: 0.004063991685868816\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006893067249911837\n",
      "Training Loss: 0.006209510922199115\n",
      "Training Loss: 0.005961336673935875\n",
      "Validation Loss: 0.004059221205730619\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006886883230763487\n",
      "Training Loss: 0.006203909683972597\n",
      "Training Loss: 0.005955878116656095\n",
      "Validation Loss: 0.004054525558407722\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006880860582459718\n",
      "Training Loss: 0.0061984622932504865\n",
      "Training Loss: 0.005950533008435741\n",
      "Validation Loss: 0.004049895241008967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006874992670491338\n",
      "Training Loss: 0.0061931652354542165\n",
      "Training Loss: 0.005945297536673024\n",
      "Validation Loss: 0.004045335142977871\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006869274046039209\n",
      "Training Loss: 0.006188013054197654\n",
      "Training Loss: 0.005940168027300388\n",
      "Validation Loss: 0.0040408431413698565\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006863698838278651\n",
      "Training Loss: 0.0061829989263787865\n",
      "Training Loss: 0.005935140503570437\n",
      "Validation Loss: 0.004036410998819877\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00685825934051536\n",
      "Training Loss: 0.006178118716925383\n",
      "Training Loss: 0.005930210524238646\n",
      "Validation Loss: 0.004032040464173835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006852949828607961\n",
      "Training Loss: 0.006173366970615462\n",
      "Training Loss: 0.005925375210354105\n",
      "Validation Loss: 0.00402773177680256\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006847764944541268\n",
      "Training Loss: 0.006168737320695073\n",
      "Training Loss: 0.00592062957060989\n",
      "Validation Loss: 0.0040234814566727515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006842697631218471\n",
      "Training Loss: 0.0061642249498981986\n",
      "Training Loss: 0.005915970416390337\n",
      "Validation Loss: 0.004019289242830964\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0068377425789367405\n",
      "Training Loss: 0.006159824478672817\n",
      "Training Loss: 0.005911394448485225\n",
      "Validation Loss: 0.0040151530299911255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006832894160179421\n",
      "Training Loss: 0.00615553084295243\n",
      "Training Loss: 0.005906896874657832\n",
      "Validation Loss: 0.004011068559837726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006828146957559511\n",
      "Training Loss: 0.0061513378762174395\n",
      "Training Loss: 0.005902474501053803\n",
      "Validation Loss: 0.004007037647926573\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006823495065327733\n",
      "Training Loss: 0.006147241202415899\n",
      "Training Loss: 0.005898123591323383\n",
      "Validation Loss: 0.004003057738352734\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006818933518370613\n",
      "Training Loss: 0.0061432353185955435\n",
      "Training Loss: 0.0058938407269306485\n",
      "Validation Loss: 0.003999127997242416\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006814458176959306\n",
      "Training Loss: 0.006139315572800115\n",
      "Training Loss: 0.0058896221563918515\n",
      "Validation Loss: 0.003995246682289904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006810063307057135\n",
      "Training Loss: 0.006135478019714356\n",
      "Training Loss: 0.005885464826715179\n",
      "Validation Loss: 0.003991413365112974\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0068057443166617305\n",
      "Training Loss: 0.006131716356612742\n",
      "Training Loss: 0.005881365103996359\n",
      "Validation Loss: 0.00398761998296956\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006801497805863619\n",
      "Training Loss: 0.00612802839721553\n",
      "Training Loss: 0.005877320532454178\n",
      "Validation Loss: 0.003983875049874605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006797318428289146\n",
      "Training Loss: 0.006124408461619169\n",
      "Training Loss: 0.005873327336739748\n",
      "Validation Loss: 0.003980169706491314\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.0067932028631912545\n",
      "Training Loss: 0.00612085297354497\n",
      "Training Loss: 0.005869383424869739\n",
      "Validation Loss: 0.003976509125453284\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006789147226372734\n",
      "Training Loss: 0.006117358158808202\n",
      "Training Loss: 0.005865484857349656\n",
      "Validation Loss: 0.003972889123003135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006785147991031408\n",
      "Training Loss: 0.006113919490016997\n",
      "Training Loss: 0.00586163007945288\n",
      "Validation Loss: 0.003969306638333528\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.00678120118798688\n",
      "Training Loss: 0.006110534673789516\n",
      "Training Loss: 0.005857815212220885\n",
      "Validation Loss: 0.003965758156094156\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.00677730331546627\n",
      "Training Loss: 0.006107198708923534\n",
      "Training Loss: 0.005854038732359186\n",
      "Validation Loss: 0.003962245510807366\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006773451577173546\n",
      "Training Loss: 0.006103910011006519\n",
      "Training Loss: 0.005850297286524437\n",
      "Validation Loss: 0.003958769206721545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006769642385770567\n",
      "Training Loss: 0.0061006642156280575\n",
      "Training Loss: 0.005846588962012902\n",
      "Validation Loss: 0.003955327504801072\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006765873673721216\n",
      "Training Loss: 0.0060974581039045005\n",
      "Training Loss: 0.005842910974170081\n",
      "Validation Loss: 0.003951917929585312\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006762141729705035\n",
      "Training Loss: 0.006094289788743481\n",
      "Training Loss: 0.00583926253952086\n",
      "Validation Loss: 0.00394853667010752\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006758444044389762\n",
      "Training Loss: 0.006091155961621552\n",
      "Training Loss: 0.005835639386205003\n",
      "Validation Loss: 0.003945183196612582\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006754777833120898\n",
      "Training Loss: 0.006088053791318089\n",
      "Training Loss: 0.005832040201639756\n",
      "Validation Loss: 0.0039418604678809225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006751139970729128\n",
      "Training Loss: 0.006084980746963993\n",
      "Training Loss: 0.005828463011421263\n",
      "Validation Loss: 0.003938561430975293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.00674752879422158\n",
      "Training Loss: 0.006081934209214523\n",
      "Training Loss: 0.005824905942426994\n",
      "Validation Loss: 0.003935286909305187\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.0067439414671389385\n",
      "Training Loss: 0.006078911105869338\n",
      "Training Loss: 0.005821367448661476\n",
      "Validation Loss: 0.003932038430004182\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00674037556629628\n",
      "Training Loss: 0.006075910531217232\n",
      "Training Loss: 0.005817843705881387\n",
      "Validation Loss: 0.003928809316849776\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006736829238361679\n",
      "Training Loss: 0.006072929289657623\n",
      "Training Loss: 0.005814334827009588\n",
      "Validation Loss: 0.003925601974668588\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.0067332998057827356\n",
      "Training Loss: 0.0060699651238974184\n",
      "Training Loss: 0.00581083802273497\n",
      "Validation Loss: 0.00392241231191987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006729784418130293\n",
      "Training Loss: 0.006067016153829172\n",
      "Training Loss: 0.005807351825060323\n",
      "Validation Loss: 0.003919242565347447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006726282216841355\n",
      "Training Loss: 0.00606408022576943\n",
      "Training Loss: 0.005803873709519394\n",
      "Validation Loss: 0.003916087966501253\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006722790466155857\n",
      "Training Loss: 0.006061155743664131\n",
      "Training Loss: 0.0058004033222096045\n",
      "Validation Loss: 0.003912947034029018\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.0067193075438262894\n",
      "Training Loss: 0.006058239650446922\n",
      "Training Loss: 0.005796937635168433\n",
      "Validation Loss: 0.003909819146613015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006715830479515716\n",
      "Training Loss: 0.006055331171955914\n",
      "Training Loss: 0.005793475597747602\n",
      "Validation Loss: 0.003906706835305465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006712358230142854\n",
      "Training Loss: 0.006052428199909627\n",
      "Training Loss: 0.005790015258826315\n",
      "Validation Loss: 0.003903606367621864\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006708888915018179\n",
      "Training Loss: 0.006049528328003362\n",
      "Training Loss: 0.005786555490340106\n",
      "Validation Loss: 0.0039005154525348393\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006705419393256307\n",
      "Training Loss: 0.006046629735501483\n",
      "Training Loss: 0.005783094319631346\n",
      "Validation Loss: 0.003897425579953562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006701949285925366\n",
      "Training Loss: 0.006043732033576816\n",
      "Training Loss: 0.005779629897442646\n",
      "Validation Loss: 0.0038943510611143925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006698476531892083\n",
      "Training Loss: 0.006040832744911313\n",
      "Training Loss: 0.0057761610910529275\n",
      "Validation Loss: 0.0038912821779540333\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.0066949984763050455\n",
      "Training Loss: 0.006037930070888251\n",
      "Training Loss: 0.0057726861699484286\n",
      "Validation Loss: 0.003888214434094183\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006691513573168777\n",
      "Training Loss: 0.006035020799608901\n",
      "Training Loss: 0.005769203528179787\n",
      "Validation Loss: 0.003885148761511435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00668801995983813\n",
      "Training Loss: 0.0060321056691464035\n",
      "Training Loss: 0.005765711782732979\n",
      "Validation Loss: 0.003882085066027102\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006684515953529626\n",
      "Training Loss: 0.006029182419879362\n",
      "Training Loss: 0.005762208097730763\n",
      "Validation Loss: 0.0038790249624132606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0066809995181392875\n",
      "Training Loss: 0.006026247940026224\n",
      "Training Loss: 0.005758693669340573\n",
      "Validation Loss: 0.0038759596135697506\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006677468995912932\n",
      "Training Loss: 0.006023302443791181\n",
      "Training Loss: 0.005755163910798729\n",
      "Validation Loss: 0.0038728938540025207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006673922185436823\n",
      "Training Loss: 0.006020343044074252\n",
      "Training Loss: 0.005751618475769647\n",
      "Validation Loss: 0.003869825058480662\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006670358270639554\n",
      "Training Loss: 0.006017369104083628\n",
      "Training Loss: 0.005748055758886039\n",
      "Validation Loss: 0.003866747357840702\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006666774530895054\n",
      "Training Loss: 0.006014378187246621\n",
      "Training Loss: 0.005744474158273078\n",
      "Validation Loss: 0.0038636623570444378\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006663169355597347\n",
      "Training Loss: 0.006011368982726708\n",
      "Training Loss: 0.005740872269961983\n",
      "Validation Loss: 0.0038605726852468897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006659540440887212\n",
      "Training Loss: 0.0060083397314883765\n",
      "Training Loss: 0.005737247769138776\n",
      "Validation Loss: 0.003857470402007483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006655886196531355\n",
      "Training Loss: 0.006005288109881803\n",
      "Training Loss: 0.005733599313534796\n",
      "Validation Loss: 0.0038543542574965553\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006652204921119846\n",
      "Training Loss: 0.006002213481115177\n",
      "Training Loss: 0.005729925156338141\n",
      "Validation Loss: 0.003851227644762924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.0066484944702824575\n",
      "Training Loss: 0.005999113661237061\n",
      "Training Loss: 0.005726224244572222\n",
      "Validation Loss: 0.003848088169444269\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006644753341679461\n",
      "Training Loss: 0.005995987164787948\n",
      "Training Loss: 0.005722493716748431\n",
      "Validation Loss: 0.0038449311696955664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006640978882205673\n",
      "Training Loss: 0.005992831698385999\n",
      "Training Loss: 0.0057187330367742106\n",
      "Validation Loss: 0.0038417574610780884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.0066371695470297705\n",
      "Training Loss: 0.005989646728849039\n",
      "Training Loss: 0.005714939909521491\n",
      "Validation Loss: 0.003838563628960401\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006633323048008606\n",
      "Training Loss: 0.005986428351607173\n",
      "Training Loss: 0.005711111811688169\n",
      "Validation Loss: 0.0038353503397875206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006629438178497367\n",
      "Training Loss: 0.005983176893787459\n",
      "Training Loss: 0.005707247843965888\n",
      "Validation Loss: 0.00383211409717568\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.0066255113523220645\n",
      "Training Loss: 0.005979889438021928\n",
      "Training Loss: 0.005703345162910409\n",
      "Validation Loss: 0.0038288497984712796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006621541160275229\n",
      "Training Loss: 0.005976564163574949\n",
      "Training Loss: 0.005699403443140909\n",
      "Validation Loss: 0.003825564838009311\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006617525632609613\n",
      "Training Loss: 0.005973198640858755\n",
      "Training Loss: 0.005695418007089757\n",
      "Validation Loss: 0.003822252933286507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006613461986999028\n",
      "Training Loss: 0.005969791712705046\n",
      "Training Loss: 0.005691390350693837\n",
      "Validation Loss: 0.0038189092916541137\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006609349502250552\n",
      "Training Loss: 0.005966340859886259\n",
      "Training Loss: 0.005687315558898263\n",
      "Validation Loss: 0.0038155364238897736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006605183909996413\n",
      "Training Loss: 0.0059628438670188185\n",
      "Training Loss: 0.005683192196302116\n",
      "Validation Loss: 0.003812128088693396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.00660096415609587\n",
      "Training Loss: 0.0059592989983502775\n",
      "Training Loss: 0.005679018194205128\n",
      "Validation Loss: 0.0038086888602715026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006596686536213383\n",
      "Training Loss: 0.005955703270155936\n",
      "Training Loss: 0.005674790967023\n",
      "Validation Loss: 0.003805209261893682\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006592348956619389\n",
      "Training Loss: 0.0059520548104774204\n",
      "Training Loss: 0.005670508838375099\n",
      "Validation Loss: 0.003801694644740626\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006587950192624703\n",
      "Training Loss: 0.005948351932456717\n",
      "Training Loss: 0.0056661688775056975\n",
      "Validation Loss: 0.0037981342835602967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006583486124873162\n",
      "Training Loss: 0.005944590844446793\n",
      "Training Loss: 0.005661768958671019\n",
      "Validation Loss: 0.0037945346204365155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.0065789544861763714\n",
      "Training Loss: 0.005940770680317655\n",
      "Training Loss: 0.0056573065283009785\n",
      "Validation Loss: 0.0037908910386515466\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006574352050665766\n",
      "Training Loss: 0.005936887413263321\n",
      "Training Loss: 0.005652778149815276\n",
      "Validation Loss: 0.003787198138627318\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006569676211802289\n",
      "Training Loss: 0.005932939189951867\n",
      "Training Loss: 0.005648182602599263\n",
      "Validation Loss: 0.0037834581276768127\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006564924730919301\n",
      "Training Loss: 0.005928922697203234\n",
      "Training Loss: 0.0056435152381891385\n",
      "Validation Loss: 0.003779664003222218\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006560093376901932\n",
      "Training Loss: 0.005924835450714454\n",
      "Training Loss: 0.0056387741136131805\n",
      "Validation Loss: 0.003775815658004473\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.0065551797783700745\n",
      "Training Loss: 0.005920674519147724\n",
      "Training Loss: 0.005633956485544331\n",
      "Validation Loss: 0.0037719102758192197\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006550180926569738\n",
      "Training Loss: 0.005916436616098508\n",
      "Training Loss: 0.005629058058839291\n",
      "Validation Loss: 0.003767946729792303\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006545092808664776\n",
      "Training Loss: 0.005912118746200576\n",
      "Training Loss: 0.0056240771804004904\n",
      "Validation Loss: 0.0037639185052438314\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006539912046282553\n",
      "Training Loss: 0.005907717868685722\n",
      "Training Loss: 0.005619009079528042\n",
      "Validation Loss: 0.0037598264759427374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006534635626012459\n",
      "Training Loss: 0.0059032298007514325\n",
      "Training Loss: 0.00561385044187773\n",
      "Validation Loss: 0.0037556652861301986\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006529259455273859\n",
      "Training Loss: 0.005898652197793126\n",
      "Training Loss: 0.005608599194674753\n",
      "Validation Loss: 0.0037514348658262177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006523780392599292\n",
      "Training Loss: 0.005893980718683451\n",
      "Training Loss: 0.005603249505511485\n",
      "Validation Loss: 0.0037471336100689985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006518193505471572\n",
      "Training Loss: 0.005889211441390216\n",
      "Training Loss: 0.005597799253882841\n",
      "Validation Loss: 0.0037427551467547173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006512495835777372\n",
      "Training Loss: 0.005884340590564534\n",
      "Training Loss: 0.005592242430429906\n",
      "Validation Loss: 0.003738292316341082\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.0065066814940655604\n",
      "Training Loss: 0.0058793642511591315\n",
      "Training Loss: 0.005586575822671875\n",
      "Validation Loss: 0.0037337439102016054\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006500748436665162\n",
      "Training Loss: 0.005874278798000887\n",
      "Training Loss: 0.005580795685527846\n",
      "Validation Loss: 0.003729106386825233\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006494691026746296\n",
      "Training Loss: 0.005869079125113785\n",
      "Training Loss: 0.005574897347250953\n",
      "Validation Loss: 0.003724379183101897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006488504420267418\n",
      "Training Loss: 0.005863761509535834\n",
      "Training Loss: 0.00556887540849857\n",
      "Validation Loss: 0.00371955233113317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006482184738270007\n",
      "Training Loss: 0.005858320848783478\n",
      "Training Loss: 0.005562726070056669\n",
      "Validation Loss: 0.0037146236549989646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006475727843353525\n",
      "Training Loss: 0.005852753432700411\n",
      "Training Loss: 0.005556444303365425\n",
      "Validation Loss: 0.0037095949081233127\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006469128539320082\n",
      "Training Loss: 0.005847054719924927\n",
      "Training Loss: 0.005550024757976644\n",
      "Validation Loss: 0.0037044528701729823\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006462381554883905\n",
      "Training Loss: 0.005841218663845211\n",
      "Training Loss: 0.005543463003705256\n",
      "Validation Loss: 0.003699200855060533\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006455481838202104\n",
      "Training Loss: 0.005835241012973711\n",
      "Training Loss: 0.005536753573105671\n",
      "Validation Loss: 0.0036938294141783557\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006448425302514806\n",
      "Training Loss: 0.005829117783578113\n",
      "Training Loss: 0.005529891276382841\n",
      "Validation Loss: 0.0036883354736387394\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006441207361058332\n",
      "Training Loss: 0.005822842399356887\n",
      "Training Loss: 0.005522871357388794\n",
      "Validation Loss: 0.00368271449455217\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006433822425897233\n",
      "Training Loss: 0.005816411746200174\n",
      "Training Loss: 0.005515688060550019\n",
      "Validation Loss: 0.0036769578654549263\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006426264894544147\n",
      "Training Loss: 0.005809820105787367\n",
      "Training Loss: 0.005508336466737092\n",
      "Validation Loss: 0.0036710683881629553\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006418532459065318\n",
      "Training Loss: 0.005803063316270709\n",
      "Training Loss: 0.0055008122115395965\n",
      "Validation Loss: 0.0036650313426782326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006410618614172563\n",
      "Training Loss: 0.005796136612771079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [20:12<04:58, 149.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0054931094212224705\n",
      "Validation Loss: 0.0036588481161743402\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.13150465864688157\n",
      "Training Loss: 0.10718462876975536\n",
      "Training Loss: 0.09323472745716571\n",
      "Validation Loss: 0.08598459892895784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.08413754904642701\n",
      "Training Loss: 0.07942399507388473\n",
      "Training Loss: 0.0770998626947403\n",
      "Validation Loss: 0.07599474929189415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.07529293091967702\n",
      "Training Loss: 0.07266986768692732\n",
      "Training Loss: 0.07148431835696101\n",
      "Validation Loss: 0.07112480427944258\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.07066534858196974\n",
      "Training Loss: 0.0685205659456551\n",
      "Training Loss: 0.06769059110432864\n",
      "Validation Loss: 0.06748923862332039\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.06705920482054353\n",
      "Training Loss: 0.06483732940629125\n",
      "Training Loss: 0.06370582388713956\n",
      "Validation Loss: 0.06273605603348004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.06203921500593424\n",
      "Training Loss: 0.059014176316559315\n",
      "Training Loss: 0.05706715913489461\n",
      "Validation Loss: 0.055123936322130515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.054254633244127035\n",
      "Training Loss: 0.050688138650730255\n",
      "Training Loss: 0.04826347596943378\n",
      "Validation Loss: 0.04609684729843997\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.04524845287203789\n",
      "Training Loss: 0.04178991229273379\n",
      "Training Loss: 0.03934047029353678\n",
      "Validation Loss: 0.03766110750815172\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.03701936707831919\n",
      "Training Loss: 0.034122003139927984\n",
      "Training Loss: 0.03192794198170304\n",
      "Validation Loss: 0.03089632699789291\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.030583553891628982\n",
      "Training Loss: 0.028278657905757426\n",
      "Training Loss: 0.02637860665563494\n",
      "Validation Loss: 0.025772316616781976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.02583305599167943\n",
      "Training Loss: 0.023979727034457027\n",
      "Training Loss: 0.022330846972763538\n",
      "Validation Loss: 0.021911129962359922\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.022336798440665006\n",
      "Training Loss: 0.020800509969703854\n",
      "Training Loss: 0.019350377642549575\n",
      "Validation Loss: 0.01895925034297986\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.01971651299856603\n",
      "Training Loss: 0.018401264930143952\n",
      "Training Loss: 0.0171042900159955\n",
      "Validation Loss: 0.016652257197400492\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.017704883010592313\n",
      "Training Loss: 0.016547742271795868\n",
      "Training Loss: 0.015369646118488163\n",
      "Validation Loss: 0.014809137246874946\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.01612789131235331\n",
      "Training Loss: 0.01508848724886775\n",
      "Training Loss: 0.014005222527775913\n",
      "Validation Loss: 0.013310335372480449\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.01487428946653381\n",
      "Training Loss: 0.013926473865285515\n",
      "Training Loss: 0.01292096474673599\n",
      "Validation Loss: 0.012076144174704056\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.01387047314317897\n",
      "Training Loss: 0.01299700178904459\n",
      "Training Loss: 0.012055732714943588\n",
      "Validation Loss: 0.011049292118319969\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.013058053620625287\n",
      "Training Loss: 0.012167428265092895\n",
      "Training Loss: 0.011033333414234221\n",
      "Validation Loss: 0.009696660934832324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.011967638910282402\n",
      "Training Loss: 0.011232002743054182\n",
      "Training Loss: 0.010404330345336348\n",
      "Validation Loss: 0.008993632725199287\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.011448369893478229\n",
      "Training Loss: 0.01074756390764378\n",
      "Training Loss: 0.009974944854620844\n",
      "Validation Loss: 0.008435292888872242\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.01105264920508489\n",
      "Training Loss: 0.010377556647872552\n",
      "Training Loss: 0.009647890577325598\n",
      "Validation Loss: 0.007981167531969926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.010743121394189075\n",
      "Training Loss: 0.010087312597315759\n",
      "Training Loss: 0.009390761002432555\n",
      "Validation Loss: 0.007608516477937862\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.010493159325560554\n",
      "Training Loss: 0.009851198446704075\n",
      "Training Loss: 0.009178623600164429\n",
      "Validation Loss: 0.007298299904096495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.010280821417691186\n",
      "Training Loss: 0.00964850963326171\n",
      "Training Loss: 0.008992074091220275\n",
      "Validation Loss: 0.007035071367732762\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.01008976029115729\n",
      "Training Loss: 0.00946447221096605\n",
      "Training Loss: 0.00881829451303929\n",
      "Validation Loss: 0.006807909127366677\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.009910274640424177\n",
      "Training Loss: 0.009291054244386032\n",
      "Training Loss: 0.008651489834301173\n",
      "Validation Loss: 0.0066103045318505905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.009738845857791603\n",
      "Training Loss: 0.009125883489614352\n",
      "Training Loss: 0.008491126790177076\n",
      "Validation Loss: 0.0064383280722508105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.009575574708869682\n",
      "Training Loss: 0.008969264036277309\n",
      "Training Loss: 0.008338593355147169\n",
      "Validation Loss: 0.006288315530531527\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.009421078586019576\n",
      "Training Loss: 0.008821265008300543\n",
      "Training Loss: 0.008194603067822754\n",
      "Validation Loss: 0.0061557675526069285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.009274839255958796\n",
      "Training Loss: 0.008680574708851054\n",
      "Training Loss: 0.008058429934317246\n",
      "Validation Loss: 0.006035666086114525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.009135248153470457\n",
      "Training Loss: 0.008544997953576968\n",
      "Training Loss: 0.007928482213756069\n",
      "Validation Loss: 0.005923583515621512\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.009000603497261181\n",
      "Training Loss: 0.008412695097504183\n",
      "Training Loss: 0.007803417867980897\n",
      "Validation Loss: 0.005816739382813528\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.00887020398164168\n",
      "Training Loss: 0.008283260203897953\n",
      "Training Loss: 0.007683116685366258\n",
      "Validation Loss: 0.00571444685233422\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.00874493115232326\n",
      "Training Loss: 0.008158116462873295\n",
      "Training Loss: 0.007568997400812805\n",
      "Validation Loss: 0.005617831299327356\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008627049720380454\n",
      "Training Loss: 0.00804003981174901\n",
      "Training Loss: 0.0074634724564384665\n",
      "Validation Loss: 0.0055287871856420315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008519132147775964\n",
      "Training Loss: 0.007931833970360458\n",
      "Training Loss: 0.007368618356995284\n",
      "Validation Loss: 0.005448568411553383\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008422593310242518\n",
      "Training Loss: 0.00783481732942164\n",
      "Training Loss: 0.007284890275914222\n",
      "Validation Loss: 0.005376807593196379\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008336883845040576\n",
      "Training Loss: 0.007748230895958841\n",
      "Training Loss: 0.007210912915179506\n",
      "Validation Loss: 0.005311759136449755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008259940420975909\n",
      "Training Loss: 0.007669919531326741\n",
      "Training Loss: 0.0071443482197355475\n",
      "Validation Loss: 0.005251283902580735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.00818930758163333\n",
      "Training Loss: 0.007597501329146325\n",
      "Training Loss: 0.007082945501897484\n",
      "Validation Loss: 0.005193703603836593\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.0081229808810167\n",
      "Training Loss: 0.007529108425369486\n",
      "Training Loss: 0.007025057188002393\n",
      "Validation Loss: 0.005138035637347467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008059636113466695\n",
      "Training Loss: 0.007463549961103127\n",
      "Training Loss: 0.006969663701020181\n",
      "Validation Loss: 0.0050838678156998884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007998510608449578\n",
      "Training Loss: 0.007400162805570289\n",
      "Training Loss: 0.006916188956238329\n",
      "Validation Loss: 0.005031114139363923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.00793919772724621\n",
      "Training Loss: 0.007338615274056793\n",
      "Training Loss: 0.006864318603184074\n",
      "Validation Loss: 0.004979827804909496\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.00788148764637299\n",
      "Training Loss: 0.0072787584993056955\n",
      "Training Loss: 0.006813877432141453\n",
      "Validation Loss: 0.004930094243524324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007825274993665516\n",
      "Training Loss: 0.007220540469279513\n",
      "Training Loss: 0.0067647641443181784\n",
      "Validation Loss: 0.004881984288045572\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0077705153508577495\n",
      "Training Loss: 0.007163966011721641\n",
      "Training Loss: 0.006716923031490296\n",
      "Validation Loss: 0.00483554518805147\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007717211573617533\n",
      "Training Loss: 0.007109080606605858\n",
      "Training Loss: 0.006670341051649303\n",
      "Validation Loss: 0.004790815322011123\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007665416719391942\n",
      "Training Loss: 0.007055967833148316\n",
      "Training Loss: 0.006625051229493692\n",
      "Validation Loss: 0.00474782287289671\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007615230156807229\n",
      "Training Loss: 0.007004744772566482\n",
      "Training Loss: 0.006581128715770319\n",
      "Validation Loss: 0.004706605378763352\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007566799222258851\n",
      "Training Loss: 0.006955557420151308\n",
      "Training Loss: 0.006538695170311257\n",
      "Validation Loss: 0.004667196428701491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007520307323429734\n",
      "Training Loss: 0.006908570542000234\n",
      "Training Loss: 0.00649790114723146\n",
      "Validation Loss: 0.0046296259898462155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007475951915839687\n",
      "Training Loss: 0.006863942774944007\n",
      "Training Loss: 0.0064589104836341\n",
      "Validation Loss: 0.004593917602945244\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.0074339164583943785\n",
      "Training Loss: 0.006821808534441516\n",
      "Training Loss: 0.006421870969934389\n",
      "Validation Loss: 0.004560071310509791\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007394341769395396\n",
      "Training Loss: 0.006782250439282506\n",
      "Training Loss: 0.00638689175248146\n",
      "Validation Loss: 0.004528054317428071\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007357302926247939\n",
      "Training Loss: 0.006745288438396529\n",
      "Training Loss: 0.0063540246442426\n",
      "Validation Loss: 0.004497818605649923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007322796400403604\n",
      "Training Loss: 0.0067108710564207285\n",
      "Training Loss: 0.006323255362221971\n",
      "Validation Loss: 0.004469271623032439\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007290743979392573\n",
      "Training Loss: 0.006678882199339569\n",
      "Training Loss: 0.006294509664876387\n",
      "Validation Loss: 0.004442318636280474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.0072610093199182305\n",
      "Training Loss: 0.006649160601664334\n",
      "Training Loss: 0.006267666229978204\n",
      "Validation Loss: 0.004416832825289306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007233410220360384\n",
      "Training Loss: 0.006621511668199674\n",
      "Training Loss: 0.006242571028415114\n",
      "Validation Loss: 0.0043927041605753255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007207745963241905\n",
      "Training Loss: 0.006595730801345781\n",
      "Training Loss: 0.006219057319685817\n",
      "Validation Loss: 0.004369819897823454\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.0071838126599323\n",
      "Training Loss: 0.006571615672437474\n",
      "Training Loss: 0.00619695606874302\n",
      "Validation Loss: 0.004348063372530755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.00716141288052313\n",
      "Training Loss: 0.006548974434845149\n",
      "Training Loss: 0.0061761065438622606\n",
      "Validation Loss: 0.0043273441097175895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.0071403659228235486\n",
      "Training Loss: 0.006527636122191325\n",
      "Training Loss: 0.006156364816124551\n",
      "Validation Loss: 0.004307572052976263\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007120511494576931\n",
      "Training Loss: 0.0065074476401787255\n",
      "Training Loss: 0.006137599985813722\n",
      "Validation Loss: 0.004288661933969706\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007101706445682794\n",
      "Training Loss: 0.006488274775911122\n",
      "Training Loss: 0.006119698395486921\n",
      "Validation Loss: 0.004270540758983096\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007083828456234187\n",
      "Training Loss: 0.00647000442724675\n",
      "Training Loss: 0.006102562675951048\n",
      "Validation Loss: 0.0042531517454621835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.00706677163601853\n",
      "Training Loss: 0.0064525381894782186\n",
      "Training Loss: 0.006086109867319465\n",
      "Validation Loss: 0.004236429272444521\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007050444659544155\n",
      "Training Loss: 0.006435791640542448\n",
      "Training Loss: 0.006070267892209813\n",
      "Validation Loss: 0.004220319109356679\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007034768908051774\n",
      "Training Loss: 0.006419692313065752\n",
      "Training Loss: 0.006054975109873339\n",
      "Validation Loss: 0.004204768385330027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007019677674397826\n",
      "Training Loss: 0.006404180536046624\n",
      "Training Loss: 0.006040179902920499\n",
      "Validation Loss: 0.0041897392987399204\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007005112867336721\n",
      "Training Loss: 0.006389201977290213\n",
      "Training Loss: 0.006025837791385129\n",
      "Validation Loss: 0.004175190295202636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006991023191949353\n",
      "Training Loss: 0.006374710615491494\n",
      "Training Loss: 0.006011908026994206\n",
      "Validation Loss: 0.004161077387385029\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006977366162464023\n",
      "Training Loss: 0.006360666831023991\n",
      "Training Loss: 0.005998358944198117\n",
      "Validation Loss: 0.004147376712762196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006964103453792631\n",
      "Training Loss: 0.0063470364699605855\n",
      "Training Loss: 0.005985159597476013\n",
      "Validation Loss: 0.004134049694043364\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00695120204356499\n",
      "Training Loss: 0.006333789440104738\n",
      "Training Loss: 0.005972285113530234\n",
      "Validation Loss: 0.004121074124678886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006938633365789429\n",
      "Training Loss: 0.006320897977566346\n",
      "Training Loss: 0.005959712852491066\n",
      "Validation Loss: 0.004108423014626618\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.006926372104790062\n",
      "Training Loss: 0.006308339813258499\n",
      "Training Loss: 0.005947423517354764\n",
      "Validation Loss: 0.004096072691027075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006914395840140059\n",
      "Training Loss: 0.006296093297423795\n",
      "Training Loss: 0.0059353989758528765\n",
      "Validation Loss: 0.004084003539206565\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006902685313252732\n",
      "Training Loss: 0.006284139106282964\n",
      "Training Loss: 0.005923623715061694\n",
      "Validation Loss: 0.004072196733237904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006891223277198151\n",
      "Training Loss: 0.006272462116321549\n",
      "Training Loss: 0.005912083156290464\n",
      "Validation Loss: 0.004060634143723782\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006879994118353352\n",
      "Training Loss: 0.006261045407736674\n",
      "Training Loss: 0.005900764486868866\n",
      "Validation Loss: 0.004049295859720056\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.00686898387153633\n",
      "Training Loss: 0.006249876037472859\n",
      "Training Loss: 0.005889656218350865\n",
      "Validation Loss: 0.004038174811939008\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.0068581822549458596\n",
      "Training Loss: 0.006238943006610498\n",
      "Training Loss: 0.005878747766255401\n",
      "Validation Loss: 0.004027252221542797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006847576892469078\n",
      "Training Loss: 0.0062282336549833416\n",
      "Training Loss: 0.00586802965903189\n",
      "Validation Loss: 0.004016515652366569\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.0068371583020780235\n",
      "Training Loss: 0.006217736803228036\n",
      "Training Loss: 0.005857492501381785\n",
      "Validation Loss: 0.0040059585537594025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006826918683946133\n",
      "Training Loss: 0.0062074441672302786\n",
      "Training Loss: 0.0058471272402675824\n",
      "Validation Loss: 0.0039955643071070985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006816848434973508\n",
      "Training Loss: 0.0061973457492422316\n",
      "Training Loss: 0.005836926975753158\n",
      "Validation Loss: 0.003985326494524527\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006806941538816318\n",
      "Training Loss: 0.00618743454804644\n",
      "Training Loss: 0.005826883902773261\n",
      "Validation Loss: 0.003975232518958241\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006797190471552313\n",
      "Training Loss: 0.006177701293490827\n",
      "Training Loss: 0.005816990527673625\n",
      "Validation Loss: 0.003965278681932708\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006787590330932289\n",
      "Training Loss: 0.0061681387526914475\n",
      "Training Loss: 0.005807241006987169\n",
      "Validation Loss: 0.0039554524943075575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006778133849147707\n",
      "Training Loss: 0.006158740696264431\n",
      "Training Loss: 0.005797628193395212\n",
      "Validation Loss: 0.003945747766450173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006768816453404725\n",
      "Training Loss: 0.006149499140447006\n",
      "Training Loss: 0.005788145568803884\n",
      "Validation Loss: 0.003936155552039279\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006759632788598538\n",
      "Training Loss: 0.006140408765058964\n",
      "Training Loss: 0.005778788492898456\n",
      "Validation Loss: 0.003926671034881447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006750578150385991\n",
      "Training Loss: 0.006131462133489549\n",
      "Training Loss: 0.005769550524419174\n",
      "Validation Loss: 0.003917289100550743\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006741648234892637\n",
      "Training Loss: 0.00612265438074246\n",
      "Training Loss: 0.0057604254630859945\n",
      "Validation Loss: 0.0039079977713493905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006732838170137256\n",
      "Training Loss: 0.006113978967769071\n",
      "Training Loss: 0.005751408823416568\n",
      "Validation Loss: 0.0038987951722058854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006724143811734393\n",
      "Training Loss: 0.006105431604664773\n",
      "Training Loss: 0.005742495814338326\n",
      "Validation Loss: 0.0038896761573013966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006715561293531209\n",
      "Training Loss: 0.006097005310002714\n",
      "Training Loss: 0.005733682028367184\n",
      "Validation Loss: 0.00388063875346162\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006707086956594139\n",
      "Training Loss: 0.0060886957781622186\n",
      "Training Loss: 0.005724961349042133\n",
      "Validation Loss: 0.00387166932178138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006698716890532523\n",
      "Training Loss: 0.006080497791990638\n",
      "Training Loss: 0.005716330307186581\n",
      "Validation Loss: 0.0038627707334026023\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006690446906723082\n",
      "Training Loss: 0.006072406145394779\n",
      "Training Loss: 0.005707783533143811\n",
      "Validation Loss: 0.0038539382591163424\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006682274035410956\n",
      "Training Loss: 0.006064416711451486\n",
      "Training Loss: 0.005699318120605312\n",
      "Validation Loss: 0.0038451656981716665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.00667419531266205\n",
      "Training Loss: 0.0060565237741684545\n",
      "Training Loss: 0.005690928901894949\n",
      "Validation Loss: 0.003836451187923425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006666205369401723\n",
      "Training Loss: 0.0060487220890354364\n",
      "Training Loss: 0.005682612000382506\n",
      "Validation Loss: 0.0038277877009410956\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006658303018193692\n",
      "Training Loss: 0.0060410095087718215\n",
      "Training Loss: 0.005674363330472261\n",
      "Validation Loss: 0.0038191787362245196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006650483660632744\n",
      "Training Loss: 0.006033378854626789\n",
      "Training Loss: 0.005666180235566571\n",
      "Validation Loss: 0.003810614399779379\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.00664274392533116\n",
      "Training Loss: 0.00602582742285449\n",
      "Training Loss: 0.0056580575706902895\n",
      "Validation Loss: 0.0038020955038968515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006635081046260894\n",
      "Training Loss: 0.006018350272788666\n",
      "Training Loss: 0.00564999311696738\n",
      "Validation Loss: 0.0037936177130872277\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006627492414554581\n",
      "Training Loss: 0.006010942856082693\n",
      "Training Loss: 0.0056419836386339735\n",
      "Validation Loss: 0.0037851820142646687\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006619973453925922\n",
      "Training Loss: 0.006003601228585466\n",
      "Training Loss: 0.005634024820174091\n",
      "Validation Loss: 0.0037767822204126315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0066125214786734434\n",
      "Training Loss: 0.005996320731355809\n",
      "Training Loss: 0.00562611360102892\n",
      "Validation Loss: 0.0037684215479948965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006605134558631107\n",
      "Training Loss: 0.0059890970069682225\n",
      "Training Loss: 0.005618247097590939\n",
      "Validation Loss: 0.003760091101703642\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006597807300277054\n",
      "Training Loss: 0.005981926306267269\n",
      "Training Loss: 0.005610422290628776\n",
      "Validation Loss: 0.0037517944452865573\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006590538549935445\n",
      "Training Loss: 0.005974804447032511\n",
      "Training Loss: 0.005602636042749509\n",
      "Validation Loss: 0.0037435289410863784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006583324988605454\n",
      "Training Loss: 0.0059677272697445\n",
      "Training Loss: 0.005594886281178333\n",
      "Validation Loss: 0.00373529293966804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006576162180281244\n",
      "Training Loss: 0.005960690265637823\n",
      "Training Loss: 0.005587169393547811\n",
      "Validation Loss: 0.0037270845836185422\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006569048690143972\n",
      "Training Loss: 0.005953689111047425\n",
      "Training Loss: 0.005579482628381811\n",
      "Validation Loss: 0.0037189053497120234\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006561980516416952\n",
      "Training Loss: 0.005946720654028468\n",
      "Training Loss: 0.00557182387332432\n",
      "Validation Loss: 0.003710754046980501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006554954974562861\n",
      "Training Loss: 0.005939779669279233\n",
      "Training Loss: 0.005564190046279691\n",
      "Validation Loss: 0.003702628473997158\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0065479684591991825\n",
      "Training Loss: 0.005932863302878105\n",
      "Training Loss: 0.0055565789941465485\n",
      "Validation Loss: 0.0036945262455559346\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006541018401039764\n",
      "Training Loss: 0.0059259655850473795\n",
      "Training Loss: 0.005548987507936544\n",
      "Validation Loss: 0.0036864490182861014\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006534101105644368\n",
      "Training Loss: 0.00591908325208351\n",
      "Training Loss: 0.00554141397238709\n",
      "Validation Loss: 0.003678398672491312\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006527214293018915\n",
      "Training Loss: 0.0059122125408612195\n",
      "Training Loss: 0.005533856424153783\n",
      "Validation Loss: 0.0036703697379594775\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006520354491076432\n",
      "Training Loss: 0.005905348707456142\n",
      "Training Loss: 0.005526311686844565\n",
      "Validation Loss: 0.00366236924212112\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006513517766143196\n",
      "Training Loss: 0.005898488392704166\n",
      "Training Loss: 0.005518777068355121\n",
      "Validation Loss: 0.0036543922268607643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006506702269543894\n",
      "Training Loss: 0.005891626727534458\n",
      "Training Loss: 0.0055112515564542264\n",
      "Validation Loss: 0.003646439769347146\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006499905112432316\n",
      "Training Loss: 0.00588476013741456\n",
      "Training Loss: 0.005503733215737157\n",
      "Validation Loss: 0.0036385130736286218\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006493122683605179\n",
      "Training Loss: 0.00587788479286246\n",
      "Training Loss: 0.00549622017890215\n",
      "Validation Loss: 0.003630614876773292\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006486352124484256\n",
      "Training Loss: 0.005870997598976828\n",
      "Training Loss: 0.0054887112742289905\n",
      "Validation Loss: 0.0036227423913536196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006479591319221072\n",
      "Training Loss: 0.005864094323478639\n",
      "Training Loss: 0.005481204156531021\n",
      "Validation Loss: 0.0036149002249310776\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006472837050096132\n",
      "Training Loss: 0.005857171785319224\n",
      "Training Loss: 0.005473698051646352\n",
      "Validation Loss: 0.00360708575096672\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.00646608661103528\n",
      "Training Loss: 0.005850226450129412\n",
      "Training Loss: 0.005466191549203359\n",
      "Validation Loss: 0.0035993019838325598\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006459338800050318\n",
      "Training Loss: 0.005843255586805754\n",
      "Training Loss: 0.005458684141049161\n",
      "Validation Loss: 0.0035915493617210996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006452590368571692\n",
      "Training Loss: 0.005836257232585922\n",
      "Training Loss: 0.005451175015768968\n",
      "Validation Loss: 0.0035838293188952663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006445839622174389\n",
      "Training Loss: 0.00582922859757673\n",
      "Training Loss: 0.005443664049962536\n",
      "Validation Loss: 0.003576141462943862\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006439084840822034\n",
      "Training Loss: 0.005822167116566562\n",
      "Training Loss: 0.005436150225577876\n",
      "Validation Loss: 0.0035684904687922825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006432324126944877\n",
      "Training Loss: 0.005815072748810053\n",
      "Training Loss: 0.0054286344948923215\n",
      "Validation Loss: 0.0035608759440293306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.0064255572471302\n",
      "Training Loss: 0.005807943905238062\n",
      "Training Loss: 0.005421117938240059\n",
      "Validation Loss: 0.00355329865908899\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006418782054097392\n",
      "Training Loss: 0.005800780321005732\n",
      "Training Loss: 0.00541360032861121\n",
      "Validation Loss: 0.0035457604818485595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.00641199940640945\n",
      "Training Loss: 0.005793580473982729\n",
      "Training Loss: 0.005406082781846635\n",
      "Validation Loss: 0.0035382641519922137\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006405207846546546\n",
      "Training Loss: 0.00578634642821271\n",
      "Training Loss: 0.005398566349758767\n",
      "Validation Loss: 0.003530807937024517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006398407034575939\n",
      "Training Loss: 0.0057790776685578745\n",
      "Training Loss: 0.005391053247149102\n",
      "Validation Loss: 0.0035233941744081676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006391598698101007\n",
      "Training Loss: 0.005771776011679322\n",
      "Training Loss: 0.005383545159711503\n",
      "Validation Loss: 0.0035160226162700924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006384782218956389\n",
      "Training Loss: 0.005764444841188379\n",
      "Training Loss: 0.00537604360259138\n",
      "Validation Loss: 0.003508694892424797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006377960014506243\n",
      "Training Loss: 0.005757083256612532\n",
      "Training Loss: 0.005368551134015433\n",
      "Validation Loss: 0.003501407678495423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006371132014901377\n",
      "Training Loss: 0.005749695871490985\n",
      "Training Loss: 0.005361068611382507\n",
      "Validation Loss: 0.0034941633884648508\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006364300030400045\n",
      "Training Loss: 0.005742284772568382\n",
      "Training Loss: 0.00535359913657885\n",
      "Validation Loss: 0.0034869615867566526\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006357466665212997\n",
      "Training Loss: 0.005734853354515508\n",
      "Training Loss: 0.005346146133961156\n",
      "Validation Loss: 0.003479801623276278\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006350632944377139\n",
      "Training Loss: 0.00572740487405099\n",
      "Training Loss: 0.005338709910283797\n",
      "Validation Loss: 0.0034726825850136746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006343800098984502\n",
      "Training Loss: 0.00571994163095951\n",
      "Training Loss: 0.005331292526680045\n",
      "Validation Loss: 0.003465601145629928\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006336970857810229\n",
      "Training Loss: 0.0057124682515859605\n",
      "Training Loss: 0.005323897376074456\n",
      "Validation Loss: 0.0034585557420204363\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006330146590480581\n",
      "Training Loss: 0.005704987609060481\n",
      "Training Loss: 0.005316525273956358\n",
      "Validation Loss: 0.0034515477567806504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006323330087470822\n",
      "Training Loss: 0.005697502305265516\n",
      "Training Loss: 0.005309179029427469\n",
      "Validation Loss: 0.0034445711323897238\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006316522873821668\n",
      "Training Loss: 0.005690016764565371\n",
      "Training Loss: 0.005301859629689716\n",
      "Validation Loss: 0.0034376273619722616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.00630972747283522\n",
      "Training Loss: 0.005682533586514183\n",
      "Training Loss: 0.005294569113175385\n",
      "Validation Loss: 0.003430710627378247\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006302944374619983\n",
      "Training Loss: 0.005675054969615303\n",
      "Training Loss: 0.005287306770915165\n",
      "Validation Loss: 0.0034238217023117574\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006296176000032574\n",
      "Training Loss: 0.005667584603070281\n",
      "Training Loss: 0.00528007504879497\n",
      "Validation Loss: 0.003416956317338967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006289423074340448\n",
      "Training Loss: 0.0056601241341559215\n",
      "Training Loss: 0.005272875320515596\n",
      "Validation Loss: 0.00341011112715442\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006282688146457076\n",
      "Training Loss: 0.005652676352765411\n",
      "Training Loss: 0.0052657067379914225\n",
      "Validation Loss: 0.0034032867694263128\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.00627597059472464\n",
      "Training Loss: 0.005645241371239535\n",
      "Training Loss: 0.005258568524150178\n",
      "Validation Loss: 0.0033964772657700554\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006269271756173112\n",
      "Training Loss: 0.005637821916025132\n",
      "Training Loss: 0.0052514621458249165\n",
      "Validation Loss: 0.0033896784029974277\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006262592185521498\n",
      "Training Loss: 0.0056304192292736845\n",
      "Training Loss: 0.005244387203129008\n",
      "Validation Loss: 0.0033828939816108747\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006255932652275078\n",
      "Training Loss: 0.005623033385490999\n",
      "Training Loss: 0.005237342804903165\n",
      "Validation Loss: 0.00337611811478842\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006249293378787115\n",
      "Training Loss: 0.005615666364901699\n",
      "Training Loss: 0.005230329292826355\n",
      "Validation Loss: 0.00336934849711428\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006242674255045131\n",
      "Training Loss: 0.005608318558661268\n",
      "Training Loss: 0.005223344609257765\n",
      "Validation Loss: 0.0033625844150540953\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006236075139022432\n",
      "Training Loss: 0.005600989690283314\n",
      "Training Loss: 0.0052163892186945305\n",
      "Validation Loss: 0.0033558210681108864\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006229495028965176\n",
      "Training Loss: 0.005593680038000457\n",
      "Training Loss: 0.005209460888872855\n",
      "Validation Loss: 0.003349057792455711\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006222935526748188\n",
      "Training Loss: 0.005586388629744761\n",
      "Training Loss: 0.005202558839228004\n",
      "Validation Loss: 0.003342294028248596\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006216395781957544\n",
      "Training Loss: 0.005579117410816252\n",
      "Training Loss: 0.005195682696648873\n",
      "Validation Loss: 0.0033355260410429803\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006209873516927473\n",
      "Training Loss: 0.005571863724035211\n",
      "Training Loss: 0.005188830348779447\n",
      "Validation Loss: 0.0033287526921923745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006203369926661253\n",
      "Training Loss: 0.005564628225984052\n",
      "Training Loss: 0.005182001750799827\n",
      "Validation Loss: 0.00332197639502564\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006196883746306412\n",
      "Training Loss: 0.005557411201880314\n",
      "Training Loss: 0.005175194892799482\n",
      "Validation Loss: 0.0033151917708933018\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.0061904143512947485\n",
      "Training Loss: 0.005550211212830618\n",
      "Training Loss: 0.0051684084517182784\n",
      "Validation Loss: 0.0033083969159470347\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006183960296330043\n",
      "Training Loss: 0.005543027100502513\n",
      "Training Loss: 0.005161640556761995\n",
      "Validation Loss: 0.0033015946490072717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006177521809586323\n",
      "Training Loss: 0.005535859477240592\n",
      "Training Loss: 0.0051548909518169235\n",
      "Validation Loss: 0.0032947811636854004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006171097516780719\n",
      "Training Loss: 0.005528707366902381\n",
      "Training Loss: 0.005148158549563959\n",
      "Validation Loss: 0.00328795718855821\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006164688395801932\n",
      "Training Loss: 0.005521569980774074\n",
      "Training Loss: 0.005141441869782284\n",
      "Validation Loss: 0.003281122539192438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006158291310421191\n",
      "Training Loss: 0.005514445871813223\n",
      "Training Loss: 0.005134738995693624\n",
      "Validation Loss: 0.0032742753267821887\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006151906128507107\n",
      "Training Loss: 0.005507336469599977\n",
      "Training Loss: 0.005128050728235394\n",
      "Validation Loss: 0.0032674153505437326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006145533787203021\n",
      "Training Loss: 0.005500239470275119\n",
      "Training Loss: 0.005121375532471575\n",
      "Validation Loss: 0.003260544489472686\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006139172450057231\n",
      "Training Loss: 0.005493154714931734\n",
      "Training Loss: 0.005114711405476555\n",
      "Validation Loss: 0.003253657744250396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006132820942439139\n",
      "Training Loss: 0.005486081130802631\n",
      "Training Loss: 0.005108056896133349\n",
      "Validation Loss: 0.003246758397396528\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006126479407539591\n",
      "Training Loss: 0.005479019538615831\n",
      "Training Loss: 0.005101412910153158\n",
      "Validation Loss: 0.00323984627886623\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006120146848261356\n",
      "Training Loss: 0.005471967986086384\n",
      "Training Loss: 0.005094777229242026\n",
      "Validation Loss: 0.00323292149657258\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006113823129562661\n",
      "Training Loss: 0.005464927624561824\n",
      "Training Loss: 0.005088151827803813\n",
      "Validation Loss: 0.0032259864671145347\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006107508889981545\n",
      "Training Loss: 0.005457897868473083\n",
      "Training Loss: 0.0050815339112887156\n",
      "Validation Loss: 0.003219033615647951\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006101202403078787\n",
      "Training Loss: 0.005450877757393755\n",
      "Training Loss: 0.005074922731728293\n",
      "Validation Loss: 0.003212071490196741\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006094903735793196\n",
      "Training Loss: 0.005443868252332323\n",
      "Training Loss: 0.005068319514975883\n",
      "Validation Loss: 0.003205097043687959\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006088613304309547\n",
      "Training Loss: 0.005436867602402345\n",
      "Training Loss: 0.0050617218669503925\n",
      "Validation Loss: 0.0031981120230054503\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006082329491619021\n",
      "Training Loss: 0.005429876258713193\n",
      "Training Loss: 0.005055131349363365\n",
      "Validation Loss: 0.0031911121986105285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006076053848373703\n",
      "Training Loss: 0.005422895539086312\n",
      "Training Loss: 0.005048547254991717\n",
      "Validation Loss: 0.0031841033853831167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006069785269210115\n",
      "Training Loss: 0.005415924228727818\n",
      "Training Loss: 0.00504196947440505\n",
      "Validation Loss: 0.003177082739650073\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006063524359487929\n",
      "Training Loss: 0.005408962825313211\n",
      "Training Loss: 0.005035397964529693\n",
      "Validation Loss: 0.0031700536701566636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006057271176250652\n",
      "Training Loss: 0.005402011587866582\n",
      "Training Loss: 0.005028832372627221\n",
      "Validation Loss: 0.0031630162723896124\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006051025397027843\n",
      "Training Loss: 0.005395071079256013\n",
      "Training Loss: 0.005022273875656538\n",
      "Validation Loss: 0.0031559683658507016\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006044787036371417\n",
      "Training Loss: 0.00538814093277324\n",
      "Training Loss: 0.005015720200608484\n",
      "Validation Loss: 0.003148913628740885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006038556506391615\n",
      "Training Loss: 0.005381221646093764\n",
      "Training Loss: 0.005009174839360639\n",
      "Validation Loss: 0.0031418528707352665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006032334173214622\n",
      "Training Loss: 0.005374314277432859\n",
      "Training Loss: 0.005002635942655615\n",
      "Validation Loss: 0.0031347887824666213\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006026120774913579\n",
      "Training Loss: 0.005367418501409702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [22:40<02:28, 148.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0049961046763928605\n",
      "Validation Loss: 0.0031277158551357605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.07479558972641825\n",
      "Training Loss: 0.07219511402770877\n",
      "Training Loss: 0.07090478278696537\n",
      "Validation Loss: 0.07143728688275547\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06996932653710246\n",
      "Training Loss: 0.0674730233848095\n",
      "Training Loss: 0.06491521185263992\n",
      "Validation Loss: 0.0626480599467674\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06030364736914635\n",
      "Training Loss: 0.05529378335922956\n",
      "Training Loss: 0.05050751239061355\n",
      "Validation Loss: 0.04630573367009337\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.0448916787840426\n",
      "Training Loss: 0.03992381458170712\n",
      "Training Loss: 0.0352806421648711\n",
      "Validation Loss: 0.030764714919365523\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.030747699569910764\n",
      "Training Loss: 0.026517911884002387\n",
      "Training Loss: 0.023593245684169234\n",
      "Validation Loss: 0.021280979078388617\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.022456769095733763\n",
      "Training Loss: 0.02017440916504711\n",
      "Training Loss: 0.01885806261561811\n",
      "Validation Loss: 0.017762767836409673\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.018958612442947925\n",
      "Training Loss: 0.017181636560708285\n",
      "Training Loss: 0.016244012850802392\n",
      "Validation Loss: 0.015471064715824101\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.01670621435623616\n",
      "Training Loss: 0.015197651714552195\n",
      "Training Loss: 0.014541349548380822\n",
      "Validation Loss: 0.013919157925679276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.01524122964590788\n",
      "Training Loss: 0.013935405989177524\n",
      "Training Loss: 0.013488908684812485\n",
      "Validation Loss: 0.012909763183947025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.014308641203679145\n",
      "Training Loss: 0.013124672099947929\n",
      "Training Loss: 0.012797813825309277\n",
      "Validation Loss: 0.012200970574167002\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.013658565202495082\n",
      "Training Loss: 0.012542274955194443\n",
      "Training Loss: 0.012280152388848364\n",
      "Validation Loss: 0.011646509215005495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.013151989578036592\n",
      "Training Loss: 0.012076768081169576\n",
      "Training Loss: 0.011852481754031032\n",
      "Validation Loss: 0.011179976576018366\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.012727141409413889\n",
      "Training Loss: 0.011679869145154953\n",
      "Training Loss: 0.011479675732553006\n",
      "Validation Loss: 0.010770949635790724\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.012355801070807502\n",
      "Training Loss: 0.011329345833510161\n",
      "Training Loss: 0.011145336972549557\n",
      "Validation Loss: 0.010403676307760179\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.012023458699695766\n",
      "Training Loss: 0.011013597035780549\n",
      "Training Loss: 0.010840637004002928\n",
      "Validation Loss: 0.010068823877732488\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.011721643105847761\n",
      "Training Loss: 0.010725763353984803\n",
      "Training Loss: 0.010560186863876879\n",
      "Validation Loss: 0.00976025883443235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.011444863202050327\n",
      "Training Loss: 0.010461336642038077\n",
      "Training Loss: 0.010300344512797892\n",
      "Validation Loss: 0.009473615609831438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.011189236748032272\n",
      "Training Loss: 0.010217104833573102\n",
      "Training Loss: 0.01005846033571288\n",
      "Validation Loss: 0.009205629344077341\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.010951859721681103\n",
      "Training Loss: 0.00999062979943119\n",
      "Training Loss: 0.00983249159879051\n",
      "Validation Loss: 0.0089537736559496\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.010730464324587956\n",
      "Training Loss: 0.009779970456147567\n",
      "Training Loss: 0.00962080235243775\n",
      "Validation Loss: 0.008716064851908956\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.010523269226541742\n",
      "Training Loss: 0.009583561599720269\n",
      "Training Loss: 0.009422078647185117\n",
      "Validation Loss: 0.00849096837134383\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.010328881568275392\n",
      "Training Loss: 0.009400135834002867\n",
      "Training Loss: 0.009235264061717317\n",
      "Validation Loss: 0.00827729027310198\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.010146229774691165\n",
      "Training Loss: 0.009228672051103786\n",
      "Training Loss: 0.009059520700247958\n",
      "Validation Loss: 0.008074145523433605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.009974504077108577\n",
      "Training Loss: 0.009068354740738869\n",
      "Training Loss: 0.00889419149258174\n",
      "Validation Loss: 0.0078808785876615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.009813096056459471\n",
      "Training Loss: 0.008918526725610719\n",
      "Training Loss: 0.008738753274083138\n",
      "Validation Loss: 0.007697005136189669\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.009661531336605548\n",
      "Training Loss: 0.008778637673240155\n",
      "Training Loss: 0.008592776976292953\n",
      "Validation Loss: 0.00752215995976513\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.00951942457119003\n",
      "Training Loss: 0.00864821212599054\n",
      "Training Loss: 0.00845589601318352\n",
      "Validation Loss: 0.007356062326109393\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.009386439767549746\n",
      "Training Loss: 0.008526811382034793\n",
      "Training Loss: 0.008327770278556272\n",
      "Validation Loss: 0.007198468214319496\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.009262251104228198\n",
      "Training Loss: 0.008414010604610667\n",
      "Training Loss: 0.008208066117949784\n",
      "Validation Loss: 0.0070491528512617\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.009146529287681915\n",
      "Training Loss: 0.008309382245643064\n",
      "Training Loss: 0.008096440997906029\n",
      "Validation Loss: 0.006907883954359993\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.009038924328051508\n",
      "Training Loss: 0.008212482432136313\n",
      "Training Loss: 0.007992529238108545\n",
      "Validation Loss: 0.006774416725737241\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008939055383671075\n",
      "Training Loss: 0.008122842499287799\n",
      "Training Loss: 0.007895930334925652\n",
      "Validation Loss: 0.006648480055037509\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008846511661540718\n",
      "Training Loss: 0.008039969610981643\n",
      "Training Loss: 0.007806207835674286\n",
      "Validation Loss: 0.006529772818988461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008760847327066586\n",
      "Training Loss: 0.00796333690872416\n",
      "Training Loss: 0.00772287628846243\n",
      "Validation Loss: 0.006417965553073066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008681586580350995\n",
      "Training Loss: 0.007892384148435667\n",
      "Training Loss: 0.007645401435438543\n",
      "Validation Loss: 0.006312699538519543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008608231423422694\n",
      "Training Loss: 0.007826539182569831\n",
      "Training Loss: 0.007573230270063505\n",
      "Validation Loss: 0.006213641936074566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008540305024944246\n",
      "Training Loss: 0.007765270174713805\n",
      "Training Loss: 0.007505882120458409\n",
      "Validation Loss: 0.0061205457961396054\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008477413919754326\n",
      "Training Loss: 0.007708209394477308\n",
      "Training Loss: 0.007443122820695862\n",
      "Validation Loss: 0.006033328606412233\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008419283094117418\n",
      "Training Loss: 0.007655230819946155\n",
      "Training Loss: 0.0073850072408095\n",
      "Validation Loss: 0.005951917128574648\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008365627733292058\n",
      "Training Loss: 0.007606289522955194\n",
      "Training Loss: 0.00733156890142709\n",
      "Validation Loss: 0.005876003425788093\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008315993378637358\n",
      "Training Loss: 0.00756112564005889\n",
      "Training Loss: 0.007282494936371222\n",
      "Validation Loss: 0.005805078820649828\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008269824022427201\n",
      "Training Loss: 0.007519225729629398\n",
      "Training Loss: 0.007237216295907274\n",
      "Validation Loss: 0.005738612919721459\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.008226567214587704\n",
      "Training Loss: 0.0074799597694072874\n",
      "Training Loss: 0.007195079426746815\n",
      "Validation Loss: 0.005676099270571735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.008185669848462567\n",
      "Training Loss: 0.007442644456168637\n",
      "Training Loss: 0.00715539698721841\n",
      "Validation Loss: 0.0056169862960978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.008146504838950932\n",
      "Training Loss: 0.007406515561742708\n",
      "Training Loss: 0.007117389088962227\n",
      "Validation Loss: 0.005560602841040726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.008108267810894177\n",
      "Training Loss: 0.007370579451089725\n",
      "Training Loss: 0.007080045751063153\n",
      "Validation Loss: 0.00550598780012407\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.008069757560733706\n",
      "Training Loss: 0.007333312552655116\n",
      "Training Loss: 0.007041798060527071\n",
      "Validation Loss: 0.005451575932720823\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.00802887584315613\n",
      "Training Loss: 0.007291914336383343\n",
      "Training Loss: 0.006999744079075753\n",
      "Validation Loss: 0.005394355495973082\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007981296048965305\n",
      "Training Loss: 0.007240394505206495\n",
      "Training Loss: 0.006947707035578787\n",
      "Validation Loss: 0.0053278925001956105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007917218744987622\n",
      "Training Loss: 0.007165880365064368\n",
      "Training Loss: 0.006874144641915336\n",
      "Validation Loss: 0.005242197417256454\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007821838792879135\n",
      "Training Loss: 0.0070591981278266755\n",
      "Training Loss: 0.006780479367589578\n",
      "Validation Loss: 0.005145610075998591\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007712844870984554\n",
      "Training Loss: 0.006960413778433577\n",
      "Training Loss: 0.006699089674511925\n",
      "Validation Loss: 0.0050590725202280815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.0076247790118213745\n",
      "Training Loss: 0.006888767012860626\n",
      "Training Loss: 0.0066305798257235435\n",
      "Validation Loss: 0.0049785223287226775\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007550818852614612\n",
      "Training Loss: 0.006827225449960679\n",
      "Training Loss: 0.006566638218937442\n",
      "Validation Loss: 0.0049006417282501215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007482436317950487\n",
      "Training Loss: 0.006768392052035779\n",
      "Training Loss: 0.006504820863483474\n",
      "Validation Loss: 0.004824751848093329\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007416518941754475\n",
      "Training Loss: 0.006710464574862272\n",
      "Training Loss: 0.006444433600408956\n",
      "Validation Loss: 0.004750722323366431\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.00735213388223201\n",
      "Training Loss: 0.006653269707458094\n",
      "Training Loss: 0.006385460236342624\n",
      "Validation Loss: 0.004678776142386238\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007289248983142898\n",
      "Training Loss: 0.006597184404963628\n",
      "Training Loss: 0.006328231121879071\n",
      "Validation Loss: 0.00460938336013743\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007228226895676926\n",
      "Training Loss: 0.006542785320198164\n",
      "Training Loss: 0.0062732336891349405\n",
      "Validation Loss: 0.004543127306982917\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007169567160308361\n",
      "Training Loss: 0.006490664541488513\n",
      "Training Loss: 0.006220960681093857\n",
      "Validation Loss: 0.004480555964884966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007113755878526718\n",
      "Training Loss: 0.006441324076149613\n",
      "Training Loss: 0.006171820887830109\n",
      "Validation Loss: 0.004422086820770265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007061164974002168\n",
      "Training Loss: 0.006395098515786231\n",
      "Training Loss: 0.006126072554616257\n",
      "Validation Loss: 0.004367942545102553\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007012010141042992\n",
      "Training Loss: 0.006352138817310333\n",
      "Training Loss: 0.0060838066332507875\n",
      "Validation Loss: 0.004318140208422928\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006966337136691436\n",
      "Training Loss: 0.0063124178082216535\n",
      "Training Loss: 0.0060449516132939606\n",
      "Validation Loss: 0.004272514720070647\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006924036549171433\n",
      "Training Loss: 0.006275767075130716\n",
      "Training Loss: 0.006009305398911238\n",
      "Validation Loss: 0.004230752112500871\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.0068848883255850524\n",
      "Training Loss: 0.006241924610221759\n",
      "Training Loss: 0.005976575451204553\n",
      "Validation Loss: 0.004192458958825453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.00684859837172553\n",
      "Training Loss: 0.006210576605517417\n",
      "Training Loss: 0.0059464189352002\n",
      "Validation Loss: 0.004157196756536036\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.006814837997080758\n",
      "Training Loss: 0.006181395360035822\n",
      "Training Loss: 0.005918479391839355\n",
      "Validation Loss: 0.00412452685793213\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006783277081558481\n",
      "Training Loss: 0.00615406998200342\n",
      "Training Loss: 0.0058924105670303106\n",
      "Validation Loss: 0.004094029401999302\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006753604380646721\n",
      "Training Loss: 0.006128318905830384\n",
      "Training Loss: 0.005867896110285074\n",
      "Validation Loss: 0.0040653341553132105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006725539853796363\n",
      "Training Loss: 0.006103893412509933\n",
      "Training Loss: 0.005844654681859538\n",
      "Validation Loss: 0.004038109221715438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006698840564349666\n",
      "Training Loss: 0.0060805865458678455\n",
      "Training Loss: 0.005822447089012712\n",
      "Validation Loss: 0.004012082999028014\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006673300492111594\n",
      "Training Loss: 0.0060582230484578755\n",
      "Training Loss: 0.005801074007758871\n",
      "Validation Loss: 0.003987028455136742\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006648746524006128\n",
      "Training Loss: 0.006036661345278844\n",
      "Training Loss: 0.005780372256413102\n",
      "Validation Loss: 0.003962759000479422\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.0066250399628188465\n",
      "Training Loss: 0.006015781561145559\n",
      "Training Loss: 0.005760211201850325\n",
      "Validation Loss: 0.003939127895504948\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.0066020654630847275\n",
      "Training Loss: 0.005995490283239633\n",
      "Training Loss: 0.005740485480055213\n",
      "Validation Loss: 0.003916013562068176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006579731153324247\n",
      "Training Loss: 0.00597571037011221\n",
      "Training Loss: 0.005721115596825257\n",
      "Validation Loss: 0.0038933295951476005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0065579643618548285\n",
      "Training Loss: 0.005956379772396758\n",
      "Training Loss: 0.005702041866024956\n",
      "Validation Loss: 0.0038710057008174365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006536709310603328\n",
      "Training Loss: 0.00593744930694811\n",
      "Training Loss: 0.005683217598125338\n",
      "Validation Loss: 0.003848988944115222\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.0065159171872073785\n",
      "Training Loss: 0.005918881287798285\n",
      "Training Loss: 0.005664613725384697\n",
      "Validation Loss: 0.0038272462434689987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006495555931469426\n",
      "Training Loss: 0.005900644727516919\n",
      "Training Loss: 0.0056462095864117144\n",
      "Validation Loss: 0.003805752332829818\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.00647560311423149\n",
      "Training Loss: 0.005882718317443505\n",
      "Training Loss: 0.00562799557694234\n",
      "Validation Loss: 0.0037844915098636253\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006456038983305917\n",
      "Training Loss: 0.005865089173894376\n",
      "Training Loss: 0.005609970551449806\n",
      "Validation Loss: 0.003763455414249865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006436856256332248\n",
      "Training Loss: 0.005847745874198154\n",
      "Training Loss: 0.005592138501815498\n",
      "Validation Loss: 0.003742649459955021\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.0064180498418863864\n",
      "Training Loss: 0.005830689484719187\n",
      "Training Loss: 0.005574510407168418\n",
      "Validation Loss: 0.003722075955664969\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.0063996211852645505\n",
      "Training Loss: 0.0058139194897376\n",
      "Training Loss: 0.005557102095335722\n",
      "Validation Loss: 0.003701748036690517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006381572485552169\n",
      "Training Loss: 0.00579743962851353\n",
      "Training Loss: 0.005539932501269504\n",
      "Validation Loss: 0.0036816788435746193\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006363912386004813\n",
      "Training Loss: 0.00578126325039193\n",
      "Training Loss: 0.005523023064015433\n",
      "Validation Loss: 0.0036618861741271247\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.0063466500979848205\n",
      "Training Loss: 0.005765398999210447\n",
      "Training Loss: 0.005506396113196388\n",
      "Validation Loss: 0.0036423885665070057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006329793565091677\n",
      "Training Loss: 0.005749859812203795\n",
      "Training Loss: 0.00549007658381015\n",
      "Validation Loss: 0.0036232031841260076\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006313354557496495\n",
      "Training Loss: 0.005734661071328446\n",
      "Training Loss: 0.00547408660990186\n",
      "Validation Loss: 0.0036043502224263934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006297338262666017\n",
      "Training Loss: 0.005719814148033037\n",
      "Training Loss: 0.00545844642620068\n",
      "Validation Loss: 0.0035858493006599838\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006281752767972648\n",
      "Training Loss: 0.005705330250784755\n",
      "Training Loss: 0.005443174878018908\n",
      "Validation Loss: 0.003567716061645135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006266599748050794\n",
      "Training Loss: 0.005691218771971762\n",
      "Training Loss: 0.005428286410751753\n",
      "Validation Loss: 0.0035499622064297286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006251881720381789\n",
      "Training Loss: 0.0056774849572684616\n",
      "Training Loss: 0.005413788962177932\n",
      "Validation Loss: 0.0035325960534771258\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006237593620899134\n",
      "Training Loss: 0.005664132913807407\n",
      "Training Loss: 0.005399689123732969\n",
      "Validation Loss: 0.0035156274496923002\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006223727345932275\n",
      "Training Loss: 0.0056511614937335254\n",
      "Training Loss: 0.0053859883511904625\n",
      "Validation Loss: 0.0034990553407971696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006210275112534873\n",
      "Training Loss: 0.0056385650474112484\n",
      "Training Loss: 0.005372682185843587\n",
      "Validation Loss: 0.0034828770466232566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006197224409552291\n",
      "Training Loss: 0.005626338041620329\n",
      "Training Loss: 0.005359764419845306\n",
      "Validation Loss: 0.003467091609033222\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006184559045359492\n",
      "Training Loss: 0.005614469795254991\n",
      "Training Loss: 0.005347223114804365\n",
      "Validation Loss: 0.0034516842655178286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0061722648201975976\n",
      "Training Loss: 0.005602946192957461\n",
      "Training Loss: 0.005335046134423465\n",
      "Validation Loss: 0.0034366478042132903\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006160322926589288\n",
      "Training Loss: 0.005591754402266815\n",
      "Training Loss: 0.005323218398261815\n",
      "Validation Loss: 0.0034219700688747374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006148716995958239\n",
      "Training Loss: 0.005580879733897746\n",
      "Training Loss: 0.005311723492341116\n",
      "Validation Loss: 0.0034076345644974975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0061374273820547385\n",
      "Training Loss: 0.005570304156281054\n",
      "Training Loss: 0.005300544875790365\n",
      "Validation Loss: 0.003393628618161958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0061264383303932845\n",
      "Training Loss: 0.0055600134626729416\n",
      "Training Loss: 0.005289664416923187\n",
      "Validation Loss: 0.003379932669477977\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006115733705810271\n",
      "Training Loss: 0.005549990878789685\n",
      "Training Loss: 0.0052790656947763635\n",
      "Validation Loss: 0.00336653181270623\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006105295321322046\n",
      "Training Loss: 0.005540220270049758\n",
      "Training Loss: 0.005268730955431238\n",
      "Validation Loss: 0.0033534136313024196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.00609510968439281\n",
      "Training Loss: 0.005530686690472067\n",
      "Training Loss: 0.0052586473053088416\n",
      "Validation Loss: 0.003340560010257648\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006085162806557492\n",
      "Training Loss: 0.00552137790073175\n",
      "Training Loss: 0.005248795364168473\n",
      "Validation Loss: 0.0033279593067316944\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006075439536944032\n",
      "Training Loss: 0.005512278752285056\n",
      "Training Loss: 0.005239164898521267\n",
      "Validation Loss: 0.0033155942592100144\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006065928963362239\n",
      "Training Loss: 0.005503377056447789\n",
      "Training Loss: 0.0052297403651755305\n",
      "Validation Loss: 0.0033034543654609345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006056619422743097\n",
      "Training Loss: 0.005494661694392562\n",
      "Training Loss: 0.005220510671497322\n",
      "Validation Loss: 0.0032915285797632727\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006047500072163529\n",
      "Training Loss: 0.005486121927388013\n",
      "Training Loss: 0.005211463775485754\n",
      "Validation Loss: 0.0032798047445641225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006038560686865821\n",
      "Training Loss: 0.005477748631383292\n",
      "Training Loss: 0.005202589451801032\n",
      "Validation Loss: 0.003268274372663307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006029794533387758\n",
      "Training Loss: 0.005469532672432251\n",
      "Training Loss: 0.005193879567086696\n",
      "Validation Loss: 0.0032569274745797845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.0060211913153761995\n",
      "Training Loss: 0.005461465948610566\n",
      "Training Loss: 0.005185322702163831\n",
      "Validation Loss: 0.003245757304765075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006012744300533086\n",
      "Training Loss: 0.005453541173483245\n",
      "Training Loss: 0.0051769148098537695\n",
      "Validation Loss: 0.0032347559463232756\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006004445843864233\n",
      "Training Loss: 0.005445751346996985\n",
      "Training Loss: 0.0051686468039406465\n",
      "Validation Loss: 0.003223919811962026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.005996292586787604\n",
      "Training Loss: 0.005438092117547057\n",
      "Training Loss: 0.005160512523725629\n",
      "Validation Loss: 0.0032132387840126254\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.00598827529756818\n",
      "Training Loss: 0.0054305551474681125\n",
      "Training Loss: 0.005152505596051924\n",
      "Validation Loss: 0.003202708109067439\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.005980391916236841\n",
      "Training Loss: 0.005423137963516638\n",
      "Training Loss: 0.005144622492371127\n",
      "Validation Loss: 0.0031923254954450754\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.005972635913640261\n",
      "Training Loss: 0.005415836580214091\n",
      "Training Loss: 0.005136857595061883\n",
      "Validation Loss: 0.003182085854226326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.005965004175668582\n",
      "Training Loss: 0.005408644455019385\n",
      "Training Loss: 0.005129206203273498\n",
      "Validation Loss: 0.003171985776665924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.0059574929461814466\n",
      "Training Loss: 0.005401559923193418\n",
      "Training Loss: 0.005121666226768866\n",
      "Validation Loss: 0.0031620213281876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.005950097253080457\n",
      "Training Loss: 0.0053945781179936605\n",
      "Training Loss: 0.005114231813349761\n",
      "Validation Loss: 0.003152189077809453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.005942814326845109\n",
      "Training Loss: 0.005387698125559837\n",
      "Training Loss: 0.005106901162071154\n",
      "Validation Loss: 0.00314248979923556\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0059356415714137255\n",
      "Training Loss: 0.005380915026762523\n",
      "Training Loss: 0.005099671551142819\n",
      "Validation Loss: 0.0031329161955797103\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.005928576222504489\n",
      "Training Loss: 0.005374226809944958\n",
      "Training Loss: 0.005092540013720281\n",
      "Validation Loss: 0.003123469942437715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.005921615657862276\n",
      "Training Loss: 0.005367630497785285\n",
      "Training Loss: 0.005085503846639767\n",
      "Validation Loss: 0.0031141445454042614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.00591475656023249\n",
      "Training Loss: 0.005361125553026796\n",
      "Training Loss: 0.005078560988185927\n",
      "Validation Loss: 0.0031049422673839196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.005907998906332068\n",
      "Training Loss: 0.005354709675884806\n",
      "Training Loss: 0.005071709199692123\n",
      "Validation Loss: 0.0030958594373699405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.0059013381315162405\n",
      "Training Loss: 0.0053483798471279445\n",
      "Training Loss: 0.00506494709814433\n",
      "Validation Loss: 0.003086893802268033\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.005894773051259108\n",
      "Training Loss: 0.005342134745442309\n",
      "Training Loss: 0.0050582729309098795\n",
      "Validation Loss: 0.0030780474327012815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.005888302198727615\n",
      "Training Loss: 0.005335972068132832\n",
      "Training Loss: 0.005051685207290575\n",
      "Validation Loss: 0.00306931266226293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005881924275890924\n",
      "Training Loss: 0.005329892229055986\n",
      "Training Loss: 0.005045182247995399\n",
      "Validation Loss: 0.0030606948892230148\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.005875635714037344\n",
      "Training Loss: 0.0053238924196921285\n",
      "Training Loss: 0.005038762431940996\n",
      "Validation Loss: 0.0030521885053715093\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.005869437626097351\n",
      "Training Loss: 0.005317971741314978\n",
      "Training Loss: 0.005032426024554297\n",
      "Validation Loss: 0.0030437935094003764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005863326492835767\n",
      "Training Loss: 0.005312128759687767\n",
      "Training Loss: 0.005026169384946115\n",
      "Validation Loss: 0.0030355107999312575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.005857301162322983\n",
      "Training Loss: 0.005306361872935667\n",
      "Training Loss: 0.005019994234316982\n",
      "Validation Loss: 0.003027335787715202\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005851361101376824\n",
      "Training Loss: 0.005300670171855018\n",
      "Training Loss: 0.0050138974183937535\n",
      "Validation Loss: 0.003019269329516657\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.005845503660966642\n",
      "Training Loss: 0.005295053232694045\n",
      "Training Loss: 0.005007879759650677\n",
      "Validation Loss: 0.0030113126594688367\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.005839728132123127\n",
      "Training Loss: 0.005289508855203167\n",
      "Training Loss: 0.0050019389222143214\n",
      "Validation Loss: 0.003003460941757756\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005834033969440497\n",
      "Training Loss: 0.00528403751202859\n",
      "Training Loss: 0.004996073682559654\n",
      "Validation Loss: 0.0029957152692485977\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.0058284194197040055\n",
      "Training Loss: 0.005278636382427066\n",
      "Training Loss: 0.00499028300633654\n",
      "Validation Loss: 0.0029880723110243174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.0058228805020917205\n",
      "Training Loss: 0.005273305407608859\n",
      "Training Loss: 0.004984567599603906\n",
      "Validation Loss: 0.0029805334549125157\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0058174216561019424\n",
      "Training Loss: 0.005268043829128146\n",
      "Training Loss: 0.004978926627081819\n",
      "Validation Loss: 0.0029730962862762928\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005812036125571467\n",
      "Training Loss: 0.005262849618447945\n",
      "Training Loss: 0.004973357387352734\n",
      "Validation Loss: 0.0029657625781602404\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.0058067260432289915\n",
      "Training Loss: 0.005257723533432\n",
      "Training Loss: 0.004967860132455826\n",
      "Validation Loss: 0.0029585298636059664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.005801490279845893\n",
      "Training Loss: 0.005252663373830728\n",
      "Training Loss: 0.004962435022462159\n",
      "Validation Loss: 0.0029513948672880114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005796325248666107\n",
      "Training Loss: 0.005247668817755766\n",
      "Training Loss: 0.004957080086460337\n",
      "Validation Loss: 0.0029443608887305254\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.00579123156843707\n",
      "Training Loss: 0.005242739372770302\n",
      "Training Loss: 0.004951794309308752\n",
      "Validation Loss: 0.002937423365499322\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005786207400378771\n",
      "Training Loss: 0.005237872487632558\n",
      "Training Loss: 0.004946578835952096\n",
      "Validation Loss: 0.0029305832805844507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005781252735177986\n",
      "Training Loss: 0.005233068937668577\n",
      "Training Loss: 0.004941430256003514\n",
      "Validation Loss: 0.002923838807311788\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005776365107158199\n",
      "Training Loss: 0.0052283274417277426\n",
      "Training Loss: 0.004936349542695098\n",
      "Validation Loss: 0.002917188964653342\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005771544239250943\n",
      "Training Loss: 0.005223646868835204\n",
      "Training Loss: 0.004931336910813116\n",
      "Validation Loss: 0.002910635250311847\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005766789340996183\n",
      "Training Loss: 0.0052190267987316475\n",
      "Training Loss: 0.0049263900960795585\n",
      "Validation Loss: 0.002904175224143677\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005762098395498469\n",
      "Training Loss: 0.005214466412435286\n",
      "Training Loss: 0.004921508273109793\n",
      "Validation Loss: 0.0028978063544425905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005757470316020772\n",
      "Training Loss: 0.005209964487003163\n",
      "Training Loss: 0.004916690640384331\n",
      "Validation Loss: 0.002891530142181417\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005752904376713559\n",
      "Training Loss: 0.005205519141163677\n",
      "Training Loss: 0.004911936387070454\n",
      "Validation Loss: 0.002885340789484718\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005748400551383383\n",
      "Training Loss: 0.005201131512294523\n",
      "Training Loss: 0.004907246115035378\n",
      "Validation Loss: 0.0028792437685266387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.00574395478121005\n",
      "Training Loss: 0.00519679942575749\n",
      "Training Loss: 0.004902617485495284\n",
      "Validation Loss: 0.00287323417612927\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005739569375291467\n",
      "Training Loss: 0.0051925237633986395\n",
      "Training Loss: 0.004898051286581904\n",
      "Validation Loss: 0.002867311712752065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005735242777736857\n",
      "Training Loss: 0.005188301582820714\n",
      "Training Loss: 0.004893546146922745\n",
      "Validation Loss: 0.002861479287470032\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005730972079909406\n",
      "Training Loss: 0.0051841334044002\n",
      "Training Loss: 0.00488910022599157\n",
      "Validation Loss: 0.0028557290880302532\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005726756465737708\n",
      "Training Loss: 0.005180017022648826\n",
      "Training Loss: 0.004884713611099869\n",
      "Validation Loss: 0.0028500633779913187\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005722596161649563\n",
      "Training Loss: 0.005175953557481989\n",
      "Training Loss: 0.004880385891883634\n",
      "Validation Loss: 0.0028444812469592506\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005718489017453976\n",
      "Training Loss: 0.005171940209693276\n",
      "Training Loss: 0.004876115299412049\n",
      "Validation Loss: 0.0028389841566657586\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005714434453402646\n",
      "Training Loss: 0.0051679773046635095\n",
      "Training Loss: 0.004871902008890174\n",
      "Validation Loss: 0.0028335676493196423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005710432911873795\n",
      "Training Loss: 0.00516406497976277\n",
      "Training Loss: 0.004867745283409022\n",
      "Validation Loss: 0.0028282298498493986\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005706482038949617\n",
      "Training Loss: 0.005160201241378673\n",
      "Training Loss: 0.004863643990829587\n",
      "Validation Loss: 0.002822973329856406\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005702579359640367\n",
      "Training Loss: 0.005156384331639856\n",
      "Training Loss: 0.004859596435562707\n",
      "Validation Loss: 0.0028177925550347464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005698726474074647\n",
      "Training Loss: 0.005152615483966656\n",
      "Training Loss: 0.004855602720053867\n",
      "Validation Loss: 0.002812693293175001\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005694921245449223\n",
      "Training Loss: 0.00514889292884618\n",
      "Training Loss: 0.0048516637331340465\n",
      "Validation Loss: 0.00280767171630594\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005691163573064841\n",
      "Training Loss: 0.005145215623779223\n",
      "Training Loss: 0.004847775966627523\n",
      "Validation Loss: 0.002802721904904655\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005687451066914945\n",
      "Training Loss: 0.00514158246864099\n",
      "Training Loss: 0.004843940577702596\n",
      "Validation Loss: 0.002797847397866125\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005683783665299416\n",
      "Training Loss: 0.005137994241085835\n",
      "Training Loss: 0.0048401541088242085\n",
      "Validation Loss: 0.002793049072883395\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005680160984047689\n",
      "Training Loss: 0.005134449797915295\n",
      "Training Loss: 0.004836418444174342\n",
      "Validation Loss: 0.0027883220503232284\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.0056765801331494\n",
      "Training Loss: 0.005130946878925897\n",
      "Training Loss: 0.004832731454516761\n",
      "Validation Loss: 0.0027836660136405923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005673041970585473\n",
      "Training Loss: 0.005127485979464837\n",
      "Training Loss: 0.004829092647996731\n",
      "Validation Loss: 0.002779082055046652\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005669544513220898\n",
      "Training Loss: 0.005124065427226015\n",
      "Training Loss: 0.004825501298764721\n",
      "Validation Loss: 0.002774564113096449\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005666089453734457\n",
      "Training Loss: 0.00512068520009052\n",
      "Training Loss: 0.004821957012754865\n",
      "Validation Loss: 0.00277011571360459\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005662672576727345\n",
      "Training Loss: 0.005117344306199811\n",
      "Training Loss: 0.004818458607187495\n",
      "Validation Loss: 0.0027657370828615313\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005659293630160391\n",
      "Training Loss: 0.005114042686764151\n",
      "Training Loss: 0.0048150048189563675\n",
      "Validation Loss: 0.002761422636201919\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.0056559535552514716\n",
      "Training Loss: 0.005110779273090884\n",
      "Training Loss: 0.004811595375067554\n",
      "Validation Loss: 0.0027571749785820837\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0056526514160213995\n",
      "Training Loss: 0.005107552876579575\n",
      "Training Loss: 0.004808231153292581\n",
      "Validation Loss: 0.002752991416099157\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005649384356220252\n",
      "Training Loss: 0.005104362635174766\n",
      "Training Loss: 0.004804908879450522\n",
      "Validation Loss: 0.002748872706106749\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005646153636625968\n",
      "Training Loss: 0.005101209844578989\n",
      "Training Loss: 0.004801628284621984\n",
      "Validation Loss: 0.002744817078176342\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.00564295724616386\n",
      "Training Loss: 0.005098091961699538\n",
      "Training Loss: 0.0047983899054815995\n",
      "Validation Loss: 0.00274082289268315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005639794749440625\n",
      "Training Loss: 0.005095008747885004\n",
      "Training Loss: 0.00479519207845442\n",
      "Validation Loss: 0.002736886345854636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.0056366661185165865\n",
      "Training Loss: 0.0050919588870601724\n",
      "Training Loss: 0.004792032896075398\n",
      "Validation Loss: 0.0027330127672888756\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005633569605997764\n",
      "Training Loss: 0.005088943354785443\n",
      "Training Loss: 0.004788914279197342\n",
      "Validation Loss: 0.0027291961171235263\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005630505234003067\n",
      "Training Loss: 0.005085960518918\n",
      "Training Loss: 0.004785834322683513\n",
      "Validation Loss: 0.0027254394633601303\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.0056274722144007685\n",
      "Training Loss: 0.005083008951041848\n",
      "Training Loss: 0.004782791562029161\n",
      "Validation Loss: 0.0027217381827407674\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005624469930771738\n",
      "Training Loss: 0.005080089378752746\n",
      "Training Loss: 0.004779785363934934\n",
      "Validation Loss: 0.0027180929763734422\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005621495353407227\n",
      "Training Loss: 0.005077200525556691\n",
      "Training Loss: 0.0047768159402767196\n",
      "Validation Loss: 0.0027145021987472024\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005618552390951663\n",
      "Training Loss: 0.005074341358267702\n",
      "Training Loss: 0.004773882818990387\n",
      "Validation Loss: 0.0027109666183339747\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005615637027076446\n",
      "Training Loss: 0.005071512868162245\n",
      "Training Loss: 0.004770983486087062\n",
      "Validation Loss: 0.0027074835824340664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005612750145955942\n",
      "Training Loss: 0.00506871344987303\n",
      "Training Loss: 0.0047681185108376664\n",
      "Validation Loss: 0.002704051346779707\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005609889900661073\n",
      "Training Loss: 0.005065942386281676\n",
      "Training Loss: 0.004765287396730855\n",
      "Validation Loss: 0.002700676257313972\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005607056248700246\n",
      "Training Loss: 0.005063199404976331\n",
      "Training Loss: 0.0047624902008101344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [25:08<00:00, 150.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.002697346647651971\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Shape of the data after splitting into sequences: (22797, 12, 5)\n",
      "Shape of the data after splitting into sequences: (5692, 12, 5)\n",
      "Shape of the data after splitting into sequences: (28500, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Loss: 0.8156773480772972\n",
      "Training Loss: 0.6427613791823387\n",
      "Training Loss: 0.5004396986961365\n",
      "Validation Loss: 0.35722908980391\n",
      "Validation Accuracy: 0.5442415730337079\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.29547644056379796\n",
      "Training Loss: 0.20034219183027743\n",
      "Training Loss: 0.1349733803421259\n",
      "Validation Loss: 0.09009511323038781\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.0779067107848823\n",
      "Training Loss: 0.06512088788673281\n",
      "Training Loss: 0.06009992368519306\n",
      "Validation Loss: 0.05818530084209496\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.05734185332432389\n",
      "Training Loss: 0.05515352001413703\n",
      "Training Loss: 0.05321700341999531\n",
      "Validation Loss: 0.052350302007091176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.05137481041252613\n",
      "Training Loss: 0.04897444006986916\n",
      "Training Loss: 0.04670118323527277\n",
      "Validation Loss: 0.04561061553364055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.04438921538181603\n",
      "Training Loss: 0.04191476285457611\n",
      "Training Loss: 0.039585250252857807\n",
      "Validation Loss: 0.038450129016229274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.03723496386781335\n",
      "Training Loss: 0.03496473262086511\n",
      "Training Loss: 0.03296322450973094\n",
      "Validation Loss: 0.03192865732387545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.031004594075493514\n",
      "Training Loss: 0.029138898504897953\n",
      "Training Loss: 0.027698834342882037\n",
      "Validation Loss: 0.026730629055645695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.026223966190591454\n",
      "Training Loss: 0.024746060352772475\n",
      "Training Loss: 0.02379108198452741\n",
      "Validation Loss: 0.022703520056341638\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.022556179193779825\n",
      "Training Loss: 0.021347971581853926\n",
      "Training Loss: 0.020703351523261516\n",
      "Validation Loss: 0.01939672955803657\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.019544013482518493\n",
      "Training Loss: 0.018572849717456846\n",
      "Training Loss: 0.0181564899883233\n",
      "Validation Loss: 0.016627670223877\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.017035345695912838\n",
      "Training Loss: 0.016280994450207798\n",
      "Training Loss: 0.0160224639903754\n",
      "Validation Loss: 0.014245140397649132\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.014877413886133582\n",
      "Training Loss: 0.014292727140709758\n",
      "Training Loss: 0.014133116088341922\n",
      "Validation Loss: 0.012079222405099132\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.012940293557476253\n",
      "Training Loss: 0.012533543449826538\n",
      "Training Loss: 0.01250940416008234\n",
      "Validation Loss: 0.010268829022586513\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.01140290532144718\n",
      "Training Loss: 0.011216343156993389\n",
      "Training Loss: 0.011350642773322761\n",
      "Validation Loss: 0.009056199243909607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.010423773836810141\n",
      "Training Loss: 0.010382198801962658\n",
      "Training Loss: 0.010593057002406567\n",
      "Validation Loss: 0.008304944670016176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.00983210615348071\n",
      "Training Loss: 0.0098475824855268\n",
      "Training Loss: 0.010080024669878184\n",
      "Validation Loss: 0.007817986749604428\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.00945758915389888\n",
      "Training Loss: 0.009489570780424401\n",
      "Training Loss: 0.009726063990965485\n",
      "Validation Loss: 0.00749208998154807\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.0092142472602427\n",
      "Training Loss: 0.00924635773175396\n",
      "Training Loss: 0.009481858043000103\n",
      "Validation Loss: 0.007268598218176389\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.009053102772450075\n",
      "Training Loss: 0.009078245057025924\n",
      "Training Loss: 0.009311141534708441\n",
      "Validation Loss: 0.007108008262722261\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008941684359451757\n",
      "Training Loss: 0.008956884075887501\n",
      "Training Loss: 0.009186655234079808\n",
      "Validation Loss: 0.006984027273942497\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.008858501657377928\n",
      "Training Loss: 0.008863060657167807\n",
      "Training Loss: 0.009089576981496065\n",
      "Validation Loss: 0.006880676351602744\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008790308694588021\n",
      "Training Loss: 0.008784895979333669\n",
      "Training Loss: 0.009008104726672173\n",
      "Validation Loss: 0.006789216975775662\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008729683721903712\n",
      "Training Loss: 0.008715756903402507\n",
      "Training Loss: 0.008935525855049491\n",
      "Validation Loss: 0.0067055523591743925\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.00867302026017569\n",
      "Training Loss: 0.008652403140440584\n",
      "Training Loss: 0.008868456962518393\n",
      "Validation Loss: 0.006628160245894465\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008619014779105783\n",
      "Training Loss: 0.008593519742134959\n",
      "Training Loss: 0.008805446550250054\n",
      "Validation Loss: 0.006556629740863285\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008567490810528398\n",
      "Training Loss: 0.008538654232397675\n",
      "Training Loss: 0.008745974214980379\n",
      "Validation Loss: 0.006490717171115822\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008518623490817844\n",
      "Training Loss: 0.008487595561891794\n",
      "Training Loss: 0.008689847744535654\n",
      "Validation Loss: 0.006430028776868425\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008472558395005762\n",
      "Training Loss: 0.008440119207371026\n",
      "Training Loss: 0.008636940896976739\n",
      "Validation Loss: 0.006374023160164694\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.00842931828694418\n",
      "Training Loss: 0.008395942691713572\n",
      "Training Loss: 0.008587108026258648\n",
      "Validation Loss: 0.006322107581275232\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.00838881985633634\n",
      "Training Loss: 0.008354747764533385\n",
      "Training Loss: 0.008540172537323087\n",
      "Validation Loss: 0.006273734740379235\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008350918458309025\n",
      "Training Loss: 0.008316215418744832\n",
      "Training Loss: 0.008495942944427952\n",
      "Validation Loss: 0.00622843016524998\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008315441204467788\n",
      "Training Loss: 0.008280044802231714\n",
      "Training Loss: 0.008454230451025068\n",
      "Validation Loss: 0.006185814238603363\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008282215924700721\n",
      "Training Loss: 0.008245973874581978\n",
      "Training Loss: 0.008414862349163742\n",
      "Validation Loss: 0.006145585794001818\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008251082327915356\n",
      "Training Loss: 0.008213782195234672\n",
      "Training Loss: 0.008377694620285182\n",
      "Validation Loss: 0.006107525078558855\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008221899131312967\n",
      "Training Loss: 0.008183293541660532\n",
      "Training Loss: 0.008342607438098639\n",
      "Validation Loss: 0.006071472285430502\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008194543248973786\n",
      "Training Loss: 0.008154370262054726\n",
      "Training Loss: 0.008309505579527468\n",
      "Validation Loss: 0.006037315043746337\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008168906619539484\n",
      "Training Loss: 0.008126906320685521\n",
      "Training Loss: 0.008278304900741204\n",
      "Validation Loss: 0.006004969843171453\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008144885330693796\n",
      "Training Loss: 0.008100816237274558\n",
      "Training Loss: 0.00824891949305311\n",
      "Validation Loss: 0.005974362516681549\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008122371935751289\n",
      "Training Loss: 0.008076024502515792\n",
      "Training Loss: 0.008221249545458704\n",
      "Validation Loss: 0.005945428570188331\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008101249773753807\n",
      "Training Loss: 0.008052460775943474\n",
      "Training Loss: 0.008195179450558499\n",
      "Validation Loss: 0.0059180944998424205\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008081399574875832\n",
      "Training Loss: 0.008030058745061979\n",
      "Training Loss: 0.008170587893109768\n",
      "Validation Loss: 0.005892285693886826\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.008062707263743505\n",
      "Training Loss: 0.008008752734167502\n",
      "Training Loss: 0.008147358816349879\n",
      "Validation Loss: 0.0058679286340837565\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.008045069363433868\n",
      "Training Loss: 0.007988485778914764\n",
      "Training Loss: 0.0081253866746556\n",
      "Validation Loss: 0.005844943917335503\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.008028400824405252\n",
      "Training Loss: 0.007969201137311756\n",
      "Training Loss: 0.008104580121580511\n",
      "Validation Loss: 0.005823259443316734\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.00801262425025925\n",
      "Training Loss: 0.007950848650652915\n",
      "Training Loss: 0.008084859898081049\n",
      "Validation Loss: 0.005802800489087286\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007997679515974597\n",
      "Training Loss: 0.007933380318572745\n",
      "Training Loss: 0.008066155277192592\n",
      "Validation Loss: 0.005783490309481289\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007983512077480555\n",
      "Training Loss: 0.007916750019649043\n",
      "Training Loss: 0.008048406281741336\n",
      "Validation Loss: 0.005765264484968581\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007970073473406955\n",
      "Training Loss: 0.007900917154038324\n",
      "Training Loss: 0.008031554621411488\n",
      "Validation Loss: 0.0057480519734260235\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007957321238936856\n",
      "Training Loss: 0.007885836765635758\n",
      "Training Loss: 0.008015547661343589\n",
      "Validation Loss: 0.005731786552848023\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007945213342318312\n",
      "Training Loss: 0.007871470341924577\n",
      "Training Loss: 0.008000336360419169\n",
      "Validation Loss: 0.00571640299272127\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007933712095255032\n",
      "Training Loss: 0.007857776681194082\n",
      "Training Loss: 0.007985871474957093\n",
      "Validation Loss: 0.005701842414361708\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007922778459032997\n",
      "Training Loss: 0.007844715398969129\n",
      "Training Loss: 0.007972106699598953\n",
      "Validation Loss: 0.005688045977018355\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007912376462481916\n",
      "Training Loss: 0.007832249407656491\n",
      "Training Loss: 0.007958996853558347\n",
      "Validation Loss: 0.005674956121816813\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.00790246888413094\n",
      "Training Loss: 0.007820338002638892\n",
      "Training Loss: 0.007946499282261356\n",
      "Validation Loss: 0.005662523875839673\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.00789302066899836\n",
      "Training Loss: 0.007808943437412381\n",
      "Training Loss: 0.007934569752542302\n",
      "Validation Loss: 0.005650689022280694\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007883996239397674\n",
      "Training Loss: 0.007798028842080385\n",
      "Training Loss: 0.007923167881090194\n",
      "Validation Loss: 0.005639408960980311\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007875361242331564\n",
      "Training Loss: 0.007787555682007223\n",
      "Training Loss: 0.007912252134410665\n",
      "Validation Loss: 0.0056286321539599235\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007867081977892668\n",
      "Training Loss: 0.007777489554136992\n",
      "Training Loss: 0.007901784917339682\n",
      "Validation Loss: 0.00561831636731042\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007859125970862807\n",
      "Training Loss: 0.007767795635154471\n",
      "Training Loss: 0.007891728471731767\n",
      "Validation Loss: 0.005608415420774077\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007851460714591666\n",
      "Training Loss: 0.007758439707104116\n",
      "Training Loss: 0.007882046406157315\n",
      "Validation Loss: 0.005598896627806211\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007844057766487823\n",
      "Training Loss: 0.00774938955437392\n",
      "Training Loss: 0.007872705406043679\n",
      "Validation Loss: 0.005589714693690367\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007836885831784457\n",
      "Training Loss: 0.007740615805378184\n",
      "Training Loss: 0.007863671828527003\n",
      "Validation Loss: 0.00558083916600961\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007829917629715055\n",
      "Training Loss: 0.0077320880256593226\n",
      "Training Loss: 0.007854915587231517\n",
      "Validation Loss: 0.0055722372287235564\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007823126922594383\n",
      "Training Loss: 0.007723778755171225\n",
      "Training Loss: 0.00784640627214685\n",
      "Validation Loss: 0.005563875648527919\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007816488516982644\n",
      "Training Loss: 0.00771566137787886\n",
      "Training Loss: 0.007838115959893912\n",
      "Validation Loss: 0.005555725020743655\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007809980075107887\n",
      "Training Loss: 0.00770771230221726\n",
      "Training Loss: 0.00783001989359036\n",
      "Validation Loss: 0.00554776162803801\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007803577712038532\n",
      "Training Loss: 0.007699905802728608\n",
      "Training Loss: 0.007822090425761416\n",
      "Validation Loss: 0.005539958387498189\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0077972626802511515\n",
      "Training Loss: 0.007692221649922431\n",
      "Training Loss: 0.007814306722721084\n",
      "Validation Loss: 0.005532290976170157\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007791014559334144\n",
      "Training Loss: 0.007684638419887051\n",
      "Training Loss: 0.007806645352393389\n",
      "Validation Loss: 0.0055247358676946065\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007784816036000848\n",
      "Training Loss: 0.007677136907586828\n",
      "Training Loss: 0.0077990854869131\n",
      "Validation Loss: 0.005517275471585604\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007778650110121816\n",
      "Training Loss: 0.007669699243269861\n",
      "Training Loss: 0.007791607107501477\n",
      "Validation Loss: 0.0055098890567596035\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007772501491708681\n",
      "Training Loss: 0.007662308583967388\n",
      "Training Loss: 0.007784191691316664\n",
      "Validation Loss: 0.005502557931363248\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007766355259809643\n",
      "Training Loss: 0.007654947515111417\n",
      "Training Loss: 0.007776822331361472\n",
      "Validation Loss: 0.005495260528477139\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007760197939351201\n",
      "Training Loss: 0.007647601251956075\n",
      "Training Loss: 0.007769480306887999\n",
      "Validation Loss: 0.0054879888219842585\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007754017433617264\n",
      "Training Loss: 0.007640255833975971\n",
      "Training Loss: 0.007762151987990364\n",
      "Validation Loss: 0.005480719241052029\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.0077478010812774304\n",
      "Training Loss: 0.0076328990911133585\n",
      "Training Loss: 0.007754821402486413\n",
      "Validation Loss: 0.00547344141425251\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007741538194241002\n",
      "Training Loss: 0.007625515651889145\n",
      "Training Loss: 0.0077474734710995105\n",
      "Validation Loss: 0.005466135084011665\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007735218363814056\n",
      "Training Loss: 0.007618095682701096\n",
      "Training Loss: 0.007740095912013203\n",
      "Validation Loss: 0.005458792413877888\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007728831364074722\n",
      "Training Loss: 0.007610627914546058\n",
      "Training Loss: 0.007732676062732935\n",
      "Validation Loss: 0.005451400759054369\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007722369035473093\n",
      "Training Loss: 0.007603100310079753\n",
      "Training Loss: 0.007725200287532061\n",
      "Validation Loss: 0.005443940132730797\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007715820824960247\n",
      "Training Loss: 0.007595504057826474\n",
      "Training Loss: 0.007717657902976498\n",
      "Validation Loss: 0.005436406469527172\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007709178992081433\n",
      "Training Loss: 0.007587828341638669\n",
      "Training Loss: 0.007710037622600794\n",
      "Validation Loss: 0.005428783395432271\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007702435411047191\n",
      "Training Loss: 0.007580065968213603\n",
      "Training Loss: 0.007702329417224973\n",
      "Validation Loss: 0.005421062040645001\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007695582039887085\n",
      "Training Loss: 0.007572206823388114\n",
      "Training Loss: 0.007694521702360362\n",
      "Validation Loss: 0.005413233064470834\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007688611233606935\n",
      "Training Loss: 0.007564244129462168\n",
      "Training Loss: 0.0076866061554756015\n",
      "Validation Loss: 0.005405286009151363\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007681515702279285\n",
      "Training Loss: 0.007556169563904405\n",
      "Training Loss: 0.007678573706652969\n",
      "Validation Loss: 0.005397207625576535\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007674287963891402\n",
      "Training Loss: 0.007547976933419704\n",
      "Training Loss: 0.007670416235923767\n",
      "Validation Loss: 0.0053889947953853715\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007666921797208488\n",
      "Training Loss: 0.007539658448658884\n",
      "Training Loss: 0.007662125100614503\n",
      "Validation Loss: 0.005380635942447554\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007659409119514749\n",
      "Training Loss: 0.007531208619475364\n",
      "Training Loss: 0.007653692556777969\n",
      "Validation Loss: 0.005372121803242755\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00765174419619143\n",
      "Training Loss: 0.00752262175665237\n",
      "Training Loss: 0.0076451137429103255\n",
      "Validation Loss: 0.005363447582506145\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0076439202774781735\n",
      "Training Loss: 0.007513892748393119\n",
      "Training Loss: 0.007636380958138034\n",
      "Validation Loss: 0.00535460735875276\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007635930905817077\n",
      "Training Loss: 0.007505017515504733\n",
      "Training Loss: 0.007627489486476406\n",
      "Validation Loss: 0.005345593943009467\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.0076277707412373275\n",
      "Training Loss: 0.0074959902139380575\n",
      "Training Loss: 0.007618434346513822\n",
      "Validation Loss: 0.005336399725115115\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007619434555526823\n",
      "Training Loss: 0.007486809893744066\n",
      "Training Loss: 0.00760921241948381\n",
      "Validation Loss: 0.005327023113188282\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007610916105331853\n",
      "Training Loss: 0.007477472465252504\n",
      "Training Loss: 0.0075998212303966285\n",
      "Validation Loss: 0.005317462166101577\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007602211091434583\n",
      "Training Loss: 0.00746797742205672\n",
      "Training Loss: 0.007590258338022977\n",
      "Validation Loss: 0.0053077113835580565\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007593316184356809\n",
      "Training Loss: 0.007458322996972129\n",
      "Training Loss: 0.0075805243267677725\n",
      "Validation Loss: 0.005297770336521476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.00758422689861618\n",
      "Training Loss: 0.007448510326212272\n",
      "Training Loss: 0.0075706190604250875\n",
      "Validation Loss: 0.0052876390145275365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007574940295889974\n",
      "Training Loss: 0.007438539997674525\n",
      "Training Loss: 0.007560546256136149\n",
      "Validation Loss: 0.005277320508468436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007565454659052193\n",
      "Training Loss: 0.007428414549212903\n",
      "Training Loss: 0.007550309009384364\n",
      "Validation Loss: 0.005266815026322108\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007555768287274987\n",
      "Training Loss: 0.007418136791093275\n",
      "Training Loss: 0.007539911919739097\n",
      "Validation Loss: 0.005256124320858566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007545881683472544\n",
      "Training Loss: 0.007407710693078115\n",
      "Training Loss: 0.007529360941844061\n",
      "Validation Loss: 0.005245257010735738\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007535793533315882\n",
      "Training Loss: 0.0073971431830432265\n",
      "Training Loss: 0.007518666234100238\n",
      "Validation Loss: 0.005234217592985944\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007525506148813293\n",
      "Training Loss: 0.007386440256377682\n",
      "Training Loss: 0.007507836044533178\n",
      "Validation Loss: 0.0052230130898075575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007515021851868369\n",
      "Training Loss: 0.007375609604641795\n",
      "Training Loss: 0.00749688180279918\n",
      "Validation Loss: 0.005211651460607693\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007504343546461314\n",
      "Training Loss: 0.007364659253507853\n",
      "Training Loss: 0.007485815874533727\n",
      "Validation Loss: 0.005200143288716339\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.007493474038783461\n",
      "Training Loss: 0.0073535994277335705\n",
      "Training Loss: 0.007474650803487748\n",
      "Validation Loss: 0.00518849885072873\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007482417450519279\n",
      "Training Loss: 0.0073424383485689755\n",
      "Training Loss: 0.007463402054272592\n",
      "Validation Loss: 0.005176728758097658\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.007471180298598484\n",
      "Training Loss: 0.007331188842654228\n",
      "Training Loss: 0.007452083077514544\n",
      "Validation Loss: 0.005164844713833058\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.0074597664218163115\n",
      "Training Loss: 0.007319860151037574\n",
      "Training Loss: 0.00744070959626697\n",
      "Validation Loss: 0.005152856097216561\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0074481801933143285\n",
      "Training Loss: 0.007308463968802244\n",
      "Training Loss: 0.007429295910988003\n",
      "Validation Loss: 0.005140776890513142\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007436427313368767\n",
      "Training Loss: 0.007297011223854497\n",
      "Training Loss: 0.007417856971733272\n",
      "Validation Loss: 0.005128617801970352\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007424511660938151\n",
      "Training Loss: 0.007285511185182258\n",
      "Training Loss: 0.007406406659865752\n",
      "Validation Loss: 0.00511638635999701\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.00741243603581097\n",
      "Training Loss: 0.0072739756607916204\n",
      "Training Loss: 0.007394957455107942\n",
      "Validation Loss: 0.0051040912100658075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.00740020350262057\n",
      "Training Loss: 0.007262412596028298\n",
      "Training Loss: 0.007383520158473402\n",
      "Validation Loss: 0.00509174312190205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.007387814265093766\n",
      "Training Loss: 0.00725083073717542\n",
      "Training Loss: 0.007372103768866509\n",
      "Validation Loss: 0.005079345608239972\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.0073752686107764025\n",
      "Training Loss: 0.007239238710608334\n",
      "Training Loss: 0.007360717143164948\n",
      "Validation Loss: 0.005066904066041572\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.007362562533235178\n",
      "Training Loss: 0.007227642262587324\n",
      "Training Loss: 0.007349363797111437\n",
      "Validation Loss: 0.005054421021780941\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.007349691878771409\n",
      "Training Loss: 0.007216047843685374\n",
      "Training Loss: 0.007338047842495143\n",
      "Validation Loss: 0.005041898876360586\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.00733665086212568\n",
      "Training Loss: 0.007204460162902251\n",
      "Training Loss: 0.0073267701570875945\n",
      "Validation Loss: 0.005029340267437772\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.00732342952338513\n",
      "Training Loss: 0.007192883357638493\n",
      "Training Loss: 0.007315529899206013\n",
      "Validation Loss: 0.005016743608363224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.007310018233256415\n",
      "Training Loss: 0.007181320209056139\n",
      "Training Loss: 0.007304323863936588\n",
      "Validation Loss: 0.005004107177759824\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.007296403860091232\n",
      "Training Loss: 0.007169773936038837\n",
      "Training Loss: 0.00729314771713689\n",
      "Validation Loss: 0.004991432727743568\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.007282573115080595\n",
      "Training Loss: 0.007158247261540965\n",
      "Training Loss: 0.00728199613513425\n",
      "Validation Loss: 0.004978723190934136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00726851261802949\n",
      "Training Loss: 0.007146743307821453\n",
      "Training Loss: 0.007270864088786766\n",
      "Validation Loss: 0.0049659843292359385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0072542096104007215\n",
      "Training Loss: 0.007135265704710037\n",
      "Training Loss: 0.007259746675845236\n",
      "Validation Loss: 0.0049532274277409895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.007239653854048811\n",
      "Training Loss: 0.007123819891130551\n",
      "Training Loss: 0.007248641225742176\n",
      "Validation Loss: 0.004940469964444009\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.007224838206311688\n",
      "Training Loss: 0.0071124127565417436\n",
      "Training Loss: 0.007237545337993652\n",
      "Validation Loss: 0.004927736985643677\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007209764490253292\n",
      "Training Loss: 0.00710105545935221\n",
      "Training Loss: 0.007226462248945608\n",
      "Validation Loss: 0.004915067894002318\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007194439277518541\n",
      "Training Loss: 0.0070897594373673205\n",
      "Training Loss: 0.007215398355619982\n",
      "Validation Loss: 0.004902503341357904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.007178880990832113\n",
      "Training Loss: 0.007078539483482018\n",
      "Training Loss: 0.007204362495103851\n",
      "Validation Loss: 0.004890098858472964\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.0071631190617335964\n",
      "Training Loss: 0.007067412128672004\n",
      "Training Loss: 0.007193367733852938\n",
      "Validation Loss: 0.004877913397935669\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.007147192419506609\n",
      "Training Loss: 0.007056393036618829\n",
      "Training Loss: 0.007182427372317762\n",
      "Validation Loss: 0.004866006234765388\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.0071311468275962395\n",
      "Training Loss: 0.007045495942002163\n",
      "Training Loss: 0.007171554362867027\n",
      "Validation Loss: 0.004854433216269683\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.007115035565220751\n",
      "Training Loss: 0.007034730397863314\n",
      "Training Loss: 0.007160758344689384\n",
      "Validation Loss: 0.004843237130965493\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00709891184058506\n",
      "Training Loss: 0.00702410054509528\n",
      "Training Loss: 0.007150046337628737\n",
      "Validation Loss: 0.004832447293931305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.007082829136634245\n",
      "Training Loss: 0.0070136034244205805\n",
      "Training Loss: 0.007139417567523196\n",
      "Validation Loss: 0.004822074277378786\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.00706683217373211\n",
      "Training Loss: 0.007003228995017707\n",
      "Training Loss: 0.007128868089057505\n",
      "Validation Loss: 0.0048121059878488606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.007050958350882866\n",
      "Training Loss: 0.006992963200900704\n",
      "Training Loss: 0.007118387650698423\n",
      "Validation Loss: 0.004802516939542308\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.007035233860951848\n",
      "Training Loss: 0.006982789789326489\n",
      "Training Loss: 0.007107961395522579\n",
      "Validation Loss: 0.004793268820699932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.007019678242504597\n",
      "Training Loss: 0.0069726872956380245\n",
      "Training Loss: 0.007097575285006315\n",
      "Validation Loss: 0.004784309599559043\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.007004298648098484\n",
      "Training Loss: 0.006962635702220723\n",
      "Training Loss: 0.00708721099072136\n",
      "Validation Loss: 0.004775585547832542\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006989095527678728\n",
      "Training Loss: 0.006952614203328267\n",
      "Training Loss: 0.0070768499863334\n",
      "Validation Loss: 0.004767037947070942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006974066524417139\n",
      "Training Loss: 0.006942604121286422\n",
      "Training Loss: 0.00706647627404891\n",
      "Validation Loss: 0.0047586126543393125\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006959202088764869\n",
      "Training Loss: 0.006932589011266827\n",
      "Training Loss: 0.007056073165731504\n",
      "Validation Loss: 0.004750258148009522\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006944493040209636\n",
      "Training Loss: 0.006922553373733535\n",
      "Training Loss: 0.007045628564665094\n",
      "Validation Loss: 0.00474192941132305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.00692992637923453\n",
      "Training Loss: 0.00691248346818611\n",
      "Training Loss: 0.007035127828130499\n",
      "Validation Loss: 0.004733583056027844\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006915491024847143\n",
      "Training Loss: 0.006902367433067411\n",
      "Training Loss: 0.007024561308789998\n",
      "Validation Loss: 0.004725182670942937\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.0069011757569387554\n",
      "Training Loss: 0.006892195508116856\n",
      "Training Loss: 0.0070139208901673555\n",
      "Validation Loss: 0.004716697220266745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006886970966006629\n",
      "Training Loss: 0.006881958489539102\n",
      "Training Loss: 0.0070031988201662895\n",
      "Validation Loss: 0.004708095001098648\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006872867078636773\n",
      "Training Loss: 0.006871648421511054\n",
      "Training Loss: 0.006992390197701752\n",
      "Validation Loss: 0.004699357577306585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006858857583720237\n",
      "Training Loss: 0.006861261313315481\n",
      "Training Loss: 0.006981492738705128\n",
      "Validation Loss: 0.004690461313964151\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.0068449384142877535\n",
      "Training Loss: 0.006850790263852105\n",
      "Training Loss: 0.0069705052976496516\n",
      "Validation Loss: 0.004681393160128945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006831103967269883\n",
      "Training Loss: 0.006840235255658626\n",
      "Training Loss: 0.00695942779770121\n",
      "Validation Loss: 0.004672134972015249\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006817354349186644\n",
      "Training Loss: 0.006829593502916395\n",
      "Training Loss: 0.006948262845398858\n",
      "Validation Loss: 0.004662680994258837\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006803689093794673\n",
      "Training Loss: 0.006818867250112817\n",
      "Training Loss: 0.006937014819122851\n",
      "Validation Loss: 0.004653022279230396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006790110875735991\n",
      "Training Loss: 0.006808057442540303\n",
      "Training Loss: 0.006925689016934484\n",
      "Validation Loss: 0.004643155201704482\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006776623527402989\n",
      "Training Loss: 0.0067971684481017295\n",
      "Training Loss: 0.006914294164162129\n",
      "Validation Loss: 0.004633079394122607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006763233511592261\n",
      "Training Loss: 0.006786208066623658\n",
      "Training Loss: 0.006902839905815199\n",
      "Validation Loss: 0.0046227986962283345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006749948334181681\n",
      "Training Loss: 0.006775184866273776\n",
      "Training Loss: 0.006891336214030161\n",
      "Validation Loss: 0.004612315988973871\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.0067367784446105364\n",
      "Training Loss: 0.006764107736526057\n",
      "Training Loss: 0.0068797970702871684\n",
      "Validation Loss: 0.0046016404181561865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.00672373432898894\n",
      "Training Loss: 0.006752989828819409\n",
      "Training Loss: 0.006868236428126693\n",
      "Validation Loss: 0.0045907833623919595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006710829073563218\n",
      "Training Loss: 0.006741846920922398\n",
      "Training Loss: 0.006856670823181048\n",
      "Validation Loss: 0.004579761164953534\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006698077399632894\n",
      "Training Loss: 0.006730694581056014\n",
      "Training Loss: 0.006845115276519209\n",
      "Validation Loss: 0.0045685895111704814\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.0066854953672736885\n",
      "Training Loss: 0.006719552244758234\n",
      "Training Loss: 0.006833590345922858\n",
      "Validation Loss: 0.004557291858076128\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006673099970212207\n",
      "Training Loss: 0.006708440517541021\n",
      "Training Loss: 0.006822114465758204\n",
      "Validation Loss: 0.004545893228424483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006660909636411816\n",
      "Training Loss: 0.0066973825974855575\n",
      "Training Loss: 0.00681070790742524\n",
      "Validation Loss: 0.0045344198947992135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006648943269392475\n",
      "Training Loss: 0.0066864022600930184\n",
      "Training Loss: 0.006799390636151656\n",
      "Validation Loss: 0.004522903138258903\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006637220565462485\n",
      "Training Loss: 0.0066755236429162325\n",
      "Training Loss: 0.006788183768512681\n",
      "Validation Loss: 0.004511374428219507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0066257583070546385\n",
      "Training Loss: 0.006664771805517376\n",
      "Training Loss: 0.006777108539827168\n",
      "Validation Loss: 0.004499865033967274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.0066145753691671414\n",
      "Training Loss: 0.006654171162517741\n",
      "Training Loss: 0.006766182342544198\n",
      "Validation Loss: 0.004488413186817105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.00660368942131754\n",
      "Training Loss: 0.006643747653579339\n",
      "Training Loss: 0.006755424467846751\n",
      "Validation Loss: 0.004477050803533724\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006593113839626312\n",
      "Training Loss: 0.006633522649062798\n",
      "Training Loss: 0.006744852805277333\n",
      "Validation Loss: 0.00446581145816514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006582863616058603\n",
      "Training Loss: 0.006623520667199045\n",
      "Training Loss: 0.006734480402665213\n",
      "Validation Loss: 0.0044547293002957875\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006572947478271089\n",
      "Training Loss: 0.006613756887381896\n",
      "Training Loss: 0.006724320549983532\n",
      "Validation Loss: 0.004443834986396427\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006563373734243214\n",
      "Training Loss: 0.006604249970987439\n",
      "Training Loss: 0.006714384593069553\n",
      "Validation Loss: 0.004433153034318657\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.0065541455725906415\n",
      "Training Loss: 0.006595012487377971\n",
      "Training Loss: 0.006704679500544443\n",
      "Validation Loss: 0.004422709126067296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006545264328597113\n",
      "Training Loss: 0.006586055887164548\n",
      "Training Loss: 0.006695211018668487\n",
      "Validation Loss: 0.004412526922729494\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006536728707142174\n",
      "Training Loss: 0.00657738727517426\n",
      "Training Loss: 0.006685981198679656\n",
      "Validation Loss: 0.004402615867680713\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006528532495722174\n",
      "Training Loss: 0.006569008183432743\n",
      "Training Loss: 0.006676990987034515\n",
      "Validation Loss: 0.004392988453985432\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006520668123266659\n",
      "Training Loss: 0.006560919836629182\n",
      "Training Loss: 0.0066682382475119085\n",
      "Validation Loss: 0.0043836549883237465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006513124640914612\n",
      "Training Loss: 0.006553120909957215\n",
      "Training Loss: 0.006659719151211902\n",
      "Validation Loss: 0.0043746156119636856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006505890958942473\n",
      "Training Loss: 0.006545604829443619\n",
      "Training Loss: 0.006651430061319843\n",
      "Validation Loss: 0.0043658737120679005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.00649895261216443\n",
      "Training Loss: 0.006538365478627383\n",
      "Training Loss: 0.006643363975454122\n",
      "Validation Loss: 0.004357421916538033\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006492294749477878\n",
      "Training Loss: 0.006531394382473081\n",
      "Training Loss: 0.006635513448854908\n",
      "Validation Loss: 0.004349257499424301\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006485903188586235\n",
      "Training Loss: 0.0065246809623204175\n",
      "Training Loss: 0.006627870538504794\n",
      "Validation Loss: 0.0043413711540411445\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006479759967187419\n",
      "Training Loss: 0.006518214306561276\n",
      "Training Loss: 0.00662043017684482\n",
      "Validation Loss: 0.0043337577012147795\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0064738530188333246\n",
      "Training Loss: 0.00651198458741419\n",
      "Training Loss: 0.006613181822467595\n",
      "Validation Loss: 0.004326403304526394\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006468164502293803\n",
      "Training Loss: 0.006505979138892144\n",
      "Training Loss: 0.0066061184194404635\n",
      "Validation Loss: 0.004319298629167626\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006462681534467265\n",
      "Training Loss: 0.00650018664659001\n",
      "Training Loss: 0.006599232484586537\n",
      "Validation Loss: 0.004312435047978305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006457388858543709\n",
      "Training Loss: 0.0064945962477941065\n",
      "Training Loss: 0.006592516270466149\n",
      "Validation Loss: 0.004305798833760736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006452274355688133\n",
      "Training Loss: 0.00648919660015963\n",
      "Training Loss: 0.006585962724639103\n",
      "Validation Loss: 0.004299380231172558\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006447325195767917\n",
      "Training Loss: 0.006483977140160278\n",
      "Training Loss: 0.006579566299915313\n",
      "Validation Loss: 0.0042931695058000055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.0064425306144403295\n",
      "Training Loss: 0.00647892831475474\n",
      "Training Loss: 0.006573318192968145\n",
      "Validation Loss: 0.004287155243055372\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006437879657605663\n",
      "Training Loss: 0.006474039307795465\n",
      "Training Loss: 0.006567213870584965\n",
      "Validation Loss: 0.004281329228464263\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006433361289673485\n",
      "Training Loss: 0.006469302335754037\n",
      "Training Loss: 0.006561246556229889\n",
      "Validation Loss: 0.004275683774037308\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006428967610700056\n",
      "Training Loss: 0.006464708938729018\n",
      "Training Loss: 0.006555411422159523\n",
      "Validation Loss: 0.004270207652890155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.0064246894797543065\n",
      "Training Loss: 0.00646025039954111\n",
      "Training Loss: 0.00654970281291753\n",
      "Validation Loss: 0.004264892250943008\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006420519077219069\n",
      "Training Loss: 0.006455919448053464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:28<22:12, 148.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0065441158751491455\n",
      "Validation Loss: 0.004259731472087919\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.11953692913055419\n",
      "Training Loss: 0.1010743436217308\n",
      "Training Loss: 0.08597028397023677\n",
      "Validation Loss: 0.07205619093742263\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06637419365346432\n",
      "Training Loss: 0.06011049525812268\n",
      "Training Loss: 0.05612170178443193\n",
      "Validation Loss: 0.05321463693477464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.05175934435799718\n",
      "Training Loss: 0.04780995709821582\n",
      "Training Loss: 0.0442838122881949\n",
      "Validation Loss: 0.04113217464156365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.0399712781328708\n",
      "Training Loss: 0.03622826224192977\n",
      "Training Loss: 0.03321001091971994\n",
      "Validation Loss: 0.030002664790352743\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.029031880837865173\n",
      "Training Loss: 0.02582746692933142\n",
      "Training Loss: 0.02364921892527491\n",
      "Validation Loss: 0.02094047234309942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.020790189048275353\n",
      "Training Loss: 0.019578428640961647\n",
      "Training Loss: 0.01910065726842731\n",
      "Validation Loss: 0.017239247244688567\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.017507334062829612\n",
      "Training Loss: 0.017032656795345248\n",
      "Training Loss: 0.016931747703347357\n",
      "Validation Loss: 0.015087930237079102\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.015524220184888691\n",
      "Training Loss: 0.015274134553037584\n",
      "Training Loss: 0.015278107174672186\n",
      "Validation Loss: 0.01340978659605712\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.013987425148952752\n",
      "Training Loss: 0.013881669342517854\n",
      "Training Loss: 0.013929858491756021\n",
      "Validation Loss: 0.012050419237485595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.01276681033661589\n",
      "Training Loss: 0.01276482837740332\n",
      "Training Loss: 0.012834869099315256\n",
      "Validation Loss: 0.010951712790332484\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.011804981441237032\n",
      "Training Loss: 0.011873189976904541\n",
      "Training Loss: 0.011955199176445603\n",
      "Validation Loss: 0.010067590426004837\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.01105272447457537\n",
      "Training Loss: 0.011163147434126585\n",
      "Training Loss: 0.01125286046648398\n",
      "Validation Loss: 0.009355893552177743\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.01046568397898227\n",
      "Training Loss: 0.010596947551239282\n",
      "Training Loss: 0.010692850679624825\n",
      "Validation Loss: 0.008780676042242499\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.010006893454119562\n",
      "Training Loss: 0.010144002290908248\n",
      "Training Loss: 0.010245735759381204\n",
      "Validation Loss: 0.008313243380705878\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.00964725666679442\n",
      "Training Loss: 0.0097804504155647\n",
      "Training Loss: 0.00988793270662427\n",
      "Validation Loss: 0.007931392943340072\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.009364439740311354\n",
      "Training Loss: 0.009487876098137348\n",
      "Training Loss: 0.009600914479233325\n",
      "Validation Loss: 0.007618104192700362\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.009141389125725255\n",
      "Training Loss: 0.009251955755753443\n",
      "Training Loss: 0.009370125798741356\n",
      "Validation Loss: 0.007360199778968615\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.008964976872084663\n",
      "Training Loss: 0.009061381014762447\n",
      "Training Loss: 0.009184062040876597\n",
      "Validation Loss: 0.007147346140576129\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.008825006287079305\n",
      "Training Loss: 0.008907106599071995\n",
      "Training Loss: 0.009033560962416231\n",
      "Validation Loss: 0.006971280574484655\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.008713488965295255\n",
      "Training Loss: 0.00878184900386259\n",
      "Training Loss: 0.008911317307502031\n",
      "Validation Loss: 0.006825322545593961\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008624160623876378\n",
      "Training Loss: 0.008679723085369915\n",
      "Training Loss: 0.008811490815132857\n",
      "Validation Loss: 0.006704012105984383\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.008552116983337328\n",
      "Training Loss: 0.008596015991643072\n",
      "Training Loss: 0.008729451702674852\n",
      "Validation Loss: 0.006602871833431921\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008493544632801785\n",
      "Training Loss: 0.008526961096795276\n",
      "Training Loss: 0.008661540909670293\n",
      "Validation Loss: 0.006518211914226413\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.00844549359055236\n",
      "Training Loss: 0.008469585293205455\n",
      "Training Loss: 0.008604885480599478\n",
      "Validation Loss: 0.00644702026542025\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.00840569859254174\n",
      "Training Loss: 0.008421538991387933\n",
      "Training Loss: 0.00855722492095083\n",
      "Validation Loss: 0.006386809779613624\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008372406137641519\n",
      "Training Loss: 0.008380963038653134\n",
      "Training Loss: 0.008516773665323853\n",
      "Validation Loss: 0.006335544547725344\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008344261883758008\n",
      "Training Loss: 0.008346392288804055\n",
      "Training Loss: 0.008482127171009778\n",
      "Validation Loss: 0.006291560858770619\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008320211454993114\n",
      "Training Loss: 0.0083166607003659\n",
      "Training Loss: 0.008452164004556834\n",
      "Validation Loss: 0.006253508700507829\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008299427016172558\n",
      "Training Loss: 0.00829084268771112\n",
      "Training Loss: 0.008425990124233068\n",
      "Validation Loss: 0.006220298294720941\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008281255557667464\n",
      "Training Loss: 0.008268193438416347\n",
      "Training Loss: 0.008402884899405763\n",
      "Validation Loss: 0.006191041413433013\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008265176044078544\n",
      "Training Loss: 0.008248117823386565\n",
      "Training Loss: 0.008382271005539224\n",
      "Validation Loss: 0.006165028004279214\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.00825077562709339\n",
      "Training Loss: 0.008230135351186618\n",
      "Training Loss: 0.00836368276970461\n",
      "Validation Loss: 0.006141687978812483\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008237721511395648\n",
      "Training Loss: 0.008213861280819402\n",
      "Training Loss: 0.008346744441660121\n",
      "Validation Loss: 0.006120563017759012\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008225748282857239\n",
      "Training Loss: 0.008198981132591144\n",
      "Training Loss: 0.008331151701277122\n",
      "Validation Loss: 0.006101282975332958\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008214644686086103\n",
      "Training Loss: 0.008185243733460084\n",
      "Training Loss: 0.008316662632860244\n",
      "Validation Loss: 0.0060835586954954635\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008204242277424783\n",
      "Training Loss: 0.008172448432305827\n",
      "Training Loss: 0.008303082006750628\n",
      "Validation Loss: 0.006067152059207974\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008194405608810485\n",
      "Training Loss: 0.008160431468859315\n",
      "Training Loss: 0.008290251846192405\n",
      "Validation Loss: 0.00605186848545426\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.00818502667127177\n",
      "Training Loss: 0.008149057999253273\n",
      "Training Loss: 0.008278046224731953\n",
      "Validation Loss: 0.006037545965963535\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008176020460668952\n",
      "Training Loss: 0.008138223589630797\n",
      "Training Loss: 0.008266362042631955\n",
      "Validation Loss: 0.006024065023298595\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008167315688915551\n",
      "Training Loss: 0.00812783760833554\n",
      "Training Loss: 0.008255118965171277\n",
      "Validation Loss: 0.00601130975227217\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008158860341645778\n",
      "Training Loss: 0.008117832087446004\n",
      "Training Loss: 0.00824425034224987\n",
      "Validation Loss: 0.005999187852538536\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.00815060691558756\n",
      "Training Loss: 0.008108144215075299\n",
      "Training Loss: 0.008233699122210964\n",
      "Validation Loss: 0.005987623243414787\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.008142519496614114\n",
      "Training Loss: 0.008098725957097486\n",
      "Training Loss: 0.008223420417634771\n",
      "Validation Loss: 0.005976545271264871\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.008134567282395437\n",
      "Training Loss: 0.008089535045437515\n",
      "Training Loss: 0.008213378080399706\n",
      "Validation Loss: 0.005965897150264446\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.008126724150497467\n",
      "Training Loss: 0.008080539054935798\n",
      "Training Loss: 0.008203538483940065\n",
      "Validation Loss: 0.005955620014809825\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.00811897023115307\n",
      "Training Loss: 0.00807170572457835\n",
      "Training Loss: 0.008193876581499353\n",
      "Validation Loss: 0.005945670986129494\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.008111285674385727\n",
      "Training Loss: 0.008063012292841449\n",
      "Training Loss: 0.008184367880458012\n",
      "Validation Loss: 0.005936008178252267\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.008103656019084155\n",
      "Training Loss: 0.008054436000529677\n",
      "Training Loss: 0.008174993507564067\n",
      "Validation Loss: 0.005926594425913658\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.008096065695863217\n",
      "Training Loss: 0.008045958932489156\n",
      "Training Loss: 0.008165736410301178\n",
      "Validation Loss: 0.005917399219143089\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.008088504711631686\n",
      "Training Loss: 0.008037564709084108\n",
      "Training Loss: 0.008156580291688443\n",
      "Validation Loss: 0.0059083913233173024\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.00808096120483242\n",
      "Training Loss: 0.008029238077579066\n",
      "Training Loss: 0.008147512632422148\n",
      "Validation Loss: 0.005899541382154638\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.008073425651527941\n",
      "Training Loss: 0.008020966248586774\n",
      "Training Loss: 0.008138519796775654\n",
      "Validation Loss: 0.005890829266268718\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00806588880252093\n",
      "Training Loss: 0.008012737456010655\n",
      "Training Loss: 0.008129593530902639\n",
      "Validation Loss: 0.005882229761932087\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.00805834318860434\n",
      "Training Loss: 0.008004543324932455\n",
      "Training Loss: 0.008120720693841576\n",
      "Validation Loss: 0.005873725975842707\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.008050782496575266\n",
      "Training Loss: 0.007996371255721897\n",
      "Training Loss: 0.008111896201735363\n",
      "Validation Loss: 0.005865296431335757\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.008043198416708038\n",
      "Training Loss: 0.007988215746590867\n",
      "Training Loss: 0.00810310833971016\n",
      "Validation Loss: 0.005856925518293813\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.008035585171310231\n",
      "Training Loss: 0.007980066917371005\n",
      "Training Loss: 0.008094351247418672\n",
      "Validation Loss: 0.005848602573596611\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.008027938213199377\n",
      "Training Loss: 0.007971920062555\n",
      "Training Loss: 0.008085618048207835\n",
      "Validation Loss: 0.005840311203612371\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.008020251103444025\n",
      "Training Loss: 0.007963767364853993\n",
      "Training Loss: 0.008076902939938008\n",
      "Validation Loss: 0.005832040348885518\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.008012520849006251\n",
      "Training Loss: 0.007955602845177054\n",
      "Training Loss: 0.00806819953257218\n",
      "Validation Loss: 0.0058237766517389026\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.008004741504555568\n",
      "Training Loss: 0.007947419607080519\n",
      "Training Loss: 0.00805950072244741\n",
      "Validation Loss: 0.00581551014231204\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007996908143395558\n",
      "Training Loss: 0.007939215099904686\n",
      "Training Loss: 0.00805080410093069\n",
      "Validation Loss: 0.005807238255543739\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007989019121741875\n",
      "Training Loss: 0.007930980641394853\n",
      "Training Loss: 0.008042101978790015\n",
      "Validation Loss: 0.005798942392451291\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.00798106734524481\n",
      "Training Loss: 0.007922713620355354\n",
      "Training Loss: 0.008033391883363947\n",
      "Validation Loss: 0.0057906232933838215\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007973051534499974\n",
      "Training Loss: 0.007914410015800968\n",
      "Training Loss: 0.00802466840017587\n",
      "Validation Loss: 0.005782270448261432\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007964966881554575\n",
      "Training Loss: 0.007906065385323019\n",
      "Training Loss: 0.008015927673550322\n",
      "Validation Loss: 0.0057738768185352845\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007956810769392177\n",
      "Training Loss: 0.007897674001287668\n",
      "Training Loss: 0.008007166746538133\n",
      "Validation Loss: 0.005765435391817284\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007948580951197073\n",
      "Training Loss: 0.00788923201849684\n",
      "Training Loss: 0.007998378876363859\n",
      "Validation Loss: 0.005756939918305097\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.00794027139316313\n",
      "Training Loss: 0.007880735014332459\n",
      "Training Loss: 0.007989562079310418\n",
      "Validation Loss: 0.005748386081475555\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007931880682008341\n",
      "Training Loss: 0.007872181906132028\n",
      "Training Loss: 0.007980714878067374\n",
      "Validation Loss: 0.00573976891471094\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007923405803740024\n",
      "Training Loss: 0.007863568369066343\n",
      "Training Loss: 0.007971832158509641\n",
      "Validation Loss: 0.005731084860149729\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.00791484399465844\n",
      "Training Loss: 0.00785488834604621\n",
      "Training Loss: 0.007962909350171686\n",
      "Validation Loss: 0.005722327241569423\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007906191603979096\n",
      "Training Loss: 0.007846139889443294\n",
      "Training Loss: 0.007953947306377813\n",
      "Validation Loss: 0.0057134927038542845\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007897446576971561\n",
      "Training Loss: 0.007837320762919263\n",
      "Training Loss: 0.007944940371671691\n",
      "Validation Loss: 0.0057045774471558884\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007888605813495814\n",
      "Training Loss: 0.007828426358755677\n",
      "Training Loss: 0.007935887495987117\n",
      "Validation Loss: 0.0056955742851171765\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00787966744042933\n",
      "Training Loss: 0.007819452843395993\n",
      "Training Loss: 0.007926785077434034\n",
      "Validation Loss: 0.005686482691907146\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007870627968804911\n",
      "Training Loss: 0.007810399363515899\n",
      "Training Loss: 0.007917632431490346\n",
      "Validation Loss: 0.0056773004268578595\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007861484965542332\n",
      "Training Loss: 0.0078012602857779716\n",
      "Training Loss: 0.007908425284549594\n",
      "Validation Loss: 0.005668016246080482\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007852235244354234\n",
      "Training Loss: 0.007792032443685457\n",
      "Training Loss: 0.007899162432877346\n",
      "Validation Loss: 0.005658630984102826\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.00784287708811462\n",
      "Training Loss: 0.007782715356443077\n",
      "Training Loss: 0.007889840747229754\n",
      "Validation Loss: 0.005649142463042746\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007833407799480482\n",
      "Training Loss: 0.00777330526150763\n",
      "Training Loss: 0.00788046113215387\n",
      "Validation Loss: 0.005639546595283606\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007823825475061313\n",
      "Training Loss: 0.007763799818931148\n",
      "Training Loss: 0.007871021389728412\n",
      "Validation Loss: 0.005629840393268158\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007814129323232919\n",
      "Training Loss: 0.007754196713212877\n",
      "Training Loss: 0.007861519440775738\n",
      "Validation Loss: 0.005620021345564739\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007804315404500812\n",
      "Training Loss: 0.007744493093341589\n",
      "Training Loss: 0.007851954617071897\n",
      "Validation Loss: 0.005610083382880169\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007794383884174749\n",
      "Training Loss: 0.007734686377225444\n",
      "Training Loss: 0.007842324624070897\n",
      "Validation Loss: 0.005600026012084374\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.0077843320253305134\n",
      "Training Loss: 0.007724777029361576\n",
      "Training Loss: 0.007832631708588451\n",
      "Validation Loss: 0.005589848771440179\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007774159323889762\n",
      "Training Loss: 0.007714762141695246\n",
      "Training Loss: 0.007822873869445175\n",
      "Validation Loss: 0.005579547323495819\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007763865117449313\n",
      "Training Loss: 0.007704640604788438\n",
      "Training Loss: 0.007813051712000743\n",
      "Validation Loss: 0.0055691184701000375\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007753447860013693\n",
      "Training Loss: 0.007694410518743098\n",
      "Training Loss: 0.007803163358476013\n",
      "Validation Loss: 0.0055585613283276394\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.0077429063699673865\n",
      "Training Loss: 0.007684069892857224\n",
      "Training Loss: 0.007793209956726059\n",
      "Validation Loss: 0.005547870972110063\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007732242401689291\n",
      "Training Loss: 0.007673620830755681\n",
      "Training Loss: 0.007783193285576999\n",
      "Validation Loss: 0.005537049214387041\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0077214541391003874\n",
      "Training Loss: 0.007663062189240008\n",
      "Training Loss: 0.007773113880539313\n",
      "Validation Loss: 0.005526095478314111\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.0077105437440332025\n",
      "Training Loss: 0.0076523937447927895\n",
      "Training Loss: 0.00776297121308744\n",
      "Validation Loss: 0.0055150052433142836\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007699510735692456\n",
      "Training Loss: 0.007641615334432572\n",
      "Training Loss: 0.007752769710496068\n",
      "Validation Loss: 0.005503778871713897\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007688358917366713\n",
      "Training Loss: 0.0076307297113817184\n",
      "Training Loss: 0.007742508460069075\n",
      "Validation Loss: 0.0054924205858574325\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.0076770882797427475\n",
      "Training Loss: 0.007619736876804382\n",
      "Training Loss: 0.007732191421091557\n",
      "Validation Loss: 0.005480924261514139\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007665701491059735\n",
      "Training Loss: 0.007608639647951349\n",
      "Training Loss: 0.007721819528378546\n",
      "Validation Loss: 0.005469291293051805\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007654202656121925\n",
      "Training Loss: 0.007597439525416121\n",
      "Training Loss: 0.007711396050872281\n",
      "Validation Loss: 0.0054575249191708445\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0076425940461922436\n",
      "Training Loss: 0.00758613943355158\n",
      "Training Loss: 0.007700925328535959\n",
      "Validation Loss: 0.005445626519850633\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007630881944205612\n",
      "Training Loss: 0.007574742917204276\n",
      "Training Loss: 0.007690408320631832\n",
      "Validation Loss: 0.005433592793604966\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007619068702915684\n",
      "Training Loss: 0.007563254663255066\n",
      "Training Loss: 0.007679850510321558\n",
      "Validation Loss: 0.0054214314787826515\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007607162302592769\n",
      "Training Loss: 0.007551678901072592\n",
      "Training Loss: 0.007669256668305024\n",
      "Validation Loss: 0.005409147426894123\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007595168558182195\n",
      "Training Loss: 0.007540021071908995\n",
      "Training Loss: 0.007658630076330155\n",
      "Validation Loss: 0.005396738649722649\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0075830947340000425\n",
      "Training Loss: 0.007528288296889514\n",
      "Training Loss: 0.00764797592884861\n",
      "Validation Loss: 0.005384216333603423\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007570948303909972\n",
      "Training Loss: 0.007516487017273903\n",
      "Training Loss: 0.007637300227070227\n",
      "Validation Loss: 0.005371582231306461\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007558738213265315\n",
      "Training Loss: 0.007504622765118256\n",
      "Training Loss: 0.007626606086269021\n",
      "Validation Loss: 0.005358843742398901\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007546471713576466\n",
      "Training Loss: 0.007492704808246344\n",
      "Training Loss: 0.007615899842930957\n",
      "Validation Loss: 0.005346007943362667\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0075341608375310894\n",
      "Training Loss: 0.007480742642655969\n",
      "Training Loss: 0.0076051885168999436\n",
      "Validation Loss: 0.0053330840453965946\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007521815204527229\n",
      "Training Loss: 0.007468743460485712\n",
      "Training Loss: 0.00759447603719309\n",
      "Validation Loss: 0.005320080971931306\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.007509443034650758\n",
      "Training Loss: 0.007456717141903937\n",
      "Training Loss: 0.007583768670447171\n",
      "Validation Loss: 0.005307005488135842\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.007497057943837717\n",
      "Training Loss: 0.007444675562437624\n",
      "Training Loss: 0.007573072682134807\n",
      "Validation Loss: 0.005293870441552796\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.007484669281402603\n",
      "Training Loss: 0.007432625542860478\n",
      "Training Loss: 0.007562395198037848\n",
      "Validation Loss: 0.00528068784127284\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007472289419965818\n",
      "Training Loss: 0.007420579544268549\n",
      "Training Loss: 0.007551740122726187\n",
      "Validation Loss: 0.005267466797775934\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007459929543547333\n",
      "Training Loss: 0.0074085490102879705\n",
      "Training Loss: 0.007541114315390587\n",
      "Validation Loss: 0.005254223488867701\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007447599364677444\n",
      "Training Loss: 0.007396543564973399\n",
      "Training Loss: 0.007530524168396368\n",
      "Validation Loss: 0.005240971732654431\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007435312055749818\n",
      "Training Loss: 0.007384574905736372\n",
      "Training Loss: 0.007519976961193606\n",
      "Validation Loss: 0.005227729548658296\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.007423077601706609\n",
      "Training Loss: 0.007372656012885273\n",
      "Training Loss: 0.0075094778602942825\n",
      "Validation Loss: 0.005214510202993837\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.007410908084129915\n",
      "Training Loss: 0.007360796700231731\n",
      "Training Loss: 0.007499034422216937\n",
      "Validation Loss: 0.005201327176293714\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.007398814185289666\n",
      "Training Loss: 0.007349009501049295\n",
      "Training Loss: 0.007488654097542167\n",
      "Validation Loss: 0.0051882013618381\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.007386807064176537\n",
      "Training Loss: 0.007337306309491396\n",
      "Training Loss: 0.007478344273986295\n",
      "Validation Loss: 0.005175149594067356\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.007374898284324445\n",
      "Training Loss: 0.0073257006297353655\n",
      "Training Loss: 0.0074681151018012315\n",
      "Validation Loss: 0.005162195905551231\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007363100377842784\n",
      "Training Loss: 0.0073142058285884555\n",
      "Training Loss: 0.007457974089775234\n",
      "Validation Loss: 0.00514935791032033\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.007351425180677325\n",
      "Training Loss: 0.007302834914298728\n",
      "Training Loss: 0.007447932288050652\n",
      "Validation Loss: 0.005136656089623072\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.007339885624824092\n",
      "Training Loss: 0.007291600377066061\n",
      "Training Loss: 0.00743800082593225\n",
      "Validation Loss: 0.005124112139874546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.007328494155080989\n",
      "Training Loss: 0.007280518183251843\n",
      "Training Loss: 0.007428191397339106\n",
      "Validation Loss: 0.005111750080564133\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.007317265074816532\n",
      "Training Loss: 0.00726960382075049\n",
      "Training Loss: 0.007418516810284928\n",
      "Validation Loss: 0.005099591053499097\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.007306212215917185\n",
      "Training Loss: 0.007258872162783519\n",
      "Training Loss: 0.007408990015974269\n",
      "Validation Loss: 0.00508765878909257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.007295350871863775\n",
      "Training Loss: 0.007248338146600872\n",
      "Training Loss: 0.00739962526015006\n",
      "Validation Loss: 0.005075970280057426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.007284694379195571\n",
      "Training Loss: 0.007238016171613708\n",
      "Training Loss: 0.007390435111010447\n",
      "Validation Loss: 0.0050645484746004756\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007274256531964056\n",
      "Training Loss: 0.007227920594159514\n",
      "Training Loss: 0.007381433190312236\n",
      "Validation Loss: 0.005053416101178259\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007264051932143047\n",
      "Training Loss: 0.0072180660604499285\n",
      "Training Loss: 0.007372633341001347\n",
      "Validation Loss: 0.005042584892231553\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.007254094936652109\n",
      "Training Loss: 0.007208466026932001\n",
      "Training Loss: 0.007364046297734603\n",
      "Validation Loss: 0.005032078335532562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.007244395520538092\n",
      "Training Loss: 0.007199131398228928\n",
      "Training Loss: 0.0073556847556028515\n",
      "Validation Loss: 0.0050219080843858085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.007234965814277529\n",
      "Training Loss: 0.007190071531804279\n",
      "Training Loss: 0.007347556563327089\n",
      "Validation Loss: 0.005012082222461952\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.00722581438778434\n",
      "Training Loss: 0.007181295006303117\n",
      "Training Loss: 0.007339670168003068\n",
      "Validation Loss: 0.005002607597990401\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.007216948214336299\n",
      "Training Loss: 0.0071728066075593235\n",
      "Training Loss: 0.0073320289142429825\n",
      "Validation Loss: 0.004993491535980171\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.0072083724796539175\n",
      "Training Loss: 0.007164611596381292\n",
      "Training Loss: 0.0073246390034910295\n",
      "Validation Loss: 0.004984736177034341\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.0072000894084339965\n",
      "Training Loss: 0.00715670952340588\n",
      "Training Loss: 0.0073175011586863545\n",
      "Validation Loss: 0.004976339505467397\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.0071921002311864865\n",
      "Training Loss: 0.007149100120877847\n",
      "Training Loss: 0.007310612882720307\n",
      "Validation Loss: 0.004968294675666002\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.007184402252896689\n",
      "Training Loss: 0.007141778600635007\n",
      "Training Loss: 0.0073039719089865686\n",
      "Validation Loss: 0.004960596766793744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.007176993490429595\n",
      "Training Loss: 0.00713474077754654\n",
      "Training Loss: 0.007297574060503393\n",
      "Validation Loss: 0.004953237301460729\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.007169867145130411\n",
      "Training Loss: 0.007127979111392051\n",
      "Training Loss: 0.007291412174236029\n",
      "Validation Loss: 0.004946201473992485\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.007163016279810109\n",
      "Training Loss: 0.007121484156232327\n",
      "Training Loss: 0.007285478567937389\n",
      "Validation Loss: 0.004939477369477031\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00715643075411208\n",
      "Training Loss: 0.007115245860768482\n",
      "Training Loss: 0.007279765303246677\n",
      "Validation Loss: 0.004933052358159984\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.007150102102896199\n",
      "Training Loss: 0.0071092523192055525\n",
      "Training Loss: 0.007274260289268569\n",
      "Validation Loss: 0.004926908533664399\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.007144019433762878\n",
      "Training Loss: 0.007103490787558258\n",
      "Training Loss: 0.007268953934544698\n",
      "Validation Loss: 0.004921033705082419\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.00713816974661313\n",
      "Training Loss: 0.007097951037576422\n",
      "Training Loss: 0.0072638344776351\n",
      "Validation Loss: 0.004915403573658694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.007132542979670689\n",
      "Training Loss: 0.007092617200687528\n",
      "Training Loss: 0.0072588913689833135\n",
      "Validation Loss: 0.004910004887667098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.007127124693943188\n",
      "Training Loss: 0.007087476400192827\n",
      "Training Loss: 0.007254113546805456\n",
      "Validation Loss: 0.004904825750508168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.007121904692030512\n",
      "Training Loss: 0.007082518558017909\n",
      "Training Loss: 0.007249489553505555\n",
      "Validation Loss: 0.0048998492741120165\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.007116869546589442\n",
      "Training Loss: 0.007077728761360049\n",
      "Training Loss: 0.007245008666068316\n",
      "Validation Loss: 0.004895055085797323\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.007112008582917042\n",
      "Training Loss: 0.007073096723761409\n",
      "Training Loss: 0.00724065997521393\n",
      "Validation Loss: 0.004890433606806766\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.007107309669954702\n",
      "Training Loss: 0.007068607992259786\n",
      "Training Loss: 0.007236434846417978\n",
      "Validation Loss: 0.004885974091613728\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.00710276258236263\n",
      "Training Loss: 0.007064254665747285\n",
      "Training Loss: 0.00723232347285375\n",
      "Validation Loss: 0.004881653616864109\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.0070983554847771305\n",
      "Training Loss: 0.00706002477905713\n",
      "Training Loss: 0.007228316337568685\n",
      "Validation Loss: 0.004877470135479496\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.007094080747338012\n",
      "Training Loss: 0.007055909186601639\n",
      "Training Loss: 0.007224407636094839\n",
      "Validation Loss: 0.004873408280637408\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.007089928035857156\n",
      "Training Loss: 0.007051899007055908\n",
      "Training Loss: 0.007220588369527832\n",
      "Validation Loss: 0.004869457713610838\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.007085889083100483\n",
      "Training Loss: 0.007047987180994824\n",
      "Training Loss: 0.007216854379512369\n",
      "Validation Loss: 0.004865616170187177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.007081956365145743\n",
      "Training Loss: 0.007044165276456624\n",
      "Training Loss: 0.007213195908116176\n",
      "Validation Loss: 0.004861868359410109\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00707812204840593\n",
      "Training Loss: 0.007040427083848044\n",
      "Training Loss: 0.007209609494311735\n",
      "Validation Loss: 0.004858208134812251\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.007074380344711244\n",
      "Training Loss: 0.007036765832453966\n",
      "Training Loss: 0.007206090154359118\n",
      "Validation Loss: 0.0048546338757353555\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.007070724024670199\n",
      "Training Loss: 0.007033177074044943\n",
      "Training Loss: 0.007202632968546823\n",
      "Validation Loss: 0.004851131346809228\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.007067148859496228\n",
      "Training Loss: 0.0070296551613137125\n",
      "Training Loss: 0.0071992346819024534\n",
      "Validation Loss: 0.004847703812895029\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.007063648293260485\n",
      "Training Loss: 0.007026195633225143\n",
      "Training Loss: 0.0071958913351409136\n",
      "Validation Loss: 0.004844340472220538\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0070602190360659735\n",
      "Training Loss: 0.007022794353542849\n",
      "Training Loss: 0.007192598461406305\n",
      "Validation Loss: 0.0048410374490711625\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.007056855470291339\n",
      "Training Loss: 0.007019448556238785\n",
      "Training Loss: 0.007189354891888797\n",
      "Validation Loss: 0.004837794339263372\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.007053553886362351\n",
      "Training Loss: 0.007016154732555151\n",
      "Training Loss: 0.007186158314580098\n",
      "Validation Loss: 0.0048346114030965925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.007050312120118179\n",
      "Training Loss: 0.007012910131597891\n",
      "Training Loss: 0.007183004497783259\n",
      "Validation Loss: 0.0048314759918499026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.007047125897370279\n",
      "Training Loss: 0.007009711866267025\n",
      "Training Loss: 0.007179892654530704\n",
      "Validation Loss: 0.004828389884454062\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.007043992672115564\n",
      "Training Loss: 0.007006556435953826\n",
      "Training Loss: 0.007176820823224262\n",
      "Validation Loss: 0.004825351653840351\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0070409099489916116\n",
      "Training Loss: 0.007003444795263931\n",
      "Training Loss: 0.007173787984065712\n",
      "Validation Loss: 0.004822356290225819\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.0070378733385587115\n",
      "Training Loss: 0.007000373270129785\n",
      "Training Loss: 0.007170791644603014\n",
      "Validation Loss: 0.004819404630754353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.00703488300612662\n",
      "Training Loss: 0.006997339262161404\n",
      "Training Loss: 0.007167830140097067\n",
      "Validation Loss: 0.0048164959991706556\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.007031936521525495\n",
      "Training Loss: 0.006994343260303139\n",
      "Training Loss: 0.007164903370430693\n",
      "Validation Loss: 0.004813624353650329\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.007029030859703198\n",
      "Training Loss: 0.006991383223794401\n",
      "Training Loss: 0.007162009242456406\n",
      "Validation Loss: 0.004810793698095622\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.007026163979317062\n",
      "Training Loss: 0.00698845817707479\n",
      "Training Loss: 0.00715914795640856\n",
      "Validation Loss: 0.0048079983713209965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.0070233358524274085\n",
      "Training Loss: 0.006985567059600726\n",
      "Training Loss: 0.0071563175600022075\n",
      "Validation Loss: 0.004805238187585151\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.007020544983097352\n",
      "Training Loss: 0.006982707458082587\n",
      "Training Loss: 0.007153517460683361\n",
      "Validation Loss: 0.004802513264611447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.007017788738012314\n",
      "Training Loss: 0.006979880137369037\n",
      "Training Loss: 0.007150747567648068\n",
      "Validation Loss: 0.004799818982412055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.007015066608437337\n",
      "Training Loss: 0.006977083237143233\n",
      "Training Loss: 0.007148006232455373\n",
      "Validation Loss: 0.004797159687595002\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.007012378157814965\n",
      "Training Loss: 0.0069743177515920254\n",
      "Training Loss: 0.007145292728673667\n",
      "Validation Loss: 0.004794531712423633\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.0070097199315205215\n",
      "Training Loss: 0.006971581582911312\n",
      "Training Loss: 0.007142607140121981\n",
      "Validation Loss: 0.004791937481999146\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.007007093197898939\n",
      "Training Loss: 0.006968874213052914\n",
      "Training Loss: 0.007139948171097785\n",
      "Validation Loss: 0.004789368381587725\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.007004496249137447\n",
      "Training Loss: 0.006966194855049252\n",
      "Training Loss: 0.007137316167354584\n",
      "Validation Loss: 0.0047868276551219354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0070019288803450765\n",
      "Training Loss: 0.006963542475132272\n",
      "Training Loss: 0.007134709750534967\n",
      "Validation Loss: 0.004784315200574863\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006999388898257166\n",
      "Training Loss: 0.0069609178532846275\n",
      "Training Loss: 0.007132129457313567\n",
      "Validation Loss: 0.004781831632724053\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006996876352350228\n",
      "Training Loss: 0.006958319909172133\n",
      "Training Loss: 0.00712957238429226\n",
      "Validation Loss: 0.004779376582702978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006994390408508479\n",
      "Training Loss: 0.006955748159671202\n",
      "Training Loss: 0.007127040984341875\n",
      "Validation Loss: 0.004776943478932123\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006991930306539871\n",
      "Training Loss: 0.006953202084405348\n",
      "Training Loss: 0.007124533914029598\n",
      "Validation Loss: 0.004774539782456384\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006989495570887811\n",
      "Training Loss: 0.006950680868467316\n",
      "Training Loss: 0.007122049445752054\n",
      "Validation Loss: 0.004772159123133993\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.0069870853197062386\n",
      "Training Loss: 0.006948184567736462\n",
      "Training Loss: 0.007119588798377663\n",
      "Validation Loss: 0.0047698048037591945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006984698384767399\n",
      "Training Loss: 0.006945714021567255\n",
      "Training Loss: 0.007117150208214298\n",
      "Validation Loss: 0.004767473237693645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006982335438369774\n",
      "Training Loss: 0.006943266921443865\n",
      "Training Loss: 0.007114734201459214\n",
      "Validation Loss: 0.00476516514043376\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.00697999531577807\n",
      "Training Loss: 0.006940843077609315\n",
      "Training Loss: 0.007112341126194224\n",
      "Validation Loss: 0.0047628837572201415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006977678383118473\n",
      "Training Loss: 0.006938442706596107\n",
      "Training Loss: 0.007109969810117036\n",
      "Validation Loss: 0.004760623013527457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006975381934316829\n",
      "Training Loss: 0.0069360669620800765\n",
      "Training Loss: 0.007107619156595319\n",
      "Validation Loss: 0.004758383131722051\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006973107886733487\n",
      "Training Loss: 0.006933712501777336\n",
      "Training Loss: 0.007105288577731699\n",
      "Validation Loss: 0.0047561672615578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.00697085480671376\n",
      "Training Loss: 0.006931380894966424\n",
      "Training Loss: 0.0071029795007780195\n",
      "Validation Loss: 0.004753972762107347\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006968622013810091\n",
      "Training Loss: 0.006929072298808023\n",
      "Training Loss: 0.007100691082887352\n",
      "Validation Loss: 0.004751796381589905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006966409785090946\n",
      "Training Loss: 0.006926784194074571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [04:55<19:43, 147.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.007098422080744058\n",
      "Validation Loss: 0.004749640777937315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.08001736253499984\n",
      "Training Loss: 0.06915318988263607\n",
      "Training Loss: 0.06256739672273398\n",
      "Validation Loss: 0.0583932651311494\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.05675311826169491\n",
      "Training Loss: 0.05270977344363928\n",
      "Training Loss: 0.04900620253756642\n",
      "Validation Loss: 0.04670684087728517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.04512997002340853\n",
      "Training Loss: 0.041393864210695026\n",
      "Training Loss: 0.03829712161794305\n",
      "Validation Loss: 0.0361556591141676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.034669252978637814\n",
      "Training Loss: 0.031303512370213865\n",
      "Training Loss: 0.02883251248858869\n",
      "Validation Loss: 0.026610768049578654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.02545493799261749\n",
      "Training Loss: 0.02307182743679732\n",
      "Training Loss: 0.021702670152299108\n",
      "Validation Loss: 0.01968591846525669\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.0193282591085881\n",
      "Training Loss: 0.018383713383227588\n",
      "Training Loss: 0.017985931895673275\n",
      "Validation Loss: 0.016087160482374806\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.01623563835164532\n",
      "Training Loss: 0.01589609095128253\n",
      "Training Loss: 0.015796195128932596\n",
      "Validation Loss: 0.013833676912204435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.014246714154724031\n",
      "Training Loss: 0.01415045247413218\n",
      "Training Loss: 0.014168883627280592\n",
      "Validation Loss: 0.012151842963034183\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.012793784646783024\n",
      "Training Loss: 0.012833667777013034\n",
      "Training Loss: 0.01291810397291556\n",
      "Validation Loss: 0.010857004354435824\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.01172513050492853\n",
      "Training Loss: 0.011841747178696096\n",
      "Training Loss: 0.011967355923261494\n",
      "Validation Loss: 0.009864555006020105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.010951826781965792\n",
      "Training Loss: 0.011104553109034895\n",
      "Training Loss: 0.011255499328253791\n",
      "Validation Loss: 0.009110507374796807\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.01040070625487715\n",
      "Training Loss: 0.010562811567215249\n",
      "Training Loss: 0.010727707889163867\n",
      "Validation Loss: 0.008540253191558498\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.01001043577445671\n",
      "Training Loss: 0.010166206760331989\n",
      "Training Loss: 0.010336839689407497\n",
      "Validation Loss: 0.008108442428186955\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.009732325013028459\n",
      "Training Loss: 0.009873898579971865\n",
      "Training Loss: 0.010044620427070185\n",
      "Validation Loss: 0.007779031899087968\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.00952984524425119\n",
      "Training Loss: 0.009654281114926562\n",
      "Training Loss: 0.00982157161924988\n",
      "Validation Loss: 0.007524362035367763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.009377050854964182\n",
      "Training Loss: 0.009484148072078823\n",
      "Training Loss: 0.00964624947286211\n",
      "Validation Loss: 0.007323963287694568\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.009256601223023608\n",
      "Training Loss: 0.0093474277574569\n",
      "Training Loss: 0.009503949638456107\n",
      "Validation Loss: 0.007163149878916362\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.009157589540118352\n",
      "Training Loss: 0.009233606124762445\n",
      "Training Loss: 0.009385108158458024\n",
      "Validation Loss: 0.007031656091436325\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.009073491427116095\n",
      "Training Loss: 0.009136089290259407\n",
      "Training Loss: 0.00928368583205156\n",
      "Validation Loss: 0.006922368381402633\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.00900046963361092\n",
      "Training Loss: 0.009050803185673431\n",
      "Training Loss: 0.009195849981624633\n",
      "Validation Loss: 0.006830329134317345\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008936205595964565\n",
      "Training Loss: 0.008975172335049138\n",
      "Training Loss: 0.009119055196642877\n",
      "Validation Loss: 0.006752008296224927\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.00887917855521664\n",
      "Training Loss: 0.008907486269017682\n",
      "Training Loss: 0.009051478538895026\n",
      "Validation Loss: 0.006684799345074158\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008828281620517374\n",
      "Training Loss: 0.008846520821098238\n",
      "Training Loss: 0.00899171425611712\n",
      "Validation Loss: 0.006626716612189422\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008782637546537443\n",
      "Training Loss: 0.008791342838667332\n",
      "Training Loss: 0.00893861317075789\n",
      "Validation Loss: 0.006576208805853731\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.008741514983121305\n",
      "Training Loss: 0.008741208874853327\n",
      "Training Loss: 0.008891207205597312\n",
      "Validation Loss: 0.0065320306849990335\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008704287420259789\n",
      "Training Loss: 0.008695499936584384\n",
      "Training Loss: 0.008848668043501675\n",
      "Validation Loss: 0.006493156962881513\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008670408526668325\n",
      "Training Loss: 0.0086536909872666\n",
      "Training Loss: 0.008810277939774095\n",
      "Validation Loss: 0.006458739765783709\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008639399834210052\n",
      "Training Loss: 0.008615323598496616\n",
      "Training Loss: 0.008775416468270123\n",
      "Validation Loss: 0.006428057017861709\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.00861083950381726\n",
      "Training Loss: 0.00857999716536142\n",
      "Training Loss: 0.00874354831292294\n",
      "Validation Loss: 0.006400502725043909\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008584358420921489\n",
      "Training Loss: 0.00854735374217853\n",
      "Training Loss: 0.00871421217219904\n",
      "Validation Loss: 0.0063755661667732715\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.00855963138048537\n",
      "Training Loss: 0.008517080867895856\n",
      "Training Loss: 0.008687015416799114\n",
      "Validation Loss: 0.00635281640753736\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008536376026459038\n",
      "Training Loss: 0.008488902000244707\n",
      "Training Loss: 0.008661624806700274\n",
      "Validation Loss: 0.006331886837175221\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008514348051976412\n",
      "Training Loss: 0.008462571330601349\n",
      "Training Loss: 0.008637758670374751\n",
      "Validation Loss: 0.006312475611685953\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008493333446094766\n",
      "Training Loss: 0.008437875588424504\n",
      "Training Loss: 0.008615181429777295\n",
      "Validation Loss: 0.0062943286592078\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008473149782512337\n",
      "Training Loss: 0.008414624545257538\n",
      "Training Loss: 0.008593696616590023\n",
      "Validation Loss: 0.006277230947560976\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008453640283551066\n",
      "Training Loss: 0.008392652419861407\n",
      "Training Loss: 0.008573139160871506\n",
      "Validation Loss: 0.006261007263837905\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008434667026158423\n",
      "Training Loss: 0.008371807847870514\n",
      "Training Loss: 0.008553374309558421\n",
      "Validation Loss: 0.006245501827333499\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008416120987385511\n",
      "Training Loss: 0.008351958760758862\n",
      "Training Loss: 0.00853429373120889\n",
      "Validation Loss: 0.006230583274475393\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008397915340028704\n",
      "Training Loss: 0.008332985632587225\n",
      "Training Loss: 0.00851581257302314\n",
      "Validation Loss: 0.006216148567239471\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.00837999019655399\n",
      "Training Loss: 0.008314779269276186\n",
      "Training Loss: 0.0084978692245204\n",
      "Validation Loss: 0.006202107401344967\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008362312813987955\n",
      "Training Loss: 0.008297242588596418\n",
      "Training Loss: 0.008480419447878376\n",
      "Validation Loss: 0.006188392818361269\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008344877091003582\n",
      "Training Loss: 0.008280289040412754\n",
      "Training Loss: 0.008463429474504664\n",
      "Validation Loss: 0.006174950526891213\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.008327688070712611\n",
      "Training Loss: 0.008263836033875123\n",
      "Training Loss: 0.008446869364706799\n",
      "Validation Loss: 0.006161735482053475\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.008310760345775634\n",
      "Training Loss: 0.008247808560263366\n",
      "Training Loss: 0.008430706306826323\n",
      "Validation Loss: 0.006148713711502679\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.008294103912776336\n",
      "Training Loss: 0.008232140800682828\n",
      "Training Loss: 0.008414903820957989\n",
      "Validation Loss: 0.006135856435801624\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.008277719137258827\n",
      "Training Loss: 0.008216773432213813\n",
      "Training Loss: 0.008399419081397354\n",
      "Validation Loss: 0.006123136990609464\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.00826159556163475\n",
      "Training Loss: 0.008201653301948681\n",
      "Training Loss: 0.008384208290372043\n",
      "Validation Loss: 0.006110526983668044\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.008245710815535857\n",
      "Training Loss: 0.008186735124327242\n",
      "Training Loss: 0.008369223319459707\n",
      "Validation Loss: 0.006097997524356993\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.008230038746260106\n",
      "Training Loss: 0.008171980513725429\n",
      "Training Loss: 0.008354420571122319\n",
      "Validation Loss: 0.0060855205068783315\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.00821454480290413\n",
      "Training Loss: 0.008157355720177293\n",
      "Training Loss: 0.008339759683003649\n",
      "Validation Loss: 0.006073072748886568\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.008199199208756908\n",
      "Training Loss: 0.008142829559510574\n",
      "Training Loss: 0.008325204056454823\n",
      "Validation Loss: 0.006060632707006978\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.008183967451332137\n",
      "Training Loss: 0.008128380075795576\n",
      "Training Loss: 0.00831072208005935\n",
      "Validation Loss: 0.006048176972603614\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.008168820586288348\n",
      "Training Loss: 0.008113984839292243\n",
      "Training Loss: 0.008296286418335512\n",
      "Validation Loss: 0.006035689838764289\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.008153732098871842\n",
      "Training Loss: 0.008099625604227185\n",
      "Training Loss: 0.008281873365631328\n",
      "Validation Loss: 0.006023153732007558\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.008138679661788046\n",
      "Training Loss: 0.008085285606794059\n",
      "Training Loss: 0.008267463891534134\n",
      "Validation Loss: 0.006010559392737203\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.008123642039718106\n",
      "Training Loss: 0.0080709529586602\n",
      "Training Loss: 0.008253042246215045\n",
      "Validation Loss: 0.005997896229775016\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.008108604259323329\n",
      "Training Loss: 0.008056616047397255\n",
      "Training Loss: 0.008238594240974634\n",
      "Validation Loss: 0.005985158244461826\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.008093551996862515\n",
      "Training Loss: 0.00804226884036325\n",
      "Training Loss: 0.008224109916482121\n",
      "Validation Loss: 0.005972340266126093\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.008078476600348949\n",
      "Training Loss: 0.008027902097674088\n",
      "Training Loss: 0.00820958084659651\n",
      "Validation Loss: 0.0059594394680991605\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.008063367868307978\n",
      "Training Loss: 0.008013511653989553\n",
      "Training Loss: 0.008195000594714657\n",
      "Validation Loss: 0.005946458225776808\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.008048221316421404\n",
      "Training Loss: 0.007999094661790877\n",
      "Training Loss: 0.008180364096770063\n",
      "Validation Loss: 0.0059333941232140985\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.00803303207270801\n",
      "Training Loss: 0.007984646757831797\n",
      "Training Loss: 0.008165667779976502\n",
      "Validation Loss: 0.00592024882292647\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.008017797027714551\n",
      "Training Loss: 0.007970166897866875\n",
      "Training Loss: 0.008150907753733918\n",
      "Validation Loss: 0.005907025321627434\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.008002514317631722\n",
      "Training Loss: 0.007955654815305024\n",
      "Training Loss: 0.008136083586141468\n",
      "Validation Loss: 0.005893729658525312\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007987183517543598\n",
      "Training Loss: 0.007941109382081778\n",
      "Training Loss: 0.008121195046696813\n",
      "Validation Loss: 0.005880360770185761\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007971805181587115\n",
      "Training Loss: 0.007926530262921005\n",
      "Training Loss: 0.008106241296045482\n",
      "Validation Loss: 0.005866925795876494\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007956378964008764\n",
      "Training Loss: 0.007911918081808834\n",
      "Training Loss: 0.008091221546055748\n",
      "Validation Loss: 0.005853428221516981\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.00794090457377024\n",
      "Training Loss: 0.007897273700218648\n",
      "Training Loss: 0.008076137219322845\n",
      "Validation Loss: 0.005839872181813201\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007925382545217872\n",
      "Training Loss: 0.00788259609369561\n",
      "Training Loss: 0.008060988883953542\n",
      "Validation Loss: 0.005826259319660034\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007909815334714949\n",
      "Training Loss: 0.007867886458989233\n",
      "Training Loss: 0.008045777091756462\n",
      "Validation Loss: 0.0058125942628935315\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007894201593007893\n",
      "Training Loss: 0.007853144782129675\n",
      "Training Loss: 0.008030502372421325\n",
      "Validation Loss: 0.005798877618443011\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.00787854302325286\n",
      "Training Loss: 0.007838369832606986\n",
      "Training Loss: 0.00801516679348424\n",
      "Validation Loss: 0.005785111931749107\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007862838980508968\n",
      "Training Loss: 0.007823562305420638\n",
      "Training Loss: 0.007999770622700453\n",
      "Validation Loss: 0.005771300849619876\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007847090093418956\n",
      "Training Loss: 0.007808722276240587\n",
      "Training Loss: 0.007984314365312457\n",
      "Validation Loss: 0.005757445515279941\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007831296409713105\n",
      "Training Loss: 0.007793847632128746\n",
      "Training Loss: 0.00796879898989573\n",
      "Validation Loss: 0.005743546145863496\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007815456101670861\n",
      "Training Loss: 0.007778937984257936\n",
      "Training Loss: 0.007953225611709058\n",
      "Validation Loss: 0.005729601981400857\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00779957028455101\n",
      "Training Loss: 0.007763991948449984\n",
      "Training Loss: 0.007937594837276265\n",
      "Validation Loss: 0.005715609770111238\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007783635749947279\n",
      "Training Loss: 0.007749007267411798\n",
      "Training Loss: 0.007921907007694245\n",
      "Validation Loss: 0.00570157373303108\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007767652116017416\n",
      "Training Loss: 0.007733981035416945\n",
      "Training Loss: 0.00790616265963763\n",
      "Validation Loss: 0.0056874873430541395\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007751617904286832\n",
      "Training Loss: 0.007718911685515195\n",
      "Training Loss: 0.007890360162127763\n",
      "Validation Loss: 0.005673352264003891\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007735528976190835\n",
      "Training Loss: 0.007703796128043905\n",
      "Training Loss: 0.007874500002944842\n",
      "Validation Loss: 0.005659161786956901\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007719384434167296\n",
      "Training Loss: 0.00768862973432988\n",
      "Training Loss: 0.00785858344985172\n",
      "Validation Loss: 0.005644912971445265\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007703180955722928\n",
      "Training Loss: 0.00767340978840366\n",
      "Training Loss: 0.007842607509810478\n",
      "Validation Loss: 0.0056306034303008675\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.00768691346864216\n",
      "Training Loss: 0.007658129684859887\n",
      "Training Loss: 0.00782657204545103\n",
      "Validation Loss: 0.005616228062178145\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.0076705778599716725\n",
      "Training Loss: 0.007642785265343264\n",
      "Training Loss: 0.007810475038131699\n",
      "Validation Loss: 0.0056017772346902426\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.00765417089802213\n",
      "Training Loss: 0.0076273702981416135\n",
      "Training Loss: 0.007794314714847133\n",
      "Validation Loss: 0.0055872507097743706\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007637686212547123\n",
      "Training Loss: 0.007611878850730136\n",
      "Training Loss: 0.007778089423663914\n",
      "Validation Loss: 0.005572638091149876\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.0076211195962969215\n",
      "Training Loss: 0.007596303864847869\n",
      "Training Loss: 0.007761795078404248\n",
      "Validation Loss: 0.005557934342873063\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.0076044632156845185\n",
      "Training Loss: 0.007580638154176995\n",
      "Training Loss: 0.007745431028306484\n",
      "Validation Loss: 0.005543134109839127\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007587711811065673\n",
      "Training Loss: 0.007564873553346843\n",
      "Training Loss: 0.007728991850744933\n",
      "Validation Loss: 0.0055282256903461694\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007570857919054106\n",
      "Training Loss: 0.00754900217638351\n",
      "Training Loss: 0.007712473782012239\n",
      "Validation Loss: 0.005513202820203445\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007553894885350018\n",
      "Training Loss: 0.007533016313100233\n",
      "Training Loss: 0.007695874205091968\n",
      "Validation Loss: 0.005498058777407147\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007536816201172769\n",
      "Training Loss: 0.007516905935481191\n",
      "Training Loss: 0.007679189114132896\n",
      "Validation Loss: 0.005482782353385446\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007519614028278738\n",
      "Training Loss: 0.007500663986429572\n",
      "Training Loss: 0.0076624144648667425\n",
      "Validation Loss: 0.005467366064856812\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0075022818823345\n",
      "Training Loss: 0.007484282298246398\n",
      "Training Loss: 0.007645546587882563\n",
      "Validation Loss: 0.005451802548879151\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007484813040355221\n",
      "Training Loss: 0.007467752061784267\n",
      "Training Loss: 0.007628581537865102\n",
      "Validation Loss: 0.005436081767574036\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007467203291598707\n",
      "Training Loss: 0.007451069207163528\n",
      "Training Loss: 0.00761151723912917\n",
      "Validation Loss: 0.005420200716379737\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007449444667436182\n",
      "Training Loss: 0.00743422448518686\n",
      "Training Loss: 0.007594349867431447\n",
      "Validation Loss: 0.005404147320803715\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007431534610223025\n",
      "Training Loss: 0.007417213376611471\n",
      "Training Loss: 0.007577078669564799\n",
      "Validation Loss: 0.005387917540318678\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007413469809107482\n",
      "Training Loss: 0.007400032882578671\n",
      "Training Loss: 0.0075597024406306446\n",
      "Validation Loss: 0.00537150645539625\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0073952500254381446\n",
      "Training Loss: 0.007382680574664846\n",
      "Training Loss: 0.0075422229198738935\n",
      "Validation Loss: 0.005354909330941318\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007376874625915662\n",
      "Training Loss: 0.007365155589068309\n",
      "Training Loss: 0.0075246411294210705\n",
      "Validation Loss: 0.005338125138836546\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007358344536041841\n",
      "Training Loss: 0.007347459250595421\n",
      "Training Loss: 0.007506960612954572\n",
      "Validation Loss: 0.005321151847699971\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007339666510233656\n",
      "Training Loss: 0.007329595491755754\n",
      "Training Loss: 0.007489186630118638\n",
      "Validation Loss: 0.005303991447056361\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007320847867522389\n",
      "Training Loss: 0.0073115709982812405\n",
      "Training Loss: 0.0074713275115936995\n",
      "Validation Loss: 0.00528664575376956\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007301896276185289\n",
      "Training Loss: 0.007293395545566454\n",
      "Training Loss: 0.0074533910676836965\n",
      "Validation Loss: 0.005269120489194822\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007282825780566782\n",
      "Training Loss: 0.007275080892723054\n",
      "Training Loss: 0.007435391804901883\n",
      "Validation Loss: 0.005251425013647237\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.007263653635745868\n",
      "Training Loss: 0.00725664462079294\n",
      "Training Loss: 0.007417343781562522\n",
      "Validation Loss: 0.00523357070835956\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.0072443999547977\n",
      "Training Loss: 0.007238104359712452\n",
      "Training Loss: 0.007399264082778245\n",
      "Validation Loss: 0.005215570991178661\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.007225087361875921\n",
      "Training Loss: 0.007219486446119845\n",
      "Training Loss: 0.007381172950845212\n",
      "Validation Loss: 0.005197441597793544\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.007205744314705953\n",
      "Training Loss: 0.007200813574017957\n",
      "Training Loss: 0.007363093132153153\n",
      "Validation Loss: 0.005179204810620024\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0071864001266658305\n",
      "Training Loss: 0.007182119470089674\n",
      "Training Loss: 0.007345051113516092\n",
      "Validation Loss: 0.005160886604663278\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007167090501170605\n",
      "Training Loss: 0.0071634373185224835\n",
      "Training Loss: 0.007327073606429622\n",
      "Validation Loss: 0.005142508331016543\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007147852296475321\n",
      "Training Loss: 0.007144803697010502\n",
      "Training Loss: 0.007309190660016611\n",
      "Validation Loss: 0.005124101877013703\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007128726735245437\n",
      "Training Loss: 0.0071262590202968564\n",
      "Training Loss: 0.007291434970684349\n",
      "Validation Loss: 0.005105699093363593\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007109753405675292\n",
      "Training Loss: 0.007107842331752181\n",
      "Training Loss: 0.007273837367538363\n",
      "Validation Loss: 0.005087334599890066\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.007090976508334279\n",
      "Training Loss: 0.007089596955338493\n",
      "Training Loss: 0.007256431025452912\n",
      "Validation Loss: 0.005069042265603549\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.007072438737377524\n",
      "Training Loss: 0.007071565337246284\n",
      "Training Loss: 0.007239249135600403\n",
      "Validation Loss: 0.005050859454590199\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.00705418435158208\n",
      "Training Loss: 0.007053787891054526\n",
      "Training Loss: 0.00722232369473204\n",
      "Validation Loss: 0.005032821075750117\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.007036251815734431\n",
      "Training Loss: 0.007036306823138148\n",
      "Training Loss: 0.007205682614585385\n",
      "Validation Loss: 0.005014966056274062\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.007018680601613596\n",
      "Training Loss: 0.007019157870672643\n",
      "Training Loss: 0.007189355695154518\n",
      "Validation Loss: 0.0049973266667508506\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.0070015053916722535\n",
      "Training Loss: 0.007002376515883952\n",
      "Training Loss: 0.007173364346381277\n",
      "Validation Loss: 0.004979938186255147\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0069847550767008215\n",
      "Training Loss: 0.0069859909551450985\n",
      "Training Loss: 0.007157731681363657\n",
      "Validation Loss: 0.004962825375933493\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.0069684555032290514\n",
      "Training Loss: 0.006970027762581594\n",
      "Training Loss: 0.007142474635038525\n",
      "Validation Loss: 0.0049460217686307225\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006952628786675632\n",
      "Training Loss: 0.006954505866160616\n",
      "Training Loss: 0.007127605279674753\n",
      "Validation Loss: 0.004929543848233193\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006937287275213748\n",
      "Training Loss: 0.006939441302092746\n",
      "Training Loss: 0.007113134315004572\n",
      "Validation Loss: 0.004913413447191876\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0069224416487850245\n",
      "Training Loss: 0.00692484402039554\n",
      "Training Loss: 0.0070990658341906964\n",
      "Validation Loss: 0.004897644895055656\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006908094668760896\n",
      "Training Loss: 0.006910720425657928\n",
      "Training Loss: 0.00708540140883997\n",
      "Validation Loss: 0.004882251826805596\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.0068942486180458215\n",
      "Training Loss: 0.006897069795522839\n",
      "Training Loss: 0.00707214237540029\n",
      "Validation Loss: 0.004867240471315434\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006880897441878915\n",
      "Training Loss: 0.006883889709715731\n",
      "Training Loss: 0.007059283156413585\n",
      "Validation Loss: 0.004852612683382095\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006868033714126796\n",
      "Training Loss: 0.006871173221734353\n",
      "Training Loss: 0.007046816213987768\n",
      "Validation Loss: 0.004838374052249062\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006855646049370989\n",
      "Training Loss: 0.006858911823946982\n",
      "Training Loss: 0.007034733475884423\n",
      "Validation Loss: 0.004824517564220207\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006843720260076225\n",
      "Training Loss: 0.006847090341034345\n",
      "Training Loss: 0.007023024293594062\n",
      "Validation Loss: 0.004811041529311307\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006832242746604607\n",
      "Training Loss: 0.006835695337504149\n",
      "Training Loss: 0.007011678797425702\n",
      "Validation Loss: 0.004797939616621712\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006821193398209289\n",
      "Training Loss: 0.0068247109698131685\n",
      "Training Loss: 0.0070006821199785916\n",
      "Validation Loss: 0.00478520374902095\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006810554589610547\n",
      "Training Loss: 0.00681412001955323\n",
      "Training Loss: 0.006990022134268656\n",
      "Validation Loss: 0.004772822426244868\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006800309254322201\n",
      "Training Loss: 0.006803904611151665\n",
      "Training Loss: 0.006979684360558167\n",
      "Validation Loss: 0.004760785384124585\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006790438058087603\n",
      "Training Loss: 0.006794047601288184\n",
      "Training Loss: 0.0069696540955919775\n",
      "Validation Loss: 0.004749084847686247\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006780919709708542\n",
      "Training Loss: 0.006784529708675109\n",
      "Training Loss: 0.006959919754881412\n",
      "Validation Loss: 0.004737705327151866\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006771736732916906\n",
      "Training Loss: 0.0067753320094197985\n",
      "Training Loss: 0.006950466291746124\n",
      "Validation Loss: 0.004726635336645701\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006762871723622083\n",
      "Training Loss: 0.00676644041785039\n",
      "Training Loss: 0.006941281409235671\n",
      "Validation Loss: 0.004715866570784762\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006754305498907343\n",
      "Training Loss: 0.0067578353849239645\n",
      "Training Loss: 0.0069323515240103\n",
      "Validation Loss: 0.004705380287356256\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006746023326413706\n",
      "Training Loss: 0.006749499982688576\n",
      "Training Loss: 0.006923663894413039\n",
      "Validation Loss: 0.004695167699619458\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00673800574732013\n",
      "Training Loss: 0.006741420496837236\n",
      "Training Loss: 0.0069152076984755695\n",
      "Validation Loss: 0.004685218843600053\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006730239902390167\n",
      "Training Loss: 0.006733582079177723\n",
      "Training Loss: 0.0069069698522798715\n",
      "Validation Loss: 0.00467552066508555\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006722711340989918\n",
      "Training Loss: 0.006725969766848721\n",
      "Training Loss: 0.0068989419750869274\n",
      "Validation Loss: 0.00466606321121995\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006715404258575291\n",
      "Training Loss: 0.006718571339151822\n",
      "Training Loss: 0.0068911127909086645\n",
      "Validation Loss: 0.004656836778328367\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006708306634100154\n",
      "Training Loss: 0.0067113744287053126\n",
      "Training Loss: 0.006883472816552967\n",
      "Validation Loss: 0.004647828443002039\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006701407183427364\n",
      "Training Loss: 0.006704367797356099\n",
      "Training Loss: 0.00687601154553704\n",
      "Validation Loss: 0.004639029933212932\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006694695941405371\n",
      "Training Loss: 0.006697540679597296\n",
      "Training Loss: 0.006868720615748316\n",
      "Validation Loss: 0.0046304307859704906\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006688158746110276\n",
      "Training Loss: 0.006690881075337529\n",
      "Training Loss: 0.006861594972433522\n",
      "Validation Loss: 0.004622024538916316\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0066817887860815975\n",
      "Training Loss: 0.006684382029343397\n",
      "Training Loss: 0.006854622666724026\n",
      "Validation Loss: 0.004613802132583903\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006675574728287757\n",
      "Training Loss: 0.0066780339472461495\n",
      "Training Loss: 0.006847799968672916\n",
      "Validation Loss: 0.004605753952898922\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006669510382926092\n",
      "Training Loss: 0.006671829372644425\n",
      "Training Loss: 0.006841119027230888\n",
      "Validation Loss: 0.004597877020152349\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006663584561320022\n",
      "Training Loss: 0.006665760506875813\n",
      "Training Loss: 0.006834573050728068\n",
      "Validation Loss: 0.004590160889666151\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006657793138874695\n",
      "Training Loss: 0.006659820156055502\n",
      "Training Loss: 0.006828158898279071\n",
      "Validation Loss: 0.004582597819167409\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.0066521259944420305\n",
      "Training Loss: 0.00665400218043942\n",
      "Training Loss: 0.006821865517413244\n",
      "Validation Loss: 0.004575183084679328\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006646578967338428\n",
      "Training Loss: 0.00664829871035181\n",
      "Training Loss: 0.006815692110685632\n",
      "Validation Loss: 0.004567910836004977\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006641144923632964\n",
      "Training Loss: 0.006642708530416712\n",
      "Training Loss: 0.006809633476659655\n",
      "Validation Loss: 0.0045607752981595695\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006635820699157194\n",
      "Training Loss: 0.006637220676639117\n",
      "Training Loss: 0.006803684681653976\n",
      "Validation Loss: 0.004553770786412897\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.00663059810991399\n",
      "Training Loss: 0.006631835147854872\n",
      "Training Loss: 0.006797839825740084\n",
      "Validation Loss: 0.0045468919083810925\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006625472175655887\n",
      "Training Loss: 0.00662654566927813\n",
      "Training Loss: 0.006792097246507183\n",
      "Validation Loss: 0.004540135720326157\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0066204405052121724\n",
      "Training Loss: 0.006621347401523963\n",
      "Training Loss: 0.006786450697109103\n",
      "Validation Loss: 0.004533493972456689\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.0066154957807157185\n",
      "Training Loss: 0.006616237576818094\n",
      "Training Loss: 0.006780898165889084\n",
      "Validation Loss: 0.004526967010094544\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0066106376913376156\n",
      "Training Loss: 0.006611210000701248\n",
      "Training Loss: 0.0067754349764436485\n",
      "Validation Loss: 0.0045205469071875545\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006605858375551179\n",
      "Training Loss: 0.006606265510199592\n",
      "Training Loss: 0.006770059670088813\n",
      "Validation Loss: 0.004514230420457178\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006601157484110445\n",
      "Training Loss: 0.006601396418409422\n",
      "Training Loss: 0.0067647665773984044\n",
      "Validation Loss: 0.00450801433605571\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006596530536189676\n",
      "Training Loss: 0.006596603405778296\n",
      "Training Loss: 0.00675955657963641\n",
      "Validation Loss: 0.004501896648761931\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006591973792528734\n",
      "Training Loss: 0.006591881123604253\n",
      "Training Loss: 0.006754423419479281\n",
      "Validation Loss: 0.004495872446895692\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006587486445205286\n",
      "Training Loss: 0.006587226445553825\n",
      "Training Loss: 0.006749364691786468\n",
      "Validation Loss: 0.00448993789921567\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006583062574500218\n",
      "Training Loss: 0.006582638709805906\n",
      "Training Loss: 0.006744380636373535\n",
      "Validation Loss: 0.004484091390949789\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006578701052349061\n",
      "Training Loss: 0.006578115141019225\n",
      "Training Loss: 0.006739465390564874\n",
      "Validation Loss: 0.0044783282909919035\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006574401038233191\n",
      "Training Loss: 0.006573653312516399\n",
      "Training Loss: 0.006734620995121077\n",
      "Validation Loss: 0.004472645872084194\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006570157710229978\n",
      "Training Loss: 0.0065692507580388335\n",
      "Training Loss: 0.006729840693296864\n",
      "Validation Loss: 0.004467047147290635\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006565970616647974\n",
      "Training Loss: 0.006564906421699561\n",
      "Training Loss: 0.006725125527009368\n",
      "Validation Loss: 0.004461521659506841\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006561836343025789\n",
      "Training Loss: 0.006560616295901127\n",
      "Training Loss: 0.006720471995649859\n",
      "Validation Loss: 0.00445606957550757\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006557753001106903\n",
      "Training Loss: 0.006556381545960903\n",
      "Training Loss: 0.00671587805612944\n",
      "Validation Loss: 0.004450693452505793\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006553721239324659\n",
      "Training Loss: 0.006552198065910488\n",
      "Training Loss: 0.0067113436141517015\n",
      "Validation Loss: 0.004445385227759442\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.0065497348655480896\n",
      "Training Loss: 0.006548067424446345\n",
      "Training Loss: 0.0067068673588801175\n",
      "Validation Loss: 0.0044401459856314605\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006545756353298202\n",
      "Training Loss: 0.006543988017947413\n",
      "Training Loss: 0.006702447215793654\n",
      "Validation Loss: 0.004434971150607289\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006541822222061455\n",
      "Training Loss: 0.00653995803440921\n",
      "Training Loss: 0.0066980816319119185\n",
      "Validation Loss: 0.004429860750155712\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006537931752391159\n",
      "Training Loss: 0.0065359718474792316\n",
      "Training Loss: 0.0066937675222288814\n",
      "Validation Loss: 0.004424809444214354\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006534085558960214\n",
      "Training Loss: 0.006532034696429037\n",
      "Training Loss: 0.006689504927489907\n",
      "Validation Loss: 0.0044198219646082336\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006530281117884442\n",
      "Training Loss: 0.0065281388181028886\n",
      "Training Loss: 0.006685289320303127\n",
      "Validation Loss: 0.004414890486004061\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006526513706194237\n",
      "Training Loss: 0.006524285392370075\n",
      "Training Loss: 0.006681123808957637\n",
      "Validation Loss: 0.004410014856669507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006522786739515141\n",
      "Training Loss: 0.006520474086282774\n",
      "Training Loss: 0.00667700311401859\n",
      "Validation Loss: 0.004405196521985816\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.0065190963295754046\n",
      "Training Loss: 0.006516701130894944\n",
      "Training Loss: 0.006672929116757586\n",
      "Validation Loss: 0.00440043063044255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006515440602088347\n",
      "Training Loss: 0.006512968228198588\n",
      "Training Loss: 0.006668896508635953\n",
      "Validation Loss: 0.004395717948549584\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006511821408057585\n",
      "Training Loss: 0.00650927142938599\n",
      "Training Loss: 0.006664908694801852\n",
      "Validation Loss: 0.004391056179393376\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006508234021021053\n",
      "Training Loss: 0.0065056144155096265\n",
      "Training Loss: 0.00666096257045865\n",
      "Validation Loss: 0.004386445644751108\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006504681145306676\n",
      "Training Loss: 0.0065019918500911445\n",
      "Training Loss: 0.006657059178687632\n",
      "Validation Loss: 0.004381882950919942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006501159321051091\n",
      "Training Loss: 0.006498404303565622\n",
      "Training Loss: 0.006653193389065564\n",
      "Validation Loss: 0.004377369172452541\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006497669602977112\n",
      "Training Loss: 0.006494851168827154\n",
      "Training Loss: 0.006649368916405365\n",
      "Validation Loss: 0.004372902030093867\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006494207441573962\n",
      "Training Loss: 0.006491333394078538\n",
      "Training Loss: 0.006645579620962963\n",
      "Validation Loss: 0.004368481557852892\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006490776971913874\n",
      "Training Loss: 0.006487846740055829\n",
      "Training Loss: 0.00664183113607578\n",
      "Validation Loss: 0.0043641068813066635\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006487372284755111\n",
      "Training Loss: 0.006484392188722268\n",
      "Training Loss: 0.0066381188295781615\n",
      "Validation Loss: 0.004359773977586393\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006483996851602569\n",
      "Training Loss: 0.006480970137636178\n",
      "Training Loss: 0.006634442089125514\n",
      "Validation Loss: 0.004355484560874992\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006480648985598236\n",
      "Training Loss: 0.006477578795747832\n",
      "Training Loss: 0.006630800800630823\n",
      "Validation Loss: 0.004351237988926135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006477327137254178\n",
      "Training Loss: 0.006474218102521263\n",
      "Training Loss: 0.006627193654421717\n",
      "Validation Loss: 0.0043470315102833125\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006474029133096337\n",
      "Training Loss: 0.006470887799514458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [07:23<17:14, 147.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.006623618976445869\n",
      "Validation Loss: 0.004342867470911464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.2152504277601838\n",
      "Training Loss: 0.1569312209263444\n",
      "Training Loss: 0.1230244160629809\n",
      "Validation Loss: 0.09902593150232615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.08909749623388052\n",
      "Training Loss: 0.0804352474398911\n",
      "Training Loss: 0.07665874149650335\n",
      "Validation Loss: 0.07550015712805679\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.07423323292285204\n",
      "Training Loss: 0.07262357987463475\n",
      "Training Loss: 0.07156724471598863\n",
      "Validation Loss: 0.07170326520134224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.07101041300222277\n",
      "Training Loss: 0.06952043075114489\n",
      "Training Loss: 0.06852234950289131\n",
      "Validation Loss: 0.06857615663261896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.06798788717016578\n",
      "Training Loss: 0.06643098805099726\n",
      "Training Loss: 0.06533639067783951\n",
      "Validation Loss: 0.06507246503920368\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.06444132218137383\n",
      "Training Loss: 0.06253574071452021\n",
      "Training Loss: 0.06098078140988946\n",
      "Validation Loss: 0.059821102789111354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.058824525345116854\n",
      "Training Loss: 0.05586676992475986\n",
      "Training Loss: 0.05310975480824709\n",
      "Validation Loss: 0.05035346354033505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.04884136939421296\n",
      "Training Loss: 0.044927498064935206\n",
      "Training Loss: 0.04161480541341007\n",
      "Validation Loss: 0.03850287300524082\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.03727170918136835\n",
      "Training Loss: 0.03408277622424066\n",
      "Training Loss: 0.031732281316071746\n",
      "Validation Loss: 0.02928426011015525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.028603499024175107\n",
      "Training Loss: 0.026542886407114566\n",
      "Training Loss: 0.025173908555880187\n",
      "Validation Loss: 0.02326391674473547\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.022872698940336705\n",
      "Training Loss: 0.021731413966044784\n",
      "Training Loss: 0.02105401014443487\n",
      "Validation Loss: 0.019529866483690362\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.019323123199865223\n",
      "Training Loss: 0.0188830157648772\n",
      "Training Loss: 0.018657484580762683\n",
      "Validation Loss: 0.017290785644475497\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.017253241010475904\n",
      "Training Loss: 0.01717467643553391\n",
      "Training Loss: 0.017141452920623124\n",
      "Validation Loss: 0.015748501852091947\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.01585138156777248\n",
      "Training Loss: 0.015922307739965617\n",
      "Training Loss: 0.01596315898001194\n",
      "Validation Loss: 0.014515135034374642\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.014735512649640441\n",
      "Training Loss: 0.014883889749180526\n",
      "Training Loss: 0.01496137611567974\n",
      "Validation Loss: 0.01346648488915787\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.013789513122756034\n",
      "Training Loss: 0.013984750020317733\n",
      "Training Loss: 0.01408384955720976\n",
      "Validation Loss: 0.012546842847165934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.012966824846807867\n",
      "Training Loss: 0.013191698014270514\n",
      "Training Loss: 0.013307857520412653\n",
      "Validation Loss: 0.011728972497427563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.012247869926504791\n",
      "Training Loss: 0.012492316411808133\n",
      "Training Loss: 0.012625328220892698\n",
      "Validation Loss: 0.011002664481750114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.01162529612192884\n",
      "Training Loss: 0.011881998665630817\n",
      "Training Loss: 0.01203111616196111\n",
      "Validation Loss: 0.010363021593415336\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.011092883916571736\n",
      "Training Loss: 0.011354902964085341\n",
      "Training Loss: 0.011517761403229087\n",
      "Validation Loss: 0.009804404686006267\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.01064216047525406\n",
      "Training Loss: 0.010902780378237366\n",
      "Training Loss: 0.011076252639759332\n",
      "Validation Loss: 0.00931980183185863\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.010263426704332232\n",
      "Training Loss: 0.010516711329109966\n",
      "Training Loss: 0.01069773560622707\n",
      "Validation Loss: 0.008901721050994198\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.009947148161008954\n",
      "Training Loss: 0.010188370776595547\n",
      "Training Loss: 0.010374330638442189\n",
      "Validation Loss: 0.008542792071075587\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.009684586010407656\n",
      "Training Loss: 0.009910376184852793\n",
      "Training Loss: 0.01009918340248987\n",
      "Validation Loss: 0.008235995046888593\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.009467909366358072\n",
      "Training Loss: 0.009676161205861717\n",
      "Training Loss: 0.009866221586707979\n",
      "Validation Loss: 0.007974671227386577\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.009290104553801939\n",
      "Training Loss: 0.009479785901494325\n",
      "Training Loss: 0.009669913640245796\n",
      "Validation Loss: 0.007752591564864172\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.009144896456273272\n",
      "Training Loss: 0.009315812835702673\n",
      "Training Loss: 0.00950515250908211\n",
      "Validation Loss: 0.007564006545947174\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.009026710616890341\n",
      "Training Loss: 0.009179285546997562\n",
      "Training Loss: 0.009367235682439058\n",
      "Validation Loss: 0.007403737459896823\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008930670911213382\n",
      "Training Loss: 0.009065745424013585\n",
      "Training Loss: 0.009251906382851303\n",
      "Validation Loss: 0.007267225090299178\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008852585471468047\n",
      "Training Loss: 0.00897126667550765\n",
      "Training Loss: 0.009155389603693038\n",
      "Validation Loss: 0.007150554951029212\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008788921517552808\n",
      "Training Loss: 0.008892461371142418\n",
      "Training Loss: 0.009074407898588106\n",
      "Validation Loss: 0.007050412539601996\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008736744250636547\n",
      "Training Loss: 0.008826450129272417\n",
      "Training Loss: 0.009006162558216601\n",
      "Validation Loss: 0.006964034211214925\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008693648620974272\n",
      "Training Loss: 0.008770821900106966\n",
      "Training Loss: 0.008948297186288983\n",
      "Validation Loss: 0.006889127032684811\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008657678924500943\n",
      "Training Loss: 0.008723573208553716\n",
      "Training Loss: 0.008898847782984377\n",
      "Validation Loss: 0.0068237878332500545\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008627260309876874\n",
      "Training Loss: 0.008683057840680703\n",
      "Training Loss: 0.00885619439301081\n",
      "Validation Loss: 0.006766447085845336\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008601131240138784\n",
      "Training Loss: 0.0086479288933333\n",
      "Training Loss: 0.008819005748955533\n",
      "Validation Loss: 0.006715803789079524\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008578288205899297\n",
      "Training Loss: 0.008617092496715486\n",
      "Training Loss: 0.008786200236063451\n",
      "Validation Loss: 0.006670782853995733\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008557938720332458\n",
      "Training Loss: 0.00858966299565509\n",
      "Training Loss: 0.0087568949686829\n",
      "Validation Loss: 0.0066304809160091065\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008539454141864553\n",
      "Training Loss: 0.008564925465034321\n",
      "Training Loss: 0.008730379769112916\n",
      "Validation Loss: 0.0065941482329782974\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008522345015080646\n",
      "Training Loss: 0.008542308886535465\n",
      "Training Loss: 0.008706080482807011\n",
      "Validation Loss: 0.0065611492191033245\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008506223993608729\n",
      "Training Loss: 0.008521351126255468\n",
      "Training Loss: 0.008683532276190817\n",
      "Validation Loss: 0.0065309588268943385\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.00849079546169378\n",
      "Training Loss: 0.008501688238466159\n",
      "Training Loss: 0.008662363296607509\n",
      "Validation Loss: 0.006503125343974052\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.008475820559542626\n",
      "Training Loss: 0.008483024489833043\n",
      "Training Loss: 0.008642275293823332\n",
      "Validation Loss: 0.006477275622610966\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.008461120662977918\n",
      "Training Loss: 0.00846512476215139\n",
      "Training Loss: 0.008623022973770276\n",
      "Validation Loss: 0.006453081418686871\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.008446551599772647\n",
      "Training Loss: 0.008447799275163561\n",
      "Training Loss: 0.00860441043623723\n",
      "Validation Loss: 0.006430270974367355\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.008431999396998435\n",
      "Training Loss: 0.008430897264042869\n",
      "Training Loss: 0.00858627907349728\n",
      "Validation Loss: 0.0064086089842021465\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.008417377328732982\n",
      "Training Loss: 0.008414297847775743\n",
      "Training Loss: 0.008568497898522764\n",
      "Validation Loss: 0.006387895100038457\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.008402612523641438\n",
      "Training Loss: 0.00839789794292301\n",
      "Training Loss: 0.008550958768464624\n",
      "Validation Loss: 0.006367955902287883\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.00838764694170095\n",
      "Training Loss: 0.00838161601452157\n",
      "Training Loss: 0.008533573203021661\n",
      "Validation Loss: 0.00634863509487863\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.008372433846816421\n",
      "Training Loss: 0.008365386829245836\n",
      "Training Loss: 0.008516264677746221\n",
      "Validation Loss: 0.006329812306984003\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.008356934871990233\n",
      "Training Loss: 0.008349154413444922\n",
      "Training Loss: 0.008498973443638534\n",
      "Validation Loss: 0.006311369786468031\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.008341118085663766\n",
      "Training Loss: 0.008332875026389956\n",
      "Training Loss: 0.008481647659791633\n",
      "Validation Loss: 0.0062932107882741626\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.008324956041760742\n",
      "Training Loss: 0.00831650932552293\n",
      "Training Loss: 0.008464241413166746\n",
      "Validation Loss: 0.006275252032282061\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.008308432118501513\n",
      "Training Loss: 0.008300033294362947\n",
      "Training Loss: 0.008446723541710525\n",
      "Validation Loss: 0.006257421306983223\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.008291527250548825\n",
      "Training Loss: 0.008283419808140024\n",
      "Training Loss: 0.008429059956688434\n",
      "Validation Loss: 0.006239658950179229\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.008274232367984951\n",
      "Training Loss: 0.008266655378974974\n",
      "Training Loss: 0.008411232316866517\n",
      "Validation Loss: 0.006221912722783477\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.00825654225773178\n",
      "Training Loss: 0.00824972813250497\n",
      "Training Loss: 0.008393221765290946\n",
      "Validation Loss: 0.006204137191427558\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.008238454434322194\n",
      "Training Loss: 0.008232630885904655\n",
      "Training Loss: 0.008375014377525076\n",
      "Validation Loss: 0.006186300150924519\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.008219973702216521\n",
      "Training Loss: 0.008215362131595612\n",
      "Training Loss: 0.008356603854335844\n",
      "Validation Loss: 0.006168371269196858\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.008201107601635158\n",
      "Training Loss: 0.008197927052387968\n",
      "Training Loss: 0.008337985799880699\n",
      "Validation Loss: 0.006150336977973413\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.008181871022097767\n",
      "Training Loss: 0.008180329641327262\n",
      "Training Loss: 0.008319161057006568\n",
      "Validation Loss: 0.0061321744676993304\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.008162279115058482\n",
      "Training Loss: 0.00816258221748285\n",
      "Training Loss: 0.008300134342862293\n",
      "Validation Loss: 0.006113875726646001\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.00814235465717502\n",
      "Training Loss: 0.008144695897353813\n",
      "Training Loss: 0.008280912827467546\n",
      "Validation Loss: 0.0060954380589485005\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.008122120315674693\n",
      "Training Loss: 0.008126685190945863\n",
      "Training Loss: 0.008261505177943036\n",
      "Validation Loss: 0.006076863698734577\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.00810160381020978\n",
      "Training Loss: 0.00810856624157168\n",
      "Training Loss: 0.008241923452587799\n",
      "Validation Loss: 0.00605814988081333\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.008080831475090235\n",
      "Training Loss: 0.008090356020256878\n",
      "Training Loss: 0.00822218015557155\n",
      "Validation Loss: 0.0060393017510035904\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.008059831782011315\n",
      "Training Loss: 0.008072068495675922\n",
      "Training Loss: 0.008202289558248595\n",
      "Validation Loss: 0.0060203291575183695\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.008038635483244435\n",
      "Training Loss: 0.008053724493365735\n",
      "Training Loss: 0.008182265760842711\n",
      "Validation Loss: 0.006001242980612128\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.008017269027186557\n",
      "Training Loss: 0.00803533651982434\n",
      "Training Loss: 0.00816212487174198\n",
      "Validation Loss: 0.005982044533554339\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007995761054335162\n",
      "Training Loss: 0.008016920624068008\n",
      "Training Loss: 0.008141880913171917\n",
      "Validation Loss: 0.005962746308101446\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007974135982804\n",
      "Training Loss: 0.007998489293968306\n",
      "Training Loss: 0.008121548647759482\n",
      "Validation Loss: 0.005943360017727684\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007952419745270162\n",
      "Training Loss: 0.007980056791566312\n",
      "Training Loss: 0.008101143734529614\n",
      "Validation Loss: 0.005923895457016534\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007930632366333156\n",
      "Training Loss: 0.00796163408900611\n",
      "Training Loss: 0.008080679922131821\n",
      "Validation Loss: 0.005904359890807295\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007908797293202952\n",
      "Training Loss: 0.007943231337703765\n",
      "Training Loss: 0.008060171846300363\n",
      "Validation Loss: 0.005884764535520016\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.00788693197304383\n",
      "Training Loss: 0.007924859911436216\n",
      "Training Loss: 0.008039635655004531\n",
      "Validation Loss: 0.005865120226935892\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007865054617868737\n",
      "Training Loss: 0.007906528442399577\n",
      "Training Loss: 0.00801908338908106\n",
      "Validation Loss: 0.005845436498030937\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007843181148637086\n",
      "Training Loss: 0.007888245545327664\n",
      "Training Loss: 0.007998533205827697\n",
      "Validation Loss: 0.00582571952274132\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007821329765720293\n",
      "Training Loss: 0.007870021286653355\n",
      "Training Loss: 0.007977998703718186\n",
      "Validation Loss: 0.0058059882244430065\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007799514655489475\n",
      "Training Loss: 0.007851867844583466\n",
      "Training Loss: 0.007957501272903756\n",
      "Validation Loss: 0.005786254062850991\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007777757263975218\n",
      "Training Loss: 0.007833796752383932\n",
      "Training Loss: 0.007937064236029983\n",
      "Validation Loss: 0.00576653799403124\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007756077112862841\n",
      "Training Loss: 0.007815824786666781\n",
      "Training Loss: 0.007916711755096912\n",
      "Validation Loss: 0.005746862261158446\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007734497516648844\n",
      "Training Loss: 0.007797968843951821\n",
      "Training Loss: 0.007896471843123436\n",
      "Validation Loss: 0.005727251166258049\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.0077130464103538545\n",
      "Training Loss: 0.007780248351627961\n",
      "Training Loss: 0.007876379049848765\n",
      "Validation Loss: 0.005707731798783028\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007691750898957252\n",
      "Training Loss: 0.007762686418136582\n",
      "Training Loss: 0.007856464269571007\n",
      "Validation Loss: 0.005688335972817175\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007670638202689588\n",
      "Training Loss: 0.007745302966795862\n",
      "Training Loss: 0.007836759957717732\n",
      "Validation Loss: 0.0056690922088063\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007649734619772062\n",
      "Training Loss: 0.0077281161048449575\n",
      "Training Loss: 0.007817293469561264\n",
      "Validation Loss: 0.005650017950736153\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007629059635801241\n",
      "Training Loss: 0.007711142167681828\n",
      "Training Loss: 0.0077980870101600885\n",
      "Validation Loss: 0.005631136336478959\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007608634162461385\n",
      "Training Loss: 0.0076943947724066675\n",
      "Training Loss: 0.007779159296769649\n",
      "Validation Loss: 0.005612461347645672\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007588468756293878\n",
      "Training Loss: 0.0076778833218850194\n",
      "Training Loss: 0.0077605188684538\n",
      "Validation Loss: 0.005594001915515139\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.00756857517757453\n",
      "Training Loss: 0.0076616132014896724\n",
      "Training Loss: 0.007742173611186445\n",
      "Validation Loss: 0.005575763871317751\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007548957670805976\n",
      "Training Loss: 0.007645586668513715\n",
      "Training Loss: 0.00772412252612412\n",
      "Validation Loss: 0.00555773800908682\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007529618317494169\n",
      "Training Loss: 0.007629803761374205\n",
      "Training Loss: 0.007706359017174691\n",
      "Validation Loss: 0.005539930947491208\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.0075105573271866885\n",
      "Training Loss: 0.007614263663999737\n",
      "Training Loss: 0.007688882636139169\n",
      "Validation Loss: 0.005522336547911753\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007491776248207316\n",
      "Training Loss: 0.007598963237833232\n",
      "Training Loss: 0.007671686226967722\n",
      "Validation Loss: 0.005504953843708872\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007473274184158072\n",
      "Training Loss: 0.007583900961326435\n",
      "Training Loss: 0.007654763086466118\n",
      "Validation Loss: 0.005487772752530873\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007455054289894178\n",
      "Training Loss: 0.0075690736901015045\n",
      "Training Loss: 0.007638109121471643\n",
      "Validation Loss: 0.005470795211581032\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007437115771463141\n",
      "Training Loss: 0.007554480507969856\n",
      "Training Loss: 0.007621721139876172\n",
      "Validation Loss: 0.005454017610676335\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.0074194574414286766\n",
      "Training Loss: 0.007540121100610122\n",
      "Training Loss: 0.007605595885543152\n",
      "Validation Loss: 0.00543744313357963\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007402086921501905\n",
      "Training Loss: 0.00752599453786388\n",
      "Training Loss: 0.007589730890467763\n",
      "Validation Loss: 0.0054210671262941166\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.0073850053583737465\n",
      "Training Loss: 0.007512104874476791\n",
      "Training Loss: 0.007574129501590505\n",
      "Validation Loss: 0.005404898815715079\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007368218096671626\n",
      "Training Loss: 0.007498451594728977\n",
      "Training Loss: 0.007558789568720386\n",
      "Validation Loss: 0.005388936897729304\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.0073517289548181\n",
      "Training Loss: 0.0074850363144651055\n",
      "Training Loss: 0.007543712425976992\n",
      "Validation Loss: 0.005373188164974615\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007335543014341965\n",
      "Training Loss: 0.0074718631338328125\n",
      "Training Loss: 0.00752890195697546\n",
      "Validation Loss: 0.005357658754762137\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007319666495313868\n",
      "Training Loss: 0.0074589360214304175\n",
      "Training Loss: 0.007514359677443281\n",
      "Validation Loss: 0.005342354793938693\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007304104652721435\n",
      "Training Loss: 0.007446256930707023\n",
      "Training Loss: 0.007500085834180936\n",
      "Validation Loss: 0.0053272796964817\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007288862514542416\n",
      "Training Loss: 0.007433826816268265\n",
      "Training Loss: 0.007486082742689177\n",
      "Validation Loss: 0.005312442219701041\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007273943398613483\n",
      "Training Loss: 0.007421651159529574\n",
      "Training Loss: 0.007472352408803999\n",
      "Validation Loss: 0.005297847612750497\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.007259354599518701\n",
      "Training Loss: 0.007409728271886707\n",
      "Training Loss: 0.007458893602015451\n",
      "Validation Loss: 0.005283500493001821\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007245095492107794\n",
      "Training Loss: 0.007398060930427164\n",
      "Training Loss: 0.007445709842722863\n",
      "Validation Loss: 0.0052694075680704095\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.007231171025196091\n",
      "Training Loss: 0.007386650769039989\n",
      "Training Loss: 0.0074327978631481524\n",
      "Validation Loss: 0.0052555709033471985\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.007217583522433415\n",
      "Training Loss: 0.007375497178873047\n",
      "Training Loss: 0.007420156446751207\n",
      "Validation Loss: 0.005241998490940235\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0072043329896405335\n",
      "Training Loss: 0.007364599095890299\n",
      "Training Loss: 0.007407786529511213\n",
      "Validation Loss: 0.0052286918795657125\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007191420786548406\n",
      "Training Loss: 0.007353955741855316\n",
      "Training Loss: 0.007395682471105829\n",
      "Validation Loss: 0.005215651868434435\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007178844106383622\n",
      "Training Loss: 0.0073435659956885505\n",
      "Training Loss: 0.007383845095755532\n",
      "Validation Loss: 0.005202888598759774\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007166603925870731\n",
      "Training Loss: 0.007333427403355017\n",
      "Training Loss: 0.007372269062325358\n",
      "Validation Loss: 0.005190393501589221\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007154694998171181\n",
      "Training Loss: 0.0073235342872794715\n",
      "Training Loss: 0.007360951113514602\n",
      "Validation Loss: 0.005178173676949371\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.007143115875078365\n",
      "Training Loss: 0.007313886106712744\n",
      "Training Loss: 0.00734988724347204\n",
      "Validation Loss: 0.005166226999279572\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.007131862215464935\n",
      "Training Loss: 0.007304477375000716\n",
      "Training Loss: 0.007339074141345919\n",
      "Validation Loss: 0.0051545582324517576\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.007120927440701053\n",
      "Training Loss: 0.00729530316486489\n",
      "Training Loss: 0.007328506965423003\n",
      "Validation Loss: 0.005143159417058812\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.0071103066788055\n",
      "Training Loss: 0.007286359082791023\n",
      "Training Loss: 0.007318181357113644\n",
      "Validation Loss: 0.0051320346616459695\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.007099993414012715\n",
      "Training Loss: 0.007277639303938485\n",
      "Training Loss: 0.007308091426966712\n",
      "Validation Loss: 0.005121181102919529\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007089980132877827\n",
      "Training Loss: 0.007269138442352414\n",
      "Training Loss: 0.007298232425237075\n",
      "Validation Loss: 0.005110596416497163\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.007080259922659025\n",
      "Training Loss: 0.007260848645819351\n",
      "Training Loss: 0.007288597166771069\n",
      "Validation Loss: 0.00510027134278266\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.007070821891538799\n",
      "Training Loss: 0.007252764020813629\n",
      "Training Loss: 0.007279182054335251\n",
      "Validation Loss: 0.005090209073387086\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.007061658927705139\n",
      "Training Loss: 0.007244878364726901\n",
      "Training Loss: 0.0072699779190588745\n",
      "Validation Loss: 0.005080402704489449\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.007052760460646823\n",
      "Training Loss: 0.007237180265947245\n",
      "Training Loss: 0.007260978235863149\n",
      "Validation Loss: 0.0050708450785095106\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.007044116485631093\n",
      "Training Loss: 0.007229667365318164\n",
      "Training Loss: 0.007252179090864956\n",
      "Validation Loss: 0.005061529911635967\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.007035717815160752\n",
      "Training Loss: 0.007222328201169148\n",
      "Training Loss: 0.007243567408295348\n",
      "Validation Loss: 0.005052454158757928\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.007027553353691473\n",
      "Training Loss: 0.007215154868317768\n",
      "Training Loss: 0.007235140891280026\n",
      "Validation Loss: 0.005043609589704553\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007019612566800788\n",
      "Training Loss: 0.007208143085008487\n",
      "Training Loss: 0.00722689137677662\n",
      "Validation Loss: 0.005034988611319176\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007011886128457263\n",
      "Training Loss: 0.007201282709720544\n",
      "Training Loss: 0.0072188092302531\n",
      "Validation Loss: 0.005026585171313098\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.007004362097941339\n",
      "Training Loss: 0.007194564815727063\n",
      "Training Loss: 0.007210889504058286\n",
      "Validation Loss: 0.005018387921154499\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006997030420461669\n",
      "Training Loss: 0.007187983189942316\n",
      "Training Loss: 0.007203122830251232\n",
      "Validation Loss: 0.005010393852357533\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006989880884066224\n",
      "Training Loss: 0.007181531257228926\n",
      "Training Loss: 0.007195504573173821\n",
      "Validation Loss: 0.0050025930565394715\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006982903996249661\n",
      "Training Loss: 0.007175199353951029\n",
      "Training Loss: 0.007188025031937286\n",
      "Validation Loss: 0.004994979311366764\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006976090725511312\n",
      "Training Loss: 0.007168982663424686\n",
      "Training Loss: 0.007180679813027382\n",
      "Validation Loss: 0.004987540415467255\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006969431744655594\n",
      "Training Loss: 0.0071628730691736565\n",
      "Training Loss: 0.007173459735931829\n",
      "Validation Loss: 0.004980276525805421\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.00696291693020612\n",
      "Training Loss: 0.007156865033903159\n",
      "Training Loss: 0.007166360204573721\n",
      "Validation Loss: 0.004973172300411493\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006956538847880438\n",
      "Training Loss: 0.007150952703668736\n",
      "Training Loss: 0.007159373899921775\n",
      "Validation Loss: 0.004966226062381619\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006950288759544492\n",
      "Training Loss: 0.007145129580749199\n",
      "Training Loss: 0.0071524955437052996\n",
      "Validation Loss: 0.00495942784054728\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006944160440471023\n",
      "Training Loss: 0.007139390108641237\n",
      "Training Loss: 0.007145720402477309\n",
      "Validation Loss: 0.00495276968073351\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006938145986059681\n",
      "Training Loss: 0.007133730972418562\n",
      "Training Loss: 0.007139043092029169\n",
      "Validation Loss: 0.0049462501571796245\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006932237880537286\n",
      "Training Loss: 0.007128143306472339\n",
      "Training Loss: 0.007132456657709554\n",
      "Validation Loss: 0.004939856922013287\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006926429831655696\n",
      "Training Loss: 0.007122626245836728\n",
      "Training Loss: 0.007125957952812314\n",
      "Validation Loss: 0.004933588252549343\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006920717738103122\n",
      "Training Loss: 0.0071171735890675335\n",
      "Training Loss: 0.0071195417025592175\n",
      "Validation Loss: 0.004927432997769603\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006915094458963722\n",
      "Training Loss: 0.007111781400162727\n",
      "Training Loss: 0.007113202840555459\n",
      "Validation Loss: 0.004921388767889879\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.0069095554773230105\n",
      "Training Loss: 0.007106446679099463\n",
      "Training Loss: 0.00710693821310997\n",
      "Validation Loss: 0.004915448125409946\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006904095616191625\n",
      "Training Loss: 0.007101165009080432\n",
      "Training Loss: 0.007100743162445724\n",
      "Validation Loss: 0.004909608718478696\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006898709194501862\n",
      "Training Loss: 0.007095933258533478\n",
      "Training Loss: 0.0070946155826095494\n",
      "Validation Loss: 0.004903862001688293\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006893394287908449\n",
      "Training Loss: 0.007090747819747775\n",
      "Training Loss: 0.007088550460757688\n",
      "Validation Loss: 0.004898209498902218\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006888145599514246\n",
      "Training Loss: 0.007085606971522793\n",
      "Training Loss: 0.007082545612938702\n",
      "Validation Loss: 0.004892640499910863\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0068829602608457205\n",
      "Training Loss: 0.007080507082864642\n",
      "Training Loss: 0.007076596871484071\n",
      "Validation Loss: 0.004887150078645667\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006877834886545316\n",
      "Training Loss: 0.007075443905196152\n",
      "Training Loss: 0.007070703043136745\n",
      "Validation Loss: 0.0048817392318310695\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.00687276620301418\n",
      "Training Loss: 0.0070704182895133276\n",
      "Training Loss: 0.007064859453821555\n",
      "Validation Loss: 0.004876400762645716\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006867749755037948\n",
      "Training Loss: 0.007065425759064965\n",
      "Training Loss: 0.007059066821821034\n",
      "Validation Loss: 0.004871128274787092\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006862785911653191\n",
      "Training Loss: 0.00706046455539763\n",
      "Training Loss: 0.007053319326369092\n",
      "Validation Loss: 0.004865927375811204\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.0068578690092545\n",
      "Training Loss: 0.0070555331464856865\n",
      "Training Loss: 0.007047616704367102\n",
      "Validation Loss: 0.00486078376102188\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.0068529999535530805\n",
      "Training Loss: 0.00705062965513207\n",
      "Training Loss: 0.007041953864973039\n",
      "Validation Loss: 0.00485570260109066\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.0068481750576756895\n",
      "Training Loss: 0.007045751895639114\n",
      "Training Loss: 0.007036333050346002\n",
      "Validation Loss: 0.004850674930729725\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006843391882721334\n",
      "Training Loss: 0.007040898619452491\n",
      "Training Loss: 0.007030751338461414\n",
      "Validation Loss: 0.0048456996171787545\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.0068386488652322445\n",
      "Training Loss: 0.007036067632143385\n",
      "Training Loss: 0.007025205460377037\n",
      "Validation Loss: 0.004840778421055986\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006833945583784953\n",
      "Training Loss: 0.00703125907573849\n",
      "Training Loss: 0.007019695552298799\n",
      "Validation Loss: 0.004835906598110045\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.00682927927817218\n",
      "Training Loss: 0.007026471691788174\n",
      "Training Loss: 0.007014219311531633\n",
      "Validation Loss: 0.004831079141174056\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006824648682959377\n",
      "Training Loss: 0.007021702564088628\n",
      "Training Loss: 0.007008777222363278\n",
      "Validation Loss: 0.004826299506082629\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006820053630508483\n",
      "Training Loss: 0.007016952501726337\n",
      "Training Loss: 0.007003366044955328\n",
      "Validation Loss: 0.004821556558822062\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.00681549206841737\n",
      "Training Loss: 0.007012219384778291\n",
      "Training Loss: 0.0069979857560247185\n",
      "Validation Loss: 0.004816856969074671\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006810963421594351\n",
      "Training Loss: 0.007007503673667088\n",
      "Training Loss: 0.006992635858478024\n",
      "Validation Loss: 0.004812197254845098\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006806467368733138\n",
      "Training Loss: 0.007002803318900987\n",
      "Training Loss: 0.006987315801670775\n",
      "Validation Loss: 0.004807574146040035\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006802001631585881\n",
      "Training Loss: 0.006998119423515164\n",
      "Training Loss: 0.006982022962765768\n",
      "Validation Loss: 0.0048029846551022335\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.0067975669295992705\n",
      "Training Loss: 0.006993450536974706\n",
      "Training Loss: 0.006976759678218513\n",
      "Validation Loss: 0.004798430197328078\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0067931614501867445\n",
      "Training Loss: 0.00698879589792341\n",
      "Training Loss: 0.006971524019027129\n",
      "Validation Loss: 0.004793909102353906\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006788784788222983\n",
      "Training Loss: 0.006984155700774863\n",
      "Training Loss: 0.006966315547470003\n",
      "Validation Loss: 0.0047894199234904365\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006784437648020685\n",
      "Training Loss: 0.0069795297185191885\n",
      "Training Loss: 0.006961133814183995\n",
      "Validation Loss: 0.00478495863329075\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.0067801183951087295\n",
      "Training Loss: 0.006974917753832415\n",
      "Training Loss: 0.006955980972852558\n",
      "Validation Loss: 0.004780535422673637\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.00677582812262699\n",
      "Training Loss: 0.006970321474364027\n",
      "Training Loss: 0.006950854369206354\n",
      "Validation Loss: 0.004776135888840124\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006771565284579993\n",
      "Training Loss: 0.00696573774155695\n",
      "Training Loss: 0.006945754961343482\n",
      "Validation Loss: 0.0047717651305197\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006767330749426037\n",
      "Training Loss: 0.006961169631103985\n",
      "Training Loss: 0.00694068360957317\n",
      "Validation Loss: 0.004767422355041745\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006763123467098921\n",
      "Training Loss: 0.006956616940442473\n",
      "Training Loss: 0.006935639439616352\n",
      "Validation Loss: 0.004763113821364856\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006758944971952587\n",
      "Training Loss: 0.006952079855254851\n",
      "Training Loss: 0.006930623695952817\n",
      "Validation Loss: 0.004758824640100173\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006754793265135959\n",
      "Training Loss: 0.006947557391831651\n",
      "Training Loss: 0.0069256351247895505\n",
      "Validation Loss: 0.004754563140829377\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.00675066833384335\n",
      "Training Loss: 0.006943051415728405\n",
      "Training Loss: 0.006920676630688832\n",
      "Validation Loss: 0.004750328427546898\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006746572979027405\n",
      "Training Loss: 0.006938561396673322\n",
      "Training Loss: 0.006915746409213171\n",
      "Validation Loss: 0.004746119988810146\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006742504113353789\n",
      "Training Loss: 0.006934088331181556\n",
      "Training Loss: 0.006910845637321472\n",
      "Validation Loss: 0.00474193886450879\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.00673846330260858\n",
      "Training Loss: 0.006929634329280816\n",
      "Training Loss: 0.006905974423279986\n",
      "Validation Loss: 0.0047377783718801445\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006734450572403148\n",
      "Training Loss: 0.006925197782693431\n",
      "Training Loss: 0.006901134605286643\n",
      "Validation Loss: 0.004733647497140624\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006730466516455635\n",
      "Training Loss: 0.006920779474312439\n",
      "Training Loss: 0.006896325859706849\n",
      "Validation Loss: 0.004729540426064324\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006726510059088468\n",
      "Training Loss: 0.0069163819501409304\n",
      "Training Loss: 0.006891549170250073\n",
      "Validation Loss: 0.004725460515075018\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.0067225820361636575\n",
      "Training Loss: 0.006912005393533036\n",
      "Training Loss: 0.0068868033215403555\n",
      "Validation Loss: 0.004721406575166777\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0067186821729410436\n",
      "Training Loss: 0.006907650270150043\n",
      "Training Loss: 0.0068820915953256186\n",
      "Validation Loss: 0.0047173748849734165\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006714811777928844\n",
      "Training Loss: 0.006903314907685853\n",
      "Training Loss: 0.006877411562018096\n",
      "Validation Loss: 0.0047133693699149435\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006710968863917515\n",
      "Training Loss: 0.00689900342898909\n",
      "Training Loss: 0.006872766264714301\n",
      "Validation Loss: 0.00470938863693161\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006707155093317851\n",
      "Training Loss: 0.006894715128000825\n",
      "Training Loss: 0.006868154506664723\n",
      "Validation Loss: 0.004705434381239786\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006703370477771387\n",
      "Training Loss: 0.006890449993661605\n",
      "Training Loss: 0.0068635780783370135\n",
      "Validation Loss: 0.0047015044602743365\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006699615396792069\n",
      "Training Loss: 0.006886209270451218\n",
      "Training Loss: 0.0068590353440959005\n",
      "Validation Loss: 0.0046976000957421205\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.0066958878107834605\n",
      "Training Loss: 0.006881993397837505\n",
      "Training Loss: 0.006854528515832499\n",
      "Validation Loss: 0.004693719495632006\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006692190559115261\n",
      "Training Loss: 0.006877802357776091\n",
      "Training Loss: 0.006850057787960395\n",
      "Validation Loss: 0.004689863919583934\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006688521023606881\n",
      "Training Loss: 0.006873638712568209\n",
      "Training Loss: 0.00684562157606706\n",
      "Validation Loss: 0.004686034556603834\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006684881236869842\n",
      "Training Loss: 0.006869501084438525\n",
      "Training Loss: 0.006841222023358569\n",
      "Validation Loss: 0.004682231015588544\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006681268918327987\n",
      "Training Loss: 0.006865390109014697\n",
      "Training Loss: 0.006836857887683436\n",
      "Validation Loss: 0.004678447373745063\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006677685697795823\n",
      "Training Loss: 0.0068613065022509545\n",
      "Training Loss: 0.0068325315893162045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [09:51<14:47, 147.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.004674696296798798\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.07484477354213595\n",
      "Training Loss: 0.07155817860737443\n",
      "Training Loss: 0.0689566183462739\n",
      "Validation Loss: 0.0679061274682538\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06662943916395307\n",
      "Training Loss: 0.06400461327284575\n",
      "Training Loss: 0.061851162128150464\n",
      "Validation Loss: 0.06038019824982359\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.059263686332851645\n",
      "Training Loss: 0.05649226764217019\n",
      "Training Loss: 0.054268747474998234\n",
      "Validation Loss: 0.05220033981826868\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.05128563843667507\n",
      "Training Loss: 0.04842355522327125\n",
      "Training Loss: 0.046149573456496\n",
      "Validation Loss: 0.043647680243247014\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.04291605900973081\n",
      "Training Loss: 0.039996406864374875\n",
      "Training Loss: 0.03768704945221543\n",
      "Validation Loss: 0.034992661902660066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.034472235590219495\n",
      "Training Loss: 0.03186762892641127\n",
      "Training Loss: 0.030033890837803483\n",
      "Validation Loss: 0.027795594776823616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.027694487581029534\n",
      "Training Loss: 0.0259231902519241\n",
      "Training Loss: 0.024894393659196795\n",
      "Validation Loss: 0.023126143765939255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.023374404474161566\n",
      "Training Loss: 0.022209608904086055\n",
      "Training Loss: 0.021690782480873166\n",
      "Validation Loss: 0.02005350692563931\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.020524876499548553\n",
      "Training Loss: 0.01970218001399189\n",
      "Training Loss: 0.019474642768036576\n",
      "Validation Loss: 0.017806752890955364\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.018440254118759186\n",
      "Training Loss: 0.01782855136319995\n",
      "Training Loss: 0.017788117974996565\n",
      "Validation Loss: 0.016026291420704192\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.016799363114405424\n",
      "Training Loss: 0.016329779482912272\n",
      "Training Loss: 0.016423323301132768\n",
      "Validation Loss: 0.014540305902679147\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.015446111476048827\n",
      "Training Loss: 0.01507849144283682\n",
      "Training Loss: 0.015276175229810179\n",
      "Validation Loss: 0.013265038696588592\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.014299977112095803\n",
      "Training Loss: 0.01401053775800392\n",
      "Training Loss: 0.01429328195285052\n",
      "Validation Loss: 0.012164391420279325\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.01332036631880328\n",
      "Training Loss: 0.013094035780522973\n",
      "Training Loss: 0.01344538913341239\n",
      "Validation Loss: 0.011221434981195946\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.012484764805994928\n",
      "Training Loss: 0.012310489621013403\n",
      "Training Loss: 0.012713415063917637\n",
      "Validation Loss: 0.010422647352529209\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.011778526396956295\n",
      "Training Loss: 0.011647555673262104\n",
      "Training Loss: 0.012084451857954264\n",
      "Validation Loss: 0.009754665524259293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.011190248415805399\n",
      "Training Loss: 0.011094215782359243\n",
      "Training Loss: 0.011547782733105123\n",
      "Validation Loss: 0.009201863614496974\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.010706955546047538\n",
      "Training Loss: 0.010637180749326944\n",
      "Training Loss: 0.011092406618408858\n",
      "Validation Loss: 0.008747177341105312\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.010313845925265923\n",
      "Training Loss: 0.010262363328365609\n",
      "Training Loss: 0.010708180870860815\n",
      "Validation Loss: 0.008375080638523266\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.00999677571700886\n",
      "Training Loss: 0.009957285977434368\n",
      "Training Loss: 0.010387005992233753\n",
      "Validation Loss: 0.008072719566544958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.009743487305240706\n",
      "Training Loss: 0.009711459955433384\n",
      "Training Loss: 0.010122109963558614\n",
      "Validation Loss: 0.00782879119545347\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.009542999707628041\n",
      "Training Loss: 0.009515181760070846\n",
      "Training Loss: 0.009906328511424362\n",
      "Validation Loss: 0.007632177154366137\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.00938468607957475\n",
      "Training Loss: 0.009358642617007718\n",
      "Training Loss: 0.009731351195368916\n",
      "Validation Loss: 0.007472077474845678\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.00925847074831836\n",
      "Training Loss: 0.009232466005487367\n",
      "Training Loss: 0.009588580100098625\n",
      "Validation Loss: 0.0073392363670648315\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.00915582457673736\n",
      "Training Loss: 0.00912881461554207\n",
      "Training Loss: 0.009470456215785817\n",
      "Validation Loss: 0.007226774781638903\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.009070391519926488\n",
      "Training Loss: 0.009041890437947587\n",
      "Training Loss: 0.009371084428858012\n",
      "Validation Loss: 0.007129980736474893\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.00899777743848972\n",
      "Training Loss: 0.00896764506585896\n",
      "Training Loss: 0.009286112306872383\n",
      "Validation Loss: 0.007045586887757514\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008934948004316538\n",
      "Training Loss: 0.008903217561310158\n",
      "Training Loss: 0.009212322793900966\n",
      "Validation Loss: 0.006971181626693251\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008879719371907412\n",
      "Training Loss: 0.008846505971159786\n",
      "Training Loss: 0.009147312538698316\n",
      "Validation Loss: 0.006904891431802528\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008830463404301554\n",
      "Training Loss: 0.00879592458368279\n",
      "Training Loss: 0.009089262409834192\n",
      "Validation Loss: 0.006845234163377559\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008785949776647612\n",
      "Training Loss: 0.008750268664443865\n",
      "Training Loss: 0.0090367917122785\n",
      "Validation Loss: 0.006791039154174204\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.00874524333863519\n",
      "Training Loss: 0.008708617166848853\n",
      "Training Loss: 0.008988847425207495\n",
      "Validation Loss: 0.006741374897184583\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008707634841557592\n",
      "Training Loss: 0.008670265519758686\n",
      "Training Loss: 0.00894461895339191\n",
      "Validation Loss: 0.006695505834624088\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008672581813298165\n",
      "Training Loss: 0.008634668717859314\n",
      "Training Loss: 0.00890347723267041\n",
      "Validation Loss: 0.0066528436021481676\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008639667422976345\n",
      "Training Loss: 0.00860140772187151\n",
      "Training Loss: 0.008864927925169469\n",
      "Validation Loss: 0.0066129137144497275\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008608567889314144\n",
      "Training Loss: 0.008570145628182218\n",
      "Training Loss: 0.008828577003441751\n",
      "Validation Loss: 0.006575322159293914\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008579025892540813\n",
      "Training Loss: 0.008540615893434734\n",
      "Training Loss: 0.008794108387082816\n",
      "Validation Loss: 0.006539752237299939\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008550839883973822\n",
      "Training Loss: 0.00851260127616115\n",
      "Training Loss: 0.008761264227796346\n",
      "Validation Loss: 0.0065059367988488815\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008523848652839661\n",
      "Training Loss: 0.008485925962449983\n",
      "Training Loss: 0.008729838273720815\n",
      "Validation Loss: 0.00647364981116706\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008497920663794503\n",
      "Training Loss: 0.008460443424992263\n",
      "Training Loss: 0.008699662009021267\n",
      "Validation Loss: 0.006442717267357315\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008472959020873532\n",
      "Training Loss: 0.00843603687477298\n",
      "Training Loss: 0.008670605064835399\n",
      "Validation Loss: 0.006412989381449611\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008448888960992917\n",
      "Training Loss: 0.00841261486755684\n",
      "Training Loss: 0.008642569014336915\n",
      "Validation Loss: 0.006384357554953169\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.008425658862106502\n",
      "Training Loss: 0.008390106083825231\n",
      "Training Loss: 0.008615483532194048\n",
      "Validation Loss: 0.0063567355241145145\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.00840323930955492\n",
      "Training Loss: 0.008368455469608307\n",
      "Training Loss: 0.008589303528424352\n",
      "Validation Loss: 0.0063300671283511465\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.008381614605896174\n",
      "Training Loss: 0.008347626402974128\n",
      "Training Loss: 0.008564002746716141\n",
      "Validation Loss: 0.006304310590258978\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.00836078357999213\n",
      "Training Loss: 0.008327590425033123\n",
      "Training Loss: 0.00853957180515863\n",
      "Validation Loss: 0.006279446531265053\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.008340753458905965\n",
      "Training Loss: 0.008308328753337264\n",
      "Training Loss: 0.008516012853942812\n",
      "Validation Loss: 0.006255458485795541\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.008321534960996359\n",
      "Training Loss: 0.008289825655519962\n",
      "Training Loss: 0.008493329972261564\n",
      "Validation Loss: 0.006232340889459664\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.008303136066533625\n",
      "Training Loss: 0.008272066843928769\n",
      "Training Loss: 0.008471529940143227\n",
      "Validation Loss: 0.006210089301470709\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.008285562501987443\n",
      "Training Loss: 0.00825503671891056\n",
      "Training Loss: 0.00845061490079388\n",
      "Validation Loss: 0.00618869792722333\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.008268812590977177\n",
      "Training Loss: 0.008238715952029452\n",
      "Training Loss: 0.0084305819531437\n",
      "Validation Loss: 0.006168156761540037\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.008252876602346077\n",
      "Training Loss: 0.00822308357222937\n",
      "Training Loss: 0.008411418900359423\n",
      "Validation Loss: 0.006148451871207256\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.008237735866568983\n",
      "Training Loss: 0.008208114560693503\n",
      "Training Loss: 0.008393109780736268\n",
      "Validation Loss: 0.006129567839697087\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.008223365703597664\n",
      "Training Loss: 0.008193784507457167\n",
      "Training Loss: 0.00837562917615287\n",
      "Validation Loss: 0.00611148808763645\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.008209736555581913\n",
      "Training Loss: 0.008180062954779714\n",
      "Training Loss: 0.008358949994435533\n",
      "Validation Loss: 0.006094186654408577\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.008196813074173406\n",
      "Training Loss: 0.008166921277297661\n",
      "Training Loss: 0.008343038145685568\n",
      "Validation Loss: 0.006077638081683034\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.008184556858614087\n",
      "Training Loss: 0.00815433167736046\n",
      "Training Loss: 0.008327858611010016\n",
      "Validation Loss: 0.006061818854872849\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.00817292919382453\n",
      "Training Loss: 0.008142267601797357\n",
      "Training Loss: 0.008313375370344147\n",
      "Validation Loss: 0.0060467025006106226\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.008161892683710903\n",
      "Training Loss: 0.008130701683694497\n",
      "Training Loss: 0.008299555475823582\n",
      "Validation Loss: 0.006032263288494158\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.008151410701684654\n",
      "Training Loss: 0.008119610153371468\n",
      "Training Loss: 0.008286362251965329\n",
      "Validation Loss: 0.006018470004828793\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.008141445037908852\n",
      "Training Loss: 0.008108966955915094\n",
      "Training Loss: 0.008273763102479278\n",
      "Validation Loss: 0.006005297439811186\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.00813196662464179\n",
      "Training Loss: 0.008098752880468964\n",
      "Training Loss: 0.008261726243654266\n",
      "Validation Loss: 0.0059927203627093955\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.008122940303292126\n",
      "Training Loss: 0.00808894531102851\n",
      "Training Loss: 0.008250221499474719\n",
      "Validation Loss: 0.005980713574184377\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.00811433799448423\n",
      "Training Loss: 0.008079525731736793\n",
      "Training Loss: 0.008239221810363233\n",
      "Validation Loss: 0.005969251889285495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.008106133460532874\n",
      "Training Loss: 0.008070473416009917\n",
      "Training Loss: 0.008228699378669262\n",
      "Validation Loss: 0.005958308018430063\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.008098300263518467\n",
      "Training Loss: 0.008061771397478878\n",
      "Training Loss: 0.008218627722235396\n",
      "Validation Loss: 0.005947857293342188\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.008090815404430032\n",
      "Training Loss: 0.008053402857622132\n",
      "Training Loss: 0.008208985179662705\n",
      "Validation Loss: 0.005937877829249404\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.008083657327806576\n",
      "Training Loss: 0.008045350682223215\n",
      "Training Loss: 0.008199747339822351\n",
      "Validation Loss: 0.005928344037017545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.008076804799493402\n",
      "Training Loss: 0.008037596885114908\n",
      "Training Loss: 0.008190892926650121\n",
      "Validation Loss: 0.00591923233665777\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.008070238427026198\n",
      "Training Loss: 0.008030129028484226\n",
      "Training Loss: 0.008182402591919526\n",
      "Validation Loss: 0.005910521987930275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.008063939990242942\n",
      "Training Loss: 0.008022930122679099\n",
      "Training Loss: 0.008174254107289017\n",
      "Validation Loss: 0.005902190224445435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.008057893587974832\n",
      "Training Loss: 0.008015987743856385\n",
      "Training Loss: 0.008166430330602452\n",
      "Validation Loss: 0.005894216406682318\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.008052081831265242\n",
      "Training Loss: 0.008009286398300902\n",
      "Training Loss: 0.008158913810038939\n",
      "Validation Loss: 0.005886580124026521\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.008046490962151438\n",
      "Training Loss: 0.008002814395586028\n",
      "Training Loss: 0.008151687510544434\n",
      "Validation Loss: 0.005879267292840176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.00804110691184178\n",
      "Training Loss: 0.007996558398008346\n",
      "Training Loss: 0.008144735725363717\n",
      "Validation Loss: 0.005872253650338964\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.008035916567314417\n",
      "Training Loss: 0.007990507476497441\n",
      "Training Loss: 0.008138041438069194\n",
      "Validation Loss: 0.005865522407863834\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.008030907126376406\n",
      "Training Loss: 0.007984649557620287\n",
      "Training Loss: 0.008131591930286959\n",
      "Validation Loss: 0.005859060054697263\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.008026065499288961\n",
      "Training Loss: 0.007978974774014205\n",
      "Training Loss: 0.008125372701324523\n",
      "Validation Loss: 0.005852846144907846\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.008021383372833953\n",
      "Training Loss: 0.007973471301374957\n",
      "Training Loss: 0.00811937048099935\n",
      "Validation Loss: 0.0058468692645618926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.00801684801466763\n",
      "Training Loss: 0.007968130714725703\n",
      "Training Loss: 0.008113573238952086\n",
      "Validation Loss: 0.0058411149912398685\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.008012451332760975\n",
      "Training Loss: 0.007962942541344092\n",
      "Training Loss: 0.008107970047276467\n",
      "Validation Loss: 0.005835569924099392\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.008008182035991922\n",
      "Training Loss: 0.007957899215398356\n",
      "Training Loss: 0.00810254753101617\n",
      "Validation Loss: 0.0058302186950099436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.008004032962489874\n",
      "Training Loss: 0.007952993339858948\n",
      "Training Loss: 0.008097297686617821\n",
      "Validation Loss: 0.005825051981589517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007999997159931809\n",
      "Training Loss: 0.007948214208008722\n",
      "Training Loss: 0.008092208739835769\n",
      "Validation Loss: 0.005820058958521218\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007996065304614603\n",
      "Training Loss: 0.007943557916441933\n",
      "Training Loss: 0.008087272471748293\n",
      "Validation Loss: 0.005815225632421756\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007992230927338823\n",
      "Training Loss: 0.007939014254370705\n",
      "Training Loss: 0.008082479471340775\n",
      "Validation Loss: 0.0058105426730609006\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007988487854599952\n",
      "Training Loss: 0.00793457796331495\n",
      "Training Loss: 0.008077821779297665\n",
      "Validation Loss: 0.005806003638354831\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007984828293556347\n",
      "Training Loss: 0.007930242890724913\n",
      "Training Loss: 0.008073291555047036\n",
      "Validation Loss: 0.005801597662437498\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007981248181313277\n",
      "Training Loss: 0.007926004596520216\n",
      "Training Loss: 0.00806888004182838\n",
      "Validation Loss: 0.005797317259411296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007977742092916741\n",
      "Training Loss: 0.007921854485757648\n",
      "Training Loss: 0.008064582750666887\n",
      "Validation Loss: 0.005793154962999181\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007974303452065214\n",
      "Training Loss: 0.007917791042709723\n",
      "Training Loss: 0.008060392010957002\n",
      "Validation Loss: 0.005789101078028508\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007970928926952184\n",
      "Training Loss: 0.007913806332508102\n",
      "Training Loss: 0.00805630131973885\n",
      "Validation Loss: 0.005785149630692819\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007967613718938083\n",
      "Training Loss: 0.007909896896453574\n",
      "Training Loss: 0.008052305042510852\n",
      "Validation Loss: 0.005781295006895919\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007964353102724999\n",
      "Training Loss: 0.007906060155946762\n",
      "Training Loss: 0.008048397335223854\n",
      "Validation Loss: 0.005777533296914248\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007961144350701943\n",
      "Training Loss: 0.007902289786143228\n",
      "Training Loss: 0.008044573841616512\n",
      "Validation Loss: 0.005773854901061885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.00795798318227753\n",
      "Training Loss: 0.007898583229398355\n",
      "Training Loss: 0.008040830211248249\n",
      "Validation Loss: 0.005770258495605059\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007954865937354044\n",
      "Training Loss: 0.00789493620977737\n",
      "Training Loss: 0.0080371606326662\n",
      "Validation Loss: 0.005766733885700866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007951789858052507\n",
      "Training Loss: 0.007891346395481377\n",
      "Training Loss: 0.008033562627388165\n",
      "Validation Loss: 0.005763279598499282\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007948753212112934\n",
      "Training Loss: 0.00788780941395089\n",
      "Training Loss: 0.008030029549263417\n",
      "Validation Loss: 0.005759893908699075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.00794575194711797\n",
      "Training Loss: 0.00788432348286733\n",
      "Training Loss: 0.008026559677673503\n",
      "Validation Loss: 0.005756569165589936\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007942783253965899\n",
      "Training Loss: 0.007880884831538424\n",
      "Training Loss: 0.008023149520158767\n",
      "Validation Loss: 0.005753302428703965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007939846109366044\n",
      "Training Loss: 0.007877491256222129\n",
      "Training Loss: 0.008019794791471213\n",
      "Validation Loss: 0.0057500905731399906\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007936937493504957\n",
      "Training Loss: 0.00787414082675241\n",
      "Training Loss: 0.008016493860632181\n",
      "Validation Loss: 0.005746931748025203\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007934055412188173\n",
      "Training Loss: 0.007870831898180768\n",
      "Training Loss: 0.008013241511071101\n",
      "Validation Loss: 0.005743819296757659\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007931198166916146\n",
      "Training Loss: 0.007867559714941307\n",
      "Training Loss: 0.008010036052437499\n",
      "Validation Loss: 0.00574075342469922\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007928363074315712\n",
      "Training Loss: 0.00786432474385947\n",
      "Training Loss: 0.008006876116851344\n",
      "Validation Loss: 0.005737730481117712\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007925551288062707\n",
      "Training Loss: 0.007861124399350956\n",
      "Training Loss: 0.008003758164122701\n",
      "Validation Loss: 0.005734746787812184\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.007922758983913808\n",
      "Training Loss: 0.007857957099331543\n",
      "Training Loss: 0.008000680141849443\n",
      "Validation Loss: 0.005731803002725407\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007919985117623582\n",
      "Training Loss: 0.007854819905478507\n",
      "Training Loss: 0.007997640580870212\n",
      "Validation Loss: 0.005728892466639367\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.007917229331796989\n",
      "Training Loss: 0.007851713158888742\n",
      "Training Loss: 0.007994636367075146\n",
      "Validation Loss: 0.005726015643907313\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.007914489480899647\n",
      "Training Loss: 0.0078486343100667\n",
      "Training Loss: 0.007991665217559785\n",
      "Validation Loss: 0.005723171362527803\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.007911764348391444\n",
      "Training Loss: 0.007845582197187468\n",
      "Training Loss: 0.00798872669809498\n",
      "Validation Loss: 0.005720357749391389\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.00790905333007686\n",
      "Training Loss: 0.007842555373208597\n",
      "Training Loss: 0.007985818550223485\n",
      "Validation Loss: 0.0057175722185082815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.00790635590441525\n",
      "Training Loss: 0.007839552180375903\n",
      "Training Loss: 0.007982939332723617\n",
      "Validation Loss: 0.005714810360484662\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007903669610386714\n",
      "Training Loss: 0.007836571668740362\n",
      "Training Loss: 0.007980087214382365\n",
      "Validation Loss: 0.00571207582212859\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007900996052194386\n",
      "Training Loss: 0.00783361453213729\n",
      "Training Loss: 0.007977261452469974\n",
      "Validation Loss: 0.005709363169853021\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.00789833296672441\n",
      "Training Loss: 0.007830676642479375\n",
      "Training Loss: 0.007974460131954401\n",
      "Validation Loss: 0.0057066717417910695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.007895679434295744\n",
      "Training Loss: 0.007827759083593264\n",
      "Training Loss: 0.00797168163000606\n",
      "Validation Loss: 0.005704003482994237\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.00789303524652496\n",
      "Training Loss: 0.007824861267581583\n",
      "Training Loss: 0.007968925408786163\n",
      "Validation Loss: 0.005701354328082518\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.007890400150790811\n",
      "Training Loss: 0.007821979915024712\n",
      "Training Loss: 0.007966189712751657\n",
      "Validation Loss: 0.005698722988639152\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.007887772547546775\n",
      "Training Loss: 0.00781911657191813\n",
      "Training Loss: 0.007963474437128753\n",
      "Validation Loss: 0.0056961074803715175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007885152822127565\n",
      "Training Loss: 0.007816270176554098\n",
      "Training Loss: 0.007960778655251488\n",
      "Validation Loss: 0.005693510860162839\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.007882540302816779\n",
      "Training Loss: 0.007813438518205658\n",
      "Training Loss: 0.007958099512616173\n",
      "Validation Loss: 0.005690928139158765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.007879934902302922\n",
      "Training Loss: 0.007810622205724939\n",
      "Training Loss: 0.007955438072094694\n",
      "Validation Loss: 0.0056883591067652855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.007877335391240194\n",
      "Training Loss: 0.007807820357847959\n",
      "Training Loss: 0.007952793405856938\n",
      "Validation Loss: 0.005685802864360759\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.007874741238774732\n",
      "Training Loss: 0.007805032865144312\n",
      "Training Loss: 0.007950163808418438\n",
      "Validation Loss: 0.00568326190244825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0078721533017233\n",
      "Training Loss: 0.007802257601870224\n",
      "Training Loss: 0.007947548117954284\n",
      "Validation Loss: 0.005680731941129636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.007869570686016231\n",
      "Training Loss: 0.007799495231593028\n",
      "Training Loss: 0.007944946326315403\n",
      "Validation Loss: 0.005678212204738782\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.007866991854971275\n",
      "Training Loss: 0.007796745207160711\n",
      "Training Loss: 0.007942358228610828\n",
      "Validation Loss: 0.0056757047625907355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007864418406970799\n",
      "Training Loss: 0.007794005948817358\n",
      "Training Loss: 0.007939782891189679\n",
      "Validation Loss: 0.005673206281806394\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007861849070759491\n",
      "Training Loss: 0.007791277541546151\n",
      "Training Loss: 0.00793721932452172\n",
      "Validation Loss: 0.00567071771594497\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.007859283902216702\n",
      "Training Loss: 0.007788559254258871\n",
      "Training Loss: 0.007934666192159056\n",
      "Validation Loss: 0.005668237360633826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.007856722541619092\n",
      "Training Loss: 0.007785852222004905\n",
      "Training Loss: 0.007932124990038574\n",
      "Validation Loss: 0.0056657656383690205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.007854164300952107\n",
      "Training Loss: 0.007783154515782371\n",
      "Training Loss: 0.007929593531880528\n",
      "Validation Loss: 0.005663301735551337\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.007851610080106184\n",
      "Training Loss: 0.007780465906253084\n",
      "Training Loss: 0.007927071745507419\n",
      "Validation Loss: 0.0056608452205284595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.007849058228312061\n",
      "Training Loss: 0.007777785846265033\n",
      "Training Loss: 0.00792455974733457\n",
      "Validation Loss: 0.005658394536735971\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00784650861285627\n",
      "Training Loss: 0.007775114479009062\n",
      "Training Loss: 0.007922056142706423\n",
      "Validation Loss: 0.005655951665850419\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.007843963073100894\n",
      "Training Loss: 0.007772451308555901\n",
      "Training Loss: 0.007919560462469236\n",
      "Validation Loss: 0.005653515030378789\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.007841419664910063\n",
      "Training Loss: 0.007769795388448983\n",
      "Training Loss: 0.00791707263677381\n",
      "Validation Loss: 0.005651082629023978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00783887863624841\n",
      "Training Loss: 0.00776714822743088\n",
      "Training Loss: 0.007914592045126482\n",
      "Validation Loss: 0.005648654490562804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.007836340486537665\n",
      "Training Loss: 0.00776450737263076\n",
      "Training Loss: 0.007912118873791769\n",
      "Validation Loss: 0.005646232913216848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.007833804230904207\n",
      "Training Loss: 0.007761872658738867\n",
      "Training Loss: 0.007909652256639674\n",
      "Validation Loss: 0.005643813206364264\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.007831269815796986\n",
      "Training Loss: 0.00775924525456503\n",
      "Training Loss: 0.007907191739650443\n",
      "Validation Loss: 0.005641400642703507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.007828736283117905\n",
      "Training Loss: 0.007756623891182244\n",
      "Training Loss: 0.007904736929340288\n",
      "Validation Loss: 0.005638989865821734\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.00782620630809106\n",
      "Training Loss: 0.007754008668707684\n",
      "Training Loss: 0.007902288094628602\n",
      "Validation Loss: 0.005636583078453799\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.007823676012922078\n",
      "Training Loss: 0.007751399255357683\n",
      "Training Loss: 0.007899844221537933\n",
      "Validation Loss: 0.005634180268827366\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.007821148351067677\n",
      "Training Loss: 0.007748795754741878\n",
      "Training Loss: 0.007897405507974327\n",
      "Validation Loss: 0.005631778204782206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.007818622442428022\n",
      "Training Loss: 0.007746196605730802\n",
      "Training Loss: 0.007894972116919235\n",
      "Validation Loss: 0.005629379215932796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.007816097276518122\n",
      "Training Loss: 0.007743603381095454\n",
      "Training Loss: 0.007892543002963066\n",
      "Validation Loss: 0.005626984696646922\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.00781357238884084\n",
      "Training Loss: 0.007741014653583988\n",
      "Training Loss: 0.007890117955394089\n",
      "Validation Loss: 0.005624591766626396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.007811049568699673\n",
      "Training Loss: 0.0077384309272747484\n",
      "Training Loss: 0.007887697153491899\n",
      "Validation Loss: 0.005622200887608394\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0078085277101490646\n",
      "Training Loss: 0.007735850682947785\n",
      "Training Loss: 0.007885278461035341\n",
      "Validation Loss: 0.005619810899363809\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.007806006419705227\n",
      "Training Loss: 0.007733275338541717\n",
      "Training Loss: 0.007882863635895774\n",
      "Validation Loss: 0.005617422950349413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.007803486777702346\n",
      "Training Loss: 0.007730703399283811\n",
      "Training Loss: 0.007880452703684569\n",
      "Validation Loss: 0.005615035489232939\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.007800967058865353\n",
      "Training Loss: 0.007728135860525072\n",
      "Training Loss: 0.007878044397803024\n",
      "Validation Loss: 0.00561265074752606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.007798448051325977\n",
      "Training Loss: 0.007725572373019531\n",
      "Training Loss: 0.007875639764824882\n",
      "Validation Loss: 0.00561026705748119\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.007795930081047117\n",
      "Training Loss: 0.007723013430368155\n",
      "Training Loss: 0.0078732371609658\n",
      "Validation Loss: 0.005607884065928251\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.007793413597391918\n",
      "Training Loss: 0.0077204563654959205\n",
      "Training Loss: 0.007870836596703156\n",
      "Validation Loss: 0.005605501197330821\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.007790896838996559\n",
      "Training Loss: 0.007717903237789869\n",
      "Training Loss: 0.007868438524892553\n",
      "Validation Loss: 0.005603120646575529\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.007788381258724258\n",
      "Training Loss: 0.007715353479143232\n",
      "Training Loss: 0.007866043160902336\n",
      "Validation Loss: 0.005600740611186942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.0077858651895076035\n",
      "Training Loss: 0.007712807197822258\n",
      "Training Loss: 0.007863649583887309\n",
      "Validation Loss: 0.005598360229168464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.007783349928213283\n",
      "Training Loss: 0.007710263504413888\n",
      "Training Loss: 0.007861257086042316\n",
      "Validation Loss: 0.005595980253949594\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.007780834978912026\n",
      "Training Loss: 0.00770772245596163\n",
      "Training Loss: 0.007858866782626137\n",
      "Validation Loss: 0.005593600147926992\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.0077783200412523\n",
      "Training Loss: 0.0077051847928669304\n",
      "Training Loss: 0.007856477932073176\n",
      "Validation Loss: 0.005591221187745085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.007775806545978412\n",
      "Training Loss: 0.0077026495768222954\n",
      "Training Loss: 0.00785409040050581\n",
      "Validation Loss: 0.005588841185057431\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.0077732921484857795\n",
      "Training Loss: 0.0077001163177192215\n",
      "Training Loss: 0.007851704363711179\n",
      "Validation Loss: 0.005586462784715499\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.007770778860431165\n",
      "Training Loss: 0.00769758655806072\n",
      "Training Loss: 0.007849319559754804\n",
      "Validation Loss: 0.005584083225452498\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.00776826526154764\n",
      "Training Loss: 0.00769505875883624\n",
      "Training Loss: 0.007846935008419677\n",
      "Validation Loss: 0.005581704082145366\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.007765751957194879\n",
      "Training Loss: 0.0076925326371565465\n",
      "Training Loss: 0.00784455205197446\n",
      "Validation Loss: 0.00557932385578333\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.007763238302432001\n",
      "Training Loss: 0.007690010025398806\n",
      "Training Loss: 0.007842169230571017\n",
      "Validation Loss: 0.0055769450290212306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0077607260446529835\n",
      "Training Loss: 0.007687488258816302\n",
      "Training Loss: 0.00783978747553192\n",
      "Validation Loss: 0.005574565240851698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.007758213084889576\n",
      "Training Loss: 0.007684969020774588\n",
      "Training Loss: 0.007837406446924433\n",
      "Validation Loss: 0.005572185319262358\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.007755700007546693\n",
      "Training Loss: 0.007682452481240034\n",
      "Training Loss: 0.007835025580134243\n",
      "Validation Loss: 0.005569805044502941\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.0077531872468534855\n",
      "Training Loss: 0.0076799367007333785\n",
      "Training Loss: 0.00783264626050368\n",
      "Validation Loss: 0.00556742446366279\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.007750673835398629\n",
      "Training Loss: 0.007677423374261707\n",
      "Training Loss: 0.007830266709206626\n",
      "Validation Loss: 0.005565042670272039\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.007748161546187475\n",
      "Training Loss: 0.007674912170041352\n",
      "Training Loss: 0.007827887093881145\n",
      "Validation Loss: 0.005562661509971354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.007745648763375357\n",
      "Training Loss: 0.007672402301104739\n",
      "Training Loss: 0.007825507955858483\n",
      "Validation Loss: 0.005560280038357786\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.007743135421769694\n",
      "Training Loss: 0.007669894493883476\n",
      "Training Loss: 0.007823128560557962\n",
      "Validation Loss: 0.005557897159296056\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.007740623082499951\n",
      "Training Loss: 0.0076673886459320785\n",
      "Training Loss: 0.00782075053662993\n",
      "Validation Loss: 0.005555514365256754\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.007738109871279448\n",
      "Training Loss: 0.007664884536061436\n",
      "Training Loss: 0.007818371437024326\n",
      "Validation Loss: 0.005553131113404387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.007735596433049068\n",
      "Training Loss: 0.007662381610134617\n",
      "Training Loss: 0.00781599326292053\n",
      "Validation Loss: 0.005550748557427877\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.00773308391449973\n",
      "Training Loss: 0.007659880389692262\n",
      "Training Loss: 0.007813615087652579\n",
      "Validation Loss: 0.0055483641348821056\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.007730570916319266\n",
      "Training Loss: 0.007657380707096309\n",
      "Training Loss: 0.007811236493289471\n",
      "Validation Loss: 0.005545979551447744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.007728058786597103\n",
      "Training Loss: 0.00765488262870349\n",
      "Training Loss: 0.007808857788331807\n",
      "Validation Loss: 0.0055435941766508005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.007725546120200306\n",
      "Training Loss: 0.007652387025300414\n",
      "Training Loss: 0.007806479603750631\n",
      "Validation Loss: 0.005541209450640371\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.00772303365287371\n",
      "Training Loss: 0.007649892596527934\n",
      "Training Loss: 0.007804100679932162\n",
      "Validation Loss: 0.005538824859357784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.007720520369475708\n",
      "Training Loss: 0.007647399888373912\n",
      "Training Loss: 0.0078017218329478055\n",
      "Validation Loss: 0.005536439767096903\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.007718007044168189\n",
      "Training Loss: 0.00764490850851871\n",
      "Training Loss: 0.007799341960344463\n",
      "Validation Loss: 0.005534053468825609\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.00771549443132244\n",
      "Training Loss: 0.007642418224131689\n",
      "Training Loss: 0.00779696291545406\n",
      "Validation Loss: 0.005531667085531889\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.007712982126977294\n",
      "Training Loss: 0.0076399305532686415\n",
      "Training Loss: 0.007794584279181436\n",
      "Validation Loss: 0.005529281292163003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.007710469559533521\n",
      "Training Loss: 0.0076374428044073284\n",
      "Training Loss: 0.00779220464406535\n",
      "Validation Loss: 0.0055268939278412905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.007707957642851398\n",
      "Training Loss: 0.007634957544505596\n",
      "Training Loss: 0.007789825616637245\n",
      "Validation Loss: 0.005524508192026046\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.0077054449217394\n",
      "Training Loss: 0.007632474047131837\n",
      "Training Loss: 0.007787445571739227\n",
      "Validation Loss: 0.005522120436601174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.007702933029504493\n",
      "Training Loss: 0.007629992024740204\n",
      "Training Loss: 0.007785065671196207\n",
      "Validation Loss: 0.0055197352894027244\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.007700421429472044\n",
      "Training Loss: 0.007627511635655537\n",
      "Training Loss: 0.007782685698475689\n",
      "Validation Loss: 0.005517348082045491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.0076979099586606025\n",
      "Training Loss: 0.0076250325480941685\n",
      "Training Loss: 0.00778030558838509\n",
      "Validation Loss: 0.005514961884493071\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0076953985600266605\n",
      "Training Loss: 0.007622554829576984\n",
      "Training Loss: 0.007777925115078688\n",
      "Validation Loss: 0.005512574401139962\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.007692887663142756\n",
      "Training Loss: 0.007620079285698011\n",
      "Training Loss: 0.0077755439374595885\n",
      "Validation Loss: 0.005510189155838714\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.007690376220270991\n",
      "Training Loss: 0.00761760534835048\n",
      "Training Loss: 0.0077731641347054395\n",
      "Validation Loss: 0.005507804966123586\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.007687865659827367\n",
      "Training Loss: 0.007615133695071563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [12:19<12:19, 147.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.007770784315653145\n",
      "Validation Loss: 0.005505418976549101\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 1.0002240473031998\n",
      "Training Loss: 0.8522652235627174\n",
      "Training Loss: 0.7307652314007282\n",
      "Validation Loss: 0.5709116078829497\n",
      "Validation Accuracy: 0.5442415730337079\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.46401362389326095\n",
      "Training Loss: 0.274301274381578\n",
      "Training Loss: 0.13017958341166377\n",
      "Validation Loss: 0.07572540065294571\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06876192124560475\n",
      "Training Loss: 0.06344868117943406\n",
      "Training Loss: 0.06012093974277377\n",
      "Validation Loss: 0.059121205021491215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.056453688740730285\n",
      "Training Loss: 0.053046414274722335\n",
      "Training Loss: 0.050149201452732085\n",
      "Validation Loss: 0.048788323224093134\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.0468870345223695\n",
      "Training Loss: 0.043628278551623226\n",
      "Training Loss: 0.04125924010761082\n",
      "Validation Loss: 0.03972488753706886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.03858162158168852\n",
      "Training Loss: 0.0357429654058069\n",
      "Training Loss: 0.03408808725886047\n",
      "Validation Loss: 0.032627020816036155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.03203346220776439\n",
      "Training Loss: 0.029710767031647264\n",
      "Training Loss: 0.028651619469746946\n",
      "Validation Loss: 0.027274301019235607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.027039919001981615\n",
      "Training Loss: 0.025189578952267765\n",
      "Training Loss: 0.024552947166375814\n",
      "Validation Loss: 0.023159644775762316\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.02317733237054199\n",
      "Training Loss: 0.02169973641168326\n",
      "Training Loss: 0.02134570276364684\n",
      "Validation Loss: 0.01981084586089749\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.0200219313101843\n",
      "Training Loss: 0.01881605023983866\n",
      "Training Loss: 0.01865286611719057\n",
      "Validation Loss: 0.016857441975159593\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.017241557105444373\n",
      "Training Loss: 0.016273097298108042\n",
      "Training Loss: 0.016321202570106833\n",
      "Validation Loss: 0.014291677443478047\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.014910905223805457\n",
      "Training Loss: 0.014342174876946955\n",
      "Training Loss: 0.01471576478332281\n",
      "Validation Loss: 0.012657030799499388\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.013490342383738608\n",
      "Training Loss: 0.013223955647554249\n",
      "Training Loss: 0.01370178945362568\n",
      "Validation Loss: 0.011610579200800551\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.0125732035911642\n",
      "Training Loss: 0.012432376816868783\n",
      "Training Loss: 0.012911081991624086\n",
      "Validation Loss: 0.01079516824376717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.011872654734179377\n",
      "Training Loss: 0.011807035780511796\n",
      "Training Loss: 0.012270783628337086\n",
      "Validation Loss: 0.010132723549391362\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.011319867796264588\n",
      "Training Loss: 0.011306272500660271\n",
      "Training Loss: 0.011751489927992225\n",
      "Validation Loss: 0.009591361412286675\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.010881680431775748\n",
      "Training Loss: 0.010905233037192374\n",
      "Training Loss: 0.011330890145618468\n",
      "Validation Loss: 0.009148915332292071\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.010534063372761011\n",
      "Training Loss: 0.01058424602029845\n",
      "Training Loss: 0.010990262021077796\n",
      "Validation Loss: 0.008787170888364232\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.010257630506530405\n",
      "Training Loss: 0.010326812416315079\n",
      "Training Loss: 0.010713622253388167\n",
      "Validation Loss: 0.008490706312606174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.01003651175647974\n",
      "Training Loss: 0.010119120224844664\n",
      "Training Loss: 0.010487490312661975\n",
      "Validation Loss: 0.008246533006293636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.00985782568110153\n",
      "Training Loss: 0.009949797621229663\n",
      "Training Loss: 0.010300721019739285\n",
      "Validation Loss: 0.008043895292441162\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.009711315790191292\n",
      "Training Loss: 0.00980970100616105\n",
      "Training Loss: 0.010144321889383719\n",
      "Validation Loss: 0.007874062762260856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.009588983177673071\n",
      "Training Loss: 0.00969164512702264\n",
      "Training Loss: 0.010011204584734514\n",
      "Validation Loss: 0.007730058754082727\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.009484728227835148\n",
      "Training Loss: 0.009590110902208834\n",
      "Training Loss: 0.009895902497228236\n",
      "Validation Loss: 0.007606399507167634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.00939400099683553\n",
      "Training Loss: 0.009500950464280322\n",
      "Training Loss: 0.009794272701255977\n",
      "Validation Loss: 0.007498812046833336\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.009313476477982476\n",
      "Training Loss: 0.009421112993732095\n",
      "Training Loss: 0.00970324686379172\n",
      "Validation Loss: 0.007404006868496202\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.009240778393577785\n",
      "Training Loss: 0.009348396220011636\n",
      "Training Loss: 0.009620565001387148\n",
      "Validation Loss: 0.007319461256056354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.009174230373464525\n",
      "Training Loss: 0.009281227512983605\n",
      "Training Loss: 0.00954459629370831\n",
      "Validation Loss: 0.007243244324057373\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.009112670933827758\n",
      "Training Loss: 0.009218511863145977\n",
      "Training Loss: 0.009474163163686171\n",
      "Validation Loss: 0.007173867832105397\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.009055298925377429\n",
      "Training Loss: 0.009159488894511014\n",
      "Training Loss: 0.009408416810911148\n",
      "Validation Loss: 0.007110186339764113\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.009001556138973682\n",
      "Training Loss: 0.009103632241021842\n",
      "Training Loss: 0.009346738660242408\n",
      "Validation Loss: 0.007051304550422879\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008951043484266848\n",
      "Training Loss: 0.009050564762437716\n",
      "Training Loss: 0.00928865282330662\n",
      "Validation Loss: 0.006996492309799271\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008903450692305342\n",
      "Training Loss: 0.009000006816349923\n",
      "Training Loss: 0.009233782839728519\n",
      "Validation Loss: 0.0069451598487166545\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008858521666843444\n",
      "Training Loss: 0.008951739795738831\n",
      "Training Loss: 0.009181811603484675\n",
      "Validation Loss: 0.006896803462930191\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008816021111560985\n",
      "Training Loss: 0.008905570384813473\n",
      "Training Loss: 0.009132453238125891\n",
      "Validation Loss: 0.006850997169156757\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.0087757223052904\n",
      "Training Loss: 0.008861321414588019\n",
      "Training Loss: 0.009085438462207093\n",
      "Validation Loss: 0.006807348234422003\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008737393133342266\n",
      "Training Loss: 0.008818817794090137\n",
      "Training Loss: 0.009040509804617614\n",
      "Validation Loss: 0.006765520586004334\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008700802859384566\n",
      "Training Loss: 0.008777884987648577\n",
      "Training Loss: 0.008997414914192632\n",
      "Validation Loss: 0.006725204701723761\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008665716545656323\n",
      "Training Loss: 0.008738348814658821\n",
      "Training Loss: 0.008955910393269732\n",
      "Validation Loss: 0.006686129052652402\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008631904326612129\n",
      "Training Loss: 0.00870003766962327\n",
      "Training Loss: 0.008915765132987872\n",
      "Validation Loss: 0.006648049848101866\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008599140995647759\n",
      "Training Loss: 0.008662782103056089\n",
      "Training Loss: 0.008876761100254952\n",
      "Validation Loss: 0.00661075161377575\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008567217841045931\n",
      "Training Loss: 0.008626425940310583\n",
      "Training Loss: 0.008838698084000498\n",
      "Validation Loss: 0.006574054310108755\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.008535942817106843\n",
      "Training Loss: 0.008590820786776022\n",
      "Training Loss: 0.008801395681221038\n",
      "Validation Loss: 0.006537809400306491\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.008505140361376107\n",
      "Training Loss: 0.00855583392083645\n",
      "Training Loss: 0.008764694643905386\n",
      "Validation Loss: 0.006501885303662399\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.008474657917395234\n",
      "Training Loss: 0.008521345328772441\n",
      "Training Loss: 0.008728454831289128\n",
      "Validation Loss: 0.006466190745332017\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.008444360035937279\n",
      "Training Loss: 0.008487248645396904\n",
      "Training Loss: 0.008692557346075774\n",
      "Validation Loss: 0.00643064439809473\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.008414134120102971\n",
      "Training Loss: 0.008453453246038406\n",
      "Training Loss: 0.008656905528623611\n",
      "Validation Loss: 0.006395193444711439\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.008383886945666745\n",
      "Training Loss: 0.00841988216037862\n",
      "Training Loss: 0.00862141759134829\n",
      "Validation Loss: 0.006359803666925749\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.008353540593525395\n",
      "Training Loss: 0.00838646933552809\n",
      "Training Loss: 0.008586028555873782\n",
      "Validation Loss: 0.006324453495202188\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.008323034287896007\n",
      "Training Loss: 0.008353160126134754\n",
      "Training Loss: 0.008550684741931037\n",
      "Validation Loss: 0.006289132821028319\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.008292319873580709\n",
      "Training Loss: 0.00831991161336191\n",
      "Training Loss: 0.008515350029338151\n",
      "Validation Loss: 0.006253846779758676\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.008261362173361703\n",
      "Training Loss: 0.008286687177605927\n",
      "Training Loss: 0.008479996321257203\n",
      "Validation Loss: 0.006218602123481923\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00823014230467379\n",
      "Training Loss: 0.008253464376321063\n",
      "Training Loss: 0.008444609849248082\n",
      "Validation Loss: 0.006183430072432907\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.008198652673745527\n",
      "Training Loss: 0.008220227006822824\n",
      "Training Loss: 0.008409189693629741\n",
      "Validation Loss: 0.006148353691233762\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.008166900869691745\n",
      "Training Loss: 0.008186977342702448\n",
      "Training Loss: 0.008373749144375324\n",
      "Validation Loss: 0.006113420923376519\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.00813491307082586\n",
      "Training Loss: 0.008153724290896208\n",
      "Training Loss: 0.008338317641755567\n",
      "Validation Loss: 0.006078682954977738\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.00810273606213741\n",
      "Training Loss: 0.008120495753828436\n",
      "Training Loss: 0.00830294615472667\n",
      "Validation Loss: 0.0060442065757312135\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.008070440081646667\n",
      "Training Loss: 0.008087340836646036\n",
      "Training Loss: 0.008267709153005853\n",
      "Validation Loss: 0.006010074308403673\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00803811982856132\n",
      "Training Loss: 0.008054326453711838\n",
      "Training Loss: 0.00823270317981951\n",
      "Validation Loss: 0.005976382792100645\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.008005897932453081\n",
      "Training Loss: 0.008021540871122852\n",
      "Training Loss: 0.008198052127845586\n",
      "Validation Loss: 0.005943240007919291\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007973917863564566\n",
      "Training Loss: 0.007989092792849989\n",
      "Training Loss: 0.008163899544160812\n",
      "Validation Loss: 0.005910771042160857\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007942345814080909\n",
      "Training Loss: 0.007957106753019616\n",
      "Training Loss: 0.008130407410208136\n",
      "Validation Loss: 0.005879097688796647\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007911357266129926\n",
      "Training Loss: 0.007925717999460175\n",
      "Training Loss: 0.00809774263878353\n",
      "Validation Loss: 0.00584832964637683\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007881123173283412\n",
      "Training Loss: 0.007895062824245543\n",
      "Training Loss: 0.008066065107705071\n",
      "Validation Loss: 0.005818578500949433\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007851803154917434\n",
      "Training Loss: 0.007865268207387999\n",
      "Training Loss: 0.008035516573581845\n",
      "Validation Loss: 0.005789921617690014\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007823527989676221\n",
      "Training Loss: 0.007836441725958139\n",
      "Training Loss: 0.008006210149032996\n",
      "Validation Loss: 0.005762414917810245\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007796393343014642\n",
      "Training Loss: 0.00780866808257997\n",
      "Training Loss: 0.007978220380609856\n",
      "Validation Loss: 0.005736086734706599\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007770454460987821\n",
      "Training Loss: 0.007782002059975639\n",
      "Training Loss: 0.007951584086986258\n",
      "Validation Loss: 0.00571093365963381\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007745727553265169\n",
      "Training Loss: 0.0077564673405140635\n",
      "Training Loss: 0.007926298737293109\n",
      "Validation Loss: 0.005686938460508173\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007722197959665209\n",
      "Training Loss: 0.007732064862502739\n",
      "Training Loss: 0.007902332921512425\n",
      "Validation Loss: 0.005664054550272361\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0076998245762661096\n",
      "Training Loss: 0.007708773881895468\n",
      "Training Loss: 0.007879633210832253\n",
      "Validation Loss: 0.00564223970809763\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007678548361873254\n",
      "Training Loss: 0.007686560017755255\n",
      "Training Loss: 0.00785813146387227\n",
      "Validation Loss: 0.005621438373790614\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007658306107623502\n",
      "Training Loss: 0.007665378989186138\n",
      "Training Loss: 0.007837751653278246\n",
      "Validation Loss: 0.0056015895705753835\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007639026590622961\n",
      "Training Loss: 0.007645183376735076\n",
      "Training Loss: 0.007818417905364186\n",
      "Validation Loss: 0.005582634854036268\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007620646700961515\n",
      "Training Loss: 0.00762592502636835\n",
      "Training Loss: 0.007800054530380293\n",
      "Validation Loss: 0.0055645271976154\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007603105921298266\n",
      "Training Loss: 0.007607556029688567\n",
      "Training Loss: 0.0077825939597096295\n",
      "Validation Loss: 0.005547211551253967\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.0075863533676601945\n",
      "Training Loss: 0.007590031099971384\n",
      "Training Loss: 0.007765972011256963\n",
      "Validation Loss: 0.005530638447596451\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007570340074598789\n",
      "Training Loss: 0.007573310220614075\n",
      "Training Loss: 0.007750131781212985\n",
      "Validation Loss: 0.0055147653885102\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007555029260693118\n",
      "Training Loss: 0.00755735069164075\n",
      "Training Loss: 0.0077350212354212996\n",
      "Validation Loss: 0.005499545958290776\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007540384766180068\n",
      "Training Loss: 0.007542114941170439\n",
      "Training Loss: 0.007720592152327299\n",
      "Validation Loss: 0.005484946724812217\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.00752637680969201\n",
      "Training Loss: 0.007527568516088649\n",
      "Training Loss: 0.007706805006600917\n",
      "Validation Loss: 0.005470930758256758\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007512978963786736\n",
      "Training Loss: 0.007513675570953638\n",
      "Training Loss: 0.007693619978381321\n",
      "Validation Loss: 0.005457460307359194\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007500167121179402\n",
      "Training Loss: 0.007500403752783313\n",
      "Training Loss: 0.007681003191974014\n",
      "Validation Loss: 0.005444508820923903\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007487917785765603\n",
      "Training Loss: 0.0074877200229093435\n",
      "Training Loss: 0.0076689216401427986\n",
      "Validation Loss: 0.005432047062354644\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.00747621034446638\n",
      "Training Loss: 0.007475594073766843\n",
      "Training Loss: 0.007657346699852496\n",
      "Validation Loss: 0.0054200444889799014\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007465020738309249\n",
      "Training Loss: 0.00746399458264932\n",
      "Training Loss: 0.007646247930824757\n",
      "Validation Loss: 0.005408480640479855\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.0074543290375731885\n",
      "Training Loss: 0.007452893253648654\n",
      "Training Loss: 0.007635601352667436\n",
      "Validation Loss: 0.005397325427417926\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.0074441138654947284\n",
      "Training Loss: 0.007442260023672134\n",
      "Training Loss: 0.007625379342352971\n",
      "Validation Loss: 0.0053865590186402456\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007434353532735258\n",
      "Training Loss: 0.0074320685979910196\n",
      "Training Loss: 0.0076155603281222286\n",
      "Validation Loss: 0.005376157791646762\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007425026230630465\n",
      "Training Loss: 0.007422290616668761\n",
      "Training Loss: 0.007606120017590002\n",
      "Validation Loss: 0.005366099660881282\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007416110243066214\n",
      "Training Loss: 0.007412900172639638\n",
      "Training Loss: 0.007597035939106718\n",
      "Validation Loss: 0.005356367344554681\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0074075846938649196\n",
      "Training Loss: 0.007403873521834612\n",
      "Training Loss: 0.007588287950493395\n",
      "Validation Loss: 0.005346941469980174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007399426402407698\n",
      "Training Loss: 0.0073951867001596835\n",
      "Training Loss: 0.007579856484662742\n",
      "Validation Loss: 0.005337802230856601\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007391615292872303\n",
      "Training Loss: 0.007386815827339887\n",
      "Training Loss: 0.007571720768464729\n",
      "Validation Loss: 0.005328935603061796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007384129363344982\n",
      "Training Loss: 0.007378740420099348\n",
      "Training Loss: 0.0075638633815106006\n",
      "Validation Loss: 0.005320323018351926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007376947555458173\n",
      "Training Loss: 0.007370938706444576\n",
      "Training Loss: 0.007556265733437612\n",
      "Validation Loss: 0.005311948734497798\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007370051018660888\n",
      "Training Loss: 0.007363391445251182\n",
      "Training Loss: 0.007548910676268861\n",
      "Validation Loss: 0.005303799195654607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007363420297042467\n",
      "Training Loss: 0.0073560818412806836\n",
      "Training Loss: 0.00754178301547654\n",
      "Validation Loss: 0.005295860492807468\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0073570352949900554\n",
      "Training Loss: 0.007348989703459665\n",
      "Training Loss: 0.007534867022186518\n",
      "Validation Loss: 0.005288115187202779\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007350879164878279\n",
      "Training Loss: 0.007342101936228573\n",
      "Training Loss: 0.007528148083947599\n",
      "Validation Loss: 0.005280557856371815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0073449333955068145\n",
      "Training Loss: 0.007335400728043169\n",
      "Training Loss: 0.007521613562712446\n",
      "Validation Loss: 0.005273175662582259\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007339184760930948\n",
      "Training Loss: 0.0073288737214170395\n",
      "Training Loss: 0.007515250714495778\n",
      "Validation Loss: 0.0052659550774580895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007333615552051924\n",
      "Training Loss: 0.007322506902273745\n",
      "Training Loss: 0.007509047501953319\n",
      "Validation Loss: 0.0052588894986809135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0073282127641141415\n",
      "Training Loss: 0.007316289823502302\n",
      "Training Loss: 0.007502992363879457\n",
      "Validation Loss: 0.005251969365805932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007322963073966094\n",
      "Training Loss: 0.0073102096724323926\n",
      "Training Loss: 0.007497075172141194\n",
      "Validation Loss: 0.005245184047105774\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007317852452397347\n",
      "Training Loss: 0.007304256409406662\n",
      "Training Loss: 0.007491285763680935\n",
      "Validation Loss: 0.005238525868979481\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.0073128715809434655\n",
      "Training Loss: 0.007298421568702906\n",
      "Training Loss: 0.007485617120983079\n",
      "Validation Loss: 0.005231990421379215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0073080092342570424\n",
      "Training Loss: 0.007292696662480011\n",
      "Training Loss: 0.007480060076341033\n",
      "Validation Loss: 0.005225571560453665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007303254703292623\n",
      "Training Loss: 0.007287073403131217\n",
      "Training Loss: 0.0074746068706735965\n",
      "Validation Loss: 0.0052192592973758075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.007298599636997096\n",
      "Training Loss: 0.00728154387907125\n",
      "Training Loss: 0.007469251269940287\n",
      "Validation Loss: 0.005213050106331036\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.007294035543454811\n",
      "Training Loss: 0.007276102817850187\n",
      "Training Loss: 0.007463986772345379\n",
      "Validation Loss: 0.0052069368971030365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.007289556020405144\n",
      "Training Loss: 0.007270744164707139\n",
      "Training Loss: 0.00745880747330375\n",
      "Validation Loss: 0.005200916596458115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007285153237753548\n",
      "Training Loss: 0.007265462146606296\n",
      "Training Loss: 0.0074537080607842655\n",
      "Validation Loss: 0.005194983987943426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0072808207792695616\n",
      "Training Loss: 0.007260251814732328\n",
      "Training Loss: 0.007448683304246515\n",
      "Validation Loss: 0.005189137064375695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007276552909752354\n",
      "Training Loss: 0.0072551091841887684\n",
      "Training Loss: 0.007443729728693143\n",
      "Validation Loss: 0.005183367088719616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007272345310193487\n",
      "Training Loss: 0.0072500293655321\n",
      "Training Loss: 0.007438841460971162\n",
      "Validation Loss: 0.005177676486076389\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.007268191886250861\n",
      "Training Loss: 0.007245010202750563\n",
      "Training Loss: 0.007434017499908805\n",
      "Validation Loss: 0.005172057590039259\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.007264090288663283\n",
      "Training Loss: 0.007240046340739354\n",
      "Training Loss: 0.007429252649890259\n",
      "Validation Loss: 0.005166511025196046\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.0072600346512626856\n",
      "Training Loss: 0.007235137684037909\n",
      "Training Loss: 0.007424544212408363\n",
      "Validation Loss: 0.005161028294536295\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.007256023262161761\n",
      "Training Loss: 0.007230278410715983\n",
      "Training Loss: 0.007419888393487781\n",
      "Validation Loss: 0.0051556106629320995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.007252052347175777\n",
      "Training Loss: 0.0072254675510339435\n",
      "Training Loss: 0.007415284654125571\n",
      "Validation Loss: 0.005150256904752485\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007248117882409133\n",
      "Training Loss: 0.007220702539198101\n",
      "Training Loss: 0.00741072844597511\n",
      "Validation Loss: 0.005144962643958586\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.007244218952255323\n",
      "Training Loss: 0.007215982463676483\n",
      "Training Loss: 0.0074062184104695915\n",
      "Validation Loss: 0.005139727233725945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.00724035273422487\n",
      "Training Loss: 0.007211302941432223\n",
      "Training Loss: 0.0074017537327017635\n",
      "Validation Loss: 0.005134547258115091\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.007236516221310012\n",
      "Training Loss: 0.007206664195982739\n",
      "Training Loss: 0.007397330119274557\n",
      "Validation Loss: 0.005129419547751588\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.007232708268566057\n",
      "Training Loss: 0.007202063692966476\n",
      "Training Loss: 0.0073929471278097484\n",
      "Validation Loss: 0.005124343129455667\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.007228926735697314\n",
      "Training Loss: 0.007197499480098486\n",
      "Training Loss: 0.0073886016930919144\n",
      "Validation Loss: 0.005119318933241864\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.007225170673918911\n",
      "Training Loss: 0.007192972388584167\n",
      "Training Loss: 0.0073842951247934255\n",
      "Validation Loss: 0.005114340774709715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.007221437642001547\n",
      "Training Loss: 0.007188478216994554\n",
      "Training Loss: 0.007380023809382692\n",
      "Validation Loss: 0.005109410803618558\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007217726868693717\n",
      "Training Loss: 0.007184017868712545\n",
      "Training Loss: 0.007375786399934441\n",
      "Validation Loss: 0.005104528315590297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007214037319063209\n",
      "Training Loss: 0.0071795888175256554\n",
      "Training Loss: 0.0073715828510466964\n",
      "Validation Loss: 0.005099688896652981\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.007210367250372655\n",
      "Training Loss: 0.007175191225251183\n",
      "Training Loss: 0.007367411605082452\n",
      "Validation Loss: 0.005094889778999633\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.007206716766231694\n",
      "Training Loss: 0.00717082284623757\n",
      "Training Loss: 0.0073632699472364034\n",
      "Validation Loss: 0.005090136088174506\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.007203084329375997\n",
      "Training Loss: 0.007166484325425699\n",
      "Training Loss: 0.007359158924082294\n",
      "Validation Loss: 0.00508542085495474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.007199468691833317\n",
      "Training Loss: 0.007162174374097958\n",
      "Training Loss: 0.0073550762631930415\n",
      "Validation Loss: 0.005080745608435965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.007195870236027985\n",
      "Training Loss: 0.007157891280949116\n",
      "Training Loss: 0.007351022540824488\n",
      "Validation Loss: 0.005076107698534563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.0071922874398296695\n",
      "Training Loss: 0.0071536355302669104\n",
      "Training Loss: 0.007346995342522859\n",
      "Validation Loss: 0.005071507849249194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.007188720321282744\n",
      "Training Loss: 0.007149405746022239\n",
      "Training Loss: 0.007342995001235977\n",
      "Validation Loss: 0.005066942048175365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.007185167972929776\n",
      "Training Loss: 0.007145201404346153\n",
      "Training Loss: 0.007339019135106355\n",
      "Validation Loss: 0.005062410964374169\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.0071816298458725215\n",
      "Training Loss: 0.007141023135045543\n",
      "Training Loss: 0.007335068364627659\n",
      "Validation Loss: 0.005057918760674388\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.007178105905186385\n",
      "Training Loss: 0.007136870033573359\n",
      "Training Loss: 0.007331142905168235\n",
      "Validation Loss: 0.005053458897543423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.00717459580511786\n",
      "Training Loss: 0.0071327401138842105\n",
      "Training Loss: 0.007327241143211722\n",
      "Validation Loss: 0.005049033870716485\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00717109905264806\n",
      "Training Loss: 0.007128635819535703\n",
      "Training Loss: 0.00732336187036708\n",
      "Validation Loss: 0.005044639558567969\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.007167615692014806\n",
      "Training Loss: 0.007124553801259026\n",
      "Training Loss: 0.007319505326449871\n",
      "Validation Loss: 0.0050402772377423025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.007164145201677456\n",
      "Training Loss: 0.007120496203424409\n",
      "Training Loss: 0.0073156711948104206\n",
      "Validation Loss: 0.005035944222838858\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.007160687177092768\n",
      "Training Loss: 0.007116461350815371\n",
      "Training Loss: 0.0073118583473842595\n",
      "Validation Loss: 0.005031644814030341\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.007157242116518319\n",
      "Training Loss: 0.007112450868589804\n",
      "Training Loss: 0.007308066947152838\n",
      "Validation Loss: 0.005027375022456929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.007153810373274609\n",
      "Training Loss: 0.007108463093172759\n",
      "Training Loss: 0.007304296812508255\n",
      "Validation Loss: 0.0050231347833707765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.0071503908501472325\n",
      "Training Loss: 0.007104498421540484\n",
      "Training Loss: 0.007300546889891848\n",
      "Validation Loss: 0.005018927486550607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.007146984119317495\n",
      "Training Loss: 0.007100556580116973\n",
      "Training Loss: 0.007296817196765915\n",
      "Validation Loss: 0.0050147447245865225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.007143590323394165\n",
      "Training Loss: 0.007096637097420171\n",
      "Training Loss: 0.007293107740115374\n",
      "Validation Loss: 0.005010590647880951\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.00714020901476033\n",
      "Training Loss: 0.007092740987427532\n",
      "Training Loss: 0.007289417493157088\n",
      "Validation Loss: 0.005006465138056514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.007136841228348203\n",
      "Training Loss: 0.0070888678974006325\n",
      "Training Loss: 0.007285747422138229\n",
      "Validation Loss: 0.0050023688170849605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.007133486032253131\n",
      "Training Loss: 0.007085017346544192\n",
      "Training Loss: 0.007282096552662551\n",
      "Validation Loss: 0.004998299599300777\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.0071301432850304994\n",
      "Training Loss: 0.007081190791213885\n",
      "Training Loss: 0.007278465372510254\n",
      "Validation Loss: 0.004994258376131399\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.007126815128140151\n",
      "Training Loss: 0.007077387644676492\n",
      "Training Loss: 0.007274851542897523\n",
      "Validation Loss: 0.004990242636145166\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.007123500347952358\n",
      "Training Loss: 0.007073607455240562\n",
      "Training Loss: 0.007271257201209664\n",
      "Validation Loss: 0.0049862542504894684\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.007120197856565937\n",
      "Training Loss: 0.007069851696724072\n",
      "Training Loss: 0.007267682122765109\n",
      "Validation Loss: 0.004982295071999176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.00711690989730414\n",
      "Training Loss: 0.007066119579831138\n",
      "Training Loss: 0.007264125882647932\n",
      "Validation Loss: 0.004978362196177411\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.007113635504501872\n",
      "Training Loss: 0.007062410279177129\n",
      "Training Loss: 0.007260587480850518\n",
      "Validation Loss: 0.004974455880053508\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.007110375955235213\n",
      "Training Loss: 0.007058725866954774\n",
      "Training Loss: 0.007257068180479109\n",
      "Validation Loss: 0.004970574527167914\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.007107130110962316\n",
      "Training Loss: 0.007055065903114155\n",
      "Training Loss: 0.007253567065345123\n",
      "Validation Loss: 0.0049667191309749744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.007103899224312045\n",
      "Training Loss: 0.007051430138526485\n",
      "Training Loss: 0.0072500843682792035\n",
      "Validation Loss: 0.004962891597285072\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.007100683175376616\n",
      "Training Loss: 0.007047819104045629\n",
      "Training Loss: 0.0072466195363085715\n",
      "Validation Loss: 0.004959090695235083\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0070974814076907936\n",
      "Training Loss: 0.007044232639018446\n",
      "Training Loss: 0.007243172853486612\n",
      "Validation Loss: 0.0049553137086855055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.007094296124414541\n",
      "Training Loss: 0.0070406710309907795\n",
      "Training Loss: 0.007239744567777961\n",
      "Validation Loss: 0.004951568022815071\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.007091124965809286\n",
      "Training Loss: 0.007037135103018954\n",
      "Training Loss: 0.007236334801418707\n",
      "Validation Loss: 0.004947842287129901\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.0070879695948679\n",
      "Training Loss: 0.007033624491887167\n",
      "Training Loss: 0.007232943230774253\n",
      "Validation Loss: 0.004944144120947406\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.007084830258390866\n",
      "Training Loss: 0.00703013846417889\n",
      "Training Loss: 0.007229569231858477\n",
      "Validation Loss: 0.0049404697531959816\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.0070817068743053825\n",
      "Training Loss: 0.007026679356349632\n",
      "Training Loss: 0.007226213881513104\n",
      "Validation Loss: 0.004936823442845155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.007078600461245515\n",
      "Training Loss: 0.007023244615411386\n",
      "Training Loss: 0.0072228761727456\n",
      "Validation Loss: 0.0049332003867818735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.007075509445276112\n",
      "Training Loss: 0.007019835746614262\n",
      "Training Loss: 0.007219556332565844\n",
      "Validation Loss: 0.00492960230834531\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.007072435851441696\n",
      "Training Loss: 0.0070164529571775346\n",
      "Training Loss: 0.007216254865052178\n",
      "Validation Loss: 0.004926034032231134\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.007069377696025185\n",
      "Training Loss: 0.007013097023591399\n",
      "Training Loss: 0.00721297156647779\n",
      "Validation Loss: 0.0049224862125126665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.007066337309079245\n",
      "Training Loss: 0.007009765977272764\n",
      "Training Loss: 0.007209706085268408\n",
      "Validation Loss: 0.004918965271653168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.007063313498510979\n",
      "Training Loss: 0.007006462266435847\n",
      "Training Loss: 0.007206458667060361\n",
      "Validation Loss: 0.004915469761655321\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.007060307298088446\n",
      "Training Loss: 0.007003184023778886\n",
      "Training Loss: 0.007203229867154732\n",
      "Validation Loss: 0.00491199851182572\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.007057317984872497\n",
      "Training Loss: 0.006999932068865746\n",
      "Training Loss: 0.007200018229195848\n",
      "Validation Loss: 0.004908551570561746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.007054346724762581\n",
      "Training Loss: 0.0069967059942428024\n",
      "Training Loss: 0.007196825243299827\n",
      "Validation Loss: 0.004905129782855511\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.007051392613793723\n",
      "Training Loss: 0.006993506188737228\n",
      "Training Loss: 0.007193649421678856\n",
      "Validation Loss: 0.0049017336706139066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.007048456257907673\n",
      "Training Loss: 0.006990333092398941\n",
      "Training Loss: 0.007190492297522724\n",
      "Validation Loss: 0.004898357863035681\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.00704553765419405\n",
      "Training Loss: 0.0069871857610996815\n",
      "Training Loss: 0.0071873519953805955\n",
      "Validation Loss: 0.0048950109617742765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.007042636558180675\n",
      "Training Loss: 0.006984064641874284\n",
      "Training Loss: 0.007184229892445728\n",
      "Validation Loss: 0.004891684146734018\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.007039753336575813\n",
      "Training Loss: 0.006980969349388033\n",
      "Training Loss: 0.0071811264345888045\n",
      "Validation Loss: 0.004888384219708989\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0070368882617913185\n",
      "Training Loss: 0.00697790086385794\n",
      "Training Loss: 0.0071780400746501985\n",
      "Validation Loss: 0.0048851067281401395\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.007034040758153424\n",
      "Training Loss: 0.0069748578418511896\n",
      "Training Loss: 0.007174972003558651\n",
      "Validation Loss: 0.0048818537727354115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.007031211905996315\n",
      "Training Loss: 0.006971840279875323\n",
      "Training Loss: 0.007171921143308282\n",
      "Validation Loss: 0.004878625993125057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.007028400634881109\n",
      "Training Loss: 0.006968849072000012\n",
      "Training Loss: 0.007168887503212318\n",
      "Validation Loss: 0.004875417960954182\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.007025607508840039\n",
      "Training Loss: 0.006965882664080709\n",
      "Training Loss: 0.007165872080950066\n",
      "Validation Loss: 0.0048722367370082585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.007022832105867564\n",
      "Training Loss: 0.006962942235404625\n",
      "Training Loss: 0.007162873827619478\n",
      "Validation Loss: 0.004869073867442065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.007020075171603821\n",
      "Training Loss: 0.006960027409950271\n",
      "Training Loss: 0.00715989367919974\n",
      "Validation Loss: 0.004865937760319519\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.007017336149001494\n",
      "Training Loss: 0.006957137145800516\n",
      "Training Loss: 0.0071569299744442105\n",
      "Validation Loss: 0.004862826354173797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.007014614996151068\n",
      "Training Loss: 0.006954273640876636\n",
      "Training Loss: 0.007153984573669732\n",
      "Validation Loss: 0.004859732058464309\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.007011911947629414\n",
      "Training Loss: 0.0069514339428860695\n",
      "Training Loss: 0.007151056801667437\n",
      "Validation Loss: 0.004856665053645547\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.007009227503440343\n",
      "Training Loss: 0.006948619392933324\n",
      "Training Loss: 0.007148145989049226\n",
      "Validation Loss: 0.004853620025161863\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.007006560413283296\n",
      "Training Loss: 0.006945829231990501\n",
      "Training Loss: 0.007145251581678167\n",
      "Validation Loss: 0.0048505935446474325\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0070039121084846554\n",
      "Training Loss: 0.006943063071230426\n",
      "Training Loss: 0.007142375073162839\n",
      "Validation Loss: 0.004847588976374252\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.007001281919656321\n",
      "Training Loss: 0.006940322582377121\n",
      "Training Loss: 0.0071395160991232845\n",
      "Validation Loss: 0.004844608890635663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006998669242602773\n",
      "Training Loss: 0.006937605274142698\n",
      "Training Loss: 0.0071366739203222096\n",
      "Validation Loss: 0.004841650931656444\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0069960744172567505\n",
      "Training Loss: 0.006934911302523688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [14:47<09:51, 147.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.007133847705554217\n",
      "Validation Loss: 0.004838714901922962\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.07344634626060724\n",
      "Training Loss: 0.06651218947023153\n",
      "Training Loss: 0.06401774564757944\n",
      "Validation Loss: 0.0630619880858432\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.062366934064775705\n",
      "Training Loss: 0.06018328733742237\n",
      "Training Loss: 0.05823693642392755\n",
      "Validation Loss: 0.056827539306008416\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.05577429526485503\n",
      "Training Loss: 0.05264188252389431\n",
      "Training Loss: 0.04946924649178982\n",
      "Validation Loss: 0.04651995306688078\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.04494273548945785\n",
      "Training Loss: 0.040588378002867104\n",
      "Training Loss: 0.03654654124751687\n",
      "Validation Loss: 0.03233257731883211\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.03103721565101296\n",
      "Training Loss: 0.02724972090218216\n",
      "Training Loss: 0.024803880718536675\n",
      "Validation Loss: 0.021663074695578453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.021558123067952694\n",
      "Training Loss: 0.02034031569492072\n",
      "Training Loss: 0.019948184911627323\n",
      "Validation Loss: 0.017575339722876133\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.018045312736649066\n",
      "Training Loss: 0.01756002549082041\n",
      "Training Loss: 0.01753375675296411\n",
      "Validation Loss: 0.015155480580739258\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.015840762176085262\n",
      "Training Loss: 0.01561129217967391\n",
      "Training Loss: 0.015722041414119304\n",
      "Validation Loss: 0.013394118485490927\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.014216151118744165\n",
      "Training Loss: 0.014162772053387016\n",
      "Training Loss: 0.014351098285987973\n",
      "Validation Loss: 0.012101272419304325\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.013028958532959222\n",
      "Training Loss: 0.013082803962752222\n",
      "Training Loss: 0.01331413930049166\n",
      "Validation Loss: 0.011128042472704325\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.01214739030227065\n",
      "Training Loss: 0.012253989716991782\n",
      "Training Loss: 0.012511245547793806\n",
      "Validation Loss: 0.010365694507517004\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.011470755671616643\n",
      "Training Loss: 0.011596484519541263\n",
      "Training Loss: 0.011872802986763418\n",
      "Validation Loss: 0.009749180942072711\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.010936978854006156\n",
      "Training Loss: 0.01106356248492375\n",
      "Training Loss: 0.011355746954213828\n",
      "Validation Loss: 0.009241455386319522\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.010509208833100274\n",
      "Training Loss: 0.01062738203909248\n",
      "Training Loss: 0.010932590963784605\n",
      "Validation Loss: 0.008819532198726797\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.010163710068445653\n",
      "Training Loss: 0.01026914132758975\n",
      "Training Loss: 0.010584023154806346\n",
      "Validation Loss: 0.008467071208307583\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.00988338647526689\n",
      "Training Loss: 0.00997441167011857\n",
      "Training Loss: 0.010295311140362173\n",
      "Validation Loss: 0.00817118568890048\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.009654925428330898\n",
      "Training Loss: 0.00973133435822092\n",
      "Training Loss: 0.010054725559893996\n",
      "Validation Loss: 0.007921329253415965\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.009467656447086484\n",
      "Training Loss: 0.009530026176944375\n",
      "Training Loss: 0.009852834831690416\n",
      "Validation Loss: 0.007708805727018985\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.009312996756052598\n",
      "Training Loss: 0.009362314558820799\n",
      "Training Loss: 0.00968208149424754\n",
      "Validation Loss: 0.0075265740494379835\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.009184117482509463\n",
      "Training Loss: 0.009221556223928928\n",
      "Training Loss: 0.009536456828936935\n",
      "Validation Loss: 0.007369002505216036\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.009075629329308867\n",
      "Training Loss: 0.009102411692729219\n",
      "Training Loss: 0.00941118575166911\n",
      "Validation Loss: 0.0072316075308939045\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.008983301183907316\n",
      "Training Loss: 0.009000621618470176\n",
      "Training Loss: 0.009302485450170934\n",
      "Validation Loss: 0.00711085909475269\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008903817679965868\n",
      "Training Loss: 0.00891279187053442\n",
      "Training Loss: 0.009207339468412101\n",
      "Validation Loss: 0.007003932839437398\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008834572690539062\n",
      "Training Loss: 0.008836219221120701\n",
      "Training Loss: 0.009123336366610602\n",
      "Validation Loss: 0.0069085802906454444\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.008773515300126745\n",
      "Training Loss: 0.008768743752734736\n",
      "Training Loss: 0.009048533686436712\n",
      "Validation Loss: 0.006822984563605337\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008719020769931377\n",
      "Training Loss: 0.008708633785136043\n",
      "Training Loss: 0.008981360911857337\n",
      "Validation Loss: 0.006745670668341303\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008669800549978391\n",
      "Training Loss: 0.008654497786192223\n",
      "Training Loss: 0.008920542143750937\n",
      "Validation Loss: 0.006675427515331781\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008624829924665392\n",
      "Training Loss: 0.008605219612363726\n",
      "Training Loss: 0.00886503740795888\n",
      "Validation Loss: 0.006611249327209642\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008583291245158761\n",
      "Training Loss: 0.008559900021646171\n",
      "Training Loss: 0.00881399558391422\n",
      "Validation Loss: 0.0065523031549632885\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008544535971013829\n",
      "Training Loss: 0.008517815496306867\n",
      "Training Loss: 0.008766715752426536\n",
      "Validation Loss: 0.006497887912823745\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008508045243797824\n",
      "Training Loss: 0.008478391221724451\n",
      "Training Loss: 0.008722627938259392\n",
      "Validation Loss: 0.006447419008826104\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008473414224572479\n",
      "Training Loss: 0.008441170959267765\n",
      "Training Loss: 0.00868126803659834\n",
      "Validation Loss: 0.00640040487992797\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008440325444098562\n",
      "Training Loss: 0.008405795735307038\n",
      "Training Loss: 0.008642248002579435\n",
      "Validation Loss: 0.006356426327635817\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008408529005246237\n",
      "Training Loss: 0.008371983717661352\n",
      "Training Loss: 0.008605258545139804\n",
      "Validation Loss: 0.006315136398973592\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008377839357126503\n",
      "Training Loss: 0.008339521014131606\n",
      "Training Loss: 0.008570043861400336\n",
      "Validation Loss: 0.006276234542281272\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008348113411339\n",
      "Training Loss: 0.008308240629266947\n",
      "Training Loss: 0.008536393446847796\n",
      "Validation Loss: 0.006239462036932452\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008319244367303327\n",
      "Training Loss: 0.00827801895677112\n",
      "Training Loss: 0.00850413705338724\n",
      "Validation Loss: 0.006204612040369028\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.00829115777858533\n",
      "Training Loss: 0.008248761848080903\n",
      "Training Loss: 0.00847313367179595\n",
      "Validation Loss: 0.006171486178266533\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008263796769315377\n",
      "Training Loss: 0.00822039862512611\n",
      "Training Loss: 0.008443266737740486\n",
      "Validation Loss: 0.006139927797458982\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008237122875871137\n",
      "Training Loss: 0.008192874533124268\n",
      "Training Loss: 0.008414436210878194\n",
      "Validation Loss: 0.006109784395182819\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008211108212126419\n",
      "Training Loss: 0.00816614657640457\n",
      "Training Loss: 0.00838655982981436\n",
      "Validation Loss: 0.006080929111438186\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008185731131816283\n",
      "Training Loss: 0.008140181502094493\n",
      "Training Loss: 0.008359566816361621\n",
      "Validation Loss: 0.006053247954696417\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.008160975209902973\n",
      "Training Loss: 0.008114945993293076\n",
      "Training Loss: 0.008333389915060252\n",
      "Validation Loss: 0.006026631583501533\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.008136823761742563\n",
      "Training Loss: 0.008090411000885069\n",
      "Training Loss: 0.008307970575988293\n",
      "Validation Loss: 0.006000981864969382\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.00811325965449214\n",
      "Training Loss: 0.008066544885514304\n",
      "Training Loss: 0.008283253906993195\n",
      "Validation Loss: 0.005976208956556373\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.008090266882209107\n",
      "Training Loss: 0.008043315258109942\n",
      "Training Loss: 0.00825918813352473\n",
      "Validation Loss: 0.005952225879012617\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.008067824124591425\n",
      "Training Loss: 0.008020689518889412\n",
      "Training Loss: 0.008235723089892417\n",
      "Validation Loss: 0.005928955188680315\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.008045907637570054\n",
      "Training Loss: 0.007998632999369875\n",
      "Training Loss: 0.008212812381098047\n",
      "Validation Loss: 0.005906326441209387\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.008024493067059666\n",
      "Training Loss: 0.007977109830826522\n",
      "Training Loss: 0.008190408085938542\n",
      "Validation Loss: 0.005884264654668278\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.00800355085521005\n",
      "Training Loss: 0.007956080072326586\n",
      "Training Loss: 0.008168467793148011\n",
      "Validation Loss: 0.005862711760048033\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.00798305455944501\n",
      "Training Loss: 0.007935508232330904\n",
      "Training Loss: 0.008146950688678772\n",
      "Validation Loss: 0.005841610894350105\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007962971664965153\n",
      "Training Loss: 0.007915357406018302\n",
      "Training Loss: 0.008125815718667582\n",
      "Validation Loss: 0.005820910900234758\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007943274730350822\n",
      "Training Loss: 0.007895594594301655\n",
      "Training Loss: 0.008105032206512987\n",
      "Validation Loss: 0.005800568690952542\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007923935109283775\n",
      "Training Loss: 0.007876183529151603\n",
      "Training Loss: 0.008084563912125304\n",
      "Validation Loss: 0.0057805395080823075\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.00790492550120689\n",
      "Training Loss: 0.007857095972867682\n",
      "Training Loss: 0.008064385817851872\n",
      "Validation Loss: 0.005760792870430297\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007886221993248909\n",
      "Training Loss: 0.007838304030010478\n",
      "Training Loss: 0.008044474102789537\n",
      "Validation Loss: 0.005741299918769032\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007867802241817117\n",
      "Training Loss: 0.007819784331368283\n",
      "Training Loss: 0.008024806481553242\n",
      "Validation Loss: 0.005722034268677653\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007849646138492972\n",
      "Training Loss: 0.007801515703322366\n",
      "Training Loss: 0.008005369794555009\n",
      "Validation Loss: 0.0057029823741216335\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007831738648237661\n",
      "Training Loss: 0.007783482120139525\n",
      "Training Loss: 0.007986150153446942\n",
      "Validation Loss: 0.005684124775429706\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007814065752318128\n",
      "Training Loss: 0.0077656691265292465\n",
      "Training Loss: 0.007967138997046277\n",
      "Validation Loss: 0.005665454014173049\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007796617011772469\n",
      "Training Loss: 0.007748067546635866\n",
      "Training Loss: 0.007948332451051102\n",
      "Validation Loss: 0.005646957943917074\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007779387042392045\n",
      "Training Loss: 0.007730668755248189\n",
      "Training Loss: 0.007929729325696825\n",
      "Validation Loss: 0.00562863480011943\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007762370925629512\n",
      "Training Loss: 0.007713469743030146\n",
      "Training Loss: 0.007911329364869744\n",
      "Validation Loss: 0.005610485760013709\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007745566022349521\n",
      "Training Loss: 0.007696469015209004\n",
      "Training Loss: 0.007893134754849597\n",
      "Validation Loss: 0.005592509486785765\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007728971419855952\n",
      "Training Loss: 0.007679667334305123\n",
      "Training Loss: 0.007875152005581185\n",
      "Validation Loss: 0.005574710230248865\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007712591401068494\n",
      "Training Loss: 0.007663066036766395\n",
      "Training Loss: 0.007857387989060953\n",
      "Validation Loss: 0.00555709163590303\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007696429110364988\n",
      "Training Loss: 0.007646670243702829\n",
      "Training Loss: 0.007839849693700671\n",
      "Validation Loss: 0.005539658284494967\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007680487121688202\n",
      "Training Loss: 0.007630485703703016\n",
      "Training Loss: 0.007822546400129795\n",
      "Validation Loss: 0.0055224225448256125\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0076647723757196215\n",
      "Training Loss: 0.007614517660113051\n",
      "Training Loss: 0.007805486143333837\n",
      "Validation Loss: 0.005505381986561618\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0076492892624810335\n",
      "Training Loss: 0.00759877122589387\n",
      "Training Loss: 0.007788677933858707\n",
      "Validation Loss: 0.005488552521977029\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0076340435596648605\n",
      "Training Loss: 0.007583254958735779\n",
      "Training Loss: 0.007772131642559544\n",
      "Validation Loss: 0.005471936924110973\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007619041715515778\n",
      "Training Loss: 0.007567973929690197\n",
      "Training Loss: 0.007755854475544765\n",
      "Validation Loss: 0.00545554607714202\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007604288883740083\n",
      "Training Loss: 0.007552937520667911\n",
      "Training Loss: 0.0077398550137877465\n",
      "Validation Loss: 0.005439383967967934\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007589787755860016\n",
      "Training Loss: 0.00753814721130766\n",
      "Training Loss: 0.0077241384889930485\n",
      "Validation Loss: 0.005423458988956186\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007575543919811026\n",
      "Training Loss: 0.007523611987708136\n",
      "Training Loss: 0.007708713669562712\n",
      "Validation Loss: 0.005407780113242818\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007561561248730868\n",
      "Training Loss: 0.007509336185175925\n",
      "Training Loss: 0.0076935836463235315\n",
      "Validation Loss: 0.005392353865318\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.0075478414807002995\n",
      "Training Loss: 0.007495322339236736\n",
      "Training Loss: 0.007678754141088575\n",
      "Validation Loss: 0.0053771820568134275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007534386602928862\n",
      "Training Loss: 0.00748157434980385\n",
      "Training Loss: 0.007664226545020938\n",
      "Validation Loss: 0.005362272020586337\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007521197134628892\n",
      "Training Loss: 0.007468094517244026\n",
      "Training Loss: 0.007650002351729199\n",
      "Validation Loss: 0.005347629340648065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007508271291153505\n",
      "Training Loss: 0.007454883778700605\n",
      "Training Loss: 0.007636084161931649\n",
      "Validation Loss: 0.005333256087621695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007495611347258091\n",
      "Training Loss: 0.007441943885060027\n",
      "Training Loss: 0.007622470813803375\n",
      "Validation Loss: 0.005319156298110408\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007483214428648352\n",
      "Training Loss: 0.007429274533642456\n",
      "Training Loss: 0.007609162334119901\n",
      "Validation Loss: 0.005305333257903963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007471078706439585\n",
      "Training Loss: 0.007416875332128256\n",
      "Training Loss: 0.007596156335202977\n",
      "Validation Loss: 0.0052917918734503595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007459201815072447\n",
      "Training Loss: 0.007404742115177214\n",
      "Training Loss: 0.0075834492384456095\n",
      "Validation Loss: 0.005278526266429866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007447579104918987\n",
      "Training Loss: 0.00739287466974929\n",
      "Training Loss: 0.007571039767935872\n",
      "Validation Loss: 0.005265539963311108\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007436208974104375\n",
      "Training Loss: 0.007381270879413933\n",
      "Training Loss: 0.007558921438176185\n",
      "Validation Loss: 0.005252833427139296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.00742508455296047\n",
      "Training Loss: 0.007369924345985055\n",
      "Training Loss: 0.007547091385349631\n",
      "Validation Loss: 0.0052404078325319495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007414203656371683\n",
      "Training Loss: 0.007358836751664057\n",
      "Training Loss: 0.007535545465070754\n",
      "Validation Loss: 0.005228263475105501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007403562502004206\n",
      "Training Loss: 0.007347999636549502\n",
      "Training Loss: 0.007524276304757223\n",
      "Validation Loss: 0.005216393291458404\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007393153284210712\n",
      "Training Loss: 0.007337409594329074\n",
      "Training Loss: 0.007513280590064824\n",
      "Validation Loss: 0.005204802059851001\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007382973378989846\n",
      "Training Loss: 0.007327063631964848\n",
      "Training Loss: 0.007502551786601543\n",
      "Validation Loss: 0.005193482140037283\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007373017031932249\n",
      "Training Loss: 0.007316956316353753\n",
      "Training Loss: 0.0074920826021116224\n",
      "Validation Loss: 0.005182434151372924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007363278662087396\n",
      "Training Loss: 0.007307081394828856\n",
      "Training Loss: 0.00748186755226925\n",
      "Validation Loss: 0.0051716559663352165\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.00735375372110866\n",
      "Training Loss: 0.007297434998909011\n",
      "Training Loss: 0.007471900075906888\n",
      "Validation Loss: 0.00516113380148086\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007344436708372086\n",
      "Training Loss: 0.007288010037736967\n",
      "Training Loss: 0.007462173842359334\n",
      "Validation Loss: 0.005150874995553259\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007335321356076747\n",
      "Training Loss: 0.007278804358793423\n",
      "Training Loss: 0.007452683509327472\n",
      "Validation Loss: 0.005140876299387702\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007326405101921409\n",
      "Training Loss: 0.00726981112617068\n",
      "Training Loss: 0.007443422074429691\n",
      "Validation Loss: 0.005131125255236716\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.00731767991092056\n",
      "Training Loss: 0.007261024310719222\n",
      "Training Loss: 0.007434382761130109\n",
      "Validation Loss: 0.005121622075002347\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007309143040329218\n",
      "Training Loss: 0.007252441301243379\n",
      "Training Loss: 0.007425560061819852\n",
      "Validation Loss: 0.005112361209990268\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007300788857974112\n",
      "Training Loss: 0.007244055150076747\n",
      "Training Loss: 0.007416946786688641\n",
      "Validation Loss: 0.005103336832240182\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00729261266766116\n",
      "Training Loss: 0.007235860637156293\n",
      "Training Loss: 0.00740853815805167\n",
      "Validation Loss: 0.005094544696516954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007284611752256751\n",
      "Training Loss: 0.007227852391079068\n",
      "Training Loss: 0.007400327587965876\n",
      "Validation Loss: 0.005085978556942362\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007276777038350701\n",
      "Training Loss: 0.007220027984585613\n",
      "Training Loss: 0.007392310118302703\n",
      "Validation Loss: 0.005077636968135164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007269108853070066\n",
      "Training Loss: 0.007212380471173674\n",
      "Training Loss: 0.007384478370659054\n",
      "Validation Loss: 0.005069510798032782\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0072615995950764045\n",
      "Training Loss: 0.00720490600913763\n",
      "Training Loss: 0.007376828622072935\n",
      "Validation Loss: 0.00506159503554423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0072542470874032\n",
      "Training Loss: 0.007197598866187036\n",
      "Training Loss: 0.007369353286921978\n",
      "Validation Loss: 0.00505388980166296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.0072470462298952045\n",
      "Training Loss: 0.007190456822281703\n",
      "Training Loss: 0.007362049386138096\n",
      "Validation Loss: 0.005046382765521117\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0072399939456954596\n",
      "Training Loss: 0.007183473512995988\n",
      "Training Loss: 0.00735491092549637\n",
      "Validation Loss: 0.0050390721651924295\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007233086105552502\n",
      "Training Loss: 0.007176646220032125\n",
      "Training Loss: 0.007347933304263279\n",
      "Validation Loss: 0.005031951139367112\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.0072263173898682\n",
      "Training Loss: 0.0071699699095916\n",
      "Training Loss: 0.0073411116446368396\n",
      "Validation Loss: 0.005025018566403161\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.0072196861275006085\n",
      "Training Loss: 0.007163440225413069\n",
      "Training Loss: 0.007334440713748336\n",
      "Validation Loss: 0.005018264332555988\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.007213189022731967\n",
      "Training Loss: 0.007157053500413894\n",
      "Training Loss: 0.007327916270587593\n",
      "Validation Loss: 0.005011685368516011\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007206821666331961\n",
      "Training Loss: 0.007150806337594986\n",
      "Training Loss: 0.007321534395450726\n",
      "Validation Loss: 0.005005278877045415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007200580285862088\n",
      "Training Loss: 0.007144696157192812\n",
      "Training Loss: 0.007315290509723127\n",
      "Validation Loss: 0.004999037452036871\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.0071944611566141245\n",
      "Training Loss: 0.00713871642947197\n",
      "Training Loss: 0.007309180188458413\n",
      "Validation Loss: 0.004992959655303341\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007188463875208982\n",
      "Training Loss: 0.007132866345345974\n",
      "Training Loss: 0.007303199560847133\n",
      "Validation Loss: 0.004987036010114413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.007182582488749176\n",
      "Training Loss: 0.007127140163211152\n",
      "Training Loss: 0.007297344710677862\n",
      "Validation Loss: 0.004981264660365126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.007176815358689055\n",
      "Training Loss: 0.0071215356967877595\n",
      "Training Loss: 0.007291611966211349\n",
      "Validation Loss: 0.0049756405851542115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.007171159618301317\n",
      "Training Loss: 0.007116048806346953\n",
      "Training Loss: 0.0072859988606069235\n",
      "Validation Loss: 0.0049701588852278725\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.007165612009703182\n",
      "Training Loss: 0.0071106784825678915\n",
      "Training Loss: 0.0072804995067417624\n",
      "Validation Loss: 0.00496481511195117\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.007160169515409507\n",
      "Training Loss: 0.007105418571736664\n",
      "Training Loss: 0.007275111066410318\n",
      "Validation Loss: 0.004959606903008698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007154829697101377\n",
      "Training Loss: 0.007100267831701785\n",
      "Training Loss: 0.0072698317468166355\n",
      "Validation Loss: 0.00495452737943217\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0071495898498687895\n",
      "Training Loss: 0.007095224284566939\n",
      "Training Loss: 0.007264657670166343\n",
      "Validation Loss: 0.004949576154042537\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.007144448283943347\n",
      "Training Loss: 0.007090282889548689\n",
      "Training Loss: 0.007259584027342498\n",
      "Validation Loss: 0.004944744319759644\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.007139402767643332\n",
      "Training Loss: 0.007085441722301767\n",
      "Training Loss: 0.00725460966466926\n",
      "Validation Loss: 0.004940031200982212\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.007134448197903111\n",
      "Training Loss: 0.0070806985627859835\n",
      "Training Loss: 0.007249731299234554\n",
      "Validation Loss: 0.004935435584505622\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.007129583979258313\n",
      "Training Loss: 0.007076049929019064\n",
      "Training Loss: 0.007244946254650131\n",
      "Validation Loss: 0.004930951161011844\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.007124807768850588\n",
      "Training Loss: 0.00707149384659715\n",
      "Training Loss: 0.0072402516310103235\n",
      "Validation Loss: 0.00492657422940927\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.007120116906007752\n",
      "Training Loss: 0.007067027658922598\n",
      "Training Loss: 0.007235644202446565\n",
      "Validation Loss: 0.004922300906135083\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007115509653813206\n",
      "Training Loss: 0.007062648197170347\n",
      "Training Loss: 0.007231121775694191\n",
      "Validation Loss: 0.004918127632019728\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007110983816091903\n",
      "Training Loss: 0.007058354028267786\n",
      "Training Loss: 0.007226681710453704\n",
      "Validation Loss: 0.004914054962979065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.007106536419596523\n",
      "Training Loss: 0.007054143472341821\n",
      "Training Loss: 0.007222321641165763\n",
      "Validation Loss: 0.004910072247665166\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.007102167313569226\n",
      "Training Loss: 0.00705001107417047\n",
      "Training Loss: 0.007218039104482159\n",
      "Validation Loss: 0.0049061807548742365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.0070978722081054\n",
      "Training Loss: 0.007045958085218445\n",
      "Training Loss: 0.007213832078268752\n",
      "Validation Loss: 0.004902379538241272\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.007093650017632172\n",
      "Training Loss: 0.007041980577632785\n",
      "Training Loss: 0.00720969726331532\n",
      "Validation Loss: 0.004898660583421588\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.007089500105939806\n",
      "Training Loss: 0.007038076306926087\n",
      "Training Loss: 0.0072056338225957\n",
      "Validation Loss: 0.004895026886474676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.0070854185672942546\n",
      "Training Loss: 0.00703424371778965\n",
      "Training Loss: 0.007201638842234388\n",
      "Validation Loss: 0.004891473682220565\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.007081405098433607\n",
      "Training Loss: 0.007030481566907838\n",
      "Training Loss: 0.007197711822809652\n",
      "Validation Loss: 0.004887993277437734\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.007077456942643039\n",
      "Training Loss: 0.00702678706147708\n",
      "Training Loss: 0.007193848631577566\n",
      "Validation Loss: 0.004884589967066736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.007073572898516432\n",
      "Training Loss: 0.0070231572445482015\n",
      "Training Loss: 0.007190048324409872\n",
      "Validation Loss: 0.004881255400597296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.007069751119124703\n",
      "Training Loss: 0.007019592979922891\n",
      "Training Loss: 0.0071863093785941605\n",
      "Validation Loss: 0.004877996190812104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.0070659894269192594\n",
      "Training Loss: 0.0070160900813061745\n",
      "Training Loss: 0.007182629505405202\n",
      "Validation Loss: 0.004874803059802422\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00706228697381448\n",
      "Training Loss: 0.0070126475766301155\n",
      "Training Loss: 0.007179005830548704\n",
      "Validation Loss: 0.004871672028518711\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.0070586416649166496\n",
      "Training Loss: 0.007009263110812753\n",
      "Training Loss: 0.007175439668353647\n",
      "Validation Loss: 0.004868605543644785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.007055052132345736\n",
      "Training Loss: 0.007005935817724094\n",
      "Training Loss: 0.007171926399460063\n",
      "Validation Loss: 0.004865596395932933\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.007051516510546208\n",
      "Training Loss: 0.00700266346684657\n",
      "Training Loss: 0.007168466525617987\n",
      "Validation Loss: 0.004862649005241274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.007048034563777037\n",
      "Training Loss: 0.0069994445063639435\n",
      "Training Loss: 0.007165056880330667\n",
      "Validation Loss: 0.004859757194363544\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.007044603720423766\n",
      "Training Loss: 0.006996278257574886\n",
      "Training Loss: 0.007161696914117784\n",
      "Validation Loss: 0.004856916655932836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.007041222800617106\n",
      "Training Loss: 0.006993162748403847\n",
      "Training Loss: 0.007158385341754183\n",
      "Validation Loss: 0.0048541319602315594\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.007037890235078521\n",
      "Training Loss: 0.006990095758810639\n",
      "Training Loss: 0.007155118203954771\n",
      "Validation Loss: 0.00485139640356843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.007034604200161994\n",
      "Training Loss: 0.0069870761723723265\n",
      "Training Loss: 0.007151898352894932\n",
      "Validation Loss: 0.004848709590262158\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.007031364215072245\n",
      "Training Loss: 0.006984103290596977\n",
      "Training Loss: 0.007148721530102193\n",
      "Validation Loss: 0.00484606940456237\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.007028169158147648\n",
      "Training Loss: 0.006981173870153725\n",
      "Training Loss: 0.007145587103441358\n",
      "Validation Loss: 0.004843477966143551\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.007025017675478012\n",
      "Training Loss: 0.006978290053084492\n",
      "Training Loss: 0.007142494876170531\n",
      "Validation Loss: 0.004840926237775829\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.007021908223978243\n",
      "Training Loss: 0.006975446857977658\n",
      "Training Loss: 0.007139441564213485\n",
      "Validation Loss: 0.004838418318848178\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.007018839460797608\n",
      "Training Loss: 0.006972646457143128\n",
      "Training Loss: 0.007136428552912548\n",
      "Validation Loss: 0.004835950371579089\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.007015810316661372\n",
      "Training Loss: 0.006969883928541094\n",
      "Training Loss: 0.007133451864356175\n",
      "Validation Loss: 0.004833523510416363\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.007012819907395169\n",
      "Training Loss: 0.006967160827480256\n",
      "Training Loss: 0.007130512435687706\n",
      "Validation Loss: 0.004831131739970925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.007009866468142718\n",
      "Training Loss: 0.006964473372790963\n",
      "Training Loss: 0.007127607961883768\n",
      "Validation Loss: 0.00482877697390138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.007006950909853913\n",
      "Training Loss: 0.006961823077872396\n",
      "Training Loss: 0.007124738482525572\n",
      "Validation Loss: 0.004826454870177831\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.007004069082322531\n",
      "Training Loss: 0.00695920783909969\n",
      "Training Loss: 0.0071219020779244606\n",
      "Validation Loss: 0.004824169541923643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.0070012223062803965\n",
      "Training Loss: 0.00695662678219378\n",
      "Training Loss: 0.007119098401162773\n",
      "Validation Loss: 0.004821916250773611\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.00699840881396085\n",
      "Training Loss: 0.006954077768605202\n",
      "Training Loss: 0.007116325935348868\n",
      "Validation Loss: 0.004819694050362732\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006995627059950493\n",
      "Training Loss: 0.00695156141766347\n",
      "Training Loss: 0.007113584682811052\n",
      "Validation Loss: 0.00481750225789552\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006992877032607794\n",
      "Training Loss: 0.006949075885349884\n",
      "Training Loss: 0.007110872362973169\n",
      "Validation Loss: 0.004815338103602944\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006990157575928606\n",
      "Training Loss: 0.0069466197397559885\n",
      "Training Loss: 0.0071081894391682\n",
      "Validation Loss: 0.004813201463221457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.00698746693902649\n",
      "Training Loss: 0.006944193684030324\n",
      "Training Loss: 0.0071055339730810374\n",
      "Validation Loss: 0.004811095904422861\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006984805060783401\n",
      "Training Loss: 0.006941794705344364\n",
      "Training Loss: 0.007102906369837001\n",
      "Validation Loss: 0.004809011241520515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006982171472627669\n",
      "Training Loss: 0.006939423870062455\n",
      "Training Loss: 0.007100305092753842\n",
      "Validation Loss: 0.004806952304442235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006979564272915013\n",
      "Training Loss: 0.00693707775673829\n",
      "Training Loss: 0.007097727491054684\n",
      "Validation Loss: 0.004804917467951649\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006976982944761403\n",
      "Training Loss: 0.006934756935806945\n",
      "Training Loss: 0.007095175547292456\n",
      "Validation Loss: 0.004802903756263844\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006974426781525835\n",
      "Training Loss: 0.006932460940442979\n",
      "Training Loss: 0.007092647730605677\n",
      "Validation Loss: 0.004800913048374435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006971895244787447\n",
      "Training Loss: 0.006930188779952004\n",
      "Training Loss: 0.00709014312014915\n",
      "Validation Loss: 0.004798943233765202\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006969387603458017\n",
      "Training Loss: 0.0069279393216129396\n",
      "Training Loss: 0.0070876607031095775\n",
      "Validation Loss: 0.004796993317019739\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006966902134590783\n",
      "Training Loss: 0.0069257120485417545\n",
      "Training Loss: 0.007085201422451064\n",
      "Validation Loss: 0.004795063522466448\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006964439081493765\n",
      "Training Loss: 0.006923507250612602\n",
      "Training Loss: 0.00708276271703653\n",
      "Validation Loss: 0.004793152609431928\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006961997797479853\n",
      "Training Loss: 0.006921322266571224\n",
      "Training Loss: 0.00708034488838166\n",
      "Validation Loss: 0.0047912576570688335\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006959577319212258\n",
      "Training Loss: 0.0069191572163254025\n",
      "Training Loss: 0.0070779462647624315\n",
      "Validation Loss: 0.004789380176160275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006957177453441546\n",
      "Training Loss: 0.0069170115608721975\n",
      "Training Loss: 0.007075566392159089\n",
      "Validation Loss: 0.004787517761879632\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006954795363708399\n",
      "Training Loss: 0.00691488501499407\n",
      "Training Loss: 0.007073206433560699\n",
      "Validation Loss: 0.004785673083931082\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006952432589605451\n",
      "Training Loss: 0.006912775045493618\n",
      "Training Loss: 0.0070708630909211935\n",
      "Validation Loss: 0.004783841740769031\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.00695008865033742\n",
      "Training Loss: 0.006910683941096067\n",
      "Training Loss: 0.007068538056919351\n",
      "Validation Loss: 0.0047820239063624415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006947762210038491\n",
      "Training Loss: 0.006908608553931117\n",
      "Training Loss: 0.007066230975324288\n",
      "Validation Loss: 0.00478022036095558\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006945452329819091\n",
      "Training Loss: 0.006906549790874124\n",
      "Training Loss: 0.007063940217485651\n",
      "Validation Loss: 0.004778428960675269\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006943159294314683\n",
      "Training Loss: 0.0069045068440027535\n",
      "Training Loss: 0.007061665096553042\n",
      "Validation Loss: 0.004776651726439177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006940881263581104\n",
      "Training Loss: 0.006902478772681206\n",
      "Training Loss: 0.007059405540348962\n",
      "Validation Loss: 0.004774882443107958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006938620635773987\n",
      "Training Loss: 0.006900464011123404\n",
      "Training Loss: 0.007057160026161\n",
      "Validation Loss: 0.004773124832047798\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006936372747877613\n",
      "Training Loss: 0.006898464044788852\n",
      "Training Loss: 0.007054929923033342\n",
      "Validation Loss: 0.004771378901106923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006934140344383195\n",
      "Training Loss: 0.00689647849299945\n",
      "Training Loss: 0.0070527134311851114\n",
      "Validation Loss: 0.004769644296461235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006931920482893475\n",
      "Training Loss: 0.0068945034523494545\n",
      "Training Loss: 0.007050511194393039\n",
      "Validation Loss: 0.004767919487053047\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006929714853176847\n",
      "Training Loss: 0.006892543849535287\n",
      "Training Loss: 0.007048321835463866\n",
      "Validation Loss: 0.0047662007155451465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006927521689212881\n",
      "Training Loss: 0.006890594277065247\n",
      "Training Loss: 0.007046145807253197\n",
      "Validation Loss: 0.004764488692201799\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006925341169117019\n",
      "Training Loss: 0.006888657178496942\n",
      "Training Loss: 0.0070439817884471265\n",
      "Validation Loss: 0.004762789400639745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.0069231727387523275\n",
      "Training Loss: 0.006886731403646991\n",
      "Training Loss: 0.0070418292679823935\n",
      "Validation Loss: 0.004761095747372575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006921015662373975\n",
      "Training Loss: 0.006884815857047215\n",
      "Training Loss: 0.007039688642835245\n",
      "Validation Loss: 0.004759409869733277\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006918870234512724\n",
      "Training Loss: 0.006882909834384918\n",
      "Training Loss: 0.007037559089949355\n",
      "Validation Loss: 0.004757729608152229\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0069167349435156214\n",
      "Training Loss: 0.006881014792015776\n",
      "Training Loss: 0.007035440183244646\n",
      "Validation Loss: 0.004756055993362843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.0069146098452620205\n",
      "Training Loss: 0.006879128476139158\n",
      "Training Loss: 0.007033331723650917\n",
      "Validation Loss: 0.004754386587183546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.00691249528026674\n",
      "Training Loss: 0.006877251926343888\n",
      "Training Loss: 0.007031234470196069\n",
      "Validation Loss: 0.004752726634189988\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006910390338744037\n",
      "Training Loss: 0.006875384377781302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [17:15<07:24, 148.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.007029145230771974\n",
      "Validation Loss: 0.00475107046534841\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.6798785389959812\n",
      "Training Loss: 0.604961868673563\n",
      "Training Loss: 0.5360102474689483\n",
      "Validation Loss: 0.4409030540270752\n",
      "Validation Accuracy: 0.5442415730337079\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.3892568515986204\n",
      "Training Loss: 0.2981566652655602\n",
      "Training Loss: 0.1984592751413584\n",
      "Validation Loss: 0.10822134881458256\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.08874802768230439\n",
      "Training Loss: 0.07046282006427645\n",
      "Training Loss: 0.06480019565671682\n",
      "Validation Loss: 0.06365002202016584\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.0625052665360272\n",
      "Training Loss: 0.06069034861400723\n",
      "Training Loss: 0.05890907131135464\n",
      "Validation Loss: 0.058454252737626604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.05713757805526257\n",
      "Training Loss: 0.054704474098980424\n",
      "Training Loss: 0.052311430033296344\n",
      "Validation Loss: 0.05129763364708156\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.04986968904733658\n",
      "Training Loss: 0.04708212280645967\n",
      "Training Loss: 0.044449634747579694\n",
      "Validation Loss: 0.04306692907356479\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.04170417399145663\n",
      "Training Loss: 0.03877735920250416\n",
      "Training Loss: 0.03620228626765311\n",
      "Validation Loss: 0.03449633944612206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.03336430111899972\n",
      "Training Loss: 0.03061322501860559\n",
      "Training Loss: 0.028453370835632086\n",
      "Validation Loss: 0.026523570708009633\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.02580488665960729\n",
      "Training Loss: 0.023601272301748397\n",
      "Training Loss: 0.022005060911178587\n",
      "Validation Loss: 0.019867335979858142\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.019543656669557093\n",
      "Training Loss: 0.018106145437341185\n",
      "Training Loss: 0.017330856609623878\n",
      "Validation Loss: 0.015455995683736179\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.015725850195158275\n",
      "Training Loss: 0.01510798701317981\n",
      "Training Loss: 0.014866533621679992\n",
      "Validation Loss: 0.012954643710380357\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.013520076340064406\n",
      "Training Loss: 0.013186695859767497\n",
      "Training Loss: 0.013104777855332941\n",
      "Validation Loss: 0.011087989476457071\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.011899155627470464\n",
      "Training Loss: 0.011718339163344353\n",
      "Training Loss: 0.011743183581857011\n",
      "Validation Loss: 0.009710687840503923\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.01075859999167733\n",
      "Training Loss: 0.010675402040360495\n",
      "Training Loss: 0.010785281122662128\n",
      "Validation Loss: 0.00877147408599934\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.010018709677970037\n",
      "Training Loss: 0.009975651440909132\n",
      "Training Loss: 0.010136254574172198\n",
      "Validation Loss: 0.008128576840043821\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.009534299711231143\n",
      "Training Loss: 0.009494985546916723\n",
      "Training Loss: 0.009680125946179032\n",
      "Validation Loss: 0.0076605377070019754\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.009196105763548986\n",
      "Training Loss: 0.009147399228531868\n",
      "Training Loss: 0.00934335466241464\n",
      "Validation Loss: 0.007301417993004905\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.008948195984121412\n",
      "Training Loss: 0.008888316350057721\n",
      "Training Loss: 0.009088699504500255\n",
      "Validation Loss: 0.0070199261581671705\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.008763876023003832\n",
      "Training Loss: 0.008694235016591847\n",
      "Training Loss: 0.008895805588690565\n",
      "Validation Loss: 0.006798976664745322\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.00862759656039998\n",
      "Training Loss: 0.008549852098803967\n",
      "Training Loss: 0.008750625250395388\n",
      "Validation Loss: 0.006626412702797587\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008527766687329859\n",
      "Training Loss: 0.00844323493191041\n",
      "Training Loss: 0.008641843171790242\n",
      "Validation Loss: 0.006492198231692813\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.008454884020611644\n",
      "Training Loss: 0.008364598125917838\n",
      "Training Loss: 0.008560116513399408\n",
      "Validation Loss: 0.006387812837571157\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008401257520308719\n",
      "Training Loss: 0.008306099843466655\n",
      "Training Loss: 0.00849798588664271\n",
      "Validation Loss: 0.006306228460744023\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.00836097751162015\n",
      "Training Loss: 0.008261736367130653\n",
      "Training Loss: 0.008449778518406675\n",
      "Validation Loss: 0.006241852769450274\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.008329739328473806\n",
      "Training Loss: 0.008227138818474486\n",
      "Training Loss: 0.008411374278366565\n",
      "Validation Loss: 0.006190380708084264\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.00830455413321033\n",
      "Training Loss: 0.008199237500084564\n",
      "Training Loss: 0.008379873053636402\n",
      "Validation Loss: 0.00614856856644907\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008283422426320613\n",
      "Training Loss: 0.008175947754643857\n",
      "Training Loss: 0.008353285706834867\n",
      "Validation Loss: 0.006114018338031314\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008265042447019368\n",
      "Training Loss: 0.008155879025580362\n",
      "Training Loss: 0.008330258479109034\n",
      "Validation Loss: 0.006084964707430997\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008248575339093804\n",
      "Training Loss: 0.008138109499122947\n",
      "Training Loss: 0.008309871571836993\n",
      "Validation Loss: 0.006060105172938176\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008233485270757228\n",
      "Training Loss: 0.008122029873775319\n",
      "Training Loss: 0.00829149729339406\n",
      "Validation Loss: 0.006038487474224792\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008219427146250381\n",
      "Training Loss: 0.008107229747110978\n",
      "Training Loss: 0.00827469784184359\n",
      "Validation Loss: 0.006019401555001903\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008206173004582524\n",
      "Training Loss: 0.008093436333583668\n",
      "Training Loss: 0.008259163098409772\n",
      "Validation Loss: 0.0060023163403436705\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008193573788739741\n",
      "Training Loss: 0.008080454367445781\n",
      "Training Loss: 0.008244671826250851\n",
      "Validation Loss: 0.005986837078974153\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.00818152354680933\n",
      "Training Loss: 0.008068149886094033\n",
      "Training Loss: 0.008231057913508266\n",
      "Validation Loss: 0.005972662749647927\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008169946616981179\n",
      "Training Loss: 0.008056423001689836\n",
      "Training Loss: 0.008218196383677424\n",
      "Validation Loss: 0.0059595562117383555\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008158790019806475\n",
      "Training Loss: 0.00804519815952517\n",
      "Training Loss: 0.008205989839043469\n",
      "Validation Loss: 0.00594733367684517\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008148009802680463\n",
      "Training Loss: 0.008034416212467477\n",
      "Training Loss: 0.008194361665518955\n",
      "Validation Loss: 0.005935862608086527\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008137570965336636\n",
      "Training Loss: 0.008024030190426856\n",
      "Training Loss: 0.008183250116417184\n",
      "Validation Loss: 0.005925023488784104\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008127444775309413\n",
      "Training Loss: 0.008014000159455463\n",
      "Training Loss: 0.008172602521954104\n",
      "Validation Loss: 0.005914728765459626\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008117607603780925\n",
      "Training Loss: 0.008004293274134398\n",
      "Training Loss: 0.008162374885287135\n",
      "Validation Loss: 0.005904903605297794\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008108036449411883\n",
      "Training Loss: 0.007994878117460757\n",
      "Training Loss: 0.008152528101345525\n",
      "Validation Loss: 0.0058954868353170795\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008098711663624272\n",
      "Training Loss: 0.007985729414504022\n",
      "Training Loss: 0.008143028295598924\n",
      "Validation Loss: 0.0058864250457374735\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.008089615226490423\n",
      "Training Loss: 0.00797682486125268\n",
      "Training Loss: 0.008133844985859468\n",
      "Validation Loss: 0.005877677848290527\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.008080729079665616\n",
      "Training Loss: 0.007968140160664916\n",
      "Training Loss: 0.008124949963530526\n",
      "Validation Loss: 0.005869208933168164\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.00807203835225664\n",
      "Training Loss: 0.007959656456951052\n",
      "Training Loss: 0.008116318519460037\n",
      "Validation Loss: 0.005860984449672481\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.008063526464393362\n",
      "Training Loss: 0.007951356945559382\n",
      "Training Loss: 0.008107927580131218\n",
      "Validation Loss: 0.005852979392053957\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.008055180880473927\n",
      "Training Loss: 0.007943223513429985\n",
      "Training Loss: 0.008099754622671753\n",
      "Validation Loss: 0.005845166551828217\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.008046987132402136\n",
      "Training Loss: 0.007935239230282604\n",
      "Training Loss: 0.008091780378017575\n",
      "Validation Loss: 0.005837527243682089\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.008038930937182158\n",
      "Training Loss: 0.007927389685064554\n",
      "Training Loss: 0.008083984241820871\n",
      "Validation Loss: 0.005830041384010502\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0080310016090516\n",
      "Training Loss: 0.007919662862550468\n",
      "Training Loss: 0.008076349821640178\n",
      "Validation Loss: 0.005822694422206266\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.0080231874412857\n",
      "Training Loss: 0.007912046237615869\n",
      "Training Loss: 0.008068859952036291\n",
      "Validation Loss: 0.005815463963362357\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.008015476632863283\n",
      "Training Loss: 0.007904525082558393\n",
      "Training Loss: 0.008061499661998824\n",
      "Validation Loss: 0.005808343765524666\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.008007861132500693\n",
      "Training Loss: 0.007897092369385064\n",
      "Training Loss: 0.008054254851303995\n",
      "Validation Loss: 0.005801320337596234\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.008000328744528815\n",
      "Training Loss: 0.007889734900090843\n",
      "Training Loss: 0.008047111597843468\n",
      "Validation Loss: 0.005794378723478301\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007992871335009113\n",
      "Training Loss: 0.007882445842260494\n",
      "Training Loss: 0.0080400580028072\n",
      "Validation Loss: 0.005787512167158087\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007985482015646994\n",
      "Training Loss: 0.007875213925726711\n",
      "Training Loss: 0.008033081137109549\n",
      "Validation Loss: 0.005780710596748198\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007978151483694091\n",
      "Training Loss: 0.007868034051498399\n",
      "Training Loss: 0.0080261728935875\n",
      "Validation Loss: 0.0057739656081088305\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007970873496960849\n",
      "Training Loss: 0.007860896583879366\n",
      "Training Loss: 0.008019321169704198\n",
      "Validation Loss: 0.005767267128044551\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00796364038134925\n",
      "Training Loss: 0.00785379553330131\n",
      "Training Loss: 0.008012517120223493\n",
      "Validation Loss: 0.005760608103618109\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007956447202013805\n",
      "Training Loss: 0.007846724302507936\n",
      "Training Loss: 0.00800575278000906\n",
      "Validation Loss: 0.005753985191486106\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007949287435039878\n",
      "Training Loss: 0.007839676978765055\n",
      "Training Loss: 0.007999018853297457\n",
      "Validation Loss: 0.0057473890431057875\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007942155856871977\n",
      "Training Loss: 0.00783264790661633\n",
      "Training Loss: 0.00799230980221182\n",
      "Validation Loss: 0.005740818595042808\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007935047160135583\n",
      "Training Loss: 0.007825633846223354\n",
      "Training Loss: 0.007985617411322891\n",
      "Validation Loss: 0.0057342667407303885\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007927957436768338\n",
      "Training Loss: 0.007818628792883829\n",
      "Training Loss: 0.007978936994913966\n",
      "Validation Loss: 0.005727724019134564\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007920882478356362\n",
      "Training Loss: 0.007811627908376977\n",
      "Training Loss: 0.007972260846290737\n",
      "Validation Loss: 0.005721189072512592\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007913817942608148\n",
      "Training Loss: 0.0078046269807964565\n",
      "Training Loss: 0.00796558311325498\n",
      "Validation Loss: 0.005714657760926344\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007906759813195094\n",
      "Training Loss: 0.007797623595688492\n",
      "Training Loss: 0.007958901287056506\n",
      "Validation Loss: 0.005708128458485426\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007899705852614715\n",
      "Training Loss: 0.007790613250108436\n",
      "Training Loss: 0.00795220908592455\n",
      "Validation Loss: 0.005701595787848398\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007892650959547609\n",
      "Training Loss: 0.007783594933571294\n",
      "Training Loss: 0.007945503444643691\n",
      "Validation Loss: 0.005695055057085381\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.00788559449953027\n",
      "Training Loss: 0.007776563283987343\n",
      "Training Loss: 0.007938778974348679\n",
      "Validation Loss: 0.0056885098214418195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.00787853273213841\n",
      "Training Loss: 0.00776951735606417\n",
      "Training Loss: 0.007932033769320697\n",
      "Validation Loss: 0.005681950235320778\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007871463249903171\n",
      "Training Loss: 0.007762454714393244\n",
      "Training Loss: 0.007925264310324564\n",
      "Validation Loss: 0.005675374382447577\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.00786438446259126\n",
      "Training Loss: 0.007755371291423217\n",
      "Training Loss: 0.007918466773116962\n",
      "Validation Loss: 0.005668780715414061\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007857293899869547\n",
      "Training Loss: 0.0077482678485102955\n",
      "Training Loss: 0.007911639676894993\n",
      "Validation Loss: 0.005662171736495632\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.00785018982598558\n",
      "Training Loss: 0.007741141120204702\n",
      "Training Loss: 0.00790477940114215\n",
      "Validation Loss: 0.005655539814602542\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007843070114031434\n",
      "Training Loss: 0.007733989449916407\n",
      "Training Loss: 0.007897884657140822\n",
      "Validation Loss: 0.005648884181916881\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00783593415748328\n",
      "Training Loss: 0.007726812160108239\n",
      "Training Loss: 0.007890955085167661\n",
      "Validation Loss: 0.0056422067167802474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00782878048135899\n",
      "Training Loss: 0.007719609010964632\n",
      "Training Loss: 0.007883987450040876\n",
      "Validation Loss: 0.00563550380246944\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007821606957586482\n",
      "Training Loss: 0.007712376547278837\n",
      "Training Loss: 0.007876980372238905\n",
      "Validation Loss: 0.005628773127682507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.00781441414146684\n",
      "Training Loss: 0.007705116143915803\n",
      "Training Loss: 0.007869933160254732\n",
      "Validation Loss: 0.005622017489003248\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007807199963135645\n",
      "Training Loss: 0.007697828597156331\n",
      "Training Loss: 0.007862846008501947\n",
      "Validation Loss: 0.005615234367151776\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007799965451704338\n",
      "Training Loss: 0.0076905119407456365\n",
      "Training Loss: 0.007855718408245593\n",
      "Validation Loss: 0.005608425070165416\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007792709378991276\n",
      "Training Loss: 0.007683166358619928\n",
      "Training Loss: 0.007848549234913661\n",
      "Validation Loss: 0.005601586951884661\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007785432344535366\n",
      "Training Loss: 0.007675792269874364\n",
      "Training Loss: 0.007841338571161031\n",
      "Validation Loss: 0.005594719200018333\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007778132444946096\n",
      "Training Loss: 0.007668389016762376\n",
      "Training Loss: 0.007834087386727332\n",
      "Validation Loss: 0.005587825456142342\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.0077708119212184104\n",
      "Training Loss: 0.007660959718050435\n",
      "Training Loss: 0.007826796107692643\n",
      "Validation Loss: 0.005580907721553793\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007763468954944983\n",
      "Training Loss: 0.007653502681059763\n",
      "Training Loss: 0.007819464596686886\n",
      "Validation Loss: 0.00557396628794501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007756105712614953\n",
      "Training Loss: 0.00764601954491809\n",
      "Training Loss: 0.007812094410182908\n",
      "Validation Loss: 0.005566996862337495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007748722876422107\n",
      "Training Loss: 0.007638513116398826\n",
      "Training Loss: 0.007804687363095582\n",
      "Validation Loss: 0.005560002828623806\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007741321126231924\n",
      "Training Loss: 0.007630983053240925\n",
      "Training Loss: 0.007797244092216715\n",
      "Validation Loss: 0.005552988795019435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007733900643652305\n",
      "Training Loss: 0.007623432013206184\n",
      "Training Loss: 0.007789766765199602\n",
      "Validation Loss: 0.005545957010040541\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007726463928120211\n",
      "Training Loss: 0.007615862723905593\n",
      "Training Loss: 0.007782256641658023\n",
      "Validation Loss: 0.00553890294002976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007719011642038822\n",
      "Training Loss: 0.007608274957165122\n",
      "Training Loss: 0.007774716606363654\n",
      "Validation Loss: 0.005531832363895988\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007711546521168202\n",
      "Training Loss: 0.007600673239212483\n",
      "Training Loss: 0.007767148376442492\n",
      "Validation Loss: 0.005524749167818116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007704068715684116\n",
      "Training Loss: 0.0075930600101128225\n",
      "Training Loss: 0.007759555574739352\n",
      "Validation Loss: 0.005517656680751132\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007696580981137231\n",
      "Training Loss: 0.007585437251254917\n",
      "Training Loss: 0.007751941147726029\n",
      "Validation Loss: 0.005510555670512945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.0076890862674918025\n",
      "Training Loss: 0.007577807903289795\n",
      "Training Loss: 0.007744307212997228\n",
      "Validation Loss: 0.005503448107007766\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007681586928665638\n",
      "Training Loss: 0.007570175640285015\n",
      "Training Loss: 0.007736656535416842\n",
      "Validation Loss: 0.005496338850902289\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.00767408491927199\n",
      "Training Loss: 0.0075625434133689854\n",
      "Training Loss: 0.007728994232602418\n",
      "Validation Loss: 0.005489234353436597\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007666582133388147\n",
      "Training Loss: 0.007554916399531066\n",
      "Training Loss: 0.007721323857549578\n",
      "Validation Loss: 0.005482134401400605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007659083347534761\n",
      "Training Loss: 0.007547295948024839\n",
      "Training Loss: 0.007713648313656449\n",
      "Validation Loss: 0.005475042712236388\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007651590624591336\n",
      "Training Loss: 0.007539687000680715\n",
      "Training Loss: 0.0077059729292523116\n",
      "Validation Loss: 0.005467964087748963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007644108104286716\n",
      "Training Loss: 0.0075320945924613626\n",
      "Training Loss: 0.0076983007648959755\n",
      "Validation Loss: 0.005460905108674105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007636636869283393\n",
      "Training Loss: 0.0075245230959262695\n",
      "Training Loss: 0.007690638108178973\n",
      "Validation Loss: 0.0054538664800439325\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0076291837589815255\n",
      "Training Loss: 0.0075169759034179155\n",
      "Training Loss: 0.007682987798471003\n",
      "Validation Loss: 0.005446854946098887\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007621749773388729\n",
      "Training Loss: 0.007509458202403039\n",
      "Training Loss: 0.007675355272367596\n",
      "Validation Loss: 0.005439879814832566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007614340314175933\n",
      "Training Loss: 0.007501974710030481\n",
      "Training Loss: 0.00766774553922005\n",
      "Validation Loss: 0.005432937557160268\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0076069585664663465\n",
      "Training Loss: 0.007494529846590012\n",
      "Training Loss: 0.0076601640321314335\n",
      "Validation Loss: 0.0054260379545851035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.00759960925555788\n",
      "Training Loss: 0.007487129509681836\n",
      "Training Loss: 0.007652615699917078\n",
      "Validation Loss: 0.005419188659125416\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.007592295677168295\n",
      "Training Loss: 0.007479777560802176\n",
      "Training Loss: 0.007645106170093641\n",
      "Validation Loss: 0.005412390320875755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.007585022473940626\n",
      "Training Loss: 0.007472479393472895\n",
      "Training Loss: 0.007637640762841329\n",
      "Validation Loss: 0.005405648280552515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0075777949416078625\n",
      "Training Loss: 0.007465240273158997\n",
      "Training Loss: 0.007630223699379712\n",
      "Validation Loss: 0.0053989691790611895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007570614413125441\n",
      "Training Loss: 0.007458064607344567\n",
      "Training Loss: 0.007622860866831616\n",
      "Validation Loss: 0.005392360746902362\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007563486903673038\n",
      "Training Loss: 0.007450957512483001\n",
      "Training Loss: 0.007615557365352288\n",
      "Validation Loss: 0.005385822574660349\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007556415389990434\n",
      "Training Loss: 0.007443924272665754\n",
      "Training Loss: 0.007608319263672456\n",
      "Validation Loss: 0.005379363861761652\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007549406279576942\n",
      "Training Loss: 0.0074369694956112654\n",
      "Training Loss: 0.0076011512719560415\n",
      "Validation Loss: 0.0053729901660568595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.007542461276752874\n",
      "Training Loss: 0.007430096734315157\n",
      "Training Loss: 0.007594057817477733\n",
      "Validation Loss: 0.005366702497350785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.007535585437435657\n",
      "Training Loss: 0.007423311790917069\n",
      "Training Loss: 0.007587045031832531\n",
      "Validation Loss: 0.005360510600521491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.007528781396104023\n",
      "Training Loss: 0.00741661838721484\n",
      "Training Loss: 0.007580116365570575\n",
      "Validation Loss: 0.00535441315968343\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.007522053815773688\n",
      "Training Loss: 0.0074100192845799025\n",
      "Training Loss: 0.007573276433395222\n",
      "Validation Loss: 0.005348417137519278\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.007515404978767038\n",
      "Training Loss: 0.007403519381769002\n",
      "Training Loss: 0.007566530130570754\n",
      "Validation Loss: 0.005342526821775383\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007508838157518767\n",
      "Training Loss: 0.007397122104885056\n",
      "Training Loss: 0.007559881075285375\n",
      "Validation Loss: 0.005336746005759983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.007502356900949962\n",
      "Training Loss: 0.0073908299487084154\n",
      "Training Loss: 0.007553332635434344\n",
      "Validation Loss: 0.005331077394494264\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.007495963091496378\n",
      "Training Loss: 0.007384645739803091\n",
      "Training Loss: 0.007546887826174497\n",
      "Validation Loss: 0.005325520566790208\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.00748965923092328\n",
      "Training Loss: 0.007378571620211005\n",
      "Training Loss: 0.007540549569530413\n",
      "Validation Loss: 0.005320080336225167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.007483447451377288\n",
      "Training Loss: 0.007372609082376584\n",
      "Training Loss: 0.007534321104176342\n",
      "Validation Loss: 0.00531475973875377\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0074773284676484765\n",
      "Training Loss: 0.007366761020384729\n",
      "Training Loss: 0.00752820310764946\n",
      "Validation Loss: 0.005309557505579812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.00747130565403495\n",
      "Training Loss: 0.007361026769503951\n",
      "Training Loss: 0.007522198992082849\n",
      "Validation Loss: 0.005304475130481917\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.0074653784389374775\n",
      "Training Loss: 0.0073554068908561025\n",
      "Training Loss: 0.0075163077062461526\n",
      "Validation Loss: 0.00529951593064274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007459547186736018\n",
      "Training Loss: 0.007349903848953545\n",
      "Training Loss: 0.007510532668093219\n",
      "Validation Loss: 0.005294675898235919\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007453814024920576\n",
      "Training Loss: 0.007344515738077461\n",
      "Training Loss: 0.007504873451543972\n",
      "Validation Loss: 0.005289959442655273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.007448177292826585\n",
      "Training Loss: 0.00733924275264144\n",
      "Training Loss: 0.00749932809965685\n",
      "Validation Loss: 0.005285363279419083\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.007442636581836268\n",
      "Training Loss: 0.007334082898451015\n",
      "Training Loss: 0.007493898422690108\n",
      "Validation Loss: 0.00528088708413409\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.007437192731886171\n",
      "Training Loss: 0.0073290363198611885\n",
      "Training Loss: 0.007488583754748106\n",
      "Validation Loss: 0.005276527554006054\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.007431844695238396\n",
      "Training Loss: 0.007324100981932134\n",
      "Training Loss: 0.007483380908379331\n",
      "Validation Loss: 0.00527228070213721\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.007426591225084849\n",
      "Training Loss: 0.007319274137262255\n",
      "Training Loss: 0.007478290207218379\n",
      "Validation Loss: 0.005268147331662476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.007421430892427452\n",
      "Training Loss: 0.0073145539581310004\n",
      "Training Loss: 0.007473309464985505\n",
      "Validation Loss: 0.005264123302643721\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.007416362079093233\n",
      "Training Loss: 0.007309938684338704\n",
      "Training Loss: 0.007468436914496124\n",
      "Validation Loss: 0.005260208015999851\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.0074113828723784535\n",
      "Training Loss: 0.007305425370577723\n",
      "Training Loss: 0.007463668711716309\n",
      "Validation Loss: 0.005256396231733346\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.007406490973662585\n",
      "Training Loss: 0.00730101078748703\n",
      "Training Loss: 0.0074590040254406634\n",
      "Validation Loss: 0.005252684923045839\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.007401686249068007\n",
      "Training Loss: 0.0072966918512247505\n",
      "Training Loss: 0.007454439164139331\n",
      "Validation Loss: 0.005249070558236556\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.007396963336504996\n",
      "Training Loss: 0.0072924655105452985\n",
      "Training Loss: 0.007449972487520427\n",
      "Validation Loss: 0.005245549291675764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.0073923231731168925\n",
      "Training Loss: 0.007288328375434503\n",
      "Training Loss: 0.00744559885119088\n",
      "Validation Loss: 0.00524211728296588\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.007387761395657435\n",
      "Training Loss: 0.0072842770535498855\n",
      "Training Loss: 0.007441316744079813\n",
      "Validation Loss: 0.005238770666856612\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.007383276191540063\n",
      "Training Loss: 0.0072803093213588\n",
      "Training Loss: 0.007437122748233378\n",
      "Validation Loss: 0.005235508260882219\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.007378864311613142\n",
      "Training Loss: 0.007276421613059938\n",
      "Training Loss: 0.00743301315465942\n",
      "Validation Loss: 0.0052323234319854315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.007374524322804063\n",
      "Training Loss: 0.007272610017098486\n",
      "Training Loss: 0.007428985828300938\n",
      "Validation Loss: 0.005229214070302047\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.0073702538677025585\n",
      "Training Loss: 0.007268871907144785\n",
      "Training Loss: 0.007425036489730701\n",
      "Validation Loss: 0.005226179133326317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.0073660496826050805\n",
      "Training Loss: 0.007265203840797767\n",
      "Training Loss: 0.0074211634485982355\n",
      "Validation Loss: 0.005223209954002935\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.0073619095067260784\n",
      "Training Loss: 0.007261602143989876\n",
      "Training Loss: 0.007417361717671156\n",
      "Validation Loss: 0.005220306358362935\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.007357831875560805\n",
      "Training Loss: 0.007258064647903666\n",
      "Training Loss: 0.007413628002395854\n",
      "Validation Loss: 0.005217461974956514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0073538129846565424\n",
      "Training Loss: 0.007254587911302224\n",
      "Training Loss: 0.007409960989607498\n",
      "Validation Loss: 0.005214677491811303\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.007349851443432271\n",
      "Training Loss: 0.007251169817754999\n",
      "Training Loss: 0.007406357177533209\n",
      "Validation Loss: 0.0052119440849075155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.007345944432308897\n",
      "Training Loss: 0.007247805934166535\n",
      "Training Loss: 0.007402812836226076\n",
      "Validation Loss: 0.005209264473654748\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.00734209043206647\n",
      "Training Loss: 0.007244495783234015\n",
      "Training Loss: 0.007399326201993972\n",
      "Validation Loss: 0.0052066322342817035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.0073382873833179475\n",
      "Training Loss: 0.0072412356710992755\n",
      "Training Loss: 0.007395893797511235\n",
      "Validation Loss: 0.005204044015596757\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.007334533078828826\n",
      "Training Loss: 0.007238023453392088\n",
      "Training Loss: 0.007392514782259241\n",
      "Validation Loss: 0.0052014993571707704\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.007330826272955164\n",
      "Training Loss: 0.007234856686554849\n",
      "Training Loss: 0.00738918416085653\n",
      "Validation Loss: 0.005198996558555224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.00732716363389045\n",
      "Training Loss: 0.007231731514912099\n",
      "Training Loss: 0.007385900804074481\n",
      "Validation Loss: 0.005196532027879625\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00732354394509457\n",
      "Training Loss: 0.007228648365708068\n",
      "Training Loss: 0.007382662910968066\n",
      "Validation Loss: 0.005194101575500426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.007319965860224329\n",
      "Training Loss: 0.007225605053827166\n",
      "Training Loss: 0.007379468044964596\n",
      "Validation Loss: 0.005191706347258322\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.007316428055637516\n",
      "Training Loss: 0.007222599849337712\n",
      "Training Loss: 0.007376314538996667\n",
      "Validation Loss: 0.005189341141088876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.007312928687315434\n",
      "Training Loss: 0.007219629073515535\n",
      "Training Loss: 0.007373200432630256\n",
      "Validation Loss: 0.005187010890908874\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.00730946525814943\n",
      "Training Loss: 0.007216693917289376\n",
      "Training Loss: 0.007370122887659818\n",
      "Validation Loss: 0.005184704802794319\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.00730603730189614\n",
      "Training Loss: 0.007213789203669876\n",
      "Training Loss: 0.007367080163676292\n",
      "Validation Loss: 0.005182422622985971\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.0073026433039922265\n",
      "Training Loss: 0.007210916104959324\n",
      "Training Loss: 0.007364072429481894\n",
      "Validation Loss: 0.0051801690434137085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.007299281634623185\n",
      "Training Loss: 0.007208071639761329\n",
      "Training Loss: 0.0073610962834209205\n",
      "Validation Loss: 0.005177937772418006\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.007295951863052323\n",
      "Training Loss: 0.007205255272565409\n",
      "Training Loss: 0.0073581510048825296\n",
      "Validation Loss: 0.005175725733495077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.007292651397874579\n",
      "Training Loss: 0.007202466471353546\n",
      "Training Loss: 0.007355234780116007\n",
      "Validation Loss: 0.005173536467501957\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.007289379856083542\n",
      "Training Loss: 0.007199702967191115\n",
      "Training Loss: 0.0073523468780331315\n",
      "Validation Loss: 0.00517136818765966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.00728613548912108\n",
      "Training Loss: 0.007196963467868045\n",
      "Training Loss: 0.007349485440645367\n",
      "Validation Loss: 0.005169219736355158\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.0072829181648558\n",
      "Training Loss: 0.007194247726583853\n",
      "Training Loss: 0.007346650902181864\n",
      "Validation Loss: 0.005167086908248452\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.007279725383268669\n",
      "Training Loss: 0.0071915539540350435\n",
      "Training Loss: 0.00734383961185813\n",
      "Validation Loss: 0.005164970473773526\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.007276557405712083\n",
      "Training Loss: 0.007188881286419928\n",
      "Training Loss: 0.0073410523741040375\n",
      "Validation Loss: 0.005162870192251513\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.007273413263610564\n",
      "Training Loss: 0.007186228120699525\n",
      "Training Loss: 0.0073382868559565394\n",
      "Validation Loss: 0.005160782924392836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.007270290449960157\n",
      "Training Loss: 0.00718359459657222\n",
      "Training Loss: 0.0073355425521731375\n",
      "Validation Loss: 0.005158707655160531\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.007267189325066283\n",
      "Training Loss: 0.007180979320546612\n",
      "Training Loss: 0.007332818938884884\n",
      "Validation Loss: 0.005156645810315281\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.0072641078813467175\n",
      "Training Loss: 0.00717838148586452\n",
      "Training Loss: 0.007330114366486669\n",
      "Validation Loss: 0.005154599227649526\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.007261045534978621\n",
      "Training Loss: 0.007175800701370463\n",
      "Training Loss: 0.007327428988646716\n",
      "Validation Loss: 0.005152563268862916\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.007258001160807908\n",
      "Training Loss: 0.007173235380323603\n",
      "Training Loss: 0.007324760630726814\n",
      "Validation Loss: 0.005150536067386189\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.007254974502138793\n",
      "Training Loss: 0.007170684904558584\n",
      "Training Loss: 0.007322108371881768\n",
      "Validation Loss: 0.005148518825305647\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.0072519637981895355\n",
      "Training Loss: 0.007168147744378075\n",
      "Training Loss: 0.007319472874514759\n",
      "Validation Loss: 0.00514650980162361\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.0072489677899284285\n",
      "Training Loss: 0.007165625209454447\n",
      "Training Loss: 0.007316852393560111\n",
      "Validation Loss: 0.00514451120561512\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.007245987158967182\n",
      "Training Loss: 0.007163115134462714\n",
      "Training Loss: 0.007314246905734762\n",
      "Validation Loss: 0.0051425205912503805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.007243018568842672\n",
      "Training Loss: 0.0071606173471082\n",
      "Training Loss: 0.007311655193334445\n",
      "Validation Loss: 0.005140539178928214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.007240064544603229\n",
      "Training Loss: 0.0071581302280537785\n",
      "Training Loss: 0.007309075475204736\n",
      "Validation Loss: 0.005138561660633161\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.007237120437202975\n",
      "Training Loss: 0.007155654119560495\n",
      "Training Loss: 0.00730650874087587\n",
      "Validation Loss: 0.005136594371189981\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.0072341882344335315\n",
      "Training Loss: 0.007153189243981614\n",
      "Training Loss: 0.007303953826194629\n",
      "Validation Loss: 0.005134630877671115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.007231265703449025\n",
      "Training Loss: 0.00715073331957683\n",
      "Training Loss: 0.00730140928295441\n",
      "Validation Loss: 0.005132673270320206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.007228352560778148\n",
      "Training Loss: 0.0071482857060618694\n",
      "Training Loss: 0.007298875754931941\n",
      "Validation Loss: 0.005130720221479371\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.0072254479554248975\n",
      "Training Loss: 0.007145847150823102\n",
      "Training Loss: 0.007296352090779692\n",
      "Validation Loss: 0.0051287730836592025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.007222551177837886\n",
      "Training Loss: 0.007143416323233395\n",
      "Training Loss: 0.007293837626930326\n",
      "Validation Loss: 0.005126829303570845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.00721966149227228\n",
      "Training Loss: 0.007140991999767721\n",
      "Training Loss: 0.007291331032756716\n",
      "Validation Loss: 0.005124888874674111\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.007216777947032824\n",
      "Training Loss: 0.00713857458322309\n",
      "Training Loss: 0.007288833562051877\n",
      "Validation Loss: 0.0051229523986661704\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.007213899824419059\n",
      "Training Loss: 0.007136164725525304\n",
      "Training Loss: 0.00728634272585623\n",
      "Validation Loss: 0.00512101827843345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.007211026008590125\n",
      "Training Loss: 0.0071337595698423685\n",
      "Training Loss: 0.007283859279705211\n",
      "Validation Loss: 0.005119087086896297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.007208157350542024\n",
      "Training Loss: 0.0071313596656545995\n",
      "Training Loss: 0.007281381456414238\n",
      "Validation Loss: 0.005117158078473438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.007205291749560274\n",
      "Training Loss: 0.007128964833682403\n",
      "Training Loss: 0.007278910798486322\n",
      "Validation Loss: 0.005115228549451724\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.007202429246972315\n",
      "Training Loss: 0.007126574207795784\n",
      "Training Loss: 0.00727644499973394\n",
      "Validation Loss: 0.005113303988355767\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.007199568363139406\n",
      "Training Loss: 0.007124186806613579\n",
      "Training Loss: 0.00727398358634673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [19:43<04:56, 148.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.005111378490705085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.14876488994807005\n",
      "Training Loss: 0.10600458825007081\n",
      "Training Loss: 0.07460961313918232\n",
      "Validation Loss: 0.057474783883335885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.054697492932900785\n",
      "Training Loss: 0.05180074276402593\n",
      "Training Loss: 0.05028439523652196\n",
      "Validation Loss: 0.04978636605141873\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.049136332208290695\n",
      "Training Loss: 0.04689816449768841\n",
      "Training Loss: 0.045243909154087304\n",
      "Validation Loss: 0.0443980877047007\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.043843934498727324\n",
      "Training Loss: 0.04151494322344661\n",
      "Training Loss: 0.03984220564365387\n",
      "Validation Loss: 0.03871666606557503\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.038322682827711105\n",
      "Training Loss: 0.036077031726017596\n",
      "Training Loss: 0.034593970160931346\n",
      "Validation Loss: 0.03331266781001278\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.03310520714148879\n",
      "Training Loss: 0.03110507706180215\n",
      "Training Loss: 0.029928713077679275\n",
      "Validation Loss: 0.028559435530450573\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.02852082050871104\n",
      "Training Loss: 0.0268409853707999\n",
      "Training Loss: 0.025989456246607007\n",
      "Validation Loss: 0.024536267524647912\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.024637530297040938\n",
      "Training Loss: 0.02328048559371382\n",
      "Training Loss: 0.02272159967571497\n",
      "Validation Loss: 0.021159350848495122\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.02138247962575406\n",
      "Training Loss: 0.020317124472931027\n",
      "Training Loss: 0.020002381466329096\n",
      "Validation Loss: 0.018308131309393585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.018652828480117022\n",
      "Training Loss: 0.017844723351299763\n",
      "Training Loss: 0.01772899378556758\n",
      "Validation Loss: 0.015899507611403973\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.016379818432033063\n",
      "Training Loss: 0.01580096099060029\n",
      "Training Loss: 0.01584606476360932\n",
      "Validation Loss: 0.013894798210560439\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.014529493376612664\n",
      "Training Loss: 0.014151691754814238\n",
      "Training Loss: 0.01431834014132619\n",
      "Validation Loss: 0.012264598472295015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.013068375941365958\n",
      "Training Loss: 0.01285580316791311\n",
      "Training Loss: 0.013098622730467469\n",
      "Validation Loss: 0.01096657570451498\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.011945164590142667\n",
      "Training Loss: 0.011853472035145387\n",
      "Training Loss: 0.012127133407630027\n",
      "Validation Loss: 0.009946184652710983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.01109533960232511\n",
      "Training Loss: 0.011076982049271464\n",
      "Training Loss: 0.011348877258133144\n",
      "Validation Loss: 0.009143345514469351\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.010452923069242387\n",
      "Training Loss: 0.010467432620935142\n",
      "Training Loss: 0.010723647624254228\n",
      "Validation Loss: 0.008508615312166512\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.009965380756184458\n",
      "Training Loss: 0.009985733045032247\n",
      "Training Loss: 0.010226375481579453\n",
      "Validation Loss: 0.008011279751944324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.009598392192274333\n",
      "Training Loss: 0.00960994927212596\n",
      "Training Loss: 0.009841169170103968\n",
      "Validation Loss: 0.00763175681264799\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.009328812308376655\n",
      "Training Loss: 0.009325457359664143\n",
      "Training Loss: 0.009553543024230749\n",
      "Validation Loss: 0.007351639895082525\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.009136669033905491\n",
      "Training Loss: 0.009117205878719687\n",
      "Training Loss: 0.009345752184744925\n",
      "Validation Loss: 0.007149531676046802\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.009002050390699878\n",
      "Training Loss: 0.008967792721232399\n",
      "Training Loss: 0.009197637072065844\n",
      "Validation Loss: 0.00700329931510424\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.00890655571478419\n",
      "Training Loss: 0.008860003841109574\n",
      "Training Loss: 0.00909049138543196\n",
      "Validation Loss: 0.006894139456562698\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008835693105356767\n",
      "Training Loss: 0.008779775956645608\n",
      "Training Loss: 0.009009893145412206\n",
      "Validation Loss: 0.0068086143426094835\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008779673294629902\n",
      "Training Loss: 0.00871723314980045\n",
      "Training Loss: 0.00894612975534983\n",
      "Validation Loss: 0.00673822172933122\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.008732616034103557\n",
      "Training Loss: 0.00866609561839141\n",
      "Training Loss: 0.008893191770184786\n",
      "Validation Loss: 0.006677955502822074\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.0086912488238886\n",
      "Training Loss: 0.008622569896979258\n",
      "Training Loss: 0.008847500982228666\n",
      "Validation Loss: 0.006624925253766306\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.00865380119299516\n",
      "Training Loss: 0.008584384296555073\n",
      "Training Loss: 0.008806927134282888\n",
      "Validation Loss: 0.006577404334963289\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008619298402918503\n",
      "Training Loss: 0.008550143195316195\n",
      "Training Loss: 0.008770165076712147\n",
      "Validation Loss: 0.006534315671855479\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008587164811324328\n",
      "Training Loss: 0.008518951379228383\n",
      "Training Loss: 0.008736377236200497\n",
      "Validation Loss: 0.00649490575253796\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008557026762282476\n",
      "Training Loss: 0.008490204814588651\n",
      "Training Loss: 0.008704995961161331\n",
      "Validation Loss: 0.006458633279773208\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008528617825359106\n",
      "Training Loss: 0.008463472040602937\n",
      "Training Loss: 0.008675618262495846\n",
      "Validation Loss: 0.006425075897465596\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008501731914002449\n",
      "Training Loss: 0.008438434213167057\n",
      "Training Loss: 0.008647948767757043\n",
      "Validation Loss: 0.00639389641059751\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008476203490281477\n",
      "Training Loss: 0.008414848619140684\n",
      "Training Loss: 0.008621760780224577\n",
      "Validation Loss: 0.0063648174328499296\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008451899022329599\n",
      "Training Loss: 0.00839252669364214\n",
      "Training Loss: 0.008596880761906504\n",
      "Validation Loss: 0.006337606542994886\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008428705784026533\n",
      "Training Loss: 0.00837131544481963\n",
      "Training Loss: 0.008573171121533961\n",
      "Validation Loss: 0.0063120728369090665\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008406529246130959\n",
      "Training Loss: 0.008351096027763561\n",
      "Training Loss: 0.008550521973520518\n",
      "Validation Loss: 0.006288051724256006\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008385292146122082\n",
      "Training Loss: 0.008331770830554888\n",
      "Training Loss: 0.00852884579449892\n",
      "Validation Loss: 0.0062654057473174474\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008364929392701014\n",
      "Training Loss: 0.008313258507987484\n",
      "Training Loss: 0.008508072789991274\n",
      "Validation Loss: 0.006244011881258967\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008345386419678106\n",
      "Training Loss: 0.00829549209563993\n",
      "Training Loss: 0.008488142347196116\n",
      "Validation Loss: 0.006223766587078153\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008326613192912191\n",
      "Training Loss: 0.008278417877154425\n",
      "Training Loss: 0.008469007624080404\n",
      "Validation Loss: 0.006204581827322921\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008308571309316903\n",
      "Training Loss: 0.008261987804435194\n",
      "Training Loss: 0.008450625034747646\n",
      "Validation Loss: 0.006186379764616238\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008291221221443266\n",
      "Training Loss: 0.008246158660622314\n",
      "Training Loss: 0.008432955882744864\n",
      "Validation Loss: 0.00616907904819282\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.00827453008852899\n",
      "Training Loss: 0.008230892040301115\n",
      "Training Loss: 0.008415967305190862\n",
      "Validation Loss: 0.0061526202905420845\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.00825846471474506\n",
      "Training Loss: 0.0082161582831759\n",
      "Training Loss: 0.008399626379832626\n",
      "Validation Loss: 0.006136941777155055\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.008242994729662313\n",
      "Training Loss: 0.00820192230399698\n",
      "Training Loss: 0.008383903962094336\n",
      "Validation Loss: 0.0061219916155750165\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.008228091545170173\n",
      "Training Loss: 0.008188158719567582\n",
      "Training Loss: 0.008368771120440216\n",
      "Validation Loss: 0.006107715123301644\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.00821372530190274\n",
      "Training Loss: 0.00817483954015188\n",
      "Training Loss: 0.008354199128225446\n",
      "Validation Loss: 0.006094066430082063\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.00819986921036616\n",
      "Training Loss: 0.008161939720157534\n",
      "Training Loss: 0.008340160551015288\n",
      "Validation Loss: 0.006081001604174714\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.008186494339024648\n",
      "Training Loss: 0.008149436112726107\n",
      "Training Loss: 0.008326629420043901\n",
      "Validation Loss: 0.0060684791062383\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.008173574897227809\n",
      "Training Loss: 0.00813730459776707\n",
      "Training Loss: 0.00831357813673094\n",
      "Validation Loss: 0.006056461876959362\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.00816108379047364\n",
      "Training Loss: 0.00812552343471907\n",
      "Training Loss: 0.008300980690401048\n",
      "Validation Loss: 0.006044913338382174\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.008148995235096664\n",
      "Training Loss: 0.00811407228349708\n",
      "Training Loss: 0.0082888127269689\n",
      "Validation Loss: 0.006033797964213018\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.008137285745469854\n",
      "Training Loss: 0.008102930813329294\n",
      "Training Loss: 0.008277049589669332\n",
      "Validation Loss: 0.006023087763761202\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.008125931173563004\n",
      "Training Loss: 0.008092079983325675\n",
      "Training Loss: 0.008265667244559153\n",
      "Validation Loss: 0.006012748355265665\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.008114908181596547\n",
      "Training Loss: 0.008081501887645573\n",
      "Training Loss: 0.008254643317777664\n",
      "Validation Loss: 0.00600275684807324\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.00810419448884204\n",
      "Training Loss: 0.00807118151220493\n",
      "Training Loss: 0.008243955345824361\n",
      "Validation Loss: 0.005993089422824259\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.008093771149870008\n",
      "Training Loss: 0.008061098888283595\n",
      "Training Loss: 0.008233582344837486\n",
      "Validation Loss: 0.005983714142479421\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.008083615520736203\n",
      "Training Loss: 0.00805124144302681\n",
      "Training Loss: 0.008223503509070725\n",
      "Validation Loss: 0.005974615174554958\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.008073711206670851\n",
      "Training Loss: 0.008041592392837629\n",
      "Training Loss: 0.008213700263295322\n",
      "Validation Loss: 0.005965772687897003\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.008064039449673146\n",
      "Training Loss: 0.008032139698043466\n",
      "Training Loss: 0.008204155405983328\n",
      "Validation Loss: 0.005957165577323333\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.008054584747878835\n",
      "Training Loss: 0.00802287019090727\n",
      "Training Loss: 0.00819485251326114\n",
      "Validation Loss: 0.005948775518539079\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.008045331871835515\n",
      "Training Loss: 0.008013772352132947\n",
      "Training Loss: 0.00818577331956476\n",
      "Validation Loss: 0.005940585813139764\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.00803626784705557\n",
      "Training Loss: 0.008004835052415728\n",
      "Training Loss: 0.008176906381268055\n",
      "Validation Loss: 0.00593258615640735\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.008027375270612537\n",
      "Training Loss: 0.007996046915650368\n",
      "Training Loss: 0.008168234530603512\n",
      "Validation Loss: 0.005924757503318402\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.008018645152915269\n",
      "Training Loss: 0.007987398452823981\n",
      "Training Loss: 0.008159747531171888\n",
      "Validation Loss: 0.005917091511210866\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.008010065043345093\n",
      "Training Loss: 0.007978880894370377\n",
      "Training Loss: 0.00815143135143444\n",
      "Validation Loss: 0.005909574146152296\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.008001624800963327\n",
      "Training Loss: 0.00797048581414856\n",
      "Training Loss: 0.00814327623695135\n",
      "Validation Loss: 0.005902193014489047\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.00799331403686665\n",
      "Training Loss: 0.007962204318027943\n",
      "Training Loss: 0.008135270946659147\n",
      "Validation Loss: 0.005894941003114152\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007985123749822378\n",
      "Training Loss: 0.007954030961263925\n",
      "Training Loss: 0.008127405549166723\n",
      "Validation Loss: 0.005887806170954882\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007977045291336254\n",
      "Training Loss: 0.007945955959148704\n",
      "Training Loss: 0.008119671751046553\n",
      "Validation Loss: 0.005880781186462035\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007969071628758683\n",
      "Training Loss: 0.00793797591002658\n",
      "Training Loss: 0.00811206005862914\n",
      "Validation Loss: 0.005873857793304023\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007961194978561252\n",
      "Training Loss: 0.0079300831630826\n",
      "Training Loss: 0.008104563332162798\n",
      "Validation Loss: 0.005867030000669903\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007953408792382106\n",
      "Training Loss: 0.007922272060532122\n",
      "Training Loss: 0.008097174433059991\n",
      "Validation Loss: 0.005860288203641605\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.00794570663711056\n",
      "Training Loss: 0.007914537412580103\n",
      "Training Loss: 0.008089884973596782\n",
      "Validation Loss: 0.005853626236131185\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007938084207708015\n",
      "Training Loss: 0.00790687583736144\n",
      "Training Loss: 0.008082691153977067\n",
      "Validation Loss: 0.005847042919597013\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007930534629849717\n",
      "Training Loss: 0.00789928222540766\n",
      "Training Loss: 0.008075586373452097\n",
      "Validation Loss: 0.0058405241233118795\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007923053574049845\n",
      "Training Loss: 0.007891749248374253\n",
      "Training Loss: 0.00806856388109736\n",
      "Validation Loss: 0.005834071564728792\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007915637773694471\n",
      "Training Loss: 0.007884276947006583\n",
      "Training Loss: 0.008061619377695024\n",
      "Validation Loss: 0.005827676732448882\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00790828127763234\n",
      "Training Loss: 0.007876861300319433\n",
      "Training Loss: 0.008054749385919421\n",
      "Validation Loss: 0.005821336375999401\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007900981289567425\n",
      "Training Loss: 0.00786949687404558\n",
      "Training Loss: 0.008047947011655197\n",
      "Validation Loss: 0.005815049323378905\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007893734241370111\n",
      "Training Loss: 0.007862181892851367\n",
      "Training Loss: 0.00804121006047353\n",
      "Validation Loss: 0.005808803991916893\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.00788653616560623\n",
      "Training Loss: 0.007854911347385496\n",
      "Training Loss: 0.00803453302825801\n",
      "Validation Loss: 0.005802600566046626\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007879384065745398\n",
      "Training Loss: 0.007847684365697206\n",
      "Training Loss: 0.008027912377147004\n",
      "Validation Loss: 0.005796436262264681\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007872275209520012\n",
      "Training Loss: 0.007840497165452688\n",
      "Training Loss: 0.008021344832377509\n",
      "Validation Loss: 0.005790304569161257\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007865206557326018\n",
      "Training Loss: 0.00783334668725729\n",
      "Training Loss: 0.008014827659353614\n",
      "Validation Loss: 0.005784202397152195\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007858174864668398\n",
      "Training Loss: 0.007826230468926951\n",
      "Training Loss: 0.008008357007056474\n",
      "Validation Loss: 0.005778126143902707\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007851179430726916\n",
      "Training Loss: 0.007819146964466199\n",
      "Training Loss: 0.008001928995363415\n",
      "Validation Loss: 0.0057720747145855525\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007844216775847599\n",
      "Training Loss: 0.007812092803651467\n",
      "Training Loss: 0.007995541639393195\n",
      "Validation Loss: 0.0057660450876345125\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007837284633424133\n",
      "Training Loss: 0.007805067577864974\n",
      "Training Loss: 0.00798919168766588\n",
      "Validation Loss: 0.005760034844488575\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007830380701925606\n",
      "Training Loss: 0.007798068487318233\n",
      "Training Loss: 0.007982876729220151\n",
      "Validation Loss: 0.0057540383448908\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007823503331746907\n",
      "Training Loss: 0.007791092781117186\n",
      "Training Loss: 0.007976594928186387\n",
      "Validation Loss: 0.00574805272816356\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007816651863977312\n",
      "Training Loss: 0.00778413797263056\n",
      "Training Loss: 0.007970342629123479\n",
      "Validation Loss: 0.005742076917792137\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007809822101844475\n",
      "Training Loss: 0.007777203161967918\n",
      "Training Loss: 0.007964117395458744\n",
      "Validation Loss: 0.00573610743439725\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007803013210650534\n",
      "Training Loss: 0.007770285964943469\n",
      "Training Loss: 0.007957917107269168\n",
      "Validation Loss: 0.005730141497091547\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007796223821351304\n",
      "Training Loss: 0.0077633849368430674\n",
      "Training Loss: 0.007951738553820177\n",
      "Validation Loss: 0.0057241756186475245\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007789451393764466\n",
      "Training Loss: 0.007756498026428744\n",
      "Training Loss: 0.007945580079685897\n",
      "Validation Loss: 0.005718205751997702\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007782693875487894\n",
      "Training Loss: 0.007749623226700351\n",
      "Training Loss: 0.007939439978217706\n",
      "Validation Loss: 0.005712235961214043\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.0077759505854919555\n",
      "Training Loss: 0.007742759230313823\n",
      "Training Loss: 0.007933314417023211\n",
      "Validation Loss: 0.005706255045572944\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007769219252513722\n",
      "Training Loss: 0.0077359047601930795\n",
      "Training Loss: 0.0079272028664127\n",
      "Validation Loss: 0.005700265993939691\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.00776249747374095\n",
      "Training Loss: 0.0077290558652020995\n",
      "Training Loss: 0.007921101436950266\n",
      "Validation Loss: 0.0056942653426314506\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007755784494802356\n",
      "Training Loss: 0.007722213411470875\n",
      "Training Loss: 0.007915010256692768\n",
      "Validation Loss: 0.005688246782984208\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007749079266795888\n",
      "Training Loss: 0.007715374488616362\n",
      "Training Loss: 0.007908924932125956\n",
      "Validation Loss: 0.005682215013468031\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007742379298433661\n",
      "Training Loss: 0.0077085385110694914\n",
      "Training Loss: 0.007902844391064718\n",
      "Validation Loss: 0.00567616229704215\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007735682998318225\n",
      "Training Loss: 0.007701702539343387\n",
      "Training Loss: 0.007896766042103992\n",
      "Validation Loss: 0.0056700845133889925\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007728987528244033\n",
      "Training Loss: 0.007694864536169917\n",
      "Training Loss: 0.007890687211183831\n",
      "Validation Loss: 0.005663983556546606\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007722293109400198\n",
      "Training Loss: 0.007688025325769558\n",
      "Training Loss: 0.007884606999577954\n",
      "Validation Loss: 0.00565785426892382\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007715597549686208\n",
      "Training Loss: 0.0076811787660699335\n",
      "Training Loss: 0.00787852258537896\n",
      "Validation Loss: 0.005651690471352318\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.007708897358970717\n",
      "Training Loss: 0.007674326746491715\n",
      "Training Loss: 0.007872430921997875\n",
      "Validation Loss: 0.005645498613764145\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007702193638542667\n",
      "Training Loss: 0.0076674679096322505\n",
      "Training Loss: 0.00786633218289353\n",
      "Validation Loss: 0.00563927073806022\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.007695482772542164\n",
      "Training Loss: 0.00766059884801507\n",
      "Training Loss: 0.007860222060699015\n",
      "Validation Loss: 0.005633004423063458\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.007688763140467927\n",
      "Training Loss: 0.0076537185802590104\n",
      "Training Loss: 0.007854099535616115\n",
      "Validation Loss: 0.0056266989572015536\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.007682033603778109\n",
      "Training Loss: 0.007646825191332027\n",
      "Training Loss: 0.007847961969673634\n",
      "Validation Loss: 0.005620351944150131\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007675292853964493\n",
      "Training Loss: 0.00763991724466905\n",
      "Training Loss: 0.007841807279037312\n",
      "Validation Loss: 0.0056139617789473925\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.007668538459111005\n",
      "Training Loss: 0.0076329944771714505\n",
      "Training Loss: 0.007835634832736105\n",
      "Validation Loss: 0.005607524496932211\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007661768437828869\n",
      "Training Loss: 0.007626054972643032\n",
      "Training Loss: 0.007829442017246037\n",
      "Validation Loss: 0.005601040128189442\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007654982546810061\n",
      "Training Loss: 0.007619095999980345\n",
      "Training Loss: 0.007823226494947448\n",
      "Validation Loss: 0.005594504547169369\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.007648178591625765\n",
      "Training Loss: 0.007612118949182332\n",
      "Training Loss: 0.00781698793405667\n",
      "Validation Loss: 0.005587918822538484\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.007641355256782845\n",
      "Training Loss: 0.007605120264925063\n",
      "Training Loss: 0.007810723023721948\n",
      "Validation Loss: 0.005581278258442795\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.007634510612115264\n",
      "Training Loss: 0.007598100128816441\n",
      "Training Loss: 0.007804431124823168\n",
      "Validation Loss: 0.005574586382658964\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.007627644675085321\n",
      "Training Loss: 0.007591055439552292\n",
      "Training Loss: 0.007798109513241797\n",
      "Validation Loss: 0.005567835073583246\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.007620753992814571\n",
      "Training Loss: 0.007583986686076969\n",
      "Training Loss: 0.007791758149396628\n",
      "Validation Loss: 0.005561027984563889\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007613839356927201\n",
      "Training Loss: 0.007576893400400877\n",
      "Training Loss: 0.007785374673549086\n",
      "Validation Loss: 0.005554161483181243\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0076068982400465755\n",
      "Training Loss: 0.007569773129653186\n",
      "Training Loss: 0.007778957422124222\n",
      "Validation Loss: 0.0055472347492959055\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.00759992976905778\n",
      "Training Loss: 0.007562627368606627\n",
      "Training Loss: 0.007772506381152198\n",
      "Validation Loss: 0.0055402517017877\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.007592932363040745\n",
      "Training Loss: 0.007555453723762184\n",
      "Training Loss: 0.007766019870759919\n",
      "Validation Loss: 0.005533205206621062\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0075859064573887735\n",
      "Training Loss: 0.007548251516418532\n",
      "Training Loss: 0.007759496155194938\n",
      "Validation Loss: 0.005526099801377467\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0075788504572119565\n",
      "Training Loss: 0.007541020401986316\n",
      "Training Loss: 0.007752934416057542\n",
      "Validation Loss: 0.0055189312545561725\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.007571763466112316\n",
      "Training Loss: 0.007533761848462745\n",
      "Training Loss: 0.007746335670817644\n",
      "Validation Loss: 0.005511701126645706\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.0075646449963096525\n",
      "Training Loss: 0.0075264741503633555\n",
      "Training Loss: 0.007739697161596269\n",
      "Validation Loss: 0.005504412297944255\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007557493872009218\n",
      "Training Loss: 0.00751915699802339\n",
      "Training Loss: 0.007733019074657932\n",
      "Validation Loss: 0.005497063928691859\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007550310490187257\n",
      "Training Loss: 0.007511811280855909\n",
      "Training Loss: 0.007726301802322268\n",
      "Validation Loss: 0.005489654270042613\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.007543093707645312\n",
      "Training Loss: 0.007504435180453584\n",
      "Training Loss: 0.007719543925486505\n",
      "Validation Loss: 0.005482185298440915\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.007535843993537128\n",
      "Training Loss: 0.007497031684033572\n",
      "Training Loss: 0.007712745325407013\n",
      "Validation Loss: 0.005474660324529232\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.007528560583014041\n",
      "Training Loss: 0.00748960018507205\n",
      "Training Loss: 0.007705906013725326\n",
      "Validation Loss: 0.0054670782351678\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.007521243488881737\n",
      "Training Loss: 0.007482140789506957\n",
      "Training Loss: 0.007699026645859704\n",
      "Validation Loss: 0.0054594395588037\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.007513891932321712\n",
      "Training Loss: 0.007474653589306399\n",
      "Training Loss: 0.007692105324240401\n",
      "Validation Loss: 0.0054517501345155445\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00750650680041872\n",
      "Training Loss: 0.007467138863867149\n",
      "Training Loss: 0.007685144909191877\n",
      "Validation Loss: 0.005444007754336331\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.00749908773927018\n",
      "Training Loss: 0.007459599917056039\n",
      "Training Loss: 0.007678145007230341\n",
      "Validation Loss: 0.005436216832892028\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.007491634988691658\n",
      "Training Loss: 0.007452034747693688\n",
      "Training Loss: 0.00767110526910983\n",
      "Validation Loss: 0.0054283775493837476\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.007484150390373543\n",
      "Training Loss: 0.007444445755099878\n",
      "Training Loss: 0.007664027138380334\n",
      "Validation Loss: 0.005420493304708533\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.007476631787139923\n",
      "Training Loss: 0.007436834615655244\n",
      "Training Loss: 0.00765691127628088\n",
      "Validation Loss: 0.0054125694631274495\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.007469082064926624\n",
      "Training Loss: 0.007429203225765377\n",
      "Training Loss: 0.0076497581251896915\n",
      "Validation Loss: 0.005404603654476866\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00746150137623772\n",
      "Training Loss: 0.007421549010323361\n",
      "Training Loss: 0.00764256913214922\n",
      "Validation Loss: 0.005396600165195093\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.007453889996977523\n",
      "Training Loss: 0.007413877743529156\n",
      "Training Loss: 0.007635344647569582\n",
      "Validation Loss: 0.005388564628998885\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.007446248041233048\n",
      "Training Loss: 0.007406189437024296\n",
      "Training Loss: 0.0076280871569179\n",
      "Validation Loss: 0.005380498764649285\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0074385791330132634\n",
      "Training Loss: 0.007398485625162721\n",
      "Training Loss: 0.007620798221323639\n",
      "Validation Loss: 0.0053724034393750385\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.007430882420158014\n",
      "Training Loss: 0.007390767817851156\n",
      "Training Loss: 0.00761347824591212\n",
      "Validation Loss: 0.005364287936317033\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.0074231603846419605\n",
      "Training Loss: 0.007383039055857807\n",
      "Training Loss: 0.007606129078194499\n",
      "Validation Loss: 0.0053561472025270875\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.007415414230199531\n",
      "Training Loss: 0.007375299850245938\n",
      "Training Loss: 0.00759875241201371\n",
      "Validation Loss: 0.005347989585899403\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.007407645202474669\n",
      "Training Loss: 0.007367553664371371\n",
      "Training Loss: 0.007591351110022515\n",
      "Validation Loss: 0.0053398194500464905\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.007399854341056198\n",
      "Training Loss: 0.007359802983701229\n",
      "Training Loss: 0.00758392576011829\n",
      "Validation Loss: 0.005331640920518071\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0073920449893921615\n",
      "Training Loss: 0.0073520479339640584\n",
      "Training Loss: 0.007576480017742142\n",
      "Validation Loss: 0.005323452680120558\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.007384218282531947\n",
      "Training Loss: 0.007344293802743778\n",
      "Training Loss: 0.007569014714099467\n",
      "Validation Loss: 0.005315266353381651\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.007376376041211188\n",
      "Training Loss: 0.007336542083648965\n",
      "Training Loss: 0.007561533546540886\n",
      "Validation Loss: 0.005307081873591445\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.0073685214028228075\n",
      "Training Loss: 0.007328795781359076\n",
      "Training Loss: 0.007554037383524701\n",
      "Validation Loss: 0.005298901149176396\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.0073606563697103415\n",
      "Training Loss: 0.007321055660722778\n",
      "Training Loss: 0.0075465296767652035\n",
      "Validation Loss: 0.005290731031636006\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.007352783425012603\n",
      "Training Loss: 0.007313326132716611\n",
      "Training Loss: 0.007539012347115204\n",
      "Validation Loss: 0.005282575165162261\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.007344904638594016\n",
      "Training Loss: 0.0073056108562741424\n",
      "Training Loss: 0.007531489313114434\n",
      "Validation Loss: 0.005274436287477278\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.007337023736909032\n",
      "Training Loss: 0.00729791174759157\n",
      "Training Loss: 0.007523963052080944\n",
      "Validation Loss: 0.005266321925027819\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.007329142359085381\n",
      "Training Loss: 0.007290231507504359\n",
      "Training Loss: 0.007516435801517218\n",
      "Validation Loss: 0.0052582320019477205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.0073212639766279605\n",
      "Training Loss: 0.007282573705306277\n",
      "Training Loss: 0.007508910736069083\n",
      "Validation Loss: 0.005250175029635848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.007313391410280019\n",
      "Training Loss: 0.007274941635550931\n",
      "Training Loss: 0.0075013904657680545\n",
      "Validation Loss: 0.00524214894139323\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.007305527671705931\n",
      "Training Loss: 0.007267337028170004\n",
      "Training Loss: 0.007493878017412498\n",
      "Validation Loss: 0.005234162911793657\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.00729767470213119\n",
      "Training Loss: 0.007259763204492628\n",
      "Training Loss: 0.00748637716169469\n",
      "Validation Loss: 0.005226215758371387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.007289836445706897\n",
      "Training Loss: 0.00725222438108176\n",
      "Training Loss: 0.007478889904450625\n",
      "Validation Loss: 0.005218313509870446\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.007282016261597164\n",
      "Training Loss: 0.007244722222676501\n",
      "Training Loss: 0.007471418685745448\n",
      "Validation Loss: 0.00521046083205997\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.00727421542396769\n",
      "Training Loss: 0.007237260490655899\n",
      "Training Loss: 0.007463967953808606\n",
      "Validation Loss: 0.005202660567305061\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.007266438638907857\n",
      "Training Loss: 0.0072298400150612\n",
      "Training Loss: 0.007456538708647713\n",
      "Validation Loss: 0.005194912498471526\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.007258688306901604\n",
      "Training Loss: 0.007222464147489518\n",
      "Training Loss: 0.007449135177303105\n",
      "Validation Loss: 0.005187223989809497\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.007250965965213254\n",
      "Training Loss: 0.007215137182502076\n",
      "Training Loss: 0.007441758892964572\n",
      "Validation Loss: 0.005179597656085585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0072432743973331526\n",
      "Training Loss: 0.007207858952460811\n",
      "Training Loss: 0.007434412917355075\n",
      "Validation Loss: 0.005172033537848947\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.0072356166772078725\n",
      "Training Loss: 0.00720063254237175\n",
      "Training Loss: 0.0074270998488646\n",
      "Validation Loss: 0.005164534546790666\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.007227995545254089\n",
      "Training Loss: 0.007193459826521575\n",
      "Training Loss: 0.007419821339426562\n",
      "Validation Loss: 0.005157106043247694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.007220413139439188\n",
      "Training Loss: 0.007186343964422122\n",
      "Training Loss: 0.007412580280797556\n",
      "Validation Loss: 0.005149746354240296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.00721287116757594\n",
      "Training Loss: 0.0071792855160310864\n",
      "Training Loss: 0.007405378890689462\n",
      "Validation Loss: 0.005142460295961898\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.00720537239103578\n",
      "Training Loss: 0.007172285589040257\n",
      "Training Loss: 0.007398218516027555\n",
      "Validation Loss: 0.005135248389011354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.007197917251032777\n",
      "Training Loss: 0.007165346414549276\n",
      "Training Loss: 0.007391100554959848\n",
      "Validation Loss: 0.0051281102874127914\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.007190507897175849\n",
      "Training Loss: 0.007158468085108325\n",
      "Training Loss: 0.007384026848012581\n",
      "Validation Loss: 0.005121045556897821\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.007183146327733993\n",
      "Training Loss: 0.007151651841122657\n",
      "Training Loss: 0.007376999204279855\n",
      "Validation Loss: 0.005114061409330226\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.007175834483932704\n",
      "Training Loss: 0.0071448995009996\n",
      "Training Loss: 0.0073700188309885565\n",
      "Validation Loss: 0.005107152365995676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.007168571537476964\n",
      "Training Loss: 0.007138209649710916\n",
      "Training Loss: 0.007363087553530931\n",
      "Validation Loss: 0.005100323066502558\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.0071613595535745846\n",
      "Training Loss: 0.007131584858288988\n",
      "Training Loss: 0.007356203939998522\n",
      "Validation Loss: 0.0050935735762527405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.00715419935353566\n",
      "Training Loss: 0.007125023049884476\n",
      "Training Loss: 0.007349370789015665\n",
      "Validation Loss: 0.005086902377268906\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.00714709181978833\n",
      "Training Loss: 0.007118526608683169\n",
      "Training Loss: 0.007342587603488937\n",
      "Validation Loss: 0.005080308388458209\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0071400361438281835\n",
      "Training Loss: 0.007112094623153098\n",
      "Training Loss: 0.007335856470745057\n",
      "Validation Loss: 0.005073795260552819\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.007133034430444241\n",
      "Training Loss: 0.007105726018780842\n",
      "Training Loss: 0.007329176104394719\n",
      "Validation Loss: 0.00506735752268663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.007126085959025658\n",
      "Training Loss: 0.007099421168095432\n",
      "Training Loss: 0.007322546407813206\n",
      "Validation Loss: 0.0050609994776484165\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.00711919107416179\n",
      "Training Loss: 0.007093178454670124\n",
      "Training Loss: 0.007315968970069662\n",
      "Validation Loss: 0.005054720135907946\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.007112348479568027\n",
      "Training Loss: 0.0070870009146165105\n",
      "Training Loss: 0.007309443305712193\n",
      "Validation Loss: 0.0050485148918658\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.00710555943835061\n",
      "Training Loss: 0.007080881760921329\n",
      "Training Loss: 0.007302967292489484\n",
      "Validation Loss: 0.00504238341655059\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.007098822851548903\n",
      "Training Loss: 0.007074824032024481\n",
      "Training Loss: 0.007296542830299586\n",
      "Validation Loss: 0.005036325052673562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.00709213876107242\n",
      "Training Loss: 0.007068825052701868\n",
      "Training Loss: 0.007290167338214815\n",
      "Validation Loss: 0.005030339116131196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.007085505276918412\n",
      "Training Loss: 0.007062885643681511\n",
      "Training Loss: 0.007283842118922621\n",
      "Validation Loss: 0.005024424606274939\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.007078922652290202\n",
      "Training Loss: 0.00705700310645625\n",
      "Training Loss: 0.007277564909309149\n",
      "Validation Loss: 0.0050185780953929835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.007072390822577291\n",
      "Training Loss: 0.007051177062676288\n",
      "Training Loss: 0.007271335950354114\n",
      "Validation Loss: 0.005012801507608232\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.007065909398952499\n",
      "Training Loss: 0.007045404048985801\n",
      "Training Loss: 0.0072651539091020826\n",
      "Validation Loss: 0.005007091082967399\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.007059474787674844\n",
      "Training Loss: 0.00703968690126203\n",
      "Training Loss: 0.0072590191743802275\n",
      "Validation Loss: 0.005001445609573903\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.007053088691900484\n",
      "Training Loss: 0.007034019936691038\n",
      "Training Loss: 0.007252927987137809\n",
      "Validation Loss: 0.004995860338598238\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.007046749237342737\n",
      "Training Loss: 0.007028403145959601\n",
      "Training Loss: 0.007246881852624938\n",
      "Validation Loss: 0.0049903377271885195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.007040454953676089\n",
      "Training Loss: 0.007022837693803012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [22:11<02:28, 148.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.007240878506563604\n",
      "Validation Loss: 0.004984873020629074\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.07748712856322527\n",
      "Training Loss: 0.07041946850717068\n",
      "Training Loss: 0.06605850579217076\n",
      "Validation Loss: 0.06498553386230148\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06287463679909706\n",
      "Training Loss: 0.060105094891041516\n",
      "Training Loss: 0.056375578120350836\n",
      "Validation Loss: 0.05478071858792492\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.052748915813863276\n",
      "Training Loss: 0.04944891856983304\n",
      "Training Loss: 0.04547012354247272\n",
      "Validation Loss: 0.04348119793020273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.04190001375041902\n",
      "Training Loss: 0.03879361533559859\n",
      "Training Loss: 0.03562024561688304\n",
      "Validation Loss: 0.03394435704005568\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.03299635626375675\n",
      "Training Loss: 0.030518998405896128\n",
      "Training Loss: 0.028346873833797873\n",
      "Validation Loss: 0.02690513584281454\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.026502666934393345\n",
      "Training Loss: 0.024678252669982612\n",
      "Training Loss: 0.023271477087400853\n",
      "Validation Loss: 0.02187295376553378\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.021911335051991044\n",
      "Training Loss: 0.020609860667027535\n",
      "Training Loss: 0.019722320705186577\n",
      "Validation Loss: 0.018235143438881534\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.01863400818314403\n",
      "Training Loss: 0.017714691760484128\n",
      "Training Loss: 0.017183516817167403\n",
      "Validation Loss: 0.015554862656745683\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.0162605368392542\n",
      "Training Loss: 0.015615044366568328\n",
      "Training Loss: 0.015338891630526631\n",
      "Validation Loss: 0.013559257909864857\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.014532937612384557\n",
      "Training Loss: 0.014080514670349658\n",
      "Training Loss: 0.013990053334273398\n",
      "Validation Loss: 0.012063673885769389\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.013274196102283895\n",
      "Training Loss: 0.012953698952915147\n",
      "Training Loss: 0.01299758136505261\n",
      "Validation Loss: 0.010930233692561978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.012351524825207889\n",
      "Training Loss: 0.012117897293064743\n",
      "Training Loss: 0.012258923730114476\n",
      "Validation Loss: 0.010055918270052316\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.01166565592517145\n",
      "Training Loss: 0.011487951093586161\n",
      "Training Loss: 0.011701917172176763\n",
      "Validation Loss: 0.009370989830683121\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.01114876937121153\n",
      "Training Loss: 0.011008679304504767\n",
      "Training Loss: 0.011281508922111242\n",
      "Validation Loss: 0.00883818921643529\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.010761755588464438\n",
      "Training Loss: 0.010650739037664607\n",
      "Training Loss: 0.010971677902853116\n",
      "Validation Loss: 0.008441097536019647\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.010482093462487683\n",
      "Training Loss: 0.010395310427993536\n",
      "Training Loss: 0.010749864781973883\n",
      "Validation Loss: 0.008160306949147515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.010285922858165576\n",
      "Training Loss: 0.010217431484488771\n",
      "Training Loss: 0.01058851885027252\n",
      "Validation Loss: 0.007964471487192374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.010144795764936133\n",
      "Training Loss: 0.010088719851337373\n",
      "Training Loss: 0.010462893412914127\n",
      "Validation Loss: 0.00782325596867778\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.010035606193123386\n",
      "Training Loss: 0.009988224043045193\n",
      "Training Loss: 0.010357827137922868\n",
      "Validation Loss: 0.007716151064240865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.009944765063701198\n",
      "Training Loss: 0.00990433171042241\n",
      "Training Loss: 0.010265831153374165\n",
      "Validation Loss: 0.007631098831940987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.009865359601099044\n",
      "Training Loss: 0.009831173982238397\n",
      "Training Loss: 0.010183176720747724\n",
      "Validation Loss: 0.007560949384109274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.00979381889803335\n",
      "Training Loss: 0.009765581000829115\n",
      "Training Loss: 0.010107668261043727\n",
      "Validation Loss: 0.007501233170290341\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.00972808136837557\n",
      "Training Loss: 0.009705594354309142\n",
      "Training Loss: 0.010037792696384712\n",
      "Validation Loss: 0.007448969321231243\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.009666829343186691\n",
      "Training Loss: 0.009649862203514204\n",
      "Training Loss: 0.009972430653870107\n",
      "Validation Loss: 0.00740209846427727\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.009609139275271445\n",
      "Training Loss: 0.00959738033474423\n",
      "Training Loss: 0.009910715953446924\n",
      "Validation Loss: 0.007359121620048131\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.009554327947553247\n",
      "Training Loss: 0.009547379201976583\n",
      "Training Loss: 0.009851971619063989\n",
      "Validation Loss: 0.007318916992190179\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.009501872339751572\n",
      "Training Loss: 0.009499255203409121\n",
      "Training Loss: 0.009795655439374969\n",
      "Validation Loss: 0.007280644764495867\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.009451365517452359\n",
      "Training Loss: 0.009452543273800984\n",
      "Training Loss: 0.009741335760336369\n",
      "Validation Loss: 0.007243687150937118\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.009402500927681103\n",
      "Training Loss: 0.009406898848246782\n",
      "Training Loss: 0.009688670316245407\n",
      "Validation Loss: 0.007207584270443558\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.00935504338121973\n",
      "Training Loss: 0.009362062632571905\n",
      "Training Loss: 0.009637392979348078\n",
      "Validation Loss: 0.007172024725109674\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.009308822300517931\n",
      "Training Loss: 0.009317863240139558\n",
      "Training Loss: 0.009587299161357804\n",
      "Validation Loss: 0.007136801484019987\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.009263720662565901\n",
      "Training Loss: 0.009274185752728955\n",
      "Training Loss: 0.009538231384940445\n",
      "Validation Loss: 0.007101772240561883\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.009219648564467206\n",
      "Training Loss: 0.009230953772785142\n",
      "Training Loss: 0.00949005717993714\n",
      "Validation Loss: 0.007066840249678811\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.009176532618002966\n",
      "Training Loss: 0.009188106380170211\n",
      "Training Loss: 0.009442655192688108\n",
      "Validation Loss: 0.00703194455029129\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.009134295562980697\n",
      "Training Loss: 0.009145583218196408\n",
      "Training Loss: 0.009395895191701129\n",
      "Validation Loss: 0.006996998804087719\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.009092843835242092\n",
      "Training Loss: 0.009103303268784658\n",
      "Training Loss: 0.009349628507625312\n",
      "Validation Loss: 0.006961897373973821\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.009052056428045034\n",
      "Training Loss: 0.00906115561723709\n",
      "Training Loss: 0.009303676133276895\n",
      "Validation Loss: 0.006926496311750138\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.009011782228481025\n",
      "Training Loss: 0.009018998369574547\n",
      "Training Loss: 0.009257846722612158\n",
      "Validation Loss: 0.006890640490730241\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.00897185978712514\n",
      "Training Loss: 0.00897668786579743\n",
      "Training Loss: 0.009211978995008394\n",
      "Validation Loss: 0.006854168445478832\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.00893215662566945\n",
      "Training Loss: 0.008934122445061803\n",
      "Training Loss: 0.009166004344588146\n",
      "Validation Loss: 0.006816991203297139\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008892635150114075\n",
      "Training Loss: 0.008891318456735462\n",
      "Training Loss: 0.009120030683698132\n",
      "Validation Loss: 0.006779140603121663\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008853418587241322\n",
      "Training Loss: 0.008848469854565338\n",
      "Training Loss: 0.00907439941773191\n",
      "Validation Loss: 0.0067408344240105725\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.008814815619261935\n",
      "Training Loss: 0.00880597326089628\n",
      "Training Loss: 0.009029671577736735\n",
      "Validation Loss: 0.006702487377889371\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.008777277251938357\n",
      "Training Loss: 0.00876436077291146\n",
      "Training Loss: 0.008986506073269993\n",
      "Validation Loss: 0.006664595630783797\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.008741276990622281\n",
      "Training Loss: 0.00872416082653217\n",
      "Training Loss: 0.00894548904383555\n",
      "Validation Loss: 0.00662764951225705\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.008707176361931489\n",
      "Training Loss: 0.00868578085443005\n",
      "Training Loss: 0.008907006134977564\n",
      "Validation Loss: 0.006592018515384348\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.008675156011013313\n",
      "Training Loss: 0.008649436096893623\n",
      "Training Loss: 0.008871191272046417\n",
      "Validation Loss: 0.0065579215729127775\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.008645207522204146\n",
      "Training Loss: 0.00861516187316738\n",
      "Training Loss: 0.008837968255393206\n",
      "Validation Loss: 0.00652543298921056\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.008617187468335032\n",
      "Training Loss: 0.00858286046772264\n",
      "Training Loss: 0.008807118573458865\n",
      "Validation Loss: 0.0064945174328815405\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.008590879085240886\n",
      "Training Loss: 0.00855236653937027\n",
      "Training Loss: 0.00877836340223439\n",
      "Validation Loss: 0.006465088859922514\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.008566043624887243\n",
      "Training Loss: 0.00852348611340858\n",
      "Training Loss: 0.008751407265663147\n",
      "Validation Loss: 0.006437025736661607\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.008542448581429198\n",
      "Training Loss: 0.008496028506197035\n",
      "Training Loss: 0.00872597202192992\n",
      "Validation Loss: 0.006410201986613317\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00851988919544965\n",
      "Training Loss: 0.008469819029560312\n",
      "Training Loss: 0.008701810098718852\n",
      "Validation Loss: 0.006384485333195228\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.008498184336349367\n",
      "Training Loss: 0.00844469956588\n",
      "Training Loss: 0.00867871019989252\n",
      "Validation Loss: 0.006359759281295237\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.008477183545473964\n",
      "Training Loss: 0.008420536164194345\n",
      "Training Loss: 0.008656490674475208\n",
      "Validation Loss: 0.006335911082020134\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.008456759976688773\n",
      "Training Loss: 0.008397211787523702\n",
      "Training Loss: 0.008635002451483161\n",
      "Validation Loss: 0.006312842262396066\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.008436809274135157\n",
      "Training Loss: 0.008374628176679834\n",
      "Training Loss: 0.008614124169107527\n",
      "Validation Loss: 0.006290459686241458\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.008417250079801306\n",
      "Training Loss: 0.008352706679143012\n",
      "Training Loss: 0.008593758067581803\n",
      "Validation Loss: 0.006268694592650268\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.008398018575971946\n",
      "Training Loss: 0.008331383022014052\n",
      "Training Loss: 0.008573831799440086\n",
      "Validation Loss: 0.00624749001659704\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.008379071074305102\n",
      "Training Loss: 0.008310610451735556\n",
      "Training Loss: 0.008554290772881359\n",
      "Validation Loss: 0.006226804562624586\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.008360380238154903\n",
      "Training Loss: 0.00829035775968805\n",
      "Training Loss: 0.00853510304936208\n",
      "Validation Loss: 0.0062066144872394\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.00834193680086173\n",
      "Training Loss: 0.008270605575526134\n",
      "Training Loss: 0.008516250187531113\n",
      "Validation Loss: 0.006186903731667259\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.00832374491263181\n",
      "Training Loss: 0.008251348516205325\n",
      "Training Loss: 0.00849772870889865\n",
      "Validation Loss: 0.006167670777276828\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.008305819608503953\n",
      "Training Loss: 0.008232586341910065\n",
      "Training Loss: 0.008479541391134262\n",
      "Validation Loss: 0.006148921952352681\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.008288182384567334\n",
      "Training Loss: 0.008214324068976566\n",
      "Training Loss: 0.008461699798936025\n",
      "Validation Loss: 0.006130655472731909\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.00827085860655643\n",
      "Training Loss: 0.008196565513499081\n",
      "Training Loss: 0.008444215937051922\n",
      "Validation Loss: 0.006112880452818582\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.008253871422493831\n",
      "Training Loss: 0.008179314014269039\n",
      "Training Loss: 0.008427097984822466\n",
      "Validation Loss: 0.006095591143788665\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.008237245169002563\n",
      "Training Loss: 0.008162568187108263\n",
      "Training Loss: 0.008410354006337002\n",
      "Validation Loss: 0.006078782232394547\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.008220990371191873\n",
      "Training Loss: 0.008146318123908713\n",
      "Training Loss: 0.008393985182046891\n",
      "Validation Loss: 0.006062443559110332\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.008205115729942918\n",
      "Training Loss: 0.008130553596420214\n",
      "Training Loss: 0.008377989445580169\n",
      "Validation Loss: 0.0060465529579355306\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.008189622510690242\n",
      "Training Loss: 0.008115255121374503\n",
      "Training Loss: 0.00836235865834169\n",
      "Validation Loss: 0.006031093976376683\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.008174503755290061\n",
      "Training Loss: 0.008100403995485976\n",
      "Training Loss: 0.008347085188142955\n",
      "Validation Loss: 0.00601604725286532\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.008159747854806483\n",
      "Training Loss: 0.00808597564813681\n",
      "Training Loss: 0.00833215583814308\n",
      "Validation Loss: 0.006001388746983466\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.008145342039642855\n",
      "Training Loss: 0.008071945658884942\n",
      "Training Loss: 0.008317558401031419\n",
      "Validation Loss: 0.00598709507102377\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.008131269724108279\n",
      "Training Loss: 0.008058292107889428\n",
      "Training Loss: 0.008303279459942131\n",
      "Validation Loss: 0.005973145762050336\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.008117511769523845\n",
      "Training Loss: 0.008044990175403655\n",
      "Training Loss: 0.008289305607322604\n",
      "Validation Loss: 0.005959521839133558\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.008104053421411662\n",
      "Training Loss: 0.0080320208449848\n",
      "Training Loss: 0.008275625461246819\n",
      "Validation Loss: 0.0059462067019717575\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.008090876891510562\n",
      "Training Loss: 0.008019364032661543\n",
      "Training Loss: 0.008262228365056216\n",
      "Validation Loss: 0.005933183806508863\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00807796482811682\n",
      "Training Loss: 0.008007002000231295\n",
      "Training Loss: 0.008249102917034179\n",
      "Validation Loss: 0.005920440349675548\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.008065302559407428\n",
      "Training Loss: 0.007994917953619734\n",
      "Training Loss: 0.00823623887146823\n",
      "Validation Loss: 0.005907960425737952\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.008052875479916111\n",
      "Training Loss: 0.007983097315300256\n",
      "Training Loss: 0.008223628357518465\n",
      "Validation Loss: 0.005895741549425162\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.008040669832844287\n",
      "Training Loss: 0.007971525026950985\n",
      "Training Loss: 0.008211261905962601\n",
      "Validation Loss: 0.0058837581538396435\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.008028672481887043\n",
      "Training Loss: 0.007960188059369102\n",
      "Training Loss: 0.008199130104621873\n",
      "Validation Loss: 0.005872008473131011\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.008016871712170541\n",
      "Training Loss: 0.007949075028300286\n",
      "Training Loss: 0.008187224514549597\n",
      "Validation Loss: 0.005860480367404859\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.008005256264004857\n",
      "Training Loss: 0.00793817391153425\n",
      "Training Loss: 0.00817553846980445\n",
      "Validation Loss: 0.00584916721799233\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007993813131470233\n",
      "Training Loss: 0.007927473235176875\n",
      "Training Loss: 0.00816406008321792\n",
      "Validation Loss: 0.005838057576950765\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007982533931499346\n",
      "Training Loss: 0.00791696191765368\n",
      "Training Loss: 0.008152781539829448\n",
      "Validation Loss: 0.005827140354739732\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.0079714055696968\n",
      "Training Loss: 0.007906629557255655\n",
      "Training Loss: 0.008141692383214832\n",
      "Validation Loss: 0.005816410527187871\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.00796042021480389\n",
      "Training Loss: 0.007896466584643349\n",
      "Training Loss: 0.00813078228617087\n",
      "Validation Loss: 0.005805855405025101\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.00794956760131754\n",
      "Training Loss: 0.007886461674934253\n",
      "Training Loss: 0.008120041391812265\n",
      "Validation Loss: 0.0057954735651068135\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00793883782462217\n",
      "Training Loss: 0.007876607641810552\n",
      "Training Loss: 0.00810945983743295\n",
      "Validation Loss: 0.005785252320779006\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007928224495844915\n",
      "Training Loss: 0.007866894929902628\n",
      "Training Loss: 0.008099028661381453\n",
      "Validation Loss: 0.005775184349648738\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007917719157412649\n",
      "Training Loss: 0.007857316503068432\n",
      "Training Loss: 0.008088737901998684\n",
      "Validation Loss: 0.005765267265855931\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007907313684700056\n",
      "Training Loss: 0.007847863537026569\n",
      "Training Loss: 0.008078579421853646\n",
      "Validation Loss: 0.005755486656137313\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007897001324454322\n",
      "Training Loss: 0.00783852924942039\n",
      "Training Loss: 0.008068544050911442\n",
      "Validation Loss: 0.0057458448579555815\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007886777620296926\n",
      "Training Loss: 0.00782930556917563\n",
      "Training Loss: 0.008058624342083931\n",
      "Validation Loss: 0.005736326632675913\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007876635685097427\n",
      "Training Loss: 0.007820188060868531\n",
      "Training Loss: 0.008048813082277775\n",
      "Validation Loss: 0.005726929314518243\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.00786656908574514\n",
      "Training Loss: 0.007811167286708951\n",
      "Training Loss: 0.008039100639289245\n",
      "Validation Loss: 0.005717644006212692\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007856572439195589\n",
      "Training Loss: 0.007802237075520679\n",
      "Training Loss: 0.008029481833800674\n",
      "Validation Loss: 0.005708463525326316\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007846639362396672\n",
      "Training Loss: 0.00779339144937694\n",
      "Training Loss: 0.00801994904060848\n",
      "Validation Loss: 0.005699380584783182\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007836762953083963\n",
      "Training Loss: 0.007784622592153028\n",
      "Training Loss: 0.008010492559988052\n",
      "Validation Loss: 0.0056903895220897175\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007826936799101531\n",
      "Training Loss: 0.007775923260487616\n",
      "Training Loss: 0.00800110726384446\n",
      "Validation Loss: 0.0056814835158312755\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007817153291543946\n",
      "Training Loss: 0.0077672856859862804\n",
      "Training Loss: 0.007991782673634588\n",
      "Validation Loss: 0.005672649820492173\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007807405287167057\n",
      "Training Loss: 0.007758702827850357\n",
      "Training Loss: 0.007982511868467554\n",
      "Validation Loss: 0.005663884383772783\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007797684834804386\n",
      "Training Loss: 0.007750168554484844\n",
      "Training Loss: 0.007973287177737803\n",
      "Validation Loss: 0.00565517181923006\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007787986696930602\n",
      "Training Loss: 0.007741672651609406\n",
      "Training Loss: 0.00796409867471084\n",
      "Validation Loss: 0.005646507837809622\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007778297928161919\n",
      "Training Loss: 0.007733206074917689\n",
      "Training Loss: 0.007954937641043216\n",
      "Validation Loss: 0.005637884494492763\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.007768609670456499\n",
      "Training Loss: 0.007724761806894093\n",
      "Training Loss: 0.007945793679682537\n",
      "Validation Loss: 0.005629292536223454\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.007758912512799725\n",
      "Training Loss: 0.00771633051102981\n",
      "Training Loss: 0.007936658322578297\n",
      "Validation Loss: 0.005620716906184059\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.007749194056959823\n",
      "Training Loss: 0.007707901736721396\n",
      "Training Loss: 0.00792751851142384\n",
      "Validation Loss: 0.005612144469063771\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.007739444678882137\n",
      "Training Loss: 0.007699465168407187\n",
      "Training Loss: 0.007918365078512579\n",
      "Validation Loss: 0.005603576322305906\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.007729650600813329\n",
      "Training Loss: 0.007691011790884658\n",
      "Training Loss: 0.007909187970217318\n",
      "Validation Loss: 0.005594991267857592\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.007719798912294209\n",
      "Training Loss: 0.007682530521415174\n",
      "Training Loss: 0.007899972561281174\n",
      "Validation Loss: 0.005586382561478387\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0077098732814192775\n",
      "Training Loss: 0.00767400880693458\n",
      "Training Loss: 0.007890705739846453\n",
      "Validation Loss: 0.005577736005731178\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.007699861271539703\n",
      "Training Loss: 0.007665434030350297\n",
      "Training Loss: 0.00788137423223816\n",
      "Validation Loss: 0.005569034997698213\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.007689744416857138\n",
      "Training Loss: 0.007656792934285477\n",
      "Training Loss: 0.007871962800854816\n",
      "Validation Loss: 0.005560267663016664\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.00767950561712496\n",
      "Training Loss: 0.007648071529110893\n",
      "Training Loss: 0.00786245745490305\n",
      "Validation Loss: 0.005551419644668865\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.007669125543907284\n",
      "Training Loss: 0.00763925486127846\n",
      "Training Loss: 0.007852839158149437\n",
      "Validation Loss: 0.005542471718430268\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.007658582165604457\n",
      "Training Loss: 0.00763032597140409\n",
      "Training Loss: 0.007843091320246458\n",
      "Validation Loss: 0.005533416692711664\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.007647854880196973\n",
      "Training Loss: 0.007621267583454027\n",
      "Training Loss: 0.007833192594116554\n",
      "Validation Loss: 0.005524222858072248\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0076369185920339075\n",
      "Training Loss: 0.007612059557577595\n",
      "Training Loss: 0.007823123434791341\n",
      "Validation Loss: 0.005514876563834508\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.007625750054139644\n",
      "Training Loss: 0.0076026829658076165\n",
      "Training Loss: 0.007812860825797543\n",
      "Validation Loss: 0.005505354553094825\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.007614320976426825\n",
      "Training Loss: 0.007593115156050771\n",
      "Training Loss: 0.007802382383961231\n",
      "Validation Loss: 0.005495639715416964\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.007602604465791956\n",
      "Training Loss: 0.007583333018701523\n",
      "Training Loss: 0.007791663861135021\n",
      "Validation Loss: 0.005485710439408261\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.007590571268228814\n",
      "Training Loss: 0.007573314169421792\n",
      "Training Loss: 0.007780678768176586\n",
      "Validation Loss: 0.005475542419119163\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.007578191321808845\n",
      "Training Loss: 0.007563031592871994\n",
      "Training Loss: 0.007769399442477152\n",
      "Validation Loss: 0.0054651134807009545\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.007565434736898169\n",
      "Training Loss: 0.007552460550796241\n",
      "Training Loss: 0.007757802051492036\n",
      "Validation Loss: 0.005454404868206449\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.0075522722699679435\n",
      "Training Loss: 0.0075415728101506825\n",
      "Training Loss: 0.007745857200352475\n",
      "Validation Loss: 0.005443391468627065\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00753867402789183\n",
      "Training Loss: 0.007530343678081408\n",
      "Training Loss: 0.007733541580382735\n",
      "Validation Loss: 0.005432054021113207\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.007524615887086838\n",
      "Training Loss: 0.007518746164860204\n",
      "Training Loss: 0.007720830739708617\n",
      "Validation Loss: 0.0054203817853704095\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.007510071395663545\n",
      "Training Loss: 0.0075067576218862085\n",
      "Training Loss: 0.007707703595515341\n",
      "Validation Loss: 0.005408351614370189\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.007495021609356627\n",
      "Training Loss: 0.007494354794034735\n",
      "Training Loss: 0.0076941429905127735\n",
      "Validation Loss: 0.005395963310598909\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.00747945184353739\n",
      "Training Loss: 0.0074815201852470635\n",
      "Training Loss: 0.007680135077098384\n",
      "Validation Loss: 0.0053832013005641905\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.007463351838523522\n",
      "Training Loss: 0.0074682381341699515\n",
      "Training Loss: 0.0076656746247317645\n",
      "Validation Loss: 0.005370065670596499\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.007446721672313288\n",
      "Training Loss: 0.007454500829335302\n",
      "Training Loss: 0.007650757918599993\n",
      "Validation Loss: 0.005356563775789704\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.007429564504418522\n",
      "Training Loss: 0.007440302050672472\n",
      "Training Loss: 0.007635390765499323\n",
      "Validation Loss: 0.005342699715532781\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.007411893417593091\n",
      "Training Loss: 0.007425645318580792\n",
      "Training Loss: 0.007619584794156253\n",
      "Validation Loss: 0.005328490074430959\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.0073937327938620005\n",
      "Training Loss: 0.00741053911857307\n",
      "Training Loss: 0.007603362377267331\n",
      "Validation Loss: 0.005313955010647436\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.007375113737070933\n",
      "Training Loss: 0.007395000887336209\n",
      "Training Loss: 0.007586745817679912\n",
      "Validation Loss: 0.0052991205658972935\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.007356074171839282\n",
      "Training Loss: 0.0073790509847458455\n",
      "Training Loss: 0.007569769606925547\n",
      "Validation Loss: 0.005284005566761734\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.007336661124136299\n",
      "Training Loss: 0.0073627154843416065\n",
      "Training Loss: 0.007552469347137958\n",
      "Validation Loss: 0.005268644326983878\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.007316927473293617\n",
      "Training Loss: 0.007346030235057696\n",
      "Training Loss: 0.007534888214431703\n",
      "Validation Loss: 0.00525306812958436\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.007296929900767282\n",
      "Training Loss: 0.007329031386179849\n",
      "Training Loss: 0.007517069024033845\n",
      "Validation Loss: 0.005237308788646975\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.007276729536242783\n",
      "Training Loss: 0.0073117600753903386\n",
      "Training Loss: 0.0074990581115707755\n",
      "Validation Loss: 0.00522140018233841\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.007256389401154592\n",
      "Training Loss: 0.007294256277382374\n",
      "Training Loss: 0.007480900202644989\n",
      "Validation Loss: 0.005205376596933001\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.007235969400499016\n",
      "Training Loss: 0.007276565223000944\n",
      "Training Loss: 0.007462638319702819\n",
      "Validation Loss: 0.005189267591458191\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.007215527473017573\n",
      "Training Loss: 0.007258727035950869\n",
      "Training Loss: 0.007444311602739617\n",
      "Validation Loss: 0.00517310030376434\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.007195119451498613\n",
      "Training Loss: 0.007240781822474673\n",
      "Training Loss: 0.007425957412924618\n",
      "Validation Loss: 0.005156896956097544\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.007174793387530371\n",
      "Training Loss: 0.007222766098566353\n",
      "Training Loss: 0.007407605878543108\n",
      "Validation Loss: 0.005140683693507833\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.007154595933388919\n",
      "Training Loss: 0.007204713143873959\n",
      "Training Loss: 0.007389283368829638\n",
      "Validation Loss: 0.005124469496979473\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.007134561282582581\n",
      "Training Loss: 0.0071866508154198525\n",
      "Training Loss: 0.0073710076452698555\n",
      "Validation Loss: 0.005108268124449035\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.007114717187359929\n",
      "Training Loss: 0.007168603454483673\n",
      "Training Loss: 0.00735279664513655\n",
      "Validation Loss: 0.005092095306373379\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.007095086881890893\n",
      "Training Loss: 0.007150591913377866\n",
      "Training Loss: 0.0073346596118062735\n",
      "Validation Loss: 0.00507594917749128\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.007075686971656978\n",
      "Training Loss: 0.0071326307696290316\n",
      "Training Loss: 0.007316600644262507\n",
      "Validation Loss: 0.005059837956201243\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.007056523510254919\n",
      "Training Loss: 0.007114732881309465\n",
      "Training Loss: 0.007298623375827446\n",
      "Validation Loss: 0.005043755874058671\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.007037602863274515\n",
      "Training Loss: 0.0070969056186731905\n",
      "Training Loss: 0.007280726339668035\n",
      "Validation Loss: 0.005027706908805066\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.007018922998104245\n",
      "Training Loss: 0.007079158630222082\n",
      "Training Loss: 0.007262908584671095\n",
      "Validation Loss: 0.005011683633404501\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.007000483188312501\n",
      "Training Loss: 0.007061495356028899\n",
      "Training Loss: 0.0072451682540122415\n",
      "Validation Loss: 0.0049956873088049586\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.00698227678774856\n",
      "Training Loss: 0.007043920897995121\n",
      "Training Loss: 0.007227502234745771\n",
      "Validation Loss: 0.0049797180318012\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006964298946550116\n",
      "Training Loss: 0.0070264383754692974\n",
      "Training Loss: 0.007209910182282329\n",
      "Validation Loss: 0.00496376894173746\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006946542048826814\n",
      "Training Loss: 0.007009052312932909\n",
      "Training Loss: 0.007192390165291726\n",
      "Validation Loss: 0.004947846410063545\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006929000130621716\n",
      "Training Loss: 0.0069917681161314246\n",
      "Training Loss: 0.0071749452280346305\n",
      "Validation Loss: 0.004931954248400217\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006911671458510682\n",
      "Training Loss: 0.006974593189661391\n",
      "Training Loss: 0.007157583381049335\n",
      "Validation Loss: 0.004916085420814709\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.0068945544282905755\n",
      "Training Loss: 0.0069575362256728114\n",
      "Training Loss: 0.007140310987597332\n",
      "Validation Loss: 0.004900263982113409\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006877647980581969\n",
      "Training Loss: 0.006940608621807769\n",
      "Training Loss: 0.007123138998867944\n",
      "Validation Loss: 0.00488449634821939\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006860954237636179\n",
      "Training Loss: 0.006923822067910805\n",
      "Training Loss: 0.007106080091325566\n",
      "Validation Loss: 0.004868788142385108\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006844477789709345\n",
      "Training Loss: 0.006907189463381655\n",
      "Training Loss: 0.007089150283718482\n",
      "Validation Loss: 0.004853154713120521\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006828226875513792\n",
      "Training Loss: 0.006890726287965662\n",
      "Training Loss: 0.007072366463253274\n",
      "Validation Loss: 0.004837618293136023\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006812210853677243\n",
      "Training Loss: 0.006874451892217621\n",
      "Training Loss: 0.007055749748833478\n",
      "Validation Loss: 0.004822188963475271\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.00679643677547574\n",
      "Training Loss: 0.006858379216864705\n",
      "Training Loss: 0.00703931680531241\n",
      "Validation Loss: 0.004806889177906965\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006780916990246624\n",
      "Training Loss: 0.006842525397660211\n",
      "Training Loss: 0.0070230883534532045\n",
      "Validation Loss: 0.00479173904358085\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006765659874072298\n",
      "Training Loss: 0.006826907599461265\n",
      "Training Loss: 0.0070070799917448315\n",
      "Validation Loss: 0.00477674424653517\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006750675231451168\n",
      "Training Loss: 0.006811540314229206\n",
      "Training Loss: 0.006991309564327821\n",
      "Validation Loss: 0.004761930560337358\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006735971590969712\n",
      "Training Loss: 0.006796436042059213\n",
      "Training Loss: 0.006975791805889458\n",
      "Validation Loss: 0.004747308385192176\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.0067215583298821\n",
      "Training Loss: 0.006781610237085261\n",
      "Training Loss: 0.006960542252054438\n",
      "Validation Loss: 0.004732891429330777\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006707440614700317\n",
      "Training Loss: 0.006767071805661544\n",
      "Training Loss: 0.006945568979717791\n",
      "Validation Loss: 0.004718687128945348\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006693624324398116\n",
      "Training Loss: 0.006752827923046425\n",
      "Training Loss: 0.006930883723543957\n",
      "Validation Loss: 0.0047047103838890455\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.0066801104298792775\n",
      "Training Loss: 0.006738888581749052\n",
      "Training Loss: 0.006916492085438221\n",
      "Validation Loss: 0.0046909635316245675\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006666903651785105\n",
      "Training Loss: 0.006725257817306556\n",
      "Training Loss: 0.0069024005474057045\n",
      "Validation Loss: 0.00467745488865322\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006654002239229157\n",
      "Training Loss: 0.006711940644308924\n",
      "Training Loss: 0.006888610816095025\n",
      "Validation Loss: 0.004664185963142035\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006641404965193942\n",
      "Training Loss: 0.006698937606415711\n",
      "Training Loss: 0.006875127165112644\n",
      "Validation Loss: 0.004651165486240153\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006629108476918191\n",
      "Training Loss: 0.006686253932421096\n",
      "Training Loss: 0.0068619486968964334\n",
      "Validation Loss: 0.004638396750277515\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006617110591614619\n",
      "Training Loss: 0.006673885555937886\n",
      "Training Loss: 0.006849075456848368\n",
      "Validation Loss: 0.004625870487809683\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.00660540570737794\n",
      "Training Loss: 0.00666183166380506\n",
      "Training Loss: 0.006836502031655982\n",
      "Validation Loss: 0.0046135908309265635\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006593987048836425\n",
      "Training Loss: 0.006650090041221119\n",
      "Training Loss: 0.0068242268124595286\n",
      "Validation Loss: 0.004601557605659191\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006582848032703623\n",
      "Training Loss: 0.006638655776623636\n",
      "Training Loss: 0.0068122411414515225\n",
      "Validation Loss: 0.004589765905559565\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006571979189757258\n",
      "Training Loss: 0.006627524354262278\n",
      "Training Loss: 0.00680054176482372\n",
      "Validation Loss: 0.0045782107313090325\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006561371043790132\n",
      "Training Loss: 0.006616690446389839\n",
      "Training Loss: 0.00678912150207907\n",
      "Validation Loss: 0.004566883608133772\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0065510161966085435\n",
      "Training Loss: 0.006606144279940054\n",
      "Training Loss: 0.006777969391550869\n",
      "Validation Loss: 0.004555782466718739\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006540900990366936\n",
      "Training Loss: 0.006595882148831151\n",
      "Training Loss: 0.006767078984994441\n",
      "Validation Loss: 0.004544901267855606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006531017445959151\n",
      "Training Loss: 0.006585892023285851\n",
      "Training Loss: 0.006756439009914175\n",
      "Validation Loss: 0.004534231460904389\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006521354209398851\n",
      "Training Loss: 0.006576167688472197\n",
      "Training Loss: 0.006746040233410895\n",
      "Validation Loss: 0.00452376081440807\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006511899542529136\n",
      "Training Loss: 0.006566698865499348\n",
      "Training Loss: 0.006735872791614384\n",
      "Validation Loss: 0.004513489341447025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006502643259009347\n",
      "Training Loss: 0.00655747915210668\n",
      "Training Loss: 0.0067259269475471225\n",
      "Validation Loss: 0.004503409401775243\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006493574572959915\n",
      "Training Loss: 0.006548496726900339\n",
      "Training Loss: 0.006716193089960143\n",
      "Validation Loss: 0.004493512015716497\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006484681807924062\n",
      "Training Loss: 0.006539746106718667\n",
      "Training Loss: 0.00670666056103073\n",
      "Validation Loss: 0.004483785803346068\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006475953821791336\n",
      "Training Loss: 0.006531212362460792\n",
      "Training Loss: 0.006697319076629356\n",
      "Validation Loss: 0.004474225395170742\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006467381616821513\n",
      "Training Loss: 0.0065228911902522665\n",
      "Training Loss: 0.00668815711978823\n",
      "Validation Loss: 0.004464826526988842\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006458955075358972\n",
      "Training Loss: 0.006514771333313547\n",
      "Training Loss: 0.006679167873226106\n",
      "Validation Loss: 0.0044555775428797755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006450663676951081\n",
      "Training Loss: 0.00650684570602607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [24:39<00:00, 148.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.006670341732678935\n",
      "Validation Loss: 0.004446476663912783\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Shape of the data after splitting into sequences: (22797, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2841, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2840, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (22797, 12, 5)\n",
      "Shape of the data after splitting into sequences: (5692, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Loss: 0.056786455828696486\n",
      "Training Loss: 0.053125640116631984\n",
      "Training Loss: 0.05057024510577321\n",
      "Training Loss: 0.048582135215401646\n",
      "Training Loss: 0.045119447531178594\n",
      "Training Loss: 0.04271987894549966\n",
      "Training Loss: 0.03864480452612042\n",
      "Validation Loss: 0.03703280087658082\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.035030191158875824\n",
      "Training Loss: 0.03142258777283132\n",
      "Training Loss: 0.028378105838783085\n",
      "Training Loss: 0.026406987397931517\n",
      "Training Loss: 0.02334502595011145\n",
      "Training Loss: 0.02125405676662922\n",
      "Training Loss: 0.019569223050493748\n",
      "Validation Loss: 0.018334155584849444\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.01848749226424843\n",
      "Training Loss: 0.01720594002166763\n",
      "Training Loss: 0.016858201045542955\n",
      "Training Loss: 0.017120526100043207\n",
      "Training Loss: 0.016920884114224464\n",
      "Training Loss: 0.01654966826317832\n",
      "Training Loss: 0.016067515378817916\n",
      "Validation Loss: 0.014734990450061774\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.015323740490712225\n",
      "Training Loss: 0.01437027558684349\n",
      "Training Loss: 0.014184787645936012\n",
      "Training Loss: 0.014628315975423902\n",
      "Training Loss: 0.0146397095057182\n",
      "Training Loss: 0.014323783945292235\n",
      "Training Loss: 0.014011644164565951\n",
      "Validation Loss: 0.012535132361359698\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.013287702074740082\n",
      "Training Loss: 0.012515578282764181\n",
      "Training Loss: 0.012406235884409398\n",
      "Training Loss: 0.012940983986482024\n",
      "Training Loss: 0.013117164289578796\n",
      "Training Loss: 0.012787826003041118\n",
      "Training Loss: 0.012594696551095694\n",
      "Validation Loss: 0.011002365249463738\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.011843925437424331\n",
      "Training Loss: 0.011186493912246078\n",
      "Training Loss: 0.011117474909406155\n",
      "Training Loss: 0.011697021762374788\n",
      "Training Loss: 0.01201153444708325\n",
      "Training Loss: 0.011654198709875345\n",
      "Training Loss: 0.01153763318201527\n",
      "Validation Loss: 0.009843239728152082\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.010751946901436896\n",
      "Training Loss: 0.010169216039357706\n",
      "Training Loss: 0.010124839134514332\n",
      "Training Loss: 0.010735744056291878\n",
      "Training Loss: 0.01117231980082579\n",
      "Training Loss: 0.010790249271085486\n",
      "Training Loss: 0.01072264204500243\n",
      "Validation Loss: 0.008930411804572082\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.0099047418567352\n",
      "Training Loss: 0.009371651324909181\n",
      "Training Loss: 0.009345741025172175\n",
      "Training Loss: 0.0099864959763363\n",
      "Training Loss: 0.010529725874075666\n",
      "Training Loss: 0.010129884344059974\n",
      "Training Loss: 0.010095135645242408\n",
      "Validation Loss: 0.008203300094573797\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.009245208359789103\n",
      "Training Loss: 0.0087463227880653\n",
      "Training Loss: 0.008736319872550666\n",
      "Training Loss: 0.009408438371028751\n",
      "Training Loss: 0.010042436779476702\n",
      "Training Loss: 0.009632220319472253\n",
      "Training Loss: 0.009621821230975911\n",
      "Validation Loss: 0.0076257119326737155\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.008735780203714967\n",
      "Training Loss: 0.008262101290747524\n",
      "Training Loss: 0.008266859914874658\n",
      "Training Loss: 0.008971634968183935\n",
      "Training Loss: 0.009681187638780103\n",
      "Training Loss: 0.009266983135603368\n",
      "Training Loss: 0.009276614611735567\n",
      "Validation Loss: 0.00717207641612864\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.008348461480345577\n",
      "Training Loss: 0.007894873326877131\n",
      "Training Loss: 0.00791342456243001\n",
      "Training Loss: 0.008650727673666552\n",
      "Training Loss: 0.009422050073044374\n",
      "Training Loss: 0.009008346168557183\n",
      "Training Loss: 0.009035332504427061\n",
      "Validation Loss: 0.006821783966287161\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.00806004824466072\n",
      "Training Loss: 0.007623275212245062\n",
      "Training Loss: 0.0076541796722449365\n",
      "Training Loss: 0.008422111409017816\n",
      "Training Loss: 0.00924288485199213\n",
      "Training Loss: 0.008832089875359089\n",
      "Training Loss: 0.008873916254378856\n",
      "Validation Loss: 0.006556206881609302\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.007849498448194936\n",
      "Training Loss: 0.007426852608332411\n",
      "Training Loss: 0.007468151815701276\n",
      "Training Loss: 0.00826318452716805\n",
      "Training Loss: 0.009122376333689317\n",
      "Training Loss: 0.008715303111821414\n",
      "Training Loss: 0.008769109470304102\n",
      "Validation Loss: 0.006357687595649186\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.007697321034502238\n",
      "Training Loss: 0.007286244716960937\n",
      "Training Loss: 0.007335782474838197\n",
      "Training Loss: 0.008153296322561801\n",
      "Training Loss: 0.009041135907173157\n",
      "Training Loss: 0.00863777445978485\n",
      "Training Loss: 0.008700518812984228\n",
      "Validation Loss: 0.0062099287063162424\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.007586365818278864\n",
      "Training Loss: 0.007184477319242433\n",
      "Training Loss: 0.007240274115465581\n",
      "Training Loss: 0.008075319167692213\n",
      "Training Loss: 0.00898347087786533\n",
      "Training Loss: 0.00858364650979638\n",
      "Training Loss: 0.008652515688445418\n",
      "Validation Loss: 0.0060988991034709\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.007502901835832745\n",
      "Training Loss: 0.007108180992072448\n",
      "Training Loss: 0.0071686585282441226\n",
      "Training Loss: 0.008016654445091262\n",
      "Training Loss: 0.008938385762739927\n",
      "Training Loss: 0.0085420865600463\n",
      "Training Loss: 0.008614772226428613\n",
      "Validation Loss: 0.006013458966162432\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.007437001344515011\n",
      "Training Loss: 0.007047892978880555\n",
      "Training Loss: 0.007111898806178942\n",
      "Training Loss: 0.007969067376106977\n",
      "Training Loss: 0.00889925372437574\n",
      "Training Loss: 0.008506662745494395\n",
      "Training Loss: 0.008581424105213955\n",
      "Validation Loss: 0.005945383646082454\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.007382094925269485\n",
      "Training Loss: 0.006997490422800183\n",
      "Training Loss: 0.0070642182661686094\n",
      "Training Loss: 0.007927700583823026\n",
      "Training Loss: 0.008862652060342953\n",
      "Training Loss: 0.008474036556435749\n",
      "Training Loss: 0.008549634028458968\n",
      "Validation Loss: 0.005888907392317436\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.0073341088462620975\n",
      "Training Loss: 0.006953260677400977\n",
      "Training Loss: 0.007022143205394968\n",
      "Training Loss: 0.007889902184833772\n",
      "Training Loss: 0.008827091825660318\n",
      "Training Loss: 0.00844269129796885\n",
      "Training Loss: 0.008518267397303134\n",
      "Validation Loss: 0.005840145811885037\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.007290597284445539\n",
      "Training Loss: 0.006913023876259103\n",
      "Training Loss: 0.00698365310090594\n",
      "Training Loss: 0.007854292085394263\n",
      "Training Loss: 0.008792069803457708\n",
      "Training Loss: 0.008412023490527644\n",
      "Training Loss: 0.0084870089602191\n",
      "Validation Loss: 0.005796524466273518\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.00725010950001888\n",
      "Training Loss: 0.0068755069118924435\n",
      "Training Loss: 0.006947586687747389\n",
      "Training Loss: 0.007820149853359907\n",
      "Training Loss: 0.008757492044242098\n",
      "Training Loss: 0.008381813981104642\n",
      "Training Loss: 0.008455845686839894\n",
      "Validation Loss: 0.0057563600152179826\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.007211763231316582\n",
      "Training Loss: 0.006839944628300145\n",
      "Training Loss: 0.006913261601002887\n",
      "Training Loss: 0.00778707362303976\n",
      "Training Loss: 0.008723384811310098\n",
      "Training Loss: 0.008351960120489822\n",
      "Training Loss: 0.008424832809250802\n",
      "Validation Loss: 0.005718544136893269\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.007174996950197965\n",
      "Training Loss: 0.00680584371322766\n",
      "Training Loss: 0.006880254765274003\n",
      "Training Loss: 0.007754800891852938\n",
      "Training Loss: 0.008689756974345073\n",
      "Training Loss: 0.008322366017382591\n",
      "Training Loss: 0.00839398373500444\n",
      "Validation Loss: 0.005682337973336602\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.007139425476780161\n",
      "Training Loss: 0.0067728599673137066\n",
      "Training Loss: 0.0068482782132923605\n",
      "Training Loss: 0.0077231199917150665\n",
      "Training Loss: 0.00865656387177296\n",
      "Training Loss: 0.008292903885012493\n",
      "Training Loss: 0.00836324320291169\n",
      "Validation Loss: 0.005647232643963441\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.007104753530584275\n",
      "Training Loss: 0.006740720159141347\n",
      "Training Loss: 0.006817103163339198\n",
      "Training Loss: 0.00769183095660992\n",
      "Training Loss: 0.008623691578395664\n",
      "Training Loss: 0.008263404235476628\n",
      "Training Loss: 0.008332489124732091\n",
      "Validation Loss: 0.005612846123828982\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007070729386759922\n",
      "Training Loss: 0.006709189204266295\n",
      "Training Loss: 0.006786527733784169\n",
      "Training Loss: 0.007660731961368583\n",
      "Training Loss: 0.008590971902012825\n",
      "Training Loss: 0.008233665908919647\n",
      "Training Loss: 0.008301540629472583\n",
      "Validation Loss: 0.005578872464311201\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.007037113362457603\n",
      "Training Loss: 0.0066780443780589845\n",
      "Training Loss: 0.006756352634401992\n",
      "Training Loss: 0.007629610818694346\n",
      "Training Loss: 0.00855818969081156\n",
      "Training Loss: 0.008203468789579347\n",
      "Training Loss: 0.008270173653727397\n",
      "Validation Loss: 0.0055450435713882305\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.0070036676817107945\n",
      "Training Loss: 0.006647066415753216\n",
      "Training Loss: 0.006726371413096785\n",
      "Training Loss: 0.0075982471992028875\n",
      "Training Loss: 0.008525098357349634\n",
      "Training Loss: 0.00817258114577271\n",
      "Training Loss: 0.00823813576134853\n",
      "Validation Loss: 0.005511097981043383\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.006970145127270371\n",
      "Training Loss: 0.006616033414611593\n",
      "Training Loss: 0.006696364907547832\n",
      "Training Loss: 0.0075664140458684415\n",
      "Training Loss: 0.008491432106820867\n",
      "Training Loss: 0.008140775908250361\n",
      "Training Loss: 0.008205158393830061\n",
      "Validation Loss: 0.005476775076432147\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006936285818228498\n",
      "Training Loss: 0.0065847218770068135\n",
      "Training Loss: 0.0066661033988930285\n",
      "Training Loss: 0.007533882661955431\n",
      "Training Loss: 0.00845691014546901\n",
      "Training Loss: 0.008107837010174989\n",
      "Training Loss: 0.008170970074133947\n",
      "Validation Loss: 0.005441811192983043\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006901822902727872\n",
      "Training Loss: 0.006552907412406057\n",
      "Training Loss: 0.006635349269490689\n",
      "Training Loss: 0.007500432560918853\n",
      "Training Loss: 0.008421255112625658\n",
      "Training Loss: 0.008073577656177803\n",
      "Training Loss: 0.00813530886778608\n",
      "Validation Loss: 0.005405941289633317\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006866488776868209\n",
      "Training Loss: 0.006520367214689031\n",
      "Training Loss: 0.006603861225303262\n",
      "Training Loss: 0.0074658629146870225\n",
      "Training Loss: 0.008384199147112668\n",
      "Training Loss: 0.008037855254951865\n",
      "Training Loss: 0.00809794144355692\n",
      "Validation Loss: 0.005368915912764991\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.006830030415439978\n",
      "Training Loss: 0.006486896587302908\n",
      "Training Loss: 0.006571411165641621\n",
      "Training Loss: 0.007430004821508192\n",
      "Training Loss: 0.008345511463703588\n",
      "Training Loss: 0.00800059138564393\n",
      "Training Loss: 0.008058689581230283\n",
      "Validation Loss: 0.00533051731950893\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.00679223334765993\n",
      "Training Loss: 0.00645231816044543\n",
      "Training Loss: 0.006537801102967933\n",
      "Training Loss: 0.007392750630970113\n",
      "Training Loss: 0.008305018516257406\n",
      "Training Loss: 0.007961791842244565\n",
      "Training Loss: 0.008017460874980316\n",
      "Validation Loss: 0.0052905917710063055\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.0067529525561258194\n",
      "Training Loss: 0.006416506936657243\n",
      "Training Loss: 0.00650288920267485\n",
      "Training Loss: 0.007354075942421332\n",
      "Training Loss: 0.008262648441595957\n",
      "Training Loss: 0.007921582442941144\n",
      "Training Loss: 0.007974289859412238\n",
      "Validation Loss: 0.005249081759832186\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.006712150267558173\n",
      "Training Loss: 0.006379417406860739\n",
      "Training Loss: 0.0064666287275031206\n",
      "Training Loss: 0.007314072310691699\n",
      "Training Loss: 0.00821847001905553\n",
      "Training Loss: 0.0078802237205673\n",
      "Training Loss: 0.007929384290473535\n",
      "Validation Loss: 0.0052060761701748165\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.00666994140134193\n",
      "Training Loss: 0.006341121590230614\n",
      "Training Loss: 0.00642909751390107\n",
      "Training Loss: 0.007272973197395913\n",
      "Training Loss: 0.008172739651054145\n",
      "Training Loss: 0.007838127072900533\n",
      "Training Loss: 0.007883143168874085\n",
      "Validation Loss: 0.005161825854816101\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006626608347287402\n",
      "Training Loss: 0.006301820746739395\n",
      "Training Loss: 0.006390522175352089\n",
      "Training Loss: 0.0072311552445171404\n",
      "Training Loss: 0.00812591110006906\n",
      "Training Loss: 0.007795832853298634\n",
      "Training Loss: 0.007836153083480894\n",
      "Validation Loss: 0.005116765505951666\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.00658260787371546\n",
      "Training Loss: 0.006261857729987242\n",
      "Training Loss: 0.006351281474344432\n",
      "Training Loss: 0.007189112356863916\n",
      "Training Loss: 0.008078607283532619\n",
      "Training Loss: 0.007753949635662139\n",
      "Training Loss: 0.007789117384236306\n",
      "Validation Loss: 0.005071461668119448\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006538514450658113\n",
      "Training Loss: 0.00622167315275874\n",
      "Training Loss: 0.006311849309131503\n",
      "Training Loss: 0.0071473856770899145\n",
      "Training Loss: 0.008031540805241093\n",
      "Training Loss: 0.0077130659471731635\n",
      "Training Loss: 0.007742757037049159\n",
      "Validation Loss: 0.005026541768890642\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006494942429708317\n",
      "Training Loss: 0.006181750567047856\n",
      "Training Loss: 0.006272731240023859\n",
      "Training Loss: 0.007106483863317407\n",
      "Training Loss: 0.007985403910279274\n",
      "Training Loss: 0.007673656973056495\n",
      "Training Loss: 0.007697687511099502\n",
      "Validation Loss: 0.004982595144481527\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0064524448208976536\n",
      "Training Loss: 0.0061425312724895775\n",
      "Training Loss: 0.006234368932200596\n",
      "Training Loss: 0.007066800005268306\n",
      "Training Loss: 0.00794075578567572\n",
      "Training Loss: 0.0076360148133244364\n",
      "Training Loss: 0.007654335488332435\n",
      "Validation Loss: 0.00494007769516889\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006411438512150198\n",
      "Training Loss: 0.006104357677977532\n",
      "Training Loss: 0.006197082854923792\n",
      "Training Loss: 0.007028575328877196\n",
      "Training Loss: 0.007897969906916841\n",
      "Training Loss: 0.007600238235900178\n",
      "Training Loss: 0.0076129080774262545\n",
      "Validation Loss: 0.004899273326529769\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.00637217489304021\n",
      "Training Loss: 0.006067440045299009\n",
      "Training Loss: 0.006161043004249222\n",
      "Training Loss: 0.006991893142112531\n",
      "Training Loss: 0.007857212158851325\n",
      "Training Loss: 0.007566260456806049\n",
      "Training Loss: 0.007573418087558821\n",
      "Validation Loss: 0.0048602832113270565\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006334741149912588\n",
      "Training Loss: 0.0060318542266031725\n",
      "Training Loss: 0.006126284060883336\n",
      "Training Loss: 0.006956712636165321\n",
      "Training Loss: 0.007818482699804008\n",
      "Training Loss: 0.007533907138276845\n",
      "Training Loss: 0.007535749527160078\n",
      "Validation Loss: 0.004823068477307561\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006299102524062619\n",
      "Training Loss: 0.005997578746755608\n",
      "Training Loss: 0.006092743711778894\n",
      "Training Loss: 0.006922920779325068\n",
      "Training Loss: 0.007781674275174737\n",
      "Training Loss: 0.007502970109926537\n",
      "Training Loss: 0.007499719100305811\n",
      "Validation Loss: 0.0047874975180399884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006265146143268794\n",
      "Training Loss: 0.005964527239557356\n",
      "Training Loss: 0.0060603047534823415\n",
      "Training Loss: 0.006890374713111669\n",
      "Training Loss: 0.007746621345868334\n",
      "Training Loss: 0.007473239328246564\n",
      "Training Loss: 0.007465132710058242\n",
      "Validation Loss: 0.004753393094466951\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006232726906309835\n",
      "Training Loss: 0.005932590311858803\n",
      "Training Loss: 0.006028842344530858\n",
      "Training Loss: 0.006858939310768619\n",
      "Training Loss: 0.007713153839576989\n",
      "Training Loss: 0.007444552392698824\n",
      "Training Loss: 0.007431819784687832\n",
      "Validation Loss: 0.004720578441912595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006201699193334207\n",
      "Training Loss: 0.005901662897085771\n",
      "Training Loss: 0.0059982418204890565\n",
      "Training Loss: 0.006828505934099667\n",
      "Training Loss: 0.007681120851775631\n",
      "Training Loss: 0.007416796906618402\n",
      "Training Loss: 0.0073996534512843935\n",
      "Validation Loss: 0.004688894372234602\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006171934544108808\n",
      "Training Loss: 0.005871660510310903\n",
      "Training Loss: 0.005968420474091545\n",
      "Training Loss: 0.006799002571497113\n",
      "Training Loss: 0.007650401188293472\n",
      "Training Loss: 0.007389911393402144\n",
      "Training Loss: 0.007368549910606817\n",
      "Validation Loss: 0.004658216966773045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.0061433367960853505\n",
      "Training Loss: 0.0058425292320316655\n",
      "Training Loss: 0.005939332531415858\n",
      "Training Loss: 0.0067703911516582595\n",
      "Training Loss: 0.007620914508588612\n",
      "Training Loss: 0.007363881756318733\n",
      "Training Loss: 0.007338467026129365\n",
      "Validation Loss: 0.004628454167169104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006115836986573413\n",
      "Training Loss: 0.0058142429823055865\n",
      "Training Loss: 0.005910961328190752\n",
      "Training Loss: 0.00674266220012214\n",
      "Training Loss: 0.007592611700529233\n",
      "Training Loss: 0.007338724578730762\n",
      "Training Loss: 0.007309387088753283\n",
      "Validation Loss: 0.004599544547990048\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006089391523855739\n",
      "Training Loss: 0.005786800283240154\n",
      "Training Loss: 0.00588331789302174\n",
      "Training Loss: 0.006715824175043963\n",
      "Training Loss: 0.0075654603098519145\n",
      "Training Loss: 0.00731447122991085\n",
      "Training Loss: 0.007281313871499151\n",
      "Validation Loss: 0.00457145666183855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006063979952596128\n",
      "Training Loss: 0.005760219327057712\n",
      "Training Loss: 0.005856433938606642\n",
      "Training Loss: 0.006689895758172497\n",
      "Training Loss: 0.007539445353904739\n",
      "Training Loss: 0.007291158461011946\n",
      "Training Loss: 0.007254257375607267\n",
      "Validation Loss: 0.0045441737026150585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006039590672007762\n",
      "Training Loss: 0.005734527129679918\n",
      "Training Loss: 0.0058303472987608984\n",
      "Training Loss: 0.006664893684792332\n",
      "Training Loss: 0.007514554358785972\n",
      "Training Loss: 0.007268818112788722\n",
      "Training Loss: 0.007228227723389864\n",
      "Validation Loss: 0.004517692967478451\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.006016221311292611\n",
      "Training Loss: 0.00570975624024868\n",
      "Training Loss: 0.00580509886960499\n",
      "Training Loss: 0.006640833942801691\n",
      "Training Loss: 0.007490774074103684\n",
      "Training Loss: 0.007247469137655571\n",
      "Training Loss: 0.007203223385149613\n",
      "Validation Loss: 0.004492012180443783\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.005993863199837506\n",
      "Training Loss: 0.005685926756123081\n",
      "Training Loss: 0.005780718621099368\n",
      "Training Loss: 0.0066177129262359816\n",
      "Training Loss: 0.00746807616436854\n",
      "Training Loss: 0.007227108055958525\n",
      "Training Loss: 0.007179233815986663\n",
      "Validation Loss: 0.004467133487564292\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.005972505861427635\n",
      "Training Loss: 0.005663059763610363\n",
      "Training Loss: 0.005757230131421238\n",
      "Training Loss: 0.006595520813716576\n",
      "Training Loss: 0.0074464305490255355\n",
      "Training Loss: 0.007207720333244651\n",
      "Training Loss: 0.007156235956354067\n",
      "Validation Loss: 0.0044430552694897695\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.005952131728408858\n",
      "Training Loss: 0.00564116187102627\n",
      "Training Loss: 0.00573464495188091\n",
      "Training Loss: 0.006574232886196114\n",
      "Training Loss: 0.007425792851718143\n",
      "Training Loss: 0.007189268941292539\n",
      "Training Loss: 0.0071341945813037455\n",
      "Validation Loss: 0.004419767509706402\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.005932709679473192\n",
      "Training Loss: 0.005620224787853658\n",
      "Training Loss: 0.005712953192996793\n",
      "Training Loss: 0.006553811754565686\n",
      "Training Loss: 0.007406108329305426\n",
      "Training Loss: 0.007171706175431609\n",
      "Training Loss: 0.007113065249286592\n",
      "Validation Loss: 0.0043972547868901535\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.0059142024436732755\n",
      "Training Loss: 0.005600231198477559\n",
      "Training Loss: 0.005692137776641175\n",
      "Training Loss: 0.006534210998215713\n",
      "Training Loss: 0.007387315845116973\n",
      "Training Loss: 0.007154972917633131\n",
      "Training Loss: 0.007092792356852442\n",
      "Validation Loss: 0.004375495063228191\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.005896563885034994\n",
      "Training Loss: 0.005581152737140656\n",
      "Training Loss: 0.005672166859149001\n",
      "Training Loss: 0.006515379467746243\n",
      "Training Loss: 0.007369350192602724\n",
      "Training Loss: 0.007139002945041284\n",
      "Training Loss: 0.007073318039765582\n",
      "Validation Loss: 0.00435445934258513\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.005879739720257931\n",
      "Training Loss: 0.005562951573519968\n",
      "Training Loss: 0.005652999323210679\n",
      "Training Loss: 0.006497258743038401\n",
      "Training Loss: 0.007352142748422921\n",
      "Training Loss: 0.007123727145371958\n",
      "Training Loss: 0.007054577600210905\n",
      "Validation Loss: 0.00433411103872101\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.0058636719203786925\n",
      "Training Loss: 0.0055455803242512045\n",
      "Training Loss: 0.00563458580058068\n",
      "Training Loss: 0.00647978849010542\n",
      "Training Loss: 0.007335622610989958\n",
      "Training Loss: 0.007109076449414715\n",
      "Training Loss: 0.007036509579047561\n",
      "Validation Loss: 0.0043144154097284945\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.005848301984951831\n",
      "Training Loss: 0.005528990838793106\n",
      "Training Loss: 0.005616874427068978\n",
      "Training Loss: 0.006462912271963432\n",
      "Training Loss: 0.007319724335102365\n",
      "Training Loss: 0.007094982663402334\n",
      "Training Loss: 0.00701905095949769\n",
      "Validation Loss: 0.004295322960783284\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.005833568350644782\n",
      "Training Loss: 0.005513128766906448\n",
      "Training Loss: 0.00559981124708429\n",
      "Training Loss: 0.006446573617286049\n",
      "Training Loss: 0.007304384401068092\n",
      "Training Loss: 0.007081385749625042\n",
      "Training Loss: 0.007002142913406715\n",
      "Validation Loss: 0.004276796670941015\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.005819414209690876\n",
      "Training Loss: 0.005497942740912549\n",
      "Training Loss: 0.005583339476725086\n",
      "Training Loss: 0.006430717222392559\n",
      "Training Loss: 0.007289540953934193\n",
      "Training Loss: 0.007068223357200623\n",
      "Training Loss: 0.006985729089938104\n",
      "Validation Loss: 0.0042587885247676\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.005805782800307497\n",
      "Training Loss: 0.005483377319760621\n",
      "Training Loss: 0.005567409642389975\n",
      "Training Loss: 0.006415295481565409\n",
      "Training Loss: 0.007275138582335785\n",
      "Training Loss: 0.00705544542754069\n",
      "Training Loss: 0.006969758687773719\n",
      "Validation Loss: 0.004241257427218422\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.005792621552245691\n",
      "Training Loss: 0.00546938425861299\n",
      "Training Loss: 0.00555196768254973\n",
      "Training Loss: 0.006400264978292398\n",
      "Training Loss: 0.007261133267311379\n",
      "Training Loss: 0.007043006658786908\n",
      "Training Loss: 0.006954185510985553\n",
      "Validation Loss: 0.004224156271945337\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.005779882621718571\n",
      "Training Loss: 0.005455910744494759\n",
      "Training Loss: 0.005536967839580029\n",
      "Training Loss: 0.0063855824724305425\n",
      "Training Loss: 0.007247473802417517\n",
      "Training Loss: 0.007030863235704601\n",
      "Training Loss: 0.006938970607006922\n",
      "Validation Loss: 0.004207450467465299\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.005767521614907309\n",
      "Training Loss: 0.005442914773593657\n",
      "Training Loss: 0.005522366363438777\n",
      "Training Loss: 0.006371214774553664\n",
      "Training Loss: 0.007234125111717731\n",
      "Training Loss: 0.007018980726134032\n",
      "Training Loss: 0.00692407745635137\n",
      "Validation Loss: 0.004191102454794592\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.005755499661900103\n",
      "Training Loss: 0.005430353236151859\n",
      "Training Loss: 0.005508124434272759\n",
      "Training Loss: 0.006357130550313741\n",
      "Training Loss: 0.00722105230903253\n",
      "Training Loss: 0.007007330234628171\n",
      "Training Loss: 0.00690947737544775\n",
      "Validation Loss: 0.004175078707192553\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.005743784796213731\n",
      "Training Loss: 0.005418188340845518\n",
      "Training Loss: 0.005494207929004915\n",
      "Training Loss: 0.006343304282054305\n",
      "Training Loss: 0.0072082292300183325\n",
      "Training Loss: 0.006995886110235006\n",
      "Training Loss: 0.006895141373388469\n",
      "Validation Loss: 0.0041593499163388925\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.005732344534480944\n",
      "Training Loss: 0.005406385057140142\n",
      "Training Loss: 0.0054805870697600765\n",
      "Training Loss: 0.00632971482060384\n",
      "Training Loss: 0.007195628588087857\n",
      "Training Loss: 0.006984629037324339\n",
      "Training Loss: 0.006881053925026208\n",
      "Validation Loss: 0.00414388905513041\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.005721156277577393\n",
      "Training Loss: 0.005394915615324862\n",
      "Training Loss: 0.0054672358074458315\n",
      "Training Loss: 0.006316344050574117\n",
      "Training Loss: 0.0071832319186069075\n",
      "Training Loss: 0.006973540639737621\n",
      "Training Loss: 0.0068671968346461654\n",
      "Validation Loss: 0.004128679116465821\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.005710198139422573\n",
      "Training Loss: 0.005383752061170526\n",
      "Training Loss: 0.005454132658196613\n",
      "Training Loss: 0.006303179226815701\n",
      "Training Loss: 0.007171023010741919\n",
      "Training Loss: 0.0069626084144692865\n",
      "Training Loss: 0.006853558517759666\n",
      "Validation Loss: 0.0041137015323193\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.005699453871930018\n",
      "Training Loss: 0.005372871042345651\n",
      "Training Loss: 0.005441258900100365\n",
      "Training Loss: 0.006290210713050328\n",
      "Training Loss: 0.0071589908411260695\n",
      "Training Loss: 0.006951823616400361\n",
      "Training Loss: 0.006840130162891001\n",
      "Validation Loss: 0.004098939833192901\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.005688907621661201\n",
      "Training Loss: 0.005362253502826206\n",
      "Training Loss: 0.005428600062150508\n",
      "Training Loss: 0.00627743054763414\n",
      "Training Loss: 0.0071471237496007235\n",
      "Training Loss: 0.006941178473643958\n",
      "Training Loss: 0.00682690832298249\n",
      "Validation Loss: 0.004084384037017794\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.005678551529417746\n",
      "Training Loss: 0.005351882545510307\n",
      "Training Loss: 0.0054161461815238\n",
      "Training Loss: 0.00626483739586547\n",
      "Training Loss: 0.007135417516110465\n",
      "Training Loss: 0.006930669824359938\n",
      "Training Loss: 0.00681389028322883\n",
      "Validation Loss: 0.004070031014096675\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.005668376788380555\n",
      "Training Loss: 0.005341745543992147\n",
      "Training Loss: 0.005403888367000036\n",
      "Training Loss: 0.006252430464955978\n",
      "Training Loss: 0.007123865580651909\n",
      "Training Loss: 0.006920293434523046\n",
      "Training Loss: 0.0068010763835627585\n",
      "Validation Loss: 0.00405587966698623\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.005658380324603058\n",
      "Training Loss: 0.005331832627998665\n",
      "Training Loss: 0.005391824535327033\n",
      "Training Loss: 0.006240211252006702\n",
      "Training Loss: 0.00711246641818434\n",
      "Training Loss: 0.006910049502039328\n",
      "Training Loss: 0.006788468731101603\n",
      "Validation Loss: 0.004041922341924063\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.005648558031534776\n",
      "Training Loss: 0.005322131906286814\n",
      "Training Loss: 0.005379948042682372\n",
      "Training Loss: 0.00622818116855342\n",
      "Training Loss: 0.007101220716722309\n",
      "Training Loss: 0.006899938186397776\n",
      "Training Loss: 0.00677607258898206\n",
      "Validation Loss: 0.004028165864684944\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.005638908399268985\n",
      "Training Loss: 0.005312637820607051\n",
      "Training Loss: 0.005368258892558515\n",
      "Training Loss: 0.006216345945140347\n",
      "Training Loss: 0.00709012831444852\n",
      "Training Loss: 0.00688996184617281\n",
      "Training Loss: 0.006763892463641241\n",
      "Validation Loss: 0.004014615336614434\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.005629434459842741\n",
      "Training Loss: 0.0053033447556663305\n",
      "Training Loss: 0.005356759039568715\n",
      "Training Loss: 0.006204712472390383\n",
      "Training Loss: 0.007079188188072294\n",
      "Training Loss: 0.006880121238064021\n",
      "Training Loss: 0.006751934327185154\n",
      "Validation Loss: 0.004001278610296603\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.005620136674260721\n",
      "Training Loss: 0.005294249955913983\n",
      "Training Loss: 0.005345450431341305\n",
      "Training Loss: 0.006193287011701614\n",
      "Training Loss: 0.007068405970931053\n",
      "Training Loss: 0.006870420284103602\n",
      "Training Loss: 0.006740205116802827\n",
      "Validation Loss: 0.003988162309196446\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.005611017937771976\n",
      "Training Loss: 0.0052853497175965455\n",
      "Training Loss: 0.0053343345905886965\n",
      "Training Loss: 0.00618207584891934\n",
      "Training Loss: 0.00705778420669958\n",
      "Training Loss: 0.006860860927263275\n",
      "Training Loss: 0.006728709323797375\n",
      "Validation Loss: 0.003975272919871053\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.005602080452954397\n",
      "Training Loss: 0.005276639370713383\n",
      "Training Loss: 0.0053234158817213025\n",
      "Training Loss: 0.006171085364767351\n",
      "Training Loss: 0.007047321882564575\n",
      "Training Loss: 0.006851445698412135\n",
      "Training Loss: 0.006717452229931951\n",
      "Validation Loss: 0.003962623678350103\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.0055933269782690335\n",
      "Training Loss: 0.005268118238891475\n",
      "Training Loss: 0.005312696438631974\n",
      "Training Loss: 0.006160320200724528\n",
      "Training Loss: 0.007037022669101134\n",
      "Training Loss: 0.006842175123747438\n",
      "Training Loss: 0.006706438703695312\n",
      "Validation Loss: 0.0039502198394555385\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.00558475986123085\n",
      "Training Loss: 0.005259783618384972\n",
      "Training Loss: 0.005302179187419824\n",
      "Training Loss: 0.006149784827721305\n",
      "Training Loss: 0.007026884574443102\n",
      "Training Loss: 0.006833052396541461\n",
      "Training Loss: 0.006695671579800546\n",
      "Validation Loss: 0.003938076344897712\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.0055763814767124135\n",
      "Training Loss: 0.0052516344015020875\n",
      "Training Loss: 0.005291867564665154\n",
      "Training Loss: 0.006139484074665234\n",
      "Training Loss: 0.007016912257531658\n",
      "Training Loss: 0.006824075671611354\n",
      "Training Loss: 0.006685151400743053\n",
      "Validation Loss: 0.003926193099189898\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0055681900953641165\n",
      "Training Loss: 0.005243665490997956\n",
      "Training Loss: 0.005281760584330187\n",
      "Training Loss: 0.006129418569034897\n",
      "Training Loss: 0.007007099814945832\n",
      "Training Loss: 0.006815244002500549\n",
      "Training Loss: 0.0066748774738516654\n",
      "Validation Loss: 0.003914579074160101\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0055601865489734335\n",
      "Training Loss: 0.005235872999764979\n",
      "Training Loss: 0.005271859018830582\n",
      "Training Loss: 0.006119587117573246\n",
      "Training Loss: 0.006997449353802949\n",
      "Training Loss: 0.006806557012023404\n",
      "Training Loss: 0.006664847466163337\n",
      "Validation Loss: 0.003903239583055932\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.005552367797936313\n",
      "Training Loss: 0.0052282535948324944\n",
      "Training Loss: 0.0052621624927269295\n",
      "Training Loss: 0.006109991340781562\n",
      "Training Loss: 0.006987956382799893\n",
      "Training Loss: 0.006798010378843173\n",
      "Training Loss: 0.006655057760654017\n",
      "Validation Loss: 0.0038921715934108593\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.005544729704852216\n",
      "Training Loss: 0.005220801418763585\n",
      "Training Loss: 0.005252667987952009\n",
      "Training Loss: 0.006100624673999846\n",
      "Training Loss: 0.006978614823892713\n",
      "Training Loss: 0.006789600187912584\n",
      "Training Loss: 0.00664550228510052\n",
      "Validation Loss: 0.0038813814744946273\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0055372682667803015\n",
      "Training Loss: 0.005213511705514975\n",
      "Training Loss: 0.005243372216937132\n",
      "Training Loss: 0.006091481636394747\n",
      "Training Loss: 0.006969420866807923\n",
      "Training Loss: 0.0067813209886662664\n",
      "Training Loss: 0.0066361739614512775\n",
      "Validation Loss: 0.0038708654931687127\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.005529976882389747\n",
      "Training Loss: 0.005206375387497246\n",
      "Training Loss: 0.0052342690509976815\n",
      "Training Loss: 0.006082557868212462\n",
      "Training Loss: 0.0069603690295480195\n",
      "Training Loss: 0.006773167495848611\n",
      "Training Loss: 0.006627062099287286\n",
      "Validation Loss: 0.0038606168855721138\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.005522846454987303\n",
      "Training Loss: 0.005199386259773746\n",
      "Training Loss: 0.005225353249697946\n",
      "Training Loss: 0.006073841513716616\n",
      "Training Loss: 0.006951450991909951\n",
      "Training Loss: 0.006765132163418457\n",
      "Training Loss: 0.006618159925565124\n",
      "Validation Loss: 0.0038506336041904065\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.00551587033609394\n",
      "Training Loss: 0.00519253580539953\n",
      "Training Loss: 0.0052166157466126606\n",
      "Training Loss: 0.006065326419775374\n",
      "Training Loss: 0.0069426583626773205\n",
      "Training Loss: 0.006757207465125248\n",
      "Training Loss: 0.0066094538068864495\n",
      "Validation Loss: 0.0038409076298827702\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.005509037873707712\n",
      "Training Loss: 0.005185815203585662\n",
      "Training Loss: 0.005208050557412207\n",
      "Training Loss: 0.006056999547290616\n",
      "Training Loss: 0.006933981089387089\n",
      "Training Loss: 0.006749384294962511\n",
      "Training Loss: 0.0066009325522463766\n",
      "Validation Loss: 0.0038314239110637193\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.005502338205697015\n",
      "Training Loss: 0.005179215628304519\n",
      "Training Loss: 0.005199646185501478\n",
      "Training Loss: 0.006048850925290026\n",
      "Training Loss: 0.00692541208001785\n",
      "Training Loss: 0.006741655772784725\n",
      "Training Loss: 0.006592582329176366\n",
      "Validation Loss: 0.0038221849591649148\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00549576262710616\n",
      "Training Loss: 0.005172729359474033\n",
      "Training Loss: 0.0051913960400270295\n",
      "Training Loss: 0.006040869301068596\n",
      "Training Loss: 0.006916941524250433\n",
      "Training Loss: 0.006734011956723407\n",
      "Training Loss: 0.006584393446100876\n",
      "Validation Loss: 0.0038131721677914375\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.005489301357301884\n",
      "Training Loss: 0.005166347027989104\n",
      "Training Loss: 0.005183291877037846\n",
      "Training Loss: 0.006033044180949219\n",
      "Training Loss: 0.006908561246236786\n",
      "Training Loss: 0.0067264455382246525\n",
      "Training Loss: 0.006576350789982826\n",
      "Validation Loss: 0.003804374617447707\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.005482941181981005\n",
      "Training Loss: 0.005160059133195318\n",
      "Training Loss: 0.00517532167606987\n",
      "Training Loss: 0.006025361508363858\n",
      "Training Loss: 0.006900260018883273\n",
      "Training Loss: 0.006718946531182155\n",
      "Training Loss: 0.006568444176809862\n",
      "Validation Loss: 0.003795784462089842\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.005476673308876343\n",
      "Training Loss: 0.005153856686665677\n",
      "Training Loss: 0.005167474891641178\n",
      "Training Loss: 0.006017810311168432\n",
      "Training Loss: 0.006892029664013535\n",
      "Training Loss: 0.006711505753919482\n",
      "Training Loss: 0.006560657247900963\n",
      "Validation Loss: 0.003787378300058094\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.005470484342076816\n",
      "Training Loss: 0.005147729384480044\n",
      "Training Loss: 0.0051597411558032035\n",
      "Training Loss: 0.006010378245264292\n",
      "Training Loss: 0.00688386197318323\n",
      "Training Loss: 0.006704117618501187\n",
      "Training Loss: 0.006552980609703809\n",
      "Validation Loss: 0.0037791523323760607\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.005464365291991271\n",
      "Training Loss: 0.005141672163153999\n",
      "Training Loss: 0.005152112641371787\n",
      "Training Loss: 0.006003053465974517\n",
      "Training Loss: 0.006875743590062484\n",
      "Training Loss: 0.006696768958354369\n",
      "Training Loss: 0.006545400572940707\n",
      "Validation Loss: 0.0037710957119262284\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.005458306872751564\n",
      "Training Loss: 0.005135674320044927\n",
      "Training Loss: 0.005144578581093811\n",
      "Training Loss: 0.005995824761339463\n",
      "Training Loss: 0.006867669832427054\n",
      "Training Loss: 0.00668945537880063\n",
      "Training Loss: 0.006537905643926934\n",
      "Validation Loss: 0.0037631851573882813\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.005452297865413129\n",
      "Training Loss: 0.00512972665426787\n",
      "Training Loss: 0.005137131055817008\n",
      "Training Loss: 0.005988682436873205\n",
      "Training Loss: 0.00685962887480855\n",
      "Training Loss: 0.00668216712307185\n",
      "Training Loss: 0.006530485273106024\n",
      "Validation Loss: 0.003755419773628677\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.005446329400292597\n",
      "Training Loss: 0.005123824858455919\n",
      "Training Loss: 0.005129760884447023\n",
      "Training Loss: 0.005981614834745415\n",
      "Training Loss: 0.006851612888276577\n",
      "Training Loss: 0.006674897337798029\n",
      "Training Loss: 0.006523127321852371\n",
      "Validation Loss: 0.0037477821947078916\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.005440392972086556\n",
      "Training Loss: 0.00511796084116213\n",
      "Training Loss: 0.005122458330588415\n",
      "Training Loss: 0.00597461273369845\n",
      "Training Loss: 0.00684361583320424\n",
      "Training Loss: 0.006667638969374821\n",
      "Training Loss: 0.00651582169928588\n",
      "Validation Loss: 0.0037402592770308814\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.005434479120303877\n",
      "Training Loss: 0.005112126944586634\n",
      "Training Loss: 0.00511521570791956\n",
      "Training Loss: 0.005967667378135957\n",
      "Training Loss: 0.006835627703694626\n",
      "Training Loss: 0.006660384751157835\n",
      "Training Loss: 0.006508559078210965\n",
      "Validation Loss: 0.00373284145361266\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.005428580260486342\n",
      "Training Loss: 0.005106316259480081\n",
      "Training Loss: 0.005108025057124905\n",
      "Training Loss: 0.005960767170763575\n",
      "Training Loss: 0.006827641610288993\n",
      "Training Loss: 0.0066531286493409425\n",
      "Training Loss: 0.006501330079045147\n",
      "Validation Loss: 0.0037255182470742627\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.005422688844264485\n",
      "Training Loss: 0.005100524500012398\n",
      "Training Loss: 0.005100880026584491\n",
      "Training Loss: 0.005953905513160862\n",
      "Training Loss: 0.006819650647230446\n",
      "Training Loss: 0.006645863009616733\n",
      "Training Loss: 0.00649412484257482\n",
      "Validation Loss: 0.003718278582263668\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.005416797390207648\n",
      "Training Loss: 0.0050947442435426635\n",
      "Training Loss: 0.00509377155744005\n",
      "Training Loss: 0.005947074781288393\n",
      "Training Loss: 0.006811647695722059\n",
      "Training Loss: 0.006638583315070719\n",
      "Training Loss: 0.00648693599156104\n",
      "Validation Loss: 0.003711111052223387\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.005410898933769203\n",
      "Training Loss: 0.005088973055244424\n",
      "Training Loss: 0.005086695730569773\n",
      "Training Loss: 0.005940265225362964\n",
      "Training Loss: 0.006803625721950084\n",
      "Training Loss: 0.006631282408488914\n",
      "Training Loss: 0.006479755020700395\n",
      "Validation Loss: 0.003704009482145226\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.005404987593647092\n",
      "Training Loss: 0.0050832020357484\n",
      "Training Loss: 0.0050796442036516965\n",
      "Training Loss: 0.005933471463504247\n",
      "Training Loss: 0.006795579075114802\n",
      "Training Loss: 0.0066239564423449335\n",
      "Training Loss: 0.006472571587655693\n",
      "Validation Loss: 0.003696961726030607\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.005399057547328993\n",
      "Training Loss: 0.005077429082593881\n",
      "Training Loss: 0.005072614075033926\n",
      "Training Loss: 0.005926686761085875\n",
      "Training Loss: 0.00678749981452711\n",
      "Training Loss: 0.006616598622640595\n",
      "Training Loss: 0.00646538183093071\n",
      "Validation Loss: 0.003689960187234766\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.0053931012272369116\n",
      "Training Loss: 0.005071648512966931\n",
      "Training Loss: 0.005065597574575804\n",
      "Training Loss: 0.00591990390967112\n",
      "Training Loss: 0.006779383659595624\n",
      "Training Loss: 0.006609205804998055\n",
      "Training Loss: 0.006458177390741184\n",
      "Validation Loss: 0.003683000398374229\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.005387116122292355\n",
      "Training Loss: 0.005065857958979905\n",
      "Training Loss: 0.0050585907534696165\n",
      "Training Loss: 0.0059131182642886415\n",
      "Training Loss: 0.006771226293640211\n",
      "Training Loss: 0.006601771850837395\n",
      "Training Loss: 0.006450949537102133\n",
      "Validation Loss: 0.00367606758647546\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.0053810942266136405\n",
      "Training Loss: 0.005060051023610868\n",
      "Training Loss: 0.00505158886895515\n",
      "Training Loss: 0.005906322679948062\n",
      "Training Loss: 0.006763018815545366\n",
      "Training Loss: 0.006594292981317267\n",
      "Training Loss: 0.006443693216424435\n",
      "Validation Loss: 0.003669163016846564\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.00537503368977923\n",
      "Training Loss: 0.005054226884967647\n",
      "Training Loss: 0.005044589610770344\n",
      "Training Loss: 0.005899512644973583\n",
      "Training Loss: 0.0067547584336716685\n",
      "Training Loss: 0.006586765112588182\n",
      "Training Loss: 0.0064364028931595384\n",
      "Validation Loss: 0.0036622737394820214\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.005368927703239023\n",
      "Training Loss: 0.00504838275606744\n",
      "Training Loss: 0.005037584902602248\n",
      "Training Loss: 0.005892681939876639\n",
      "Training Loss: 0.0067464426625519994\n",
      "Training Loss: 0.006579184565925971\n",
      "Training Loss: 0.006429073282051831\n",
      "Validation Loss: 0.003655395338011815\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.00536277282226365\n",
      "Training Loss: 0.005042512103100308\n",
      "Training Loss: 0.005030573971453123\n",
      "Training Loss: 0.005885827989550308\n",
      "Training Loss: 0.00673806470585987\n",
      "Training Loss: 0.0065715500735677775\n",
      "Training Loss: 0.006421697947662324\n",
      "Validation Loss: 0.003648524099353994\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.005356565134134144\n",
      "Training Loss: 0.005036615547724068\n",
      "Training Loss: 0.005023553284700028\n",
      "Training Loss: 0.005878944292198866\n",
      "Training Loss: 0.0067296197242103515\n",
      "Training Loss: 0.00656385590438731\n",
      "Training Loss: 0.00641427164315246\n",
      "Validation Loss: 0.003641650272090309\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.005350301035214215\n",
      "Training Loss: 0.005030688112019561\n",
      "Training Loss: 0.005016518993070349\n",
      "Training Loss: 0.00587202955968678\n",
      "Training Loss: 0.006721104743191973\n",
      "Training Loss: 0.006556097969878465\n",
      "Training Loss: 0.006406789208995178\n",
      "Validation Loss: 0.0036347707465800285\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.005343975484720431\n",
      "Training Loss: 0.005024727919371799\n",
      "Training Loss: 0.005009469057549723\n",
      "Training Loss: 0.005865074343746528\n",
      "Training Loss: 0.0067125182610470804\n",
      "Training Loss: 0.0065482766064815226\n",
      "Training Loss: 0.006399245666107163\n",
      "Validation Loss: 0.0036278806255314123\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0053375872661126774\n",
      "Training Loss: 0.005018733678152785\n",
      "Training Loss: 0.005002400585799478\n",
      "Training Loss: 0.0058580807829275725\n",
      "Training Loss: 0.006703852729406208\n",
      "Training Loss: 0.006540388435823843\n",
      "Training Loss: 0.006391636778134853\n",
      "Validation Loss: 0.0036209778679702222\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.005331132201827131\n",
      "Training Loss: 0.005012704373220913\n",
      "Training Loss: 0.004995313006802462\n",
      "Training Loss: 0.005851043274160475\n",
      "Training Loss: 0.006695109155261889\n",
      "Training Loss: 0.006532431876985356\n",
      "Training Loss: 0.006383960644016042\n",
      "Validation Loss: 0.0036140530185306565\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.005324606862850487\n",
      "Training Loss: 0.0050066381425131114\n",
      "Training Loss: 0.004988202860113233\n",
      "Training Loss: 0.005843959194608033\n",
      "Training Loss: 0.006686283986782655\n",
      "Training Loss: 0.006524405342061073\n",
      "Training Loss: 0.006376211342867464\n",
      "Validation Loss: 0.0036071079019247814\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.005318010990740731\n",
      "Training Loss: 0.005000533131533302\n",
      "Training Loss: 0.004981070840149187\n",
      "Training Loss: 0.005836826650192961\n",
      "Training Loss: 0.006677374948048964\n",
      "Training Loss: 0.006516307648271322\n",
      "Training Loss: 0.006368384562665597\n",
      "Validation Loss: 0.003600135920412336\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.0053113416489213704\n",
      "Training Loss: 0.004994386971811764\n",
      "Training Loss: 0.004973912884597666\n",
      "Training Loss: 0.005829642448225059\n",
      "Training Loss: 0.0066683807852678\n",
      "Training Loss: 0.006508138878270984\n",
      "Training Loss: 0.006360479331342503\n",
      "Validation Loss: 0.00359313057653592\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.0053045949275838215\n",
      "Training Loss: 0.004988202576641925\n",
      "Training Loss: 0.004966729565057904\n",
      "Training Loss: 0.005822405070066452\n",
      "Training Loss: 0.006659302132902667\n",
      "Training Loss: 0.006499897661851719\n",
      "Training Loss: 0.0063524933939334005\n",
      "Validation Loss: 0.0035860960394464964\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.005297770764445886\n",
      "Training Loss: 0.004981975201517343\n",
      "Training Loss: 0.004959519898984581\n",
      "Training Loss: 0.005815112144337036\n",
      "Training Loss: 0.0066501345823053275\n",
      "Training Loss: 0.006491586469346658\n",
      "Training Loss: 0.006344423446571454\n",
      "Validation Loss: 0.0035790264273362323\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.005290867283474654\n",
      "Training Loss: 0.004975707320263609\n",
      "Training Loss: 0.004952284506289289\n",
      "Training Loss: 0.005807763611664995\n",
      "Training Loss: 0.006640881216153503\n",
      "Training Loss: 0.006483204161049798\n",
      "Training Loss: 0.006336267647566274\n",
      "Validation Loss: 0.0035719196242367506\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005283883227966726\n",
      "Training Loss: 0.004969399450928904\n",
      "Training Loss: 0.004945024183252826\n",
      "Training Loss: 0.005800357302068733\n",
      "Training Loss: 0.006631540799280629\n",
      "Training Loss: 0.0064747518207877875\n",
      "Training Loss: 0.006328024584800005\n",
      "Validation Loss: 0.003564774425009663\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.0052768200309947135\n",
      "Training Loss: 0.00496305238630157\n",
      "Training Loss: 0.0049377382942475375\n",
      "Training Loss: 0.005792894703918136\n",
      "Training Loss: 0.006622114554047585\n",
      "Training Loss: 0.006466232514940202\n",
      "Training Loss: 0.006319694359553978\n",
      "Validation Loss: 0.003557591539919237\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.005269676857860759\n",
      "Training Loss: 0.004956665939535015\n",
      "Training Loss: 0.004930429291562177\n",
      "Training Loss: 0.0057853759470162915\n",
      "Training Loss: 0.006612604768015444\n",
      "Training Loss: 0.006457648143405095\n",
      "Training Loss: 0.006311275821644813\n",
      "Validation Loss: 0.003550368312777679\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005262452281313017\n",
      "Training Loss: 0.004950242572231218\n",
      "Training Loss: 0.004923097075079568\n",
      "Training Loss: 0.005777799020288512\n",
      "Training Loss: 0.006603012562263757\n",
      "Training Loss: 0.006449002558365464\n",
      "Training Loss: 0.0063027714996133\n",
      "Validation Loss: 0.0035431082499970436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.005255149442818947\n",
      "Training Loss: 0.0049437853007111695\n",
      "Training Loss: 0.00491574498184491\n",
      "Training Loss: 0.005770168296876364\n",
      "Training Loss: 0.006593342926353216\n",
      "Training Loss: 0.006440299118403345\n",
      "Training Loss: 0.006294180733384565\n",
      "Validation Loss: 0.0035358007394705204\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005247767421533354\n",
      "Training Loss: 0.004937294178525917\n",
      "Training Loss: 0.004908374668448232\n",
      "Training Loss: 0.005762483846046962\n",
      "Training Loss: 0.006583598862634972\n",
      "Training Loss: 0.00643154195509851\n",
      "Training Loss: 0.00628550676163286\n",
      "Validation Loss: 0.0035284606173934504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.005240311030647717\n",
      "Training Loss: 0.004930772690568119\n",
      "Training Loss: 0.004900988928275183\n",
      "Training Loss: 0.00575474898389075\n",
      "Training Loss: 0.006573784470092505\n",
      "Training Loss: 0.006422737961402163\n",
      "Training Loss: 0.00627674967981875\n",
      "Validation Loss: 0.0035210789956522178\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.0052327785978559405\n",
      "Training Loss: 0.004924223309499212\n",
      "Training Loss: 0.0048935901949880645\n",
      "Training Loss: 0.005746963761630468\n",
      "Training Loss: 0.006563905618386343\n",
      "Training Loss: 0.006413891490083188\n",
      "Training Loss: 0.006267914997879416\n",
      "Validation Loss: 0.003513659095942751\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005225175363593734\n",
      "Training Loss: 0.004917652467847802\n",
      "Training Loss: 0.004886182647314854\n",
      "Training Loss: 0.005739133854513056\n",
      "Training Loss: 0.006553967095678672\n",
      "Training Loss: 0.006405009196605534\n",
      "Training Loss: 0.006259004898602143\n",
      "Validation Loss: 0.003506207009794867\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005217505766195245\n",
      "Training Loss: 0.004911061977036297\n",
      "Training Loss: 0.004878771523945034\n",
      "Training Loss: 0.00573126407340169\n",
      "Training Loss: 0.0065439759433502335\n",
      "Training Loss: 0.006396099888952449\n",
      "Training Loss: 0.006250026453053579\n",
      "Validation Loss: 0.00349872529529356\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005209773006499745\n",
      "Training Loss: 0.0049044582521310075\n",
      "Training Loss: 0.004871361198602244\n",
      "Training Loss: 0.005723357783281245\n",
      "Training Loss: 0.006533941057277844\n",
      "Training Loss: 0.006387171323876828\n",
      "Training Loss: 0.0062409859651234\n",
      "Validation Loss: 0.0034912175386411586\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005201982588623651\n",
      "Training Loss: 0.0048978450131835416\n",
      "Training Loss: 0.0048639570263912905\n",
      "Training Loss: 0.005715422024368308\n",
      "Training Loss: 0.006523872035322711\n",
      "Training Loss: 0.00637823241064325\n",
      "Training Loss: 0.0062318870739545675\n",
      "Validation Loss: 0.0034836864714556082\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005194141369429417\n",
      "Training Loss: 0.0048912312864558775\n",
      "Training Loss: 0.004856565907830373\n",
      "Training Loss: 0.005707461377023719\n",
      "Training Loss: 0.0065137764520477506\n",
      "Training Loss: 0.00636929259635508\n",
      "Training Loss: 0.0062227378645911815\n",
      "Validation Loss: 0.0034761376568196475\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005186255043372512\n",
      "Training Loss: 0.0048846210091141986\n",
      "Training Loss: 0.004849191704997793\n",
      "Training Loss: 0.00569948339543771\n",
      "Training Loss: 0.006503665969939902\n",
      "Training Loss: 0.006360361310653389\n",
      "Training Loss: 0.006213548610685393\n",
      "Validation Loss: 0.0034685787402114338\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.005178331524366513\n",
      "Training Loss: 0.00487802094547078\n",
      "Training Loss: 0.004841843049507588\n",
      "Training Loss: 0.005691498554660938\n",
      "Training Loss: 0.006493550777668134\n",
      "Training Loss: 0.006351452341768891\n",
      "Training Loss: 0.006204330490436405\n",
      "Validation Loss: 0.0034610131469447066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005170377765898593\n",
      "Training Loss: 0.0048714405915234234\n",
      "Training Loss: 0.004834527149214409\n",
      "Training Loss: 0.005683510273229331\n",
      "Training Loss: 0.006483444276964292\n",
      "Training Loss: 0.00634257716475986\n",
      "Training Loss: 0.006195090091787279\n",
      "Validation Loss: 0.0034534481536310987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.0051624047406949106\n",
      "Training Loss: 0.004864887101575732\n",
      "Training Loss: 0.004827251256210729\n",
      "Training Loss: 0.005675531703745946\n",
      "Training Loss: 0.006473358400398865\n",
      "Training Loss: 0.006333745931042359\n",
      "Training Loss: 0.006185841769911349\n",
      "Validation Loss: 0.0034458990747715983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005154420834151097\n",
      "Training Loss: 0.0048583683231845495\n",
      "Training Loss: 0.004820025212829933\n",
      "Training Loss: 0.0056675691477721555\n",
      "Training Loss: 0.006463304998469539\n",
      "Training Loss: 0.0063249725417699664\n",
      "Training Loss: 0.006176596187287942\n",
      "Validation Loss: 0.0034383615205929464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005146437705261633\n",
      "Training Loss: 0.004851890935678966\n",
      "Training Loss: 0.004812854161136784\n",
      "Training Loss: 0.00565963487024419\n",
      "Training Loss: 0.006453300967696123\n",
      "Training Loss: 0.006316270974930376\n",
      "Training Loss: 0.006167366101872176\n",
      "Validation Loss: 0.0034308497883791056\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005138463908224367\n",
      "Training Loss: 0.004845466646365822\n",
      "Training Loss: 0.004805750138475559\n",
      "Training Loss: 0.005651740618050099\n",
      "Training Loss: 0.006443358925171196\n",
      "Training Loss: 0.00630765225738287\n",
      "Training Loss: 0.006158165048109367\n",
      "Validation Loss: 0.003423376875660951\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005130511993775145\n",
      "Training Loss: 0.004839104491402395\n",
      "Training Loss: 0.004798721352126449\n",
      "Training Loss: 0.0056438944477122275\n",
      "Training Loss: 0.006433492606738582\n",
      "Training Loss: 0.006299132267013192\n",
      "Training Loss: 0.006149008658248931\n",
      "Validation Loss: 0.0034159444493140454\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005122595021966845\n",
      "Training Loss: 0.004832812750246376\n",
      "Training Loss: 0.004791773830074817\n",
      "Training Loss: 0.0056361084146192295\n",
      "Training Loss: 0.006423720200546086\n",
      "Training Loss: 0.0062907218839973215\n",
      "Training Loss: 0.006139909048797563\n",
      "Validation Loss: 0.0034085693681954444\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.00511472187936306\n",
      "Training Loss: 0.0048266002116724846\n",
      "Training Loss: 0.004784920202801004\n",
      "Training Loss: 0.005628393199294805\n",
      "Training Loss: 0.00641405084868893\n",
      "Training Loss: 0.006282435246976093\n",
      "Training Loss: 0.006130881096469238\n",
      "Validation Loss: 0.003401254451125581\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005106907291337848\n",
      "Training Loss: 0.0048204742441885174\n",
      "Training Loss: 0.004778167284675874\n",
      "Training Loss: 0.005620761018944904\n",
      "Training Loss: 0.006404503286466934\n",
      "Training Loss: 0.006274283976526931\n",
      "Training Loss: 0.006121939570875839\n",
      "Validation Loss: 0.003394018001470729\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005099163427948952\n",
      "Training Loss: 0.004814443617942743\n",
      "Training Loss: 0.004771523251547478\n",
      "Training Loss: 0.005613221675739624\n",
      "Training Loss: 0.006395089626312256\n",
      "Training Loss: 0.006266279622213915\n",
      "Training Loss: 0.006113098395289853\n",
      "Validation Loss: 0.00338686027691424\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005091502195573412\n",
      "Training Loss: 0.004808517033816315\n",
      "Training Loss: 0.004764995155273937\n",
      "Training Loss: 0.005605785477673635\n",
      "Training Loss: 0.006385824680910446\n",
      "Training Loss: 0.006258435234194622\n",
      "Training Loss: 0.0061043692484963686\n",
      "Validation Loss: 0.0033797915416249958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005083932557026855\n",
      "Training Loss: 0.004802698771818541\n",
      "Training Loss: 0.004758589459815994\n",
      "Training Loss: 0.005598463360220194\n",
      "Training Loss: 0.006376719656400382\n",
      "Training Loss: 0.006250758733367548\n",
      "Training Loss: 0.006095769687090069\n",
      "Validation Loss: 0.003372820633139168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.0050764677219558504\n",
      "Training Loss: 0.004796998881502077\n",
      "Training Loss: 0.004752316522062756\n",
      "Training Loss: 0.005591261952649802\n",
      "Training Loss: 0.006367786474875174\n",
      "Training Loss: 0.0062432592012919485\n",
      "Training Loss: 0.006087308838032186\n",
      "Validation Loss: 0.0033659610390656173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0050691190548241135\n",
      "Training Loss: 0.004791420721448958\n",
      "Training Loss: 0.004746178528876044\n",
      "Training Loss: 0.005584191677044146\n",
      "Training Loss: 0.0063590366835705936\n",
      "Training Loss: 0.006235944508807734\n",
      "Training Loss: 0.00607899869675748\n",
      "Validation Loss: 0.003359211736955167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005061892000958324\n",
      "Training Loss: 0.004785967086208984\n",
      "Training Loss: 0.004740179572836496\n",
      "Training Loss: 0.005577257370459847\n",
      "Training Loss: 0.006350477416417561\n",
      "Training Loss: 0.0062288201390765605\n",
      "Training Loss: 0.0060708494007121775\n",
      "Validation Loss: 0.003352583880878092\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0050547997251851485\n",
      "Training Loss: 0.004780644844286144\n",
      "Training Loss: 0.004734326296602376\n",
      "Training Loss: 0.005570468239020556\n",
      "Training Loss: 0.0063421161501901226\n",
      "Training Loss: 0.006221888842992485\n",
      "Training Loss: 0.006062867456348613\n",
      "Validation Loss: 0.0033460835706484453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005047844566288404\n",
      "Training Loss: 0.004775451891473495\n",
      "Training Loss: 0.004728618549415841\n",
      "Training Loss: 0.005563827156438492\n",
      "Training Loss: 0.006333961241180078\n",
      "Training Loss: 0.006215157342376187\n",
      "Training Loss: 0.006055065526161343\n",
      "Validation Loss: 0.0033397105934921552\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.00504103526705876\n",
      "Training Loss: 0.0047703925345558675\n",
      "Training Loss: 0.004723059507668949\n",
      "Training Loss: 0.00555733700399287\n",
      "Training Loss: 0.0063260168535634875\n",
      "Training Loss: 0.006208626542356797\n",
      "Training Loss: 0.00604744661715813\n",
      "Validation Loss: 0.0033334764387730636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005034376306575723\n",
      "Training Loss: 0.004765466725220904\n",
      "Training Loss: 0.004717650226084515\n",
      "Training Loss: 0.005551004388253205\n",
      "Training Loss: 0.0063182857353240255\n",
      "Training Loss: 0.006202295674593188\n",
      "Training Loss: 0.006040015548933297\n",
      "Validation Loss: 0.0033273757121452456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.00502787041128613\n",
      "Training Loss: 0.00476067517476622\n",
      "Training Loss: 0.004712391258799471\n",
      "Training Loss: 0.005544827868579887\n",
      "Training Loss: 0.0063107692881021644\n",
      "Training Loss: 0.006196161682601087\n",
      "Training Loss: 0.0060327746591065075\n",
      "Validation Loss: 0.003321417579898255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005021521830931306\n",
      "Training Loss: 0.004756013796431944\n",
      "Training Loss: 0.004707278509740718\n",
      "Training Loss: 0.0055388088250765575\n",
      "Training Loss: 0.006303467745310627\n",
      "Training Loss: 0.006190224607707933\n",
      "Training Loss: 0.006025727193336934\n",
      "Validation Loss: 0.0033155962094936647\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0050153288402361796\n",
      "Training Loss: 0.004751479676924646\n",
      "Training Loss: 0.004702311021392233\n",
      "Training Loss: 0.0055329456325853245\n",
      "Training Loss: 0.006296379960840568\n",
      "Training Loss: 0.00618448086665012\n",
      "Training Loss: 0.006018873136490584\n",
      "Validation Loss: 0.003309913771401407\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.0050092931295512245\n",
      "Training Loss: 0.004747070078155957\n",
      "Training Loss: 0.004697485935175791\n",
      "Training Loss: 0.00552723653963767\n",
      "Training Loss: 0.006289505325257778\n",
      "Training Loss: 0.006178924842388369\n",
      "Training Loss: 0.006012211525812745\n",
      "Validation Loss: 0.0033043706105073236\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005003412274527363\n",
      "Training Loss: 0.00474278480745852\n",
      "Training Loss: 0.0046928001183550805\n",
      "Training Loss: 0.005521680152742192\n",
      "Training Loss: 0.006282839454361238\n",
      "Training Loss: 0.006173552150139585\n",
      "Training Loss: 0.006005740885157138\n",
      "Validation Loss: 0.003298965184199397\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.004997685633716173\n",
      "Training Loss: 0.004738613349036314\n",
      "Training Loss: 0.004688249722821638\n",
      "Training Loss: 0.0055162729555740956\n",
      "Training Loss: 0.006276377091417089\n",
      "Training Loss: 0.006168356214184314\n",
      "Training Loss: 0.005999456697609275\n",
      "Validation Loss: 0.0032936958657152084\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.004992108410806395\n",
      "Training Loss: 0.004734554614988156\n",
      "Training Loss: 0.004683828642591834\n",
      "Training Loss: 0.005511008903267793\n",
      "Training Loss: 0.006270114741055295\n",
      "Training Loss: 0.006163329146802426\n",
      "Training Loss: 0.0059933556325268\n",
      "Validation Loss: 0.0032885629275625342\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.004986678311834112\n",
      "Training Loss: 0.004730604142532684\n",
      "Training Loss: 0.0046795323654077944\n",
      "Training Loss: 0.005505888676852919\n",
      "Training Loss: 0.006264046435244381\n",
      "Training Loss: 0.006158464681357145\n",
      "Training Loss: 0.005987434260314331\n",
      "Validation Loss: 0.003283556797088225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.004981391591718421\n",
      "Training Loss: 0.004726755266310647\n",
      "Training Loss: 0.004675356290536001\n",
      "Training Loss: 0.005500901890336536\n",
      "Training Loss: 0.00625816679908894\n",
      "Training Loss: 0.00615375776134897\n",
      "Training Loss: 0.005981686742743477\n",
      "Validation Loss: 0.0032786753971290107\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.0049762423161882905\n",
      "Training Loss: 0.004723002893733792\n",
      "Training Loss: 0.004671293208375573\n",
      "Training Loss: 0.005496047259657644\n",
      "Training Loss: 0.0062524689815472816\n",
      "Training Loss: 0.006149199859937653\n",
      "Training Loss: 0.005976107367314398\n",
      "Validation Loss: 0.0032739167128714003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.004971226184861735\n",
      "Training Loss: 0.004719341672025621\n",
      "Training Loss: 0.004667338536819443\n",
      "Training Loss: 0.005491318499553017\n",
      "Training Loss: 0.006246944946469739\n",
      "Training Loss: 0.006144784041680396\n",
      "Training Loss: 0.005970691536786035\n",
      "Validation Loss: 0.0032692805956351513\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.004966337998048402\n",
      "Training Loss: 0.004715767667512409\n",
      "Training Loss: 0.004663488161168061\n",
      "Training Loss: 0.005486712192650884\n",
      "Training Loss: 0.0062415911792777475\n",
      "Training Loss: 0.00614050293690525\n",
      "Training Loss: 0.0059654332837089895\n",
      "Validation Loss: 0.0032647601433433175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.004961573999607936\n",
      "Training Loss: 0.004712277660728432\n",
      "Training Loss: 0.004659736635512672\n",
      "Training Loss: 0.005482222707360052\n",
      "Training Loss: 0.006236397814936936\n",
      "Training Loss: 0.006136349826701917\n",
      "Training Loss: 0.005960324996849522\n",
      "Validation Loss: 0.0032603532596480814\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.004956927411258221\n",
      "Training Loss: 0.0047088640386937185\n",
      "Training Loss: 0.004656078320695087\n",
      "Training Loss: 0.005477842220570892\n",
      "Training Loss: 0.006231360520469025\n",
      "Training Loss: 0.006132318490417674\n",
      "Training Loss: 0.0059553597203921525\n",
      "Validation Loss: 0.003256053819010655\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.004952394206775353\n",
      "Training Loss: 0.004705523051670752\n",
      "Training Loss: 0.00465250788256526\n",
      "Training Loss: 0.005473568143206649\n",
      "Training Loss: 0.006226471391273663\n",
      "Training Loss: 0.0061284027737565335\n",
      "Training Loss: 0.005950535257579759\n",
      "Validation Loss: 0.0032518542496389833\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.004947967635816894\n",
      "Training Loss: 0.0047022517619188875\n",
      "Training Loss: 0.004649020279757678\n",
      "Training Loss: 0.00546939475403633\n",
      "Training Loss: 0.006221726002986543\n",
      "Training Loss: 0.00612459726864472\n",
      "Training Loss: 0.005945843125227839\n",
      "Validation Loss: 0.003247760895514086\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.004943645307794214\n",
      "Training Loss: 0.004699044864391908\n",
      "Training Loss: 0.004645610320731066\n",
      "Training Loss: 0.005465319593204185\n",
      "Training Loss: 0.006217117651249282\n",
      "Training Loss: 0.006120897480868735\n",
      "Training Loss: 0.005941279148682952\n",
      "Validation Loss: 0.0032437573167868657\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.004939420285518281\n",
      "Training Loss: 0.004695900573278777\n",
      "Training Loss: 0.0046422749949852\n",
      "Training Loss: 0.0054613359173526985\n",
      "Training Loss: 0.00621263820969034\n",
      "Training Loss: 0.006117296771262773\n",
      "Training Loss: 0.005936836153268814\n",
      "Validation Loss: 0.0032398550791233388\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.00493528937920928\n",
      "Training Loss: 0.004692813861183822\n",
      "Training Loss: 0.004639010308310389\n",
      "Training Loss: 0.0054574401094578204\n",
      "Training Loss: 0.006208285465254449\n",
      "Training Loss: 0.006113789349910803\n",
      "Training Loss: 0.005932510043494404\n",
      "Validation Loss: 0.0032360394688476617\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.004931248771608807\n",
      "Training Loss: 0.004689782002824359\n",
      "Training Loss: 0.004635812051128596\n",
      "Training Loss: 0.00545362877950538\n",
      "Training Loss: 0.006204053033143282\n",
      "Training Loss: 0.006110373993869871\n",
      "Training Loss: 0.00592829366447404\n",
      "Validation Loss: 0.0032323073837313143\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.004927291542408056\n",
      "Training Loss: 0.004686803926015273\n",
      "Training Loss: 0.004632677467889153\n",
      "Training Loss: 0.0054498974239686505\n",
      "Training Loss: 0.006199936034390703\n",
      "Training Loss: 0.006107044446980581\n",
      "Training Loss: 0.005924184925388545\n",
      "Validation Loss: 0.0032286563380673696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.0049234155117301275\n",
      "Training Loss: 0.004683873709873296\n",
      "Training Loss: 0.004629600893240422\n",
      "Training Loss: 0.0054462424601661045\n",
      "Training Loss: 0.006195929787936621\n",
      "Training Loss: 0.006103796815732494\n",
      "Training Loss: 0.00592017751885578\n",
      "Validation Loss: 0.003225087678698258\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.004919619687716477\n",
      "Training Loss: 0.004680993775255046\n",
      "Training Loss: 0.004626583346980624\n",
      "Training Loss: 0.00544266180077102\n",
      "Training Loss: 0.00619202810281422\n",
      "Training Loss: 0.006100629389984533\n",
      "Training Loss: 0.00591626959387213\n",
      "Validation Loss: 0.003221596990125986\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.00491589706914965\n",
      "Training Loss: 0.004678159414906986\n",
      "Training Loss: 0.004623618900659494\n",
      "Training Loss: 0.00543915276590269\n",
      "Training Loss: 0.006188229252002202\n",
      "Training Loss: 0.0060975359135773035\n",
      "Training Loss: 0.005912453278433531\n",
      "Validation Loss: 0.003218177277402959\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.0049122468894347545\n",
      "Training Loss: 0.004675367796444334\n",
      "Training Loss: 0.004620706723071635\n",
      "Training Loss: 0.005435709965531714\n",
      "Training Loss: 0.006184527269797399\n",
      "Training Loss: 0.0060945158987306056\n",
      "Training Loss: 0.005908727706409991\n",
      "Validation Loss: 0.0032148288849790053\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.004908665019320324\n",
      "Training Loss: 0.004672620072960853\n",
      "Training Loss: 0.004617843519081361\n",
      "Training Loss: 0.005432332273339852\n",
      "Training Loss: 0.006180920420447365\n",
      "Training Loss: 0.006091564241796732\n",
      "Training Loss: 0.005905087243299931\n",
      "Validation Loss: 0.0032115503779371803\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.004905148121179081\n",
      "Training Loss: 0.004669912469107657\n",
      "Training Loss: 0.004615026869578287\n",
      "Training Loss: 0.0054290173423942175\n",
      "Training Loss: 0.006177404699847102\n",
      "Training Loss: 0.006088682128465734\n",
      "Training Loss: 0.0059015306609217074\n",
      "Validation Loss: 0.003208337488151696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.004901696457527578\n",
      "Training Loss: 0.004667244440061041\n",
      "Training Loss: 0.004612256550462917\n",
      "Training Loss: 0.005425762674422003\n",
      "Training Loss: 0.006173974192352034\n",
      "Training Loss: 0.0060858620307408275\n",
      "Training Loss: 0.005898052171105519\n",
      "Validation Loss: 0.0032051884432319788\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.004898302814690396\n",
      "Training Loss: 0.004664615414803848\n",
      "Training Loss: 0.004609527182765305\n",
      "Training Loss: 0.00542256448592525\n",
      "Training Loss: 0.006170628978288732\n",
      "Training Loss: 0.006083106541773305\n",
      "Training Loss: 0.005894650593400001\n",
      "Validation Loss: 0.0032020963794701777\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.004894969126326032\n",
      "Training Loss: 0.004662022428819909\n",
      "Training Loss: 0.004606839781044983\n",
      "Training Loss: 0.00541942315932829\n",
      "Training Loss: 0.0061673646239796655\n",
      "Training Loss: 0.006080411621369421\n",
      "Training Loss: 0.005891321426024661\n",
      "Validation Loss: 0.0031990649820434467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.004891690468066372\n",
      "Training Loss: 0.004659463857533411\n",
      "Training Loss: 0.004604191919788718\n",
      "Training Loss: 0.005416333991452121\n",
      "Training Loss: 0.006164177374448627\n",
      "Training Loss: 0.006077772390563041\n",
      "Training Loss: 0.005888063660822809\n",
      "Validation Loss: 0.0031960935398154936\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.004888466903357766\n",
      "Training Loss: 0.004656941308639944\n",
      "Training Loss: 0.0046015815023565665\n",
      "Training Loss: 0.005413296055630781\n",
      "Training Loss: 0.006161066773347557\n",
      "Training Loss: 0.006075192005373538\n",
      "Training Loss: 0.005884873036993667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [04:50<43:33, 290.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0031931716063642378\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.5862308865785599\n",
      "Training Loss: 0.46895305812358856\n",
      "Training Loss: 0.3622805516421795\n",
      "Training Loss: 0.24134528383612633\n",
      "Training Loss: 0.1425418009608984\n",
      "Training Loss: 0.08753643760457636\n",
      "Training Loss: 0.07262229058891535\n",
      "Validation Loss: 0.06901516650946399\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06882957268506289\n",
      "Training Loss: 0.06638377822935582\n",
      "Training Loss: 0.06547016635537148\n",
      "Training Loss: 0.06494133757427335\n",
      "Training Loss: 0.06261815577745437\n",
      "Training Loss: 0.06250960256904364\n",
      "Training Loss: 0.06028001418337226\n",
      "Validation Loss: 0.05856702857640352\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.05848550038412213\n",
      "Training Loss: 0.0559947319701314\n",
      "Training Loss: 0.05466791328042746\n",
      "Training Loss: 0.05392538316547871\n",
      "Training Loss: 0.05148589219897985\n",
      "Training Loss: 0.0509364277869463\n",
      "Training Loss: 0.04847632167860866\n",
      "Validation Loss: 0.04699811498435696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.04640915792435408\n",
      "Training Loss: 0.04403334333561361\n",
      "Training Loss: 0.04243634353391826\n",
      "Training Loss: 0.04181394968181849\n",
      "Training Loss: 0.03958366187289357\n",
      "Training Loss: 0.03897374266758561\n",
      "Training Loss: 0.036701790429651736\n",
      "Validation Loss: 0.03540695333776626\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.03480436933226883\n",
      "Training Loss: 0.03300765688531101\n",
      "Training Loss: 0.03154855646193028\n",
      "Training Loss: 0.03135402080602944\n",
      "Training Loss: 0.02965246746316552\n",
      "Training Loss: 0.029117689658887683\n",
      "Training Loss: 0.02731530287768692\n",
      "Validation Loss: 0.025999783276674453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.025699914880096913\n",
      "Training Loss: 0.024453402706421913\n",
      "Training Loss: 0.023319828012026846\n",
      "Training Loss: 0.023523114831186832\n",
      "Training Loss: 0.022504864600487052\n",
      "Training Loss: 0.02202982227317989\n",
      "Training Loss: 0.020775103019550444\n",
      "Validation Loss: 0.019373146091563424\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.019430013783276082\n",
      "Training Loss: 0.01857849619584158\n",
      "Training Loss: 0.017786844184156506\n",
      "Training Loss: 0.018259670543484388\n",
      "Training Loss: 0.017880673371255398\n",
      "Training Loss: 0.0174187823664397\n",
      "Training Loss: 0.01655101775424555\n",
      "Validation Loss: 0.014993031519684898\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.015367295446339994\n",
      "Training Loss: 0.01469549045432359\n",
      "Training Loss: 0.014152502042707056\n",
      "Training Loss: 0.014769806519616395\n",
      "Training Loss: 0.014895993422251196\n",
      "Training Loss: 0.014425783874467015\n",
      "Training Loss: 0.013792677936144172\n",
      "Validation Loss: 0.012006186189527592\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.012686960101127624\n",
      "Training Loss: 0.012081702528521419\n",
      "Training Loss: 0.011727974409004673\n",
      "Training Loss: 0.012456647688522934\n",
      "Training Loss: 0.012965445495210589\n",
      "Training Loss: 0.012505767766851932\n",
      "Training Loss: 0.012031619414919988\n",
      "Validation Loss: 0.009967507538124082\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.010934835060033947\n",
      "Training Loss: 0.010358547359937803\n",
      "Training Loss: 0.010144746354781091\n",
      "Training Loss: 0.010974585852818564\n",
      "Training Loss: 0.011739114315714687\n",
      "Training Loss: 0.011301343247760087\n",
      "Training Loss: 0.010932044606888666\n",
      "Validation Loss: 0.008570720572443156\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.009779213026631624\n",
      "Training Loss: 0.009214619569247589\n",
      "Training Loss: 0.009081132229184732\n",
      "Training Loss: 0.00998084957129322\n",
      "Training Loss: 0.010879262939561159\n",
      "Training Loss: 0.010454608873696997\n",
      "Training Loss: 0.010153342228149995\n",
      "Validation Loss: 0.007526352057191595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.008917676458368077\n",
      "Training Loss: 0.008360935342498124\n",
      "Training Loss: 0.008263592006405815\n",
      "Training Loss: 0.009195156194036826\n",
      "Training Loss: 0.010151697284309194\n",
      "Training Loss: 0.009747656438266859\n",
      "Training Loss: 0.009524038997478784\n",
      "Validation Loss: 0.006708125822762314\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.008229807346360758\n",
      "Training Loss: 0.007713897826615721\n",
      "Training Loss: 0.007646021065302194\n",
      "Training Loss: 0.00860783228999935\n",
      "Training Loss: 0.009607353829778731\n",
      "Training Loss: 0.009268959398614243\n",
      "Training Loss: 0.009136987702222541\n",
      "Validation Loss: 0.006208110876269778\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.007802023359108717\n",
      "Training Loss: 0.0073511374380905185\n",
      "Training Loss: 0.007301402031444013\n",
      "Training Loss: 0.008290833264472894\n",
      "Training Loss: 0.009320596160832792\n",
      "Training Loss: 0.009036249566124753\n",
      "Training Loss: 0.00895357087487355\n",
      "Validation Loss: 0.005954232973775613\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.0075730454258155075\n",
      "Training Loss: 0.00716742912074551\n",
      "Training Loss: 0.007117746331496164\n",
      "Training Loss: 0.008120922566158697\n",
      "Training Loss: 0.009169184500351549\n",
      "Training Loss: 0.008911177228437737\n",
      "Training Loss: 0.008850860355887563\n",
      "Validation Loss: 0.005808999199482999\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.007433820631122217\n",
      "Training Loss: 0.007056404909817502\n",
      "Training Loss: 0.007003249892732128\n",
      "Training Loss: 0.008013040523510427\n",
      "Training Loss: 0.009072890797397122\n",
      "Training Loss: 0.008828502546530217\n",
      "Training Loss: 0.008780032501090319\n",
      "Validation Loss: 0.0057146633523498915\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.007337119117146358\n",
      "Training Loss: 0.006978689129464328\n",
      "Training Loss: 0.006922910956200212\n",
      "Training Loss: 0.007934790550498292\n",
      "Training Loss: 0.009001540328608826\n",
      "Training Loss: 0.008765695850597695\n",
      "Training Loss: 0.00872420820291154\n",
      "Validation Loss: 0.005647113180478637\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.007262574916239828\n",
      "Training Loss: 0.006918149086413905\n",
      "Training Loss: 0.006860910053364933\n",
      "Training Loss: 0.007871950949192979\n",
      "Training Loss: 0.008943037842400373\n",
      "Training Loss: 0.00871318052872084\n",
      "Training Loss: 0.008675901755923405\n",
      "Validation Loss: 0.005593405147030783\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.007199579743901268\n",
      "Training Loss: 0.006866493704728782\n",
      "Training Loss: 0.006808842092286795\n",
      "Training Loss: 0.00781739600468427\n",
      "Training Loss: 0.008891706763533876\n",
      "Training Loss: 0.008666256119031459\n",
      "Training Loss: 0.008631253901403397\n",
      "Validation Loss: 0.005546566358791866\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.007142518146429211\n",
      "Training Loss: 0.006819401398533955\n",
      "Training Loss: 0.0067623113945592195\n",
      "Training Loss: 0.007767547586699948\n",
      "Training Loss: 0.008844626387581229\n",
      "Training Loss: 0.00862252374063246\n",
      "Training Loss: 0.008588150786235928\n",
      "Validation Loss: 0.005503139034792614\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.007088666977360845\n",
      "Training Loss: 0.006774856569245457\n",
      "Training Loss: 0.006719258013181389\n",
      "Training Loss: 0.007720836872467771\n",
      "Training Loss: 0.008800262664444745\n",
      "Training Loss: 0.00858076986274682\n",
      "Training Loss: 0.008545471377437934\n",
      "Validation Loss: 0.005461742956721492\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0070371042843908075\n",
      "Training Loss: 0.006732304325560108\n",
      "Training Loss: 0.006679022602038458\n",
      "Training Loss: 0.007676860915962607\n",
      "Training Loss: 0.008757815058343112\n",
      "Training Loss: 0.008540362834464759\n",
      "Training Loss: 0.008502724438440055\n",
      "Validation Loss: 0.005422152679113581\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.006987963871797547\n",
      "Training Loss: 0.006692039306508377\n",
      "Training Loss: 0.006641644311603159\n",
      "Training Loss: 0.0076357090007513765\n",
      "Training Loss: 0.008716748274164274\n",
      "Training Loss: 0.008500837786123157\n",
      "Training Loss: 0.008459782438585535\n",
      "Validation Loss: 0.005384552359671684\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.006941696483991109\n",
      "Training Loss: 0.006654565745266155\n",
      "Training Loss: 0.0066072085220366715\n",
      "Training Loss: 0.0075973561295541005\n",
      "Training Loss: 0.008676477583358065\n",
      "Training Loss: 0.008461651903344318\n",
      "Training Loss: 0.008416626636171714\n",
      "Validation Loss: 0.00534893281422974\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.006898447063867934\n",
      "Training Loss: 0.00662006551050581\n",
      "Training Loss: 0.00657542870962061\n",
      "Training Loss: 0.007561389891779981\n",
      "Training Loss: 0.008636358154471963\n",
      "Training Loss: 0.008422246783738955\n",
      "Training Loss: 0.008373256460763513\n",
      "Validation Loss: 0.005314986247653037\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.006857923182542436\n",
      "Training Loss: 0.00658831145381555\n",
      "Training Loss: 0.00654572946834378\n",
      "Training Loss: 0.00752721233991906\n",
      "Training Loss: 0.00859600793919526\n",
      "Training Loss: 0.008382399565307423\n",
      "Training Loss: 0.008329878057120368\n",
      "Validation Loss: 0.00528248510853656\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.006819765884429217\n",
      "Training Loss: 0.006559008276090026\n",
      "Training Loss: 0.00651766968308948\n",
      "Training Loss: 0.007494459044537507\n",
      "Training Loss: 0.008555639832047745\n",
      "Training Loss: 0.008342487781774252\n",
      "Training Loss: 0.00828711511567235\n",
      "Validation Loss: 0.005251699584197909\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.006783948137308471\n",
      "Training Loss: 0.006532108262181282\n",
      "Training Loss: 0.006491234983550385\n",
      "Training Loss: 0.007463197060860694\n",
      "Training Loss: 0.00851606355747208\n",
      "Training Loss: 0.008303438005968928\n",
      "Training Loss: 0.008245976642938331\n",
      "Validation Loss: 0.005223343019767871\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.006750803093309514\n",
      "Training Loss: 0.00650779961142689\n",
      "Training Loss: 0.006466753209242598\n",
      "Training Loss: 0.007433779600542039\n",
      "Training Loss: 0.008478334016399459\n",
      "Training Loss: 0.008266332115745172\n",
      "Training Loss: 0.008207526779733599\n",
      "Validation Loss: 0.005198122642250655\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006720733405090868\n",
      "Training Loss: 0.006486259349621833\n",
      "Training Loss: 0.00644457928603515\n",
      "Training Loss: 0.007406559308292344\n",
      "Training Loss: 0.008443349982844666\n",
      "Training Loss: 0.008232015626272187\n",
      "Training Loss: 0.008172525037080049\n",
      "Validation Loss: 0.005176352330026555\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006693945399601944\n",
      "Training Loss: 0.006467469137860462\n",
      "Training Loss: 0.006424857638776303\n",
      "Training Loss: 0.007381713769864291\n",
      "Training Loss: 0.008411613404750823\n",
      "Training Loss: 0.008200902253156528\n",
      "Training Loss: 0.008141270200721919\n",
      "Validation Loss: 0.005157891141201896\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006670372987864539\n",
      "Training Loss: 0.006451205427292734\n",
      "Training Loss: 0.0064074885216541585\n",
      "Training Loss: 0.0073592329048551616\n",
      "Training Loss: 0.008383237638045103\n",
      "Training Loss: 0.00817302979528904\n",
      "Training Loss: 0.00811367435264401\n",
      "Validation Loss: 0.005142346559064498\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.00664977818785701\n",
      "Training Loss: 0.006437147418037057\n",
      "Training Loss: 0.006392228028271347\n",
      "Training Loss: 0.007338982952060178\n",
      "Training Loss: 0.008358060106402262\n",
      "Training Loss: 0.008148198010167107\n",
      "Training Loss: 0.008089426216902212\n",
      "Validation Loss: 0.00512924045411528\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006631835310836323\n",
      "Training Loss: 0.0064249545824714\n",
      "Training Loss: 0.006378781734965742\n",
      "Training Loss: 0.007320776164997369\n",
      "Training Loss: 0.008335785183589906\n",
      "Training Loss: 0.008126109256409109\n",
      "Training Loss: 0.008068126286379993\n",
      "Validation Loss: 0.005118111353402448\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.00661619973834604\n",
      "Training Loss: 0.006414312204578891\n",
      "Training Loss: 0.006366863077273592\n",
      "Training Loss: 0.007304403417510912\n",
      "Training Loss: 0.008316068007843569\n",
      "Training Loss: 0.008106434521032498\n",
      "Training Loss: 0.008049363772151992\n",
      "Validation Loss: 0.0051085610249627395\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.006602544384077191\n",
      "Training Loss: 0.006404945828253403\n",
      "Training Loss: 0.0063562171184457835\n",
      "Training Loss: 0.007289660344831646\n",
      "Training Loss: 0.008298565831501037\n",
      "Training Loss: 0.008088861936703324\n",
      "Training Loss: 0.008032752897124738\n",
      "Validation Loss: 0.0051002576729206\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.00659056531265378\n",
      "Training Loss: 0.0063966184272430835\n",
      "Training Loss: 0.006346623839344829\n",
      "Training Loss: 0.0072763450921047475\n",
      "Training Loss: 0.008282956819748506\n",
      "Training Loss: 0.008073100410401822\n",
      "Training Loss: 0.0080179488286376\n",
      "Validation Loss: 0.005092929862962847\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006579989453894086\n",
      "Training Loss: 0.006389128292212263\n",
      "Training Loss: 0.006337896932382137\n",
      "Training Loss: 0.007264272670727223\n",
      "Training Loss: 0.008268955645617098\n",
      "Training Loss: 0.008058892133412883\n",
      "Training Loss: 0.008004652911331504\n",
      "Validation Loss: 0.005086350059380915\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006570575755322352\n",
      "Training Loss: 0.006382301204139367\n",
      "Training Loss: 0.006329873409122229\n",
      "Training Loss: 0.007253269568318501\n",
      "Training Loss: 0.008256309446878732\n",
      "Training Loss: 0.00804600703297183\n",
      "Training Loss: 0.007992601234000175\n",
      "Validation Loss: 0.005080339053213206\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006562114099506289\n",
      "Training Loss: 0.006375995691632852\n",
      "Training Loss: 0.0063224262720905245\n",
      "Training Loss: 0.007243176047923044\n",
      "Training Loss: 0.008244795432547107\n",
      "Training Loss: 0.008034240417182446\n",
      "Training Loss: 0.007981571385171264\n",
      "Validation Loss: 0.005074750060871355\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006554423249326646\n",
      "Training Loss: 0.006370089428965002\n",
      "Training Loss: 0.006315441855695099\n",
      "Training Loss: 0.00723385208286345\n",
      "Training Loss: 0.008234224823536352\n",
      "Training Loss: 0.008023418050725014\n",
      "Training Loss: 0.007971375504275784\n",
      "Validation Loss: 0.005069470525743642\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0065473532903706656\n",
      "Training Loss: 0.006364489072002471\n",
      "Training Loss: 0.006308831311762333\n",
      "Training Loss: 0.007225173854967579\n",
      "Training Loss: 0.008224434645380824\n",
      "Training Loss: 0.008013384519144892\n",
      "Training Loss: 0.007961856704205274\n",
      "Validation Loss: 0.005064400989904107\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006540771559812129\n",
      "Training Loss: 0.00635911296994891\n",
      "Training Loss: 0.006302517844596877\n",
      "Training Loss: 0.007217033586930484\n",
      "Training Loss: 0.008215290168300271\n",
      "Training Loss: 0.008004012809833512\n",
      "Training Loss: 0.007952883505495265\n",
      "Validation Loss: 0.005059473201709238\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0065345745766535405\n",
      "Training Loss: 0.00635390053037554\n",
      "Training Loss: 0.00629644412198104\n",
      "Training Loss: 0.007209337224485353\n",
      "Training Loss: 0.008206676437985151\n",
      "Training Loss: 0.00799518944695592\n",
      "Training Loss: 0.007944346455624328\n",
      "Validation Loss: 0.0050546330249822986\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.0065286769496742635\n",
      "Training Loss: 0.00634880434256047\n",
      "Training Loss: 0.006290558746550232\n",
      "Training Loss: 0.007202006978914142\n",
      "Training Loss: 0.008198497637640684\n",
      "Training Loss: 0.007986821457743645\n",
      "Training Loss: 0.007936158834490925\n",
      "Validation Loss: 0.005049835463239348\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006523006540373899\n",
      "Training Loss: 0.006343786991783418\n",
      "Training Loss: 0.0062848232372198255\n",
      "Training Loss: 0.0071949742618016895\n",
      "Training Loss: 0.008190672976197674\n",
      "Training Loss: 0.007978827484184875\n",
      "Training Loss: 0.00792824586154893\n",
      "Validation Loss: 0.005045046865040165\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.00651750647870358\n",
      "Training Loss: 0.006338821633253246\n",
      "Training Loss: 0.006279205305036157\n",
      "Training Loss: 0.00718818384106271\n",
      "Training Loss: 0.008183138150488958\n",
      "Training Loss: 0.007971142255701124\n",
      "Training Loss: 0.00792055077268742\n",
      "Validation Loss: 0.005040237570202418\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006512127016321756\n",
      "Training Loss: 0.006333885780768469\n",
      "Training Loss: 0.0062736797530669715\n",
      "Training Loss: 0.0071815872262232005\n",
      "Training Loss: 0.008175838576862588\n",
      "Training Loss: 0.007963709923205897\n",
      "Training Loss: 0.007913025544257834\n",
      "Validation Loss: 0.0050353952465502175\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006506836201297119\n",
      "Training Loss: 0.006328965629218146\n",
      "Training Loss: 0.006268225777894258\n",
      "Training Loss: 0.007175146588124335\n",
      "Training Loss: 0.008168730856850744\n",
      "Training Loss: 0.007956484700553119\n",
      "Training Loss: 0.007905633829068393\n",
      "Validation Loss: 0.00503049936879789\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0065015996486181395\n",
      "Training Loss: 0.006324048821697943\n",
      "Training Loss: 0.006262827484169975\n",
      "Training Loss: 0.007168827268760652\n",
      "Training Loss: 0.008161773980828003\n",
      "Training Loss: 0.007949425026308745\n",
      "Training Loss: 0.007898340547690168\n",
      "Validation Loss: 0.005025546517299858\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006496397829032503\n",
      "Training Loss: 0.006319129798794165\n",
      "Training Loss: 0.0062574734329245985\n",
      "Training Loss: 0.007162601888412609\n",
      "Training Loss: 0.008154939871747046\n",
      "Training Loss: 0.007942500478820875\n",
      "Training Loss: 0.007891125885071232\n",
      "Validation Loss: 0.005020524720026508\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006491209726082161\n",
      "Training Loss: 0.006314201522618532\n",
      "Training Loss: 0.00625215133652091\n",
      "Training Loss: 0.007156449807807803\n",
      "Training Loss: 0.008148204304743558\n",
      "Training Loss: 0.00793568481807597\n",
      "Training Loss: 0.007883968723472207\n",
      "Validation Loss: 0.005015425224464094\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006486021265154704\n",
      "Training Loss: 0.006309261115966365\n",
      "Training Loss: 0.0062468537618406116\n",
      "Training Loss: 0.007150349830044434\n",
      "Training Loss: 0.00814154560212046\n",
      "Training Loss: 0.007928954644594342\n",
      "Training Loss: 0.00787685440503992\n",
      "Validation Loss: 0.0050102501404768185\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006480821429286152\n",
      "Training Loss: 0.006304307138780132\n",
      "Training Loss: 0.006241574569139629\n",
      "Training Loss: 0.007144288313575089\n",
      "Training Loss: 0.008134946728823707\n",
      "Training Loss: 0.007922291127033531\n",
      "Training Loss: 0.007869769704993814\n",
      "Validation Loss: 0.005004994254227909\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006475601757410914\n",
      "Training Loss: 0.00629933656193316\n",
      "Training Loss: 0.006236306899227202\n",
      "Training Loss: 0.007138251955620944\n",
      "Training Loss: 0.008128394822124392\n",
      "Training Loss: 0.007915681730955838\n",
      "Training Loss: 0.007862707023741677\n",
      "Validation Loss: 0.0049996546562063445\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.00647035448171664\n",
      "Training Loss: 0.006294350174139254\n",
      "Training Loss: 0.006231048631016165\n",
      "Training Loss: 0.007132230004062876\n",
      "Training Loss: 0.008121877416269853\n",
      "Training Loss: 0.00790911024203524\n",
      "Training Loss: 0.007855658432235941\n",
      "Validation Loss: 0.004994233015467751\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.00646507760277018\n",
      "Training Loss: 0.00628934919659514\n",
      "Training Loss: 0.006225797890219837\n",
      "Training Loss: 0.007126215206808411\n",
      "Training Loss: 0.008115387123543769\n",
      "Training Loss: 0.007902569338912145\n",
      "Training Loss: 0.00784861926571466\n",
      "Validation Loss: 0.004988730716787642\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006459766958141699\n",
      "Training Loss: 0.006284331534407102\n",
      "Training Loss: 0.006220550504513085\n",
      "Training Loss: 0.0071202003385405985\n",
      "Training Loss: 0.008108914480544627\n",
      "Training Loss: 0.007896049701375887\n",
      "Training Loss: 0.007841585736023261\n",
      "Validation Loss: 0.004983145364713663\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00645442049659323\n",
      "Training Loss: 0.0062793009262532\n",
      "Training Loss: 0.006215307459933683\n",
      "Training Loss: 0.007114180480130017\n",
      "Training Loss: 0.008102452593157068\n",
      "Training Loss: 0.0078895422979258\n",
      "Training Loss: 0.007834553541615606\n",
      "Validation Loss: 0.0049774850411724286\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006449039530707523\n",
      "Training Loss: 0.006274256666074507\n",
      "Training Loss: 0.006210066978819668\n",
      "Training Loss: 0.007108150521526113\n",
      "Training Loss: 0.00809599989792332\n",
      "Training Loss: 0.007883044673362746\n",
      "Training Loss: 0.007827524025924504\n",
      "Validation Loss: 0.004971742026127652\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006443620501086116\n",
      "Training Loss: 0.006269199192174711\n",
      "Training Loss: 0.006204827696783468\n",
      "Training Loss: 0.007102108369581401\n",
      "Training Loss: 0.008089549688156695\n",
      "Training Loss: 0.00787655150401406\n",
      "Training Loss: 0.007820493094623088\n",
      "Validation Loss: 0.004965924151234803\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006438166158040985\n",
      "Training Loss: 0.006264128423645161\n",
      "Training Loss: 0.006199590653413907\n",
      "Training Loss: 0.007096050989348441\n",
      "Training Loss: 0.008083098462084309\n",
      "Training Loss: 0.00787005693768151\n",
      "Training Loss: 0.007813462364720181\n",
      "Validation Loss: 0.00496003346095622\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006432678165729158\n",
      "Training Loss: 0.00625904755492229\n",
      "Training Loss: 0.006194356471532956\n",
      "Training Loss: 0.00708997608628124\n",
      "Training Loss: 0.008076643701642752\n",
      "Training Loss: 0.007863559135003015\n",
      "Training Loss: 0.00780642983620055\n",
      "Validation Loss: 0.004954069583373291\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006427155208075419\n",
      "Training Loss: 0.006253953811828979\n",
      "Training Loss: 0.006189122513169423\n",
      "Training Loss: 0.007083882974693551\n",
      "Training Loss: 0.008070184108801186\n",
      "Training Loss: 0.007857056332286448\n",
      "Training Loss: 0.007799395051551983\n",
      "Validation Loss: 0.00494803431616531\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006421599888708443\n",
      "Training Loss: 0.006248847887036391\n",
      "Training Loss: 0.0061838908982463185\n",
      "Training Loss: 0.00707776837691199\n",
      "Training Loss: 0.008063716102624312\n",
      "Training Loss: 0.007850545330438764\n",
      "Training Loss: 0.007792360532330349\n",
      "Validation Loss: 0.004941926318594021\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006416010748362169\n",
      "Training Loss: 0.006243730110581964\n",
      "Training Loss: 0.006178661270532757\n",
      "Training Loss: 0.00707163208629936\n",
      "Training Loss: 0.008057236783206462\n",
      "Training Loss: 0.007844023916404694\n",
      "Training Loss: 0.007785324649885297\n",
      "Validation Loss: 0.004935752301326908\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.006410390983801335\n",
      "Training Loss: 0.006238599861972034\n",
      "Training Loss: 0.006173433276126161\n",
      "Training Loss: 0.0070654735114658255\n",
      "Training Loss: 0.008050743838539346\n",
      "Training Loss: 0.007837490719975904\n",
      "Training Loss: 0.007778284361120313\n",
      "Validation Loss: 0.004929512721741021\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.006404740436701104\n",
      "Training Loss: 0.006233455467154272\n",
      "Training Loss: 0.006168206064030528\n",
      "Training Loss: 0.007059290348552167\n",
      "Training Loss: 0.008044234899571165\n",
      "Training Loss: 0.007830943464068697\n",
      "Training Loss: 0.007771240881411358\n",
      "Validation Loss: 0.004923209540148297\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006399061377160251\n",
      "Training Loss: 0.006228298795758746\n",
      "Training Loss: 0.0061629815096966925\n",
      "Training Loss: 0.007053083921200596\n",
      "Training Loss: 0.008037709473865107\n",
      "Training Loss: 0.007824382161488756\n",
      "Training Loss: 0.007764193620532751\n",
      "Validation Loss: 0.004916846590842145\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0063933553820243105\n",
      "Training Loss: 0.006223128864075988\n",
      "Training Loss: 0.006157759147463367\n",
      "Training Loss: 0.007046852912171744\n",
      "Training Loss: 0.008031162738334388\n",
      "Training Loss: 0.007817805974045768\n",
      "Training Loss: 0.007757143066264689\n",
      "Validation Loss: 0.004910421597401608\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.00638761815673206\n",
      "Training Loss: 0.006217945059761405\n",
      "Training Loss: 0.006152537311427295\n",
      "Training Loss: 0.007040596958249807\n",
      "Training Loss: 0.008024596001487225\n",
      "Training Loss: 0.007811216168338433\n",
      "Training Loss: 0.00775008850148879\n",
      "Validation Loss: 0.004903941143178538\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006381853518541902\n",
      "Training Loss: 0.0062127473001601175\n",
      "Training Loss: 0.006147314028348774\n",
      "Training Loss: 0.0070343198318732906\n",
      "Training Loss: 0.008018009875668213\n",
      "Training Loss: 0.007804615882923827\n",
      "Training Loss: 0.007743029627017677\n",
      "Validation Loss: 0.0048974074775611194\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006376060881884769\n",
      "Training Loss: 0.006207540309405886\n",
      "Training Loss: 0.0061420912819448855\n",
      "Training Loss: 0.007028020338038914\n",
      "Training Loss: 0.008011402919655665\n",
      "Training Loss: 0.007798006092198193\n",
      "Training Loss: 0.007735968943452462\n",
      "Validation Loss: 0.004890825753344383\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.00637024179333821\n",
      "Training Loss: 0.006202325368067249\n",
      "Training Loss: 0.006136867642635479\n",
      "Training Loss: 0.00702170591976028\n",
      "Training Loss: 0.008004780923947692\n",
      "Training Loss: 0.007791391588980332\n",
      "Training Loss: 0.0077289047534577545\n",
      "Validation Loss: 0.004884212378984534\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006364401149330661\n",
      "Training Loss: 0.006197109289932996\n",
      "Training Loss: 0.006131648144219071\n",
      "Training Loss: 0.007015378022915683\n",
      "Training Loss: 0.00799814790370874\n",
      "Training Loss: 0.007784777283668518\n",
      "Training Loss: 0.00772184414905496\n",
      "Validation Loss: 0.004877566618928581\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006358539809007198\n",
      "Training Loss: 0.006191896771197207\n",
      "Training Loss: 0.006126434002071619\n",
      "Training Loss: 0.00700904110330157\n",
      "Training Loss: 0.007991510431747884\n",
      "Training Loss: 0.007778167283395305\n",
      "Training Loss: 0.007714788414305076\n",
      "Validation Loss: 0.004870904561599598\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006352663577417843\n",
      "Training Loss: 0.0061866920220199975\n",
      "Training Loss: 0.006121229456039145\n",
      "Training Loss: 0.007002701446181163\n",
      "Training Loss: 0.007984874637331814\n",
      "Training Loss: 0.007771565577713773\n",
      "Training Loss: 0.0077077406959142536\n",
      "Validation Loss: 0.0048642326298968565\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.006346779062878341\n",
      "Training Loss: 0.006181499959202483\n",
      "Training Loss: 0.006116039205808193\n",
      "Training Loss: 0.006996361459605396\n",
      "Training Loss: 0.007978247785940766\n",
      "Training Loss: 0.0077649730746634305\n",
      "Training Loss: 0.007700705910101533\n",
      "Validation Loss: 0.004857558354191231\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.0063408926443662495\n",
      "Training Loss: 0.006176319405203685\n",
      "Training Loss: 0.006110867437673733\n",
      "Training Loss: 0.00699002351379022\n",
      "Training Loss: 0.007971632262924687\n",
      "Training Loss: 0.007758388981455937\n",
      "Training Loss: 0.007693684193072841\n",
      "Validation Loss: 0.004850887782411294\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006335009512258694\n",
      "Training Loss: 0.006171149500878528\n",
      "Training Loss: 0.006105715440353379\n",
      "Training Loss: 0.0069836867752019315\n",
      "Training Loss: 0.007965031939093023\n",
      "Training Loss: 0.007751812351634726\n",
      "Training Loss: 0.00768667705822736\n",
      "Validation Loss: 0.004844222190329449\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.00632913509674836\n",
      "Training Loss: 0.006165986384730786\n",
      "Training Loss: 0.0061005829344503585\n",
      "Training Loss: 0.006977351604728028\n",
      "Training Loss: 0.007958447011187673\n",
      "Training Loss: 0.007745239350479096\n",
      "Training Loss: 0.00767968617961742\n",
      "Validation Loss: 0.004837552735177375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006323266885592602\n",
      "Training Loss: 0.006160822417587042\n",
      "Training Loss: 0.006095467160921544\n",
      "Training Loss: 0.0069710151216713714\n",
      "Training Loss: 0.007951877107843756\n",
      "Training Loss: 0.0077386675076559185\n",
      "Training Loss: 0.007672707204474136\n",
      "Validation Loss: 0.004830889830240205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006317411746713333\n",
      "Training Loss: 0.006155654235626571\n",
      "Training Loss: 0.006090365041745827\n",
      "Training Loss: 0.00696467560948804\n",
      "Training Loss: 0.007945319580612705\n",
      "Training Loss: 0.007732090351637453\n",
      "Training Loss: 0.007665737735806033\n",
      "Validation Loss: 0.0048242262686682994\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006311569375684485\n",
      "Training Loss: 0.006150478441850282\n",
      "Training Loss: 0.006085275821387768\n",
      "Training Loss: 0.006958330395282247\n",
      "Training Loss: 0.007938773956848309\n",
      "Training Loss: 0.007725505476118996\n",
      "Training Loss: 0.007658777390606702\n",
      "Validation Loss: 0.004817564456814321\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006305739275994711\n",
      "Training Loss: 0.006145288376719691\n",
      "Training Loss: 0.006080193768721074\n",
      "Training Loss: 0.00695197643712163\n",
      "Training Loss: 0.007932236556662246\n",
      "Training Loss: 0.007718908457318321\n",
      "Training Loss: 0.007651820078026503\n",
      "Validation Loss: 0.004810900924455249\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.00629992073518224\n",
      "Training Loss: 0.00614008168631699\n",
      "Training Loss: 0.006075115551939234\n",
      "Training Loss: 0.006945609601680189\n",
      "Training Loss: 0.007925703328801319\n",
      "Training Loss: 0.007712293866788968\n",
      "Training Loss: 0.00764486238360405\n",
      "Validation Loss: 0.0048042387559430935\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006294114171760156\n",
      "Training Loss: 0.00613485632580705\n",
      "Training Loss: 0.006070037876488641\n",
      "Training Loss: 0.006939229277195409\n",
      "Training Loss: 0.007919174992712214\n",
      "Training Loss: 0.007705660467036068\n",
      "Training Loss: 0.007637902235146612\n",
      "Validation Loss: 0.004797576686405762\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.00628831714973785\n",
      "Training Loss: 0.0061296085163485255\n",
      "Training Loss: 0.006064956306945532\n",
      "Training Loss: 0.006932831677258946\n",
      "Training Loss: 0.007912646981421858\n",
      "Training Loss: 0.007699004728347063\n",
      "Training Loss: 0.007630938087822869\n",
      "Validation Loss: 0.004790909842532198\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006282525819260627\n",
      "Training Loss: 0.006124333624611609\n",
      "Training Loss: 0.006059868298470974\n",
      "Training Loss: 0.006926412978209555\n",
      "Training Loss: 0.007906117653474211\n",
      "Training Loss: 0.007692324249073863\n",
      "Training Loss: 0.007623962273355573\n",
      "Validation Loss: 0.004784241987545783\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006276739570312202\n",
      "Training Loss: 0.0061190317099681125\n",
      "Training Loss: 0.006054770140908658\n",
      "Training Loss: 0.0069199706258950755\n",
      "Training Loss: 0.00789958275272511\n",
      "Training Loss: 0.007685614109504968\n",
      "Training Loss: 0.007616974437842146\n",
      "Validation Loss: 0.004777572961865963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00627095865434967\n",
      "Training Loss: 0.006113701851572842\n",
      "Training Loss: 0.006049660455901175\n",
      "Training Loss: 0.006913502605748363\n",
      "Training Loss: 0.007893040530616418\n",
      "Training Loss: 0.007678872995311395\n",
      "Training Loss: 0.007609970371704549\n",
      "Validation Loss: 0.004770901734323314\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006265176346059889\n",
      "Training Loss: 0.006108339607599191\n",
      "Training Loss: 0.0060445345565676685\n",
      "Training Loss: 0.006907005243701861\n",
      "Training Loss: 0.007886489654192702\n",
      "Training Loss: 0.0076721003325656055\n",
      "Training Loss: 0.00760294686537236\n",
      "Validation Loss: 0.004764225862812153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006259391675703227\n",
      "Training Loss: 0.006102943794103339\n",
      "Training Loss: 0.006039390274090692\n",
      "Training Loss: 0.0069004759867675606\n",
      "Training Loss: 0.00787992688594386\n",
      "Training Loss: 0.007665292957099155\n",
      "Training Loss: 0.007595902143511921\n",
      "Validation Loss: 0.0047575448925715035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006253602541983127\n",
      "Training Loss: 0.006097513572312891\n",
      "Training Loss: 0.006034225127659738\n",
      "Training Loss: 0.006893912145751528\n",
      "Training Loss: 0.007873350093141197\n",
      "Training Loss: 0.007658448317088186\n",
      "Training Loss: 0.0075888323469553145\n",
      "Validation Loss: 0.0047508607420561075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006247805290622637\n",
      "Training Loss: 0.0060920459328917785\n",
      "Training Loss: 0.006029037018306553\n",
      "Training Loss: 0.006887311962200329\n",
      "Training Loss: 0.007866758212912828\n",
      "Training Loss: 0.0076515667396597565\n",
      "Training Loss: 0.007581735837738961\n",
      "Validation Loss: 0.004744168425463698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006241996621247381\n",
      "Training Loss: 0.006086539843236097\n",
      "Training Loss: 0.006023824252188206\n",
      "Training Loss: 0.006880672192783095\n",
      "Training Loss: 0.007860147242899984\n",
      "Training Loss: 0.007644645302789286\n",
      "Training Loss: 0.007574609492439777\n",
      "Validation Loss: 0.004737467124398923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006236173589713872\n",
      "Training Loss: 0.006080994040821679\n",
      "Training Loss: 0.0060185839072801174\n",
      "Training Loss: 0.006873989910818636\n",
      "Training Loss: 0.007853516977047548\n",
      "Training Loss: 0.007637682990171015\n",
      "Training Loss: 0.007567449937341735\n",
      "Validation Loss: 0.004730760901189699\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006230335837462917\n",
      "Training Loss: 0.006075405910960399\n",
      "Training Loss: 0.006013314790325239\n",
      "Training Loss: 0.00686726376414299\n",
      "Training Loss: 0.007846864258171991\n",
      "Training Loss: 0.0076306783955078575\n",
      "Training Loss: 0.007560256599681452\n",
      "Validation Loss: 0.004724045932442926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0062244794936850666\n",
      "Training Loss: 0.006069774283096194\n",
      "Training Loss: 0.006008014105027541\n",
      "Training Loss: 0.006860491440165788\n",
      "Training Loss: 0.007840189090929926\n",
      "Training Loss: 0.007623631027527154\n",
      "Training Loss: 0.007553025024244562\n",
      "Validation Loss: 0.0047173205953203005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.0062186009692959485\n",
      "Training Loss: 0.0060640962177421895\n",
      "Training Loss: 0.00600267844623886\n",
      "Training Loss: 0.0068536697921808805\n",
      "Training Loss: 0.007833490550983696\n",
      "Training Loss: 0.007616539649898186\n",
      "Training Loss: 0.007545755780301988\n",
      "Validation Loss: 0.0047105828100424746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006212697108276188\n",
      "Training Loss: 0.00605837041279301\n",
      "Training Loss: 0.005997306845383719\n",
      "Training Loss: 0.0068467980239074674\n",
      "Training Loss: 0.007826766029465944\n",
      "Training Loss: 0.007609402689849958\n",
      "Training Loss: 0.007538445359095931\n",
      "Validation Loss: 0.004703833158250047\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.0062067675043363125\n",
      "Training Loss: 0.006052596062072552\n",
      "Training Loss: 0.005991899479413405\n",
      "Training Loss: 0.006839871800038964\n",
      "Training Loss: 0.007820013093296439\n",
      "Training Loss: 0.00760221851291135\n",
      "Training Loss: 0.007531092209974304\n",
      "Validation Loss: 0.004697068431327461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006200806668493897\n",
      "Training Loss: 0.006046769132954068\n",
      "Training Loss: 0.0059864508244208995\n",
      "Training Loss: 0.006832891377853229\n",
      "Training Loss: 0.00781323304749094\n",
      "Training Loss: 0.007594987920019776\n",
      "Training Loss: 0.007523694739211351\n",
      "Validation Loss: 0.0046902893809601665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0061948143818881364\n",
      "Training Loss: 0.006040889522992075\n",
      "Training Loss: 0.0059809621260501445\n",
      "Training Loss: 0.006825853053014725\n",
      "Training Loss: 0.007806423342553898\n",
      "Training Loss: 0.007587708096252754\n",
      "Training Loss: 0.007516249649925157\n",
      "Validation Loss: 0.004683492637644016\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006188786063576117\n",
      "Training Loss: 0.006034953735070303\n",
      "Training Loss: 0.005975426844088361\n",
      "Training Loss: 0.006818755365675316\n",
      "Training Loss: 0.007799584849271924\n",
      "Training Loss: 0.007580380477011203\n",
      "Training Loss: 0.007508755872258917\n",
      "Validation Loss: 0.004676677838616659\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0061827205365989355\n",
      "Training Loss: 0.0060289604880381376\n",
      "Training Loss: 0.0059698454581666735\n",
      "Training Loss: 0.006811596219195053\n",
      "Training Loss: 0.007792715226532892\n",
      "Training Loss: 0.007573002970311791\n",
      "Training Loss: 0.007501212409697473\n",
      "Validation Loss: 0.004669844178127104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006176614465657621\n",
      "Training Loss: 0.006022908234153874\n",
      "Training Loss: 0.005964218553854153\n",
      "Training Loss: 0.006804375047795475\n",
      "Training Loss: 0.0077858131949324165\n",
      "Training Loss: 0.007565574690233916\n",
      "Training Loss: 0.0074936149932909755\n",
      "Validation Loss: 0.004662992013705553\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0061704679753165695\n",
      "Training Loss: 0.006016794880270027\n",
      "Training Loss: 0.005958540582796558\n",
      "Training Loss: 0.006797087640734389\n",
      "Training Loss: 0.0077788802096620206\n",
      "Training Loss: 0.007558096600696444\n",
      "Training Loss: 0.007485966564854607\n",
      "Validation Loss: 0.004656116060009191\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.0061642761901021005\n",
      "Training Loss: 0.006010619119042531\n",
      "Training Loss: 0.005952811994357035\n",
      "Training Loss: 0.006789734461344779\n",
      "Training Loss: 0.007771913598990068\n",
      "Training Loss: 0.007550566233694553\n",
      "Training Loss: 0.007478260339703411\n",
      "Validation Loss: 0.004649220928741612\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006158038813155144\n",
      "Training Loss: 0.00600437906512525\n",
      "Training Loss: 0.005947030545212328\n",
      "Training Loss: 0.006782313335570507\n",
      "Training Loss: 0.007764915643492714\n",
      "Training Loss: 0.007542985612526536\n",
      "Training Loss: 0.007470497745089233\n",
      "Validation Loss: 0.004642297649818859\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006151751687284559\n",
      "Training Loss: 0.005998071305220946\n",
      "Training Loss: 0.005941191585734486\n",
      "Training Loss: 0.006774822334409692\n",
      "Training Loss: 0.007757883089361713\n",
      "Training Loss: 0.0075353527150582526\n",
      "Training Loss: 0.0074626769998576495\n",
      "Validation Loss: 0.004635349556556048\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006145412608748302\n",
      "Training Loss: 0.005991694503463805\n",
      "Training Loss: 0.005935295356903225\n",
      "Training Loss: 0.006767260123160668\n",
      "Training Loss: 0.007750816725892946\n",
      "Training Loss: 0.007527667827671394\n",
      "Training Loss: 0.007454796287929639\n",
      "Validation Loss: 0.004628374489383556\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006139021300477907\n",
      "Training Loss: 0.0059852488187607375\n",
      "Training Loss: 0.005929341114824638\n",
      "Training Loss: 0.006759625914273783\n",
      "Training Loss: 0.007743717461125925\n",
      "Training Loss: 0.007519932015566155\n",
      "Training Loss: 0.007446854653535411\n",
      "Validation Loss: 0.00462137105480562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006132575540104881\n",
      "Training Loss: 0.005978729520575143\n",
      "Training Loss: 0.005923323871102184\n",
      "Training Loss: 0.006751918239169754\n",
      "Training Loss: 0.007736584860831499\n",
      "Training Loss: 0.007512143542990088\n",
      "Training Loss: 0.007438850750913844\n",
      "Validation Loss: 0.004614337393229179\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006126073049381375\n",
      "Training Loss: 0.0059721371921477836\n",
      "Training Loss: 0.005917243208969012\n",
      "Training Loss: 0.006744136289344169\n",
      "Training Loss: 0.007729418035596609\n",
      "Training Loss: 0.007504305823240429\n",
      "Training Loss: 0.0074307842727284875\n",
      "Validation Loss: 0.004607272958330643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006119512263685465\n",
      "Training Loss: 0.0059654696809593585\n",
      "Training Loss: 0.005911098786164075\n",
      "Training Loss: 0.006736279248725623\n",
      "Training Loss: 0.007722218254348263\n",
      "Training Loss: 0.00749641663627699\n",
      "Training Loss: 0.007422653114190325\n",
      "Validation Loss: 0.004600179300570254\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006112892708042637\n",
      "Training Loss: 0.005958727074903436\n",
      "Training Loss: 0.005904888806398958\n",
      "Training Loss: 0.006728345416486263\n",
      "Training Loss: 0.007714983624173328\n",
      "Training Loss: 0.007488475855207071\n",
      "Training Loss: 0.007414454228710383\n",
      "Validation Loss: 0.004593052610071299\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006106212104205042\n",
      "Training Loss: 0.005951903460081667\n",
      "Training Loss: 0.005898608792340383\n",
      "Training Loss: 0.006720335192512721\n",
      "Training Loss: 0.007707717200974002\n",
      "Training Loss: 0.007480486937565729\n",
      "Training Loss: 0.00740619138116017\n",
      "Validation Loss: 0.004585891109647069\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.0060994681459851565\n",
      "Training Loss: 0.0059450010920409115\n",
      "Training Loss: 0.005892258824314922\n",
      "Training Loss: 0.006712248595431447\n",
      "Training Loss: 0.007700417881133035\n",
      "Training Loss: 0.007472448509652167\n",
      "Training Loss: 0.007397862060461193\n",
      "Validation Loss: 0.004578696828063452\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006092663531890139\n",
      "Training Loss: 0.005938019407331012\n",
      "Training Loss: 0.005885838073445484\n",
      "Training Loss: 0.006704084987286478\n",
      "Training Loss: 0.007693086877698079\n",
      "Training Loss: 0.007464362285099923\n",
      "Training Loss: 0.007389465688029304\n",
      "Validation Loss: 0.004571467339347225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006085793380625546\n",
      "Training Loss: 0.005930954435025342\n",
      "Training Loss: 0.00587934422888793\n",
      "Training Loss: 0.006695843185298145\n",
      "Training Loss: 0.007685723400209099\n",
      "Training Loss: 0.007456229149829597\n",
      "Training Loss: 0.007381000306922942\n",
      "Validation Loss: 0.004564204225351524\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006078860472189263\n",
      "Training Loss: 0.005923807310173288\n",
      "Training Loss: 0.005872774735325948\n",
      "Training Loss: 0.006687525422312319\n",
      "Training Loss: 0.007678330043563619\n",
      "Training Loss: 0.007448050997918471\n",
      "Training Loss: 0.0073724695621058345\n",
      "Validation Loss: 0.004556903250215485\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006071861913660541\n",
      "Training Loss: 0.005916575850569643\n",
      "Training Loss: 0.0058661311015021055\n",
      "Training Loss: 0.006679130055126734\n",
      "Training Loss: 0.007670906769344583\n",
      "Training Loss: 0.007439828334609047\n",
      "Training Loss: 0.007363870254484936\n",
      "Validation Loss: 0.0045495688159207116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.0060647988878190515\n",
      "Training Loss: 0.005909259585314431\n",
      "Training Loss: 0.005859408751130104\n",
      "Training Loss: 0.006670658240909688\n",
      "Training Loss: 0.007663454164285213\n",
      "Training Loss: 0.007431563065620139\n",
      "Training Loss: 0.007355203948682174\n",
      "Validation Loss: 0.004542197828522922\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006057671047747135\n",
      "Training Loss: 0.0059018583630677315\n",
      "Training Loss: 0.005852608825080097\n",
      "Training Loss: 0.006662110918550752\n",
      "Training Loss: 0.00765597345191054\n",
      "Training Loss: 0.007423256691545248\n",
      "Training Loss: 0.007346471617929637\n",
      "Validation Loss: 0.004534793441700104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006050479855621234\n",
      "Training Loss: 0.005894371250760742\n",
      "Training Loss: 0.005845730374567211\n",
      "Training Loss: 0.006653490069438704\n",
      "Training Loss: 0.007648468168918043\n",
      "Training Loss: 0.007414913009852171\n",
      "Training Loss: 0.007337673763977363\n",
      "Validation Loss: 0.00452735261426548\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006043224339373409\n",
      "Training Loss: 0.005886798012652434\n",
      "Training Loss: 0.005838771879207343\n",
      "Training Loss: 0.006644795247120782\n",
      "Training Loss: 0.007640935676172376\n",
      "Training Loss: 0.007406531299930066\n",
      "Training Loss: 0.007328808919992298\n",
      "Validation Loss: 0.004519880393935081\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006035906298784539\n",
      "Training Loss: 0.00587913878262043\n",
      "Training Loss: 0.005831733143422752\n",
      "Training Loss: 0.006636027575586923\n",
      "Training Loss: 0.007633379967883229\n",
      "Training Loss: 0.007398114822572097\n",
      "Training Loss: 0.007319882127922029\n",
      "Validation Loss: 0.004512373651011607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006028526056325063\n",
      "Training Loss: 0.005871392916887998\n",
      "Training Loss: 0.005824613164877519\n",
      "Training Loss: 0.006627189384307713\n",
      "Training Loss: 0.007625801587710157\n",
      "Training Loss: 0.007389665112132206\n",
      "Training Loss: 0.007310891780070961\n",
      "Validation Loss: 0.004504834839809086\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006021084036910906\n",
      "Training Loss: 0.005863560487632639\n",
      "Training Loss: 0.005817412104224786\n",
      "Training Loss: 0.00661828180484008\n",
      "Training Loss: 0.007618203028105199\n",
      "Training Loss: 0.007381184953264892\n",
      "Training Loss: 0.007301840753061697\n",
      "Validation Loss: 0.004497263301076709\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006013582720188424\n",
      "Training Loss: 0.005855641915113665\n",
      "Training Loss: 0.005810128543525934\n",
      "Training Loss: 0.0066093065607128665\n",
      "Training Loss: 0.007610584486974404\n",
      "Training Loss: 0.0073726766847539696\n",
      "Training Loss: 0.007292729543987662\n",
      "Validation Loss: 0.004489666249512343\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006006024251691997\n",
      "Training Loss: 0.005847638738923706\n",
      "Training Loss: 0.005802765509579331\n",
      "Training Loss: 0.006600266777095385\n",
      "Training Loss: 0.007602948674466461\n",
      "Training Loss: 0.007364143264712766\n",
      "Training Loss: 0.00728356070118025\n",
      "Validation Loss: 0.004482039576570751\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.005998409013263881\n",
      "Training Loss: 0.005839549684314989\n",
      "Training Loss: 0.005795320029137656\n",
      "Training Loss: 0.006591163359116763\n",
      "Training Loss: 0.0075952971470542256\n",
      "Training Loss: 0.007355586192570627\n",
      "Training Loss: 0.00727433710009791\n",
      "Validation Loss: 0.004474387654584696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.005990738715045154\n",
      "Training Loss: 0.0058313756569987165\n",
      "Training Loss: 0.005787794151110574\n",
      "Training Loss: 0.006581999933114275\n",
      "Training Loss: 0.007587633037474006\n",
      "Training Loss: 0.007347009769873693\n",
      "Training Loss: 0.007265060723293573\n",
      "Validation Loss: 0.004466711022901569\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005983014694647864\n",
      "Training Loss: 0.005823117013205774\n",
      "Training Loss: 0.005780188321368769\n",
      "Training Loss: 0.006572777816909365\n",
      "Training Loss: 0.007579956988338381\n",
      "Training Loss: 0.007338415529811754\n",
      "Training Loss: 0.0072557330247946085\n",
      "Validation Loss: 0.00445901261632111\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.005975239872932434\n",
      "Training Loss: 0.005814776698243805\n",
      "Training Loss: 0.005772504230262712\n",
      "Training Loss: 0.006563499810290523\n",
      "Training Loss: 0.007572270394302904\n",
      "Training Loss: 0.007329804273322225\n",
      "Training Loss: 0.0072463549871463325\n",
      "Validation Loss: 0.00445130050935772\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.005967417240608484\n",
      "Training Loss: 0.005806355239474215\n",
      "Training Loss: 0.005764743239851668\n",
      "Training Loss: 0.006554169331793674\n",
      "Training Loss: 0.007564576773438603\n",
      "Training Loss: 0.0073211817338597026\n",
      "Training Loss: 0.007236931077204645\n",
      "Validation Loss: 0.004443568832411916\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005959546632366255\n",
      "Training Loss: 0.005797850875533186\n",
      "Training Loss: 0.005756905839079991\n",
      "Training Loss: 0.0065447902784217145\n",
      "Training Loss: 0.0075568775634747\n",
      "Training Loss: 0.00731254989397712\n",
      "Training Loss: 0.0072274639771785585\n",
      "Validation Loss: 0.004435824685074277\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.005951629891642369\n",
      "Training Loss: 0.005789266882347874\n",
      "Training Loss: 0.00574899256345816\n",
      "Training Loss: 0.006535364296287299\n",
      "Training Loss: 0.007549175060121343\n",
      "Training Loss: 0.007303910636110232\n",
      "Training Loss: 0.007217955207452178\n",
      "Validation Loss: 0.004428070613657463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005943670389242471\n",
      "Training Loss: 0.005780605369363911\n",
      "Training Loss: 0.0057410083350259815\n",
      "Training Loss: 0.006525896819657646\n",
      "Training Loss: 0.0075414723111316565\n",
      "Training Loss: 0.0072952657472342254\n",
      "Training Loss: 0.007208405963610858\n",
      "Validation Loss: 0.004420310134601811\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.00593566756520886\n",
      "Training Loss: 0.0057718644256237895\n",
      "Training Loss: 0.005732952242251486\n",
      "Training Loss: 0.006516389531898312\n",
      "Training Loss: 0.007533769892761484\n",
      "Training Loss: 0.007286617959616706\n",
      "Training Loss: 0.007198818090837449\n",
      "Validation Loss: 0.004412548106829967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.005927623411989771\n",
      "Training Loss: 0.0057630463369423525\n",
      "Training Loss: 0.0057248286076355724\n",
      "Training Loss: 0.006506848029675894\n",
      "Training Loss: 0.007526071880711242\n",
      "Training Loss: 0.007277971028815955\n",
      "Training Loss: 0.007189193043159321\n",
      "Validation Loss: 0.004404784157115283\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.0059195380320306865\n",
      "Training Loss: 0.005754151182482019\n",
      "Training Loss: 0.005716638419544324\n",
      "Training Loss: 0.006497276646550745\n",
      "Training Loss: 0.007518380786059424\n",
      "Training Loss: 0.00726932582561858\n",
      "Training Loss: 0.007179531936999411\n",
      "Validation Loss: 0.00439702625707322\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005911411591805518\n",
      "Training Loss: 0.005745178767829202\n",
      "Training Loss: 0.005708383732708171\n",
      "Training Loss: 0.006487678221310489\n",
      "Training Loss: 0.007510697948746383\n",
      "Training Loss: 0.007260682978667319\n",
      "Training Loss: 0.007169832113431767\n",
      "Validation Loss: 0.0043892807022874215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005903243503416889\n",
      "Training Loss: 0.005736129654105753\n",
      "Training Loss: 0.005700069058220834\n",
      "Training Loss: 0.006478059189976193\n",
      "Training Loss: 0.007503030488733202\n",
      "Training Loss: 0.007252046106150374\n",
      "Training Loss: 0.0071600940870121125\n",
      "Validation Loss: 0.004381546341685041\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005895030670217238\n",
      "Training Loss: 0.005727001007762737\n",
      "Training Loss: 0.005691695384448394\n",
      "Training Loss: 0.006468423994374461\n",
      "Training Loss: 0.007495379546890035\n",
      "Training Loss: 0.007243413331452757\n",
      "Training Loss: 0.007150311397854239\n",
      "Validation Loss: 0.004373831273325151\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005886769199278206\n",
      "Training Loss: 0.005717789383488707\n",
      "Training Loss: 0.005683264480903744\n",
      "Training Loss: 0.006458775661303662\n",
      "Training Loss: 0.00748775017564185\n",
      "Training Loss: 0.007234783488092944\n",
      "Training Loss: 0.007140476825879887\n",
      "Validation Loss: 0.004366142686616894\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005878452979959548\n",
      "Training Loss: 0.005708488856907934\n",
      "Training Loss: 0.005674777431413531\n",
      "Training Loss: 0.0064491191657725724\n",
      "Training Loss: 0.007480146512389183\n",
      "Training Loss: 0.007226153751835227\n",
      "Training Loss: 0.007130579688819126\n",
      "Validation Loss: 0.004358484980053781\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.00587007129448466\n",
      "Training Loss: 0.005699090226553381\n",
      "Training Loss: 0.005666235928656533\n",
      "Training Loss: 0.006439458311069757\n",
      "Training Loss: 0.007472573551349342\n",
      "Training Loss: 0.007217518516117707\n",
      "Training Loss: 0.0071206036710646\n",
      "Validation Loss: 0.004350866450080543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.0058616085833637045\n",
      "Training Loss: 0.0056895805202657355\n",
      "Training Loss: 0.005657640296267346\n",
      "Training Loss: 0.006429794849827886\n",
      "Training Loss: 0.007465039180824533\n",
      "Training Loss: 0.007208866752916947\n",
      "Training Loss: 0.00711052238359116\n",
      "Validation Loss: 0.004343295229637249\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005853043164825067\n",
      "Training Loss: 0.005679938758257777\n",
      "Training Loss: 0.005648989980109036\n",
      "Training Loss: 0.006420126479933969\n",
      "Training Loss: 0.007457547725643963\n",
      "Training Loss: 0.007200180542422458\n",
      "Training Loss: 0.007100303453626111\n",
      "Validation Loss: 0.0043357878370552366\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005844344208016991\n",
      "Training Loss: 0.005670136026456021\n",
      "Training Loss: 0.005640281863743439\n",
      "Training Loss: 0.006410448825336062\n",
      "Training Loss: 0.00745010843151249\n",
      "Training Loss: 0.007191433521220461\n",
      "Training Loss: 0.007089899857528508\n",
      "Validation Loss: 0.004328356121844622\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005835468112491071\n",
      "Training Loss: 0.005660132837947458\n",
      "Training Loss: 0.0056315155734773725\n",
      "Training Loss: 0.006400748257292434\n",
      "Training Loss: 0.007442728214664384\n",
      "Training Loss: 0.007182591835735366\n",
      "Training Loss: 0.007079261259641498\n",
      "Validation Loss: 0.004321019804848128\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005826361160725355\n",
      "Training Loss: 0.005649883847800083\n",
      "Training Loss: 0.0056226902804337444\n",
      "Training Loss: 0.00639100237400271\n",
      "Training Loss: 0.00743540704715997\n",
      "Training Loss: 0.007173603117698804\n",
      "Training Loss: 0.00706833514617756\n",
      "Validation Loss: 0.004313795320682702\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.0058169648807961495\n",
      "Training Loss: 0.0056393483595456925\n",
      "Training Loss: 0.005613813359523192\n",
      "Training Loss: 0.006381173404515721\n",
      "Training Loss: 0.007428137410897762\n",
      "Training Loss: 0.007164413071004674\n",
      "Training Loss: 0.007057097705546767\n",
      "Validation Loss: 0.004306695216042356\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005807243532617576\n",
      "Training Loss: 0.0056285112298792225\n",
      "Training Loss: 0.0056048993114382025\n",
      "Training Loss: 0.006371214188984595\n",
      "Training Loss: 0.007420889761997387\n",
      "Training Loss: 0.007154975908342749\n",
      "Training Loss: 0.007045596410753205\n",
      "Validation Loss: 0.004299701093382129\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005797215553466231\n",
      "Training Loss: 0.005617424651281908\n",
      "Training Loss: 0.005595978302881122\n",
      "Training Loss: 0.00636107670608908\n",
      "Training Loss: 0.007413604180328548\n",
      "Training Loss: 0.0071452889987267555\n",
      "Training Loss: 0.0070339851884637025\n",
      "Validation Loss: 0.004292751749635636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005786995897069574\n",
      "Training Loss: 0.005606224647490308\n",
      "Training Loss: 0.005587098230607808\n",
      "Training Loss: 0.0063507401297101754\n",
      "Training Loss: 0.007406200307887047\n",
      "Training Loss: 0.0071354253136087205\n",
      "Training Loss: 0.007022509792586788\n",
      "Validation Loss: 0.0042857464624408365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005776778245344758\n",
      "Training Loss: 0.005595106977270916\n",
      "Training Loss: 0.00557832031394355\n",
      "Training Loss: 0.006340231440844946\n",
      "Training Loss: 0.007398619907908141\n",
      "Training Loss: 0.007125535921659321\n",
      "Training Loss: 0.00701141019584611\n",
      "Validation Loss: 0.004278612542419593\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005766770507907495\n",
      "Training Loss: 0.0055842590902466325\n",
      "Training Loss: 0.005569715150631965\n",
      "Training Loss: 0.006329632277484052\n",
      "Training Loss: 0.007390861881431192\n",
      "Training Loss: 0.007115794443525374\n",
      "Training Loss: 0.007000813956838101\n",
      "Validation Loss: 0.004271335718448689\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005757100690389052\n",
      "Training Loss: 0.005573790197377093\n",
      "Training Loss: 0.005561331502394751\n",
      "Training Loss: 0.00631904540641699\n",
      "Training Loss: 0.007382989145116881\n",
      "Training Loss: 0.007106333053670823\n",
      "Training Loss: 0.0069907164457254116\n",
      "Validation Loss: 0.004263967071129672\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.0057478044828167185\n",
      "Training Loss: 0.005563723751693033\n",
      "Training Loss: 0.005553187790792435\n",
      "Training Loss: 0.006308564769569785\n",
      "Training Loss: 0.007375091795111075\n",
      "Training Loss: 0.007097210108768195\n",
      "Training Loss: 0.006981037176446989\n",
      "Validation Loss: 0.004256565974456736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.00573884847399313\n",
      "Training Loss: 0.005554020227282308\n",
      "Training Loss: 0.005545270437141881\n",
      "Training Loss: 0.006298249957617372\n",
      "Training Loss: 0.007367246189387515\n",
      "Training Loss: 0.0070884285250213\n",
      "Training Loss: 0.006971688773483038\n",
      "Validation Loss: 0.0042491758500368855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005730181926046498\n",
      "Training Loss: 0.005544621139997616\n",
      "Training Loss: 0.005537550590233877\n",
      "Training Loss: 0.006288131140172482\n",
      "Training Loss: 0.007359499873127788\n",
      "Training Loss: 0.007079958993708715\n",
      "Training Loss: 0.006962595272343605\n",
      "Validation Loss: 0.004241827978274684\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0057217571459477765\n",
      "Training Loss: 0.005535472292685881\n",
      "Training Loss: 0.005529996877303347\n",
      "Training Loss: 0.0062782204110408205\n",
      "Training Loss: 0.0073518735100515185\n",
      "Training Loss: 0.0070717665867414325\n",
      "Training Loss: 0.006953706542262807\n",
      "Validation Loss: 0.004234541273389835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005713533736998216\n",
      "Training Loss: 0.005526532877702266\n",
      "Training Loss: 0.005522577964002267\n",
      "Training Loss: 0.006268514675321057\n",
      "Training Loss: 0.007344368083868176\n",
      "Training Loss: 0.007063806662335992\n",
      "Training Loss: 0.006944983238354325\n",
      "Validation Loss: 0.004227329825560391\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005705481028999202\n",
      "Training Loss: 0.005517776028136722\n",
      "Training Loss: 0.005515270900214091\n",
      "Training Loss: 0.006259009628556669\n",
      "Training Loss: 0.007336976629449055\n",
      "Training Loss: 0.00705604761140421\n",
      "Training Loss: 0.006936403830768541\n",
      "Validation Loss: 0.004220193418960195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005697573227807879\n",
      "Training Loss: 0.005509175457409583\n",
      "Training Loss: 0.005508051564684138\n",
      "Training Loss: 0.006249693424324505\n",
      "Training Loss: 0.00732968672644347\n",
      "Training Loss: 0.007048458746867254\n",
      "Training Loss: 0.006927950417157262\n",
      "Validation Loss: 0.0042131378237778945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005689788553863764\n",
      "Training Loss: 0.005500718774856068\n",
      "Training Loss: 0.005500907623209059\n",
      "Training Loss: 0.006240556208067574\n",
      "Training Loss: 0.007322484463220463\n",
      "Training Loss: 0.00704101521987468\n",
      "Training Loss: 0.006919614039361477\n",
      "Validation Loss: 0.004206168727359773\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005682115773088299\n",
      "Training Loss: 0.005492396331392229\n",
      "Training Loss: 0.005493828745093196\n",
      "Training Loss: 0.006231587716611102\n",
      "Training Loss: 0.007315359138883651\n",
      "Training Loss: 0.007033698401646689\n",
      "Training Loss: 0.006911390094319359\n",
      "Validation Loss: 0.004199286079128388\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0056745421688538045\n",
      "Training Loss: 0.005484198870835826\n",
      "Training Loss: 0.005486808359855786\n",
      "Training Loss: 0.0062227776768850164\n",
      "Training Loss: 0.007308302945457399\n",
      "Training Loss: 0.0070264958194456995\n",
      "Training Loss: 0.006903277497040108\n",
      "Validation Loss: 0.004192486971752781\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005667057083337567\n",
      "Training Loss: 0.005476118981605395\n",
      "Training Loss: 0.005479839933104813\n",
      "Training Loss: 0.006214118015486747\n",
      "Training Loss: 0.007301309974864125\n",
      "Training Loss: 0.007019394644303247\n",
      "Training Loss: 0.006895277011208237\n",
      "Validation Loss: 0.0041857734924245086\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005659654330811463\n",
      "Training Loss: 0.005468154868576675\n",
      "Training Loss: 0.00547292385599576\n",
      "Training Loss: 0.006205599586828612\n",
      "Training Loss: 0.0072943731083069\n",
      "Training Loss: 0.007012386756250635\n",
      "Training Loss: 0.006887387133901939\n",
      "Validation Loss: 0.00417914539305249\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005652332248282619\n",
      "Training Loss: 0.005460303042200394\n",
      "Training Loss: 0.005466059827012941\n",
      "Training Loss: 0.0061972154199611395\n",
      "Training Loss: 0.0072874930640682576\n",
      "Training Loss: 0.00700546620413661\n",
      "Training Loss: 0.006879613682394847\n",
      "Validation Loss: 0.004172599332037378\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005645080979447812\n",
      "Training Loss: 0.005452557205571793\n",
      "Training Loss: 0.00545924743404612\n",
      "Training Loss: 0.006188957705162466\n",
      "Training Loss: 0.007280665363650769\n",
      "Training Loss: 0.006998629391891882\n",
      "Training Loss: 0.006871953927911818\n",
      "Validation Loss: 0.004166135835210175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005637900503352285\n",
      "Training Loss: 0.005444917238783091\n",
      "Training Loss: 0.00545248590875417\n",
      "Training Loss: 0.006180820706067607\n",
      "Training Loss: 0.007273892440134659\n",
      "Training Loss: 0.006991871376521885\n",
      "Training Loss: 0.006864411736605689\n",
      "Validation Loss: 0.004159750737780042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005630786514957436\n",
      "Training Loss: 0.005437378212809563\n",
      "Training Loss: 0.0054457785212434825\n",
      "Training Loss: 0.0061727981630247085\n",
      "Training Loss: 0.007267173215514049\n",
      "Training Loss: 0.006985191478161141\n",
      "Training Loss: 0.006856988527579233\n",
      "Validation Loss: 0.004153444437826339\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005623737700516358\n",
      "Training Loss: 0.005429939824971371\n",
      "Training Loss: 0.005439125685952604\n",
      "Training Loss: 0.006164886535843834\n",
      "Training Loss: 0.007260509653715417\n",
      "Training Loss: 0.006978586741024628\n",
      "Training Loss: 0.006849684502230957\n",
      "Validation Loss: 0.004147213304673465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005616753128706477\n",
      "Training Loss: 0.005422599518788047\n",
      "Training Loss: 0.005432528534438461\n",
      "Training Loss: 0.006157079886761494\n",
      "Training Loss: 0.007253904350800439\n",
      "Training Loss: 0.00697205665288493\n",
      "Training Loss: 0.006842501076171174\n",
      "Validation Loss: 0.0041410524907350985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.0056098290398949755\n",
      "Training Loss: 0.005415354238357395\n",
      "Training Loss: 0.005425988904898986\n",
      "Training Loss: 0.006149375500390306\n",
      "Training Loss: 0.007247358738677576\n",
      "Training Loss: 0.006965601929696277\n",
      "Training Loss: 0.006835438149282709\n",
      "Validation Loss: 0.004134964302517055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.00560296569892671\n",
      "Training Loss: 0.0054082025680691\n",
      "Training Loss: 0.005419507955666631\n",
      "Training Loss: 0.006141767493681982\n",
      "Training Loss: 0.007240875223651529\n",
      "Training Loss: 0.006959222628502175\n",
      "Training Loss: 0.006828497714595869\n",
      "Validation Loss: 0.004128942763596801\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.0055961615964770315\n",
      "Training Loss: 0.005401141879847273\n",
      "Training Loss: 0.005413086147746072\n",
      "Training Loss: 0.006134255804354325\n",
      "Training Loss: 0.007234456022270024\n",
      "Training Loss: 0.006952918018214405\n",
      "Training Loss: 0.006821677661500871\n",
      "Validation Loss: 0.004122982886864033\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.0055894168402301145\n",
      "Training Loss: 0.005394170788349584\n",
      "Training Loss: 0.005406724923523143\n",
      "Training Loss: 0.006126835478935391\n",
      "Training Loss: 0.007228103650268167\n",
      "Training Loss: 0.00694668990909122\n",
      "Training Loss: 0.0068149779189843685\n",
      "Validation Loss: 0.004117089433138597\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005582732959883287\n",
      "Training Loss: 0.0053872882359428335\n",
      "Training Loss: 0.005400425383122638\n",
      "Training Loss: 0.006119505629176274\n",
      "Training Loss: 0.007221823021536693\n",
      "Training Loss: 0.006940539029892534\n",
      "Training Loss: 0.006808402263559401\n",
      "Validation Loss: 0.004111254662763649\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005576106334920041\n",
      "Training Loss: 0.0053804931085323915\n",
      "Training Loss: 0.0053941884287633\n",
      "Training Loss: 0.006112265372066759\n",
      "Training Loss: 0.0072156165749765935\n",
      "Training Loss: 0.006934467067476362\n",
      "Training Loss: 0.006801947358762845\n",
      "Validation Loss: 0.004105484017174454\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005569542587618343\n",
      "Training Loss: 0.005373784868861548\n",
      "Training Loss: 0.005388017151271924\n",
      "Training Loss: 0.006105112339719198\n",
      "Training Loss: 0.007209486056817696\n",
      "Training Loss: 0.006928475602762774\n",
      "Training Loss: 0.006795615472365171\n",
      "Validation Loss: 0.004099769155453011\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005563037659157999\n",
      "Training Loss: 0.005367160860914737\n",
      "Training Loss: 0.005381909959251061\n",
      "Training Loss: 0.006098045730614104\n",
      "Training Loss: 0.007203435521805659\n",
      "Training Loss: 0.006922564950073138\n",
      "Training Loss: 0.006789403693983331\n",
      "Validation Loss: 0.004094114658782042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005556596256210469\n",
      "Training Loss: 0.005360622419975698\n",
      "Training Loss: 0.005375869401032105\n",
      "Training Loss: 0.0060910652484744785\n",
      "Training Loss: 0.007197465331992134\n",
      "Training Loss: 0.006916736040730029\n",
      "Training Loss: 0.006783314374042675\n",
      "Validation Loss: 0.004088517881438053\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005550216299598105\n",
      "Training Loss: 0.005354169018100947\n",
      "Training Loss: 0.005369896407937631\n",
      "Training Loss: 0.006084169749519788\n",
      "Training Loss: 0.0071915816003456715\n",
      "Training Loss: 0.00691099134972319\n",
      "Training Loss: 0.006777347542811185\n",
      "Validation Loss: 0.004082981124694707\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005543901297496632\n",
      "Training Loss: 0.0053478009410901\n",
      "Training Loss: 0.0053639938007108865\n",
      "Training Loss: 0.0060773603519191964\n",
      "Training Loss: 0.00718578563304618\n",
      "Training Loss: 0.006905331948073581\n",
      "Training Loss: 0.00677150183240883\n",
      "Validation Loss: 0.004077504730821772\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005537651186459698\n",
      "Training Loss: 0.005341518323402852\n",
      "Training Loss: 0.005358161076437682\n",
      "Training Loss: 0.006070636007934809\n",
      "Training Loss: 0.007180081201950088\n",
      "Training Loss: 0.00689975916291587\n",
      "Training Loss: 0.006765778560657054\n",
      "Validation Loss: 0.004072089017672318\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005531467970577068\n",
      "Training Loss: 0.005335321081220173\n",
      "Training Loss: 0.005352399969706312\n",
      "Training Loss: 0.006063996632583439\n",
      "Training Loss: 0.007174468509620055\n",
      "Training Loss: 0.0068942729267291725\n",
      "Training Loss: 0.006760176988318562\n",
      "Validation Loss: 0.004066733635128354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005525350988609717\n",
      "Training Loss: 0.0053292089374735955\n",
      "Training Loss: 0.005346711257006973\n",
      "Training Loss: 0.006057441072771326\n",
      "Training Loss: 0.007168951620114967\n",
      "Training Loss: 0.006888875914737583\n",
      "Training Loss: 0.0067546955228317525\n",
      "Validation Loss: 0.0040614461990191916\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.0055193052359391\n",
      "Training Loss: 0.005323185153538361\n",
      "Training Loss: 0.005341099834768101\n",
      "Training Loss: 0.00605097166262567\n",
      "Training Loss: 0.007163531817495823\n",
      "Training Loss: 0.006883567380718887\n",
      "Training Loss: 0.006749336099019274\n",
      "Validation Loss: 0.004056221432286255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005513328560627997\n",
      "Training Loss: 0.00531724751344882\n",
      "Training Loss: 0.005335563434055075\n",
      "Training Loss: 0.006044588199001737\n",
      "Training Loss: 0.00715821381774731\n",
      "Training Loss: 0.0068783499184064565\n",
      "Training Loss: 0.00674409740138799\n",
      "Validation Loss: 0.004051062032538518\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005507423342205584\n",
      "Training Loss: 0.005311395706376061\n",
      "Training Loss: 0.005330103217856958\n",
      "Training Loss: 0.006038290160358883\n",
      "Training Loss: 0.007152994927018881\n",
      "Training Loss: 0.0068732220970559865\n",
      "Training Loss: 0.006738977417116984\n",
      "Validation Loss: 0.004045972526457147\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0055015914113027975\n",
      "Training Loss: 0.005305633236421272\n",
      "Training Loss: 0.0053247224050574\n",
      "Training Loss: 0.006032079766737297\n",
      "Training Loss: 0.007147879952099174\n",
      "Training Loss: 0.006868187427753583\n",
      "Training Loss: 0.006733977347612381\n",
      "Validation Loss: 0.004040952276591951\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005495832819142379\n",
      "Training Loss: 0.005299959720578045\n",
      "Training Loss: 0.005319422150496394\n",
      "Training Loss: 0.006025955338845961\n",
      "Training Loss: 0.00714286882779561\n",
      "Training Loss: 0.00686324343085289\n",
      "Training Loss: 0.006729094891343265\n",
      "Validation Loss: 0.00403600259185228\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005490147927775979\n",
      "Training Loss: 0.005294371833442711\n",
      "Training Loss: 0.005314200954744592\n",
      "Training Loss: 0.006019917700323276\n",
      "Training Loss: 0.00713796304888092\n",
      "Training Loss: 0.006858392319409177\n",
      "Training Loss: 0.006724329862045124\n",
      "Validation Loss: 0.004031123940079483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005484536966541782\n",
      "Training Loss: 0.00528887253778521\n",
      "Training Loss: 0.005309060791041702\n",
      "Training Loss: 0.006013965363963507\n",
      "Training Loss: 0.007133163551334292\n",
      "Training Loss: 0.006853632431011647\n",
      "Training Loss: 0.0067196797940414395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [09:41<38:45, 290.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.004026316490882401\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.3139440955221653\n",
      "Training Loss: 0.23827197164297104\n",
      "Training Loss: 0.16812228713184596\n",
      "Training Loss: 0.11199446182698011\n",
      "Training Loss: 0.08494390470907093\n",
      "Training Loss: 0.07663045078516006\n",
      "Training Loss: 0.07281620774418116\n",
      "Validation Loss: 0.07071368082353238\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.07057866474613547\n",
      "Training Loss: 0.06740441827103495\n",
      "Training Loss: 0.06572338245809078\n",
      "Training Loss: 0.06454225111752748\n",
      "Training Loss: 0.061631638072431084\n",
      "Training Loss: 0.06087569991126657\n",
      "Training Loss: 0.05771827567368746\n",
      "Validation Loss: 0.05633845632330755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.05538824137300253\n",
      "Training Loss: 0.05217781636863947\n",
      "Training Loss: 0.05002515367232263\n",
      "Training Loss: 0.04861506004817784\n",
      "Training Loss: 0.045611024089157584\n",
      "Training Loss: 0.04442688408307731\n",
      "Training Loss: 0.04123863922432065\n",
      "Validation Loss: 0.04016894883430852\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.03885782742872834\n",
      "Training Loss: 0.03642285272479057\n",
      "Training Loss: 0.03450819648802281\n",
      "Training Loss: 0.03390000787563622\n",
      "Training Loss: 0.031851337542757395\n",
      "Training Loss: 0.03117873152717948\n",
      "Training Loss: 0.029005201100371778\n",
      "Validation Loss: 0.02812127833523219\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.02731664464343339\n",
      "Training Loss: 0.025955194686539472\n",
      "Training Loss: 0.024607215677388012\n",
      "Training Loss: 0.024722171681933105\n",
      "Training Loss: 0.02355351678095758\n",
      "Training Loss: 0.02313462699763477\n",
      "Training Loss: 0.02171704836655408\n",
      "Validation Loss: 0.02065449520554882\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.02038900485262275\n",
      "Training Loss: 0.019542852255981416\n",
      "Training Loss: 0.018584916566032915\n",
      "Training Loss: 0.019061672799289227\n",
      "Training Loss: 0.018569232751615344\n",
      "Training Loss: 0.01820852903649211\n",
      "Training Loss: 0.017225647657178343\n",
      "Validation Loss: 0.015904794680826673\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.01609181935666129\n",
      "Training Loss: 0.015455805761739612\n",
      "Training Loss: 0.014761922815814614\n",
      "Training Loss: 0.01543206226080656\n",
      "Training Loss: 0.0154429522017017\n",
      "Training Loss: 0.015076687103137374\n",
      "Training Loss: 0.014331767549738288\n",
      "Validation Loss: 0.012700355658830886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.01327893348876387\n",
      "Training Loss: 0.012716188526246697\n",
      "Training Loss: 0.012202811751049012\n",
      "Training Loss: 0.012991543773096055\n",
      "Training Loss: 0.013361944954376669\n",
      "Training Loss: 0.012969330113846808\n",
      "Training Loss: 0.012367389968130738\n",
      "Validation Loss: 0.010357473350796249\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.011281698553357273\n",
      "Training Loss: 0.010736748049966991\n",
      "Training Loss: 0.010353861283510924\n",
      "Training Loss: 0.01122202652390115\n",
      "Training Loss: 0.011858586033340543\n",
      "Training Loss: 0.011439083212753758\n",
      "Training Loss: 0.010967503227293491\n",
      "Validation Loss: 0.008579827521575357\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.00980578168760985\n",
      "Training Loss: 0.009299326692707837\n",
      "Training Loss: 0.009085977253271267\n",
      "Training Loss: 0.010050877764588222\n",
      "Training Loss: 0.010910885613411666\n",
      "Training Loss: 0.010524620700161904\n",
      "Training Loss: 0.010214979239972308\n",
      "Validation Loss: 0.007574091130895505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.009001022117445245\n",
      "Training Loss: 0.00855544875143096\n",
      "Training Loss: 0.008455900603439658\n",
      "Training Loss: 0.009474197051022202\n",
      "Training Loss: 0.01042640877654776\n",
      "Training Loss: 0.010068479283945634\n",
      "Training Loss: 0.009842378536704928\n",
      "Validation Loss: 0.007020945034656074\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.00855311360443011\n",
      "Training Loss: 0.008133363595698028\n",
      "Training Loss: 0.008072906588204205\n",
      "Training Loss: 0.009114744368707762\n",
      "Training Loss: 0.010091111055808143\n",
      "Training Loss: 0.009750137757509946\n",
      "Training Loss: 0.009573973289225251\n",
      "Validation Loss: 0.006649031620368027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.008232179238693788\n",
      "Training Loss: 0.007830079414416104\n",
      "Training Loss: 0.007783193562645465\n",
      "Training Loss: 0.008834261287702247\n",
      "Training Loss: 0.009808348818914965\n",
      "Training Loss: 0.009482136704027653\n",
      "Training Loss: 0.00934219333343208\n",
      "Validation Loss: 0.00636946722736045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.007972484502242878\n",
      "Training Loss: 0.007586635260377079\n",
      "Training Loss: 0.007543461112072691\n",
      "Training Loss: 0.008596127176424488\n",
      "Training Loss: 0.009558613378321751\n",
      "Training Loss: 0.00924741524620913\n",
      "Training Loss: 0.00913655872340314\n",
      "Validation Loss: 0.006151641653641389\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.007756904640700668\n",
      "Training Loss: 0.007387696533696726\n",
      "Training Loss: 0.007344508477253839\n",
      "Training Loss: 0.00839503892348148\n",
      "Training Loss: 0.009344608729006722\n",
      "Training Loss: 0.009048282337607816\n",
      "Training Loss: 0.00896132572554052\n",
      "Validation Loss: 0.005984791325168049\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.007582840797258541\n",
      "Training Loss: 0.007230148643720895\n",
      "Training Loss: 0.0071857006510254\n",
      "Training Loss: 0.008232342777773738\n",
      "Training Loss: 0.00917117581819184\n",
      "Training Loss: 0.00888843772583641\n",
      "Training Loss: 0.00882048140396364\n",
      "Validation Loss: 0.005862227662846669\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.007448682005051523\n",
      "Training Loss: 0.007111056144349277\n",
      "Training Loss: 0.007065029215300456\n",
      "Training Loss: 0.008106863077264279\n",
      "Training Loss: 0.009038128899410367\n",
      "Training Loss: 0.00876681192778051\n",
      "Training Loss: 0.008713057140121237\n",
      "Validation Loss: 0.005775745329359888\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.007349390938179568\n",
      "Training Loss: 0.007024365508696065\n",
      "Training Loss: 0.006976729645393789\n",
      "Training Loss: 0.008013212732039391\n",
      "Training Loss: 0.008939886273583397\n",
      "Training Loss: 0.008677561450749636\n",
      "Training Loss: 0.00863371231709607\n",
      "Validation Loss: 0.005715688233602136\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.00727718435227871\n",
      "Training Loss: 0.006962068836437538\n",
      "Training Loss: 0.006912879018345848\n",
      "Training Loss: 0.007943774739978836\n",
      "Training Loss: 0.008868273147381842\n",
      "Training Loss: 0.008612789656035602\n",
      "Training Loss: 0.008575411000056193\n",
      "Validation Loss: 0.005673073376058026\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.007224011202342808\n",
      "Training Loss: 0.00691650333814323\n",
      "Training Loss: 0.0068658495182171465\n",
      "Training Loss: 0.007891228779917582\n",
      "Training Loss: 0.008815402272157371\n",
      "Training Loss: 0.008565115509554744\n",
      "Training Loss: 0.008531697796424851\n",
      "Validation Loss: 0.0056410717877318686\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.007183330467669293\n",
      "Training Loss: 0.006881737490184605\n",
      "Training Loss: 0.006829722861293703\n",
      "Training Loss: 0.007849876816617325\n",
      "Training Loss: 0.008775090703275056\n",
      "Training Loss: 0.008528859373182058\n",
      "Training Loss: 0.008497650614008307\n",
      "Validation Loss: 0.005615254871944865\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.007150589246302843\n",
      "Training Loss: 0.006853767808061093\n",
      "Training Loss: 0.006800491893664003\n",
      "Training Loss: 0.007815804426791147\n",
      "Training Loss: 0.008743025343865157\n",
      "Training Loss: 0.008500101019162685\n",
      "Training Loss: 0.008469886666862294\n",
      "Validation Loss: 0.005593057192534153\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.007122893463820219\n",
      "Training Loss: 0.006830102223902941\n",
      "Training Loss: 0.0067756622412707655\n",
      "Training Loss: 0.007786493381718174\n",
      "Training Loss: 0.008716378756798805\n",
      "Training Loss: 0.008476291103288531\n",
      "Training Loss: 0.00844619668321684\n",
      "Validation Loss: 0.005573109944249863\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.0070984919462352995\n",
      "Training Loss: 0.006809261704329401\n",
      "Training Loss: 0.006753750768257305\n",
      "Training Loss: 0.007760371594922617\n",
      "Training Loss: 0.008693369550164788\n",
      "Training Loss: 0.00845582141308114\n",
      "Training Loss: 0.008425190980779007\n",
      "Validation Loss: 0.005554749242340525\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.007076364923268557\n",
      "Training Loss: 0.006790405856445431\n",
      "Training Loss: 0.006733889118768275\n",
      "Training Loss: 0.0077365098940208555\n",
      "Training Loss: 0.008672909940360114\n",
      "Training Loss: 0.00843769832397811\n",
      "Training Loss: 0.008406050823396072\n",
      "Validation Loss: 0.005537687565377924\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.0070559476607013494\n",
      "Training Loss: 0.006773067964240908\n",
      "Training Loss: 0.006715539699653164\n",
      "Training Loss: 0.007714428862091154\n",
      "Training Loss: 0.008654354906175285\n",
      "Training Loss: 0.008421295274747536\n",
      "Training Loss: 0.008388331116875633\n",
      "Validation Loss: 0.005521800802420643\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.0070369138882961126\n",
      "Training Loss: 0.006756949697155505\n",
      "Training Loss: 0.006698346916818991\n",
      "Training Loss: 0.007693879210855812\n",
      "Training Loss: 0.008637277893722057\n",
      "Training Loss: 0.008406186661450193\n",
      "Training Loss: 0.008371765811461955\n",
      "Validation Loss: 0.005507006221851997\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007019034093245864\n",
      "Training Loss: 0.006741835337597877\n",
      "Training Loss: 0.006682078740559518\n",
      "Training Loss: 0.007674674377776683\n",
      "Training Loss: 0.008621373726055027\n",
      "Training Loss: 0.008392087055835874\n",
      "Training Loss: 0.008356170513434336\n",
      "Validation Loss: 0.005493208275905597\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.00700212546158582\n",
      "Training Loss: 0.006727574366377667\n",
      "Training Loss: 0.006666587420040742\n",
      "Training Loss: 0.007656644033268094\n",
      "Training Loss: 0.008606428116327152\n",
      "Training Loss: 0.008378809896530583\n",
      "Training Loss: 0.008341414338210597\n",
      "Validation Loss: 0.005480314079979051\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006986050216946751\n",
      "Training Loss: 0.0067140620248392225\n",
      "Training Loss: 0.006651776271173731\n",
      "Training Loss: 0.0076396274205762895\n",
      "Training Loss: 0.008592290087835863\n",
      "Training Loss: 0.008366223288467154\n",
      "Training Loss: 0.008327398024266585\n",
      "Validation Loss: 0.005468230963988083\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.00697069677291438\n",
      "Training Loss: 0.006701217921217904\n",
      "Training Loss: 0.00663757735514082\n",
      "Training Loss: 0.007623485623626038\n",
      "Training Loss: 0.008578853289363905\n",
      "Training Loss: 0.008354239369509742\n",
      "Training Loss: 0.008314040271798148\n",
      "Validation Loss: 0.005456874149673608\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006955974093871191\n",
      "Training Loss: 0.0066889691411051895\n",
      "Training Loss: 0.00662393408943899\n",
      "Training Loss: 0.007608101484365761\n",
      "Training Loss: 0.008566034623654559\n",
      "Training Loss: 0.008342790609458462\n",
      "Training Loss: 0.008301274229306727\n",
      "Validation Loss: 0.005446170136674346\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.006941807703115046\n",
      "Training Loss: 0.006677247832994908\n",
      "Training Loss: 0.0066107894503511485\n",
      "Training Loss: 0.00759338345262222\n",
      "Training Loss: 0.008553774819010868\n",
      "Training Loss: 0.00833182603935711\n",
      "Training Loss: 0.008289040550589561\n",
      "Validation Loss: 0.005436045379283723\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006928137711947784\n",
      "Training Loss: 0.006665996059309691\n",
      "Training Loss: 0.0065980970102828\n",
      "Training Loss: 0.0075792616489343346\n",
      "Training Loss: 0.008542015990242362\n",
      "Training Loss: 0.008321303981356323\n",
      "Training Loss: 0.00827728807227686\n",
      "Validation Loss: 0.005426417966460467\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006914916951209307\n",
      "Training Loss: 0.006655162069946528\n",
      "Training Loss: 0.006585812133271247\n",
      "Training Loss: 0.007565680864499882\n",
      "Training Loss: 0.00853070848970674\n",
      "Training Loss: 0.008311192122055217\n",
      "Training Loss: 0.008265970614738763\n",
      "Validation Loss: 0.005417186847089158\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.006902097633574158\n",
      "Training Loss: 0.006644699393073097\n",
      "Training Loss: 0.006573895768960938\n",
      "Training Loss: 0.00755259427241981\n",
      "Training Loss: 0.008519804518437013\n",
      "Training Loss: 0.008301459095673636\n",
      "Training Loss: 0.0082550405676011\n",
      "Validation Loss: 0.005408265225917156\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.00688964098924771\n",
      "Training Loss: 0.006634569414891303\n",
      "Training Loss: 0.006562313068425283\n",
      "Training Loss: 0.00753995988285169\n",
      "Training Loss: 0.00850925715873018\n",
      "Training Loss: 0.008292071816977114\n",
      "Training Loss: 0.008244456296088174\n",
      "Validation Loss: 0.005399578043402692\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006877506759483367\n",
      "Training Loss: 0.0066247392992954705\n",
      "Training Loss: 0.006551029646070674\n",
      "Training Loss: 0.0075277389190159735\n",
      "Training Loss: 0.008499027681536972\n",
      "Training Loss: 0.008282998447539285\n",
      "Training Loss: 0.008234180411091075\n",
      "Validation Loss: 0.0053910775605435675\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.0068656642897985875\n",
      "Training Loss: 0.006615180617664009\n",
      "Training Loss: 0.006540017925435677\n",
      "Training Loss: 0.007515891072107479\n",
      "Training Loss: 0.008489083640743047\n",
      "Training Loss: 0.008274206147762015\n",
      "Training Loss: 0.008224177702795715\n",
      "Validation Loss: 0.005382737950578834\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006854084451915697\n",
      "Training Loss: 0.006605867455946281\n",
      "Training Loss: 0.006529252182226628\n",
      "Training Loss: 0.007504381331382319\n",
      "Training Loss: 0.008479395251488313\n",
      "Training Loss: 0.00826566325733438\n",
      "Training Loss: 0.008214418236166239\n",
      "Validation Loss: 0.005374547158636516\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.0068427449511364105\n",
      "Training Loss: 0.006596777458908036\n",
      "Training Loss: 0.006518710317322984\n",
      "Training Loss: 0.0074931829248089344\n",
      "Training Loss: 0.008469940730137751\n",
      "Training Loss: 0.008257342692231759\n",
      "Training Loss: 0.008204878310207277\n",
      "Validation Loss: 0.0053664894839086\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006831624069018289\n",
      "Training Loss: 0.006587887509958819\n",
      "Training Loss: 0.006508374669356272\n",
      "Training Loss: 0.007482263536658138\n",
      "Training Loss: 0.008460699433926492\n",
      "Training Loss: 0.0082492197281681\n",
      "Training Loss: 0.008195532319368794\n",
      "Validation Loss: 0.005358555082978213\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006820701165124774\n",
      "Training Loss: 0.006579179537948221\n",
      "Training Loss: 0.006498228051932529\n",
      "Training Loss: 0.007471593604423106\n",
      "Training Loss: 0.0084516494453419\n",
      "Training Loss: 0.008241272259037941\n",
      "Training Loss: 0.008186356174992398\n",
      "Validation Loss: 0.005350742630367534\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.00680996427196078\n",
      "Training Loss: 0.00657063573715277\n",
      "Training Loss: 0.006488252701237798\n",
      "Training Loss: 0.007461152366595343\n",
      "Training Loss: 0.008442772742127999\n",
      "Training Loss: 0.008233478317270055\n",
      "Training Loss: 0.0081773307104595\n",
      "Validation Loss: 0.005343039921216304\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006799396058777347\n",
      "Training Loss: 0.006562240577768535\n",
      "Training Loss: 0.006478435152675956\n",
      "Training Loss: 0.00745092065189965\n",
      "Training Loss: 0.008434049944626168\n",
      "Training Loss: 0.00822581773623824\n",
      "Training Loss: 0.008168436118867249\n",
      "Validation Loss: 0.0053354406193917275\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006788983416045085\n",
      "Training Loss: 0.006553978868760168\n",
      "Training Loss: 0.006468759402632713\n",
      "Training Loss: 0.0074408816534560175\n",
      "Training Loss: 0.00842546405736357\n",
      "Training Loss: 0.008218273877864703\n",
      "Training Loss: 0.008159652715548874\n",
      "Validation Loss: 0.005327941899533185\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0067787157685961575\n",
      "Training Loss: 0.006545836876612156\n",
      "Training Loss: 0.006459211600013077\n",
      "Training Loss: 0.007431021480588242\n",
      "Training Loss: 0.008416998450411484\n",
      "Training Loss: 0.008210828771116212\n",
      "Training Loss: 0.008150964507367462\n",
      "Validation Loss: 0.005320529025292184\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006768578684423119\n",
      "Training Loss: 0.00653780230670236\n",
      "Training Loss: 0.006449777474626899\n",
      "Training Loss: 0.0074213284580037\n",
      "Training Loss: 0.008408635277301074\n",
      "Training Loss: 0.00820346181630157\n",
      "Training Loss: 0.00814234752091579\n",
      "Validation Loss: 0.005313211864502298\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006758561622118578\n",
      "Training Loss: 0.006529864919139072\n",
      "Training Loss: 0.006440440417500213\n",
      "Training Loss: 0.007411746639991179\n",
      "Training Loss: 0.008400383595144376\n",
      "Training Loss: 0.008196111749857665\n",
      "Training Loss: 0.00813377126120031\n",
      "Validation Loss: 0.005306051199913918\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006748643525643274\n",
      "Training Loss: 0.006522007691673934\n",
      "Training Loss: 0.006431161677464842\n",
      "Training Loss: 0.007402102794731036\n",
      "Training Loss: 0.008392140850191935\n",
      "Training Loss: 0.008188883552793413\n",
      "Training Loss: 0.008125283202389255\n",
      "Validation Loss: 0.005298981236377626\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006738863585051149\n",
      "Training Loss: 0.006514246055157855\n",
      "Training Loss: 0.0064220634032972155\n",
      "Training Loss: 0.007392685594968498\n",
      "Training Loss: 0.008384007150307298\n",
      "Training Loss: 0.008181671927450224\n",
      "Training Loss: 0.008116820896975695\n",
      "Validation Loss: 0.0052918629269098185\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006729161170078442\n",
      "Training Loss: 0.006506545178126544\n",
      "Training Loss: 0.006413048059912398\n",
      "Training Loss: 0.007383392371120862\n",
      "Training Loss: 0.008375924101565034\n",
      "Training Loss: 0.008174495580606162\n",
      "Training Loss: 0.008108378291362897\n",
      "Validation Loss: 0.005284761133593287\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006719535408774391\n",
      "Training Loss: 0.006498895927798003\n",
      "Training Loss: 0.006404092739103362\n",
      "Training Loss: 0.007374188086832874\n",
      "Training Loss: 0.00836788014625199\n",
      "Training Loss: 0.008167337158229203\n",
      "Training Loss: 0.008099942352855579\n",
      "Validation Loss: 0.005277691442128759\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.0067099803080782295\n",
      "Training Loss: 0.006491291584679857\n",
      "Training Loss: 0.0063951904175337405\n",
      "Training Loss: 0.007365061575546861\n",
      "Training Loss: 0.008359864634694531\n",
      "Training Loss: 0.008160185009473934\n",
      "Training Loss: 0.008091499474830925\n",
      "Validation Loss: 0.0052706560194980675\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006700487773632631\n",
      "Training Loss: 0.0064837233244907115\n",
      "Training Loss: 0.00638633269816637\n",
      "Training Loss: 0.007355997284175828\n",
      "Training Loss: 0.008351863629650324\n",
      "Training Loss: 0.008153023879276588\n",
      "Training Loss: 0.00808303255820647\n",
      "Validation Loss: 0.005263650979958335\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.006691050654044375\n",
      "Training Loss: 0.006476183200720698\n",
      "Training Loss: 0.006377509030280635\n",
      "Training Loss: 0.007346982311573811\n",
      "Training Loss: 0.008343865760834887\n",
      "Training Loss: 0.008145842928206548\n",
      "Training Loss: 0.008074529470177368\n",
      "Validation Loss: 0.005256666172267904\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006681658253073692\n",
      "Training Loss: 0.00646866197581403\n",
      "Training Loss: 0.006368709746748209\n",
      "Training Loss: 0.007338003985350952\n",
      "Training Loss: 0.008335857314523309\n",
      "Training Loss: 0.00813863064511679\n",
      "Training Loss: 0.00806597797316499\n",
      "Validation Loss: 0.005249689373253995\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006672299798810855\n",
      "Training Loss: 0.006461148930247873\n",
      "Training Loss: 0.006359926022123545\n",
      "Training Loss: 0.007329048975370824\n",
      "Training Loss: 0.008327828897163271\n",
      "Training Loss: 0.00813137499964796\n",
      "Training Loss: 0.00805736395646818\n",
      "Validation Loss: 0.0052427121486919425\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006662964539136737\n",
      "Training Loss: 0.006453635172219947\n",
      "Training Loss: 0.006351145267253742\n",
      "Training Loss: 0.007320104645914398\n",
      "Training Loss: 0.008319766161730514\n",
      "Training Loss: 0.008124064785661175\n",
      "Training Loss: 0.008048675052123144\n",
      "Validation Loss: 0.005235728377839097\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006653642887249589\n",
      "Training Loss: 0.006446111659752205\n",
      "Training Loss: 0.0063423610478639605\n",
      "Training Loss: 0.007311160186072811\n",
      "Training Loss: 0.008311658705351874\n",
      "Training Loss: 0.008116687232395635\n",
      "Training Loss: 0.00803989939042367\n",
      "Validation Loss: 0.005228723359227962\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006644323026994243\n",
      "Training Loss: 0.006438567863078788\n",
      "Training Loss: 0.0063335604057647285\n",
      "Training Loss: 0.007302202168502845\n",
      "Training Loss: 0.00830349461059086\n",
      "Training Loss: 0.008109233046416192\n",
      "Training Loss: 0.008031023427611217\n",
      "Validation Loss: 0.005221692586233944\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006634995163185522\n",
      "Training Loss: 0.006430994463153183\n",
      "Training Loss: 0.006324735241942108\n",
      "Training Loss: 0.007293218283448369\n",
      "Training Loss: 0.00829525944427587\n",
      "Training Loss: 0.008101687480229885\n",
      "Training Loss: 0.008022034115856513\n",
      "Validation Loss: 0.005214624066770858\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006625647902255878\n",
      "Training Loss: 0.006423381326021627\n",
      "Training Loss: 0.006315873177954927\n",
      "Training Loss: 0.00728419687715359\n",
      "Training Loss: 0.00828694501775317\n",
      "Training Loss: 0.008094041591975838\n",
      "Training Loss: 0.008012917842715978\n",
      "Validation Loss: 0.005207510345897982\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006616268537472933\n",
      "Training Loss: 0.006415717338677495\n",
      "Training Loss: 0.006306963219540194\n",
      "Training Loss: 0.007275124997831881\n",
      "Training Loss: 0.008278535191202537\n",
      "Training Loss: 0.008086281380383298\n",
      "Training Loss: 0.00800366569077596\n",
      "Validation Loss: 0.005200333589834444\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006606845095520839\n",
      "Training Loss: 0.006407992808963172\n",
      "Training Loss: 0.0062979941349476576\n",
      "Training Loss: 0.0072659910068614405\n",
      "Training Loss: 0.0082700212567579\n",
      "Training Loss: 0.008078397617209703\n",
      "Training Loss: 0.00799426186014898\n",
      "Validation Loss: 0.005193086552053467\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006597363949986175\n",
      "Training Loss: 0.006400194991147146\n",
      "Training Loss: 0.006288954330375418\n",
      "Training Loss: 0.0072567812231136486\n",
      "Training Loss: 0.008261386653175577\n",
      "Training Loss: 0.008070374320959672\n",
      "Training Loss: 0.00798469289089553\n",
      "Validation Loss: 0.005185757127105632\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.006587812965735793\n",
      "Training Loss: 0.0063923134235665206\n",
      "Training Loss: 0.006279830455314368\n",
      "Training Loss: 0.007247483576647938\n",
      "Training Loss: 0.008252620574785397\n",
      "Training Loss: 0.008062201786087827\n",
      "Training Loss: 0.007974946676986292\n",
      "Validation Loss: 0.005178324879959839\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.006578175142640248\n",
      "Training Loss: 0.006384335783077404\n",
      "Training Loss: 0.006270609315251931\n",
      "Training Loss: 0.007238084517302923\n",
      "Training Loss: 0.008243708284571766\n",
      "Training Loss: 0.0080538656830322\n",
      "Training Loss: 0.007965009205508976\n",
      "Validation Loss: 0.005170780832247899\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006568437920650467\n",
      "Training Loss: 0.0063762486766790975\n",
      "Training Loss: 0.006261277762241661\n",
      "Training Loss: 0.007228568908176385\n",
      "Training Loss: 0.008234636423876509\n",
      "Training Loss: 0.00804535225732252\n",
      "Training Loss: 0.007954865575302393\n",
      "Validation Loss: 0.005163109086956201\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006558583973674104\n",
      "Training Loss: 0.006368039484950714\n",
      "Training Loss: 0.006251820654142648\n",
      "Training Loss: 0.007218923209584318\n",
      "Training Loss: 0.008225389138096943\n",
      "Training Loss: 0.00803664870094508\n",
      "Training Loss: 0.007944499542936682\n",
      "Validation Loss: 0.0051552887669370304\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006548595904605463\n",
      "Training Loss: 0.006359692591358907\n",
      "Training Loss: 0.006242219579871744\n",
      "Training Loss: 0.007209133073920384\n",
      "Training Loss: 0.008215950083686039\n",
      "Training Loss: 0.008027739198878408\n",
      "Training Loss: 0.00793389638303779\n",
      "Validation Loss: 0.005147304364099875\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006538455783156678\n",
      "Training Loss: 0.006351192463189364\n",
      "Training Loss: 0.006232459807069973\n",
      "Training Loss: 0.007199183886987157\n",
      "Training Loss: 0.008206306378124282\n",
      "Training Loss: 0.008018611979205162\n",
      "Training Loss: 0.007923042246839032\n",
      "Validation Loss: 0.005139130001978369\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.00652814376517199\n",
      "Training Loss: 0.006342523089842871\n",
      "Training Loss: 0.006222524225013331\n",
      "Training Loss: 0.007189059032825753\n",
      "Training Loss: 0.008196438165614382\n",
      "Training Loss: 0.008009249051101506\n",
      "Training Loss: 0.007911918316967786\n",
      "Validation Loss: 0.005130747890392883\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006517640864476561\n",
      "Training Loss: 0.006333667129511013\n",
      "Training Loss: 0.006212391172302887\n",
      "Training Loss: 0.007178743475233205\n",
      "Training Loss: 0.008186330591561273\n",
      "Training Loss: 0.00799963901634328\n",
      "Training Loss: 0.007900507964659482\n",
      "Validation Loss: 0.0051221295586030245\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006506922927219421\n",
      "Training Loss: 0.0063246046472340825\n",
      "Training Loss: 0.006202041598735377\n",
      "Training Loss: 0.00716822093934752\n",
      "Training Loss: 0.008175964979454876\n",
      "Training Loss: 0.00798976575722918\n",
      "Training Loss: 0.007888794465688988\n",
      "Validation Loss: 0.005113251908413703\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006495970068499446\n",
      "Training Loss: 0.006315316396066919\n",
      "Training Loss: 0.006191453590290621\n",
      "Training Loss: 0.0071574761183001104\n",
      "Training Loss: 0.00816532403579913\n",
      "Training Loss: 0.007979616562370211\n",
      "Training Loss: 0.007876758617348968\n",
      "Validation Loss: 0.005104088240167063\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00648475916008465\n",
      "Training Loss: 0.006305779864778743\n",
      "Training Loss: 0.006180604037363082\n",
      "Training Loss: 0.0071464933251263576\n",
      "Training Loss: 0.008154390070121736\n",
      "Training Loss: 0.007969179840292781\n",
      "Training Loss: 0.007864385505672544\n",
      "Validation Loss: 0.005094611091183477\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.006473268471891061\n",
      "Training Loss: 0.006295974206877873\n",
      "Training Loss: 0.006169471389148385\n",
      "Training Loss: 0.007135258501511999\n",
      "Training Loss: 0.008143146414076909\n",
      "Training Loss: 0.007958442805102096\n",
      "Training Loss: 0.007851656271377579\n",
      "Validation Loss: 0.005084796231379558\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006461475606774911\n",
      "Training Loss: 0.006285875935573131\n",
      "Training Loss: 0.00615803288645111\n",
      "Training Loss: 0.007123759288224392\n",
      "Training Loss: 0.008131576509913429\n",
      "Training Loss: 0.007947398633696139\n",
      "Training Loss: 0.007838556234491988\n",
      "Validation Loss: 0.005074615997509954\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006449358524987474\n",
      "Training Loss: 0.006275459369062446\n",
      "Training Loss: 0.006146265537245199\n",
      "Training Loss: 0.007111985491355881\n",
      "Training Loss: 0.008119670363375917\n",
      "Training Loss: 0.00793604291160591\n",
      "Training Loss: 0.00782507421565242\n",
      "Validation Loss: 0.005064053745235731\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006436903185676784\n",
      "Training Loss: 0.006264702673070133\n",
      "Training Loss: 0.006134151252917945\n",
      "Training Loss: 0.007099928740644828\n",
      "Training Loss: 0.008107415921986104\n",
      "Training Loss: 0.007924375113798305\n",
      "Training Loss: 0.007811200206633657\n",
      "Validation Loss: 0.005053088340161836\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006424091848311946\n",
      "Training Loss: 0.006253580574411899\n",
      "Training Loss: 0.006121668496634811\n",
      "Training Loss: 0.007087587470887229\n",
      "Training Loss: 0.008094808552414179\n",
      "Training Loss: 0.007912401675712317\n",
      "Training Loss: 0.007796932276105509\n",
      "Validation Loss: 0.005041715321880992\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.0064109184278640895\n",
      "Training Loss: 0.006242074730107561\n",
      "Training Loss: 0.006108811397571117\n",
      "Training Loss: 0.007074961815378629\n",
      "Training Loss: 0.008081847624853254\n",
      "Training Loss: 0.007900131662609056\n",
      "Training Loss: 0.007782271875767037\n",
      "Validation Loss: 0.005029939420256164\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006397383264265954\n",
      "Training Loss: 0.00623016694909893\n",
      "Training Loss: 0.006095571648329496\n",
      "Training Loss: 0.007062062056502327\n",
      "Training Loss: 0.008068545394344255\n",
      "Training Loss: 0.007887586414581165\n",
      "Training Loss: 0.007767233243212104\n",
      "Validation Loss: 0.005017769341583779\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006383491286542266\n",
      "Training Loss: 0.006217846480431035\n",
      "Training Loss: 0.0060819558496586976\n",
      "Training Loss: 0.00704889944056049\n",
      "Training Loss: 0.00805491480161436\n",
      "Training Loss: 0.007874788519693539\n",
      "Training Loss: 0.007751837805844843\n",
      "Validation Loss: 0.0050052362127920216\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006369260784704238\n",
      "Training Loss: 0.00620510991604533\n",
      "Training Loss: 0.0060679779574275014\n",
      "Training Loss: 0.0070355028903577475\n",
      "Training Loss: 0.008040984716499224\n",
      "Training Loss: 0.00786177029251121\n",
      "Training Loss: 0.007736118828179314\n",
      "Validation Loss: 0.004992374700358074\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.0063547186320647596\n",
      "Training Loss: 0.006191964236204512\n",
      "Training Loss: 0.006053664308274165\n",
      "Training Loss: 0.007021887209266425\n",
      "Training Loss: 0.008026788476854563\n",
      "Training Loss: 0.007848571027861908\n",
      "Training Loss: 0.0077201246633194385\n",
      "Validation Loss: 0.004979234736426716\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006339896379504353\n",
      "Training Loss: 0.006178430299041793\n",
      "Training Loss: 0.006039050559047609\n",
      "Training Loss: 0.007008130020694807\n",
      "Training Loss: 0.008012378835119308\n",
      "Training Loss: 0.007835233241785318\n",
      "Training Loss: 0.0077039125736337154\n",
      "Validation Loss: 0.004965844080166033\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006324848231161013\n",
      "Training Loss: 0.00616454200586304\n",
      "Training Loss: 0.006024205475114286\n",
      "Training Loss: 0.00699414529663045\n",
      "Training Loss: 0.007997799386503175\n",
      "Training Loss: 0.007821773178875446\n",
      "Training Loss: 0.007687559355981648\n",
      "Validation Loss: 0.004952352124262838\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006309598238440231\n",
      "Training Loss: 0.0061503531562630085\n",
      "Training Loss: 0.006009137269575149\n",
      "Training Loss: 0.006980669492622838\n",
      "Training Loss: 0.007983244906645268\n",
      "Training Loss: 0.007808260989841074\n",
      "Training Loss: 0.007671170075191185\n",
      "Validation Loss: 0.004938398613437508\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0062943200662266465\n",
      "Training Loss: 0.006135912146419287\n",
      "Training Loss: 0.005994216875405982\n",
      "Training Loss: 0.006967365376767703\n",
      "Training Loss: 0.007969251653412357\n",
      "Training Loss: 0.007795165771385655\n",
      "Training Loss: 0.007655127461766824\n",
      "Validation Loss: 0.004925006077091178\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006278889722889289\n",
      "Training Loss: 0.006121580093167722\n",
      "Training Loss: 0.005979189887875691\n",
      "Training Loss: 0.006952490255353041\n",
      "Training Loss: 0.00795385498786345\n",
      "Training Loss: 0.007781329421559349\n",
      "Training Loss: 0.007638853851240128\n",
      "Validation Loss: 0.004911072741433532\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006263694219524041\n",
      "Training Loss: 0.006107010947889648\n",
      "Training Loss: 0.005964373333845288\n",
      "Training Loss: 0.006940529196872376\n",
      "Training Loss: 0.007940496762748807\n",
      "Training Loss: 0.007768484797561541\n",
      "Training Loss: 0.007623096841853112\n",
      "Validation Loss: 0.004897329773167881\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006248361292527989\n",
      "Training Loss: 0.006092643641168251\n",
      "Training Loss: 0.005949558886932209\n",
      "Training Loss: 0.006924916275893338\n",
      "Training Loss: 0.007925451142946259\n",
      "Training Loss: 0.007754890057258308\n",
      "Training Loss: 0.007607512201648205\n",
      "Validation Loss: 0.00488350372447559\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006233267218340189\n",
      "Training Loss: 0.006078234467422589\n",
      "Training Loss: 0.0059349445765838026\n",
      "Training Loss: 0.00691210511547979\n",
      "Training Loss: 0.007911879837047308\n",
      "Training Loss: 0.007741811410523951\n",
      "Training Loss: 0.007592084993375465\n",
      "Validation Loss: 0.004869425737330418\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006218151242937893\n",
      "Training Loss: 0.0060639107809402045\n",
      "Training Loss: 0.005920516289770603\n",
      "Training Loss: 0.006898204993340187\n",
      "Training Loss: 0.00789759554550983\n",
      "Training Loss: 0.007728579099057243\n",
      "Training Loss: 0.007576740818331018\n",
      "Validation Loss: 0.0048554169584400534\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.0062032764765899626\n",
      "Training Loss: 0.0060497784893959764\n",
      "Training Loss: 0.0059063410339877005\n",
      "Training Loss: 0.006885494974558242\n",
      "Training Loss: 0.00788424635771662\n",
      "Training Loss: 0.007715514106675983\n",
      "Training Loss: 0.00756165266269818\n",
      "Validation Loss: 0.004841315999543399\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006188505952013657\n",
      "Training Loss: 0.006035863447468728\n",
      "Training Loss: 0.0058924316731281574\n",
      "Training Loss: 0.006870825714431703\n",
      "Training Loss: 0.007869562736013904\n",
      "Training Loss: 0.007702412890503183\n",
      "Training Loss: 0.007546760229161009\n",
      "Validation Loss: 0.004827487912056933\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.00617410664097406\n",
      "Training Loss: 0.006022157317493111\n",
      "Training Loss: 0.005878754501463845\n",
      "Training Loss: 0.006859286578837782\n",
      "Training Loss: 0.00785716194077395\n",
      "Training Loss: 0.007689386839047074\n",
      "Training Loss: 0.007532003946835175\n",
      "Validation Loss: 0.00481333338990389\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006159642843995243\n",
      "Training Loss: 0.006008625725517049\n",
      "Training Loss: 0.005865305557381362\n",
      "Training Loss: 0.006842301147989929\n",
      "Training Loss: 0.007840952271362767\n",
      "Training Loss: 0.007676436569308862\n",
      "Training Loss: 0.007517318108584732\n",
      "Validation Loss: 0.004799899012931668\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006145834596827627\n",
      "Training Loss: 0.005995315634063445\n",
      "Training Loss: 0.005852033062838018\n",
      "Training Loss: 0.006834551825304516\n",
      "Training Loss: 0.007831093556596898\n",
      "Training Loss: 0.007663459274917841\n",
      "Training Loss: 0.0075028418027795855\n",
      "Validation Loss: 0.0047854369938764236\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006131441686302423\n",
      "Training Loss: 0.005982045732089318\n",
      "Training Loss: 0.005838957346277311\n",
      "Training Loss: 0.006813571359380148\n",
      "Training Loss: 0.007812447816831991\n",
      "Training Loss: 0.007650607475079596\n",
      "Training Loss: 0.007488337910035625\n",
      "Validation Loss: 0.004772664558348221\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.00611830954789184\n",
      "Training Loss: 0.005969064317177981\n",
      "Training Loss: 0.005826238841982558\n",
      "Training Loss: 0.006809102288680151\n",
      "Training Loss: 0.007803315391065553\n",
      "Training Loss: 0.0076379584532696755\n",
      "Training Loss: 0.00747413286473602\n",
      "Validation Loss: 0.0047583909947704174\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006104317987337709\n",
      "Training Loss: 0.00595624863752164\n",
      "Training Loss: 0.005813446975080297\n",
      "Training Loss: 0.006787346206256189\n",
      "Training Loss: 0.007785688790609129\n",
      "Training Loss: 0.007625154602574184\n",
      "Training Loss: 0.007459986041067168\n",
      "Validation Loss: 0.004745565814817191\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006091428963700309\n",
      "Training Loss: 0.0059436907398048786\n",
      "Training Loss: 0.005801025021355599\n",
      "Training Loss: 0.0067816505802329625\n",
      "Training Loss: 0.007776218832004815\n",
      "Training Loss: 0.007612233002437279\n",
      "Training Loss: 0.007446085640694946\n",
      "Validation Loss: 0.00473144158332047\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006077686900971458\n",
      "Training Loss: 0.0059310041891876605\n",
      "Training Loss: 0.005788621277315542\n",
      "Training Loss: 0.006759821742307394\n",
      "Training Loss: 0.007758161250967533\n",
      "Training Loss: 0.007599786504870281\n",
      "Training Loss: 0.00743203807505779\n",
      "Validation Loss: 0.004718885580008005\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006065180277219042\n",
      "Training Loss: 0.005918457372463309\n",
      "Training Loss: 0.005776556625496596\n",
      "Training Loss: 0.006757544947322458\n",
      "Training Loss: 0.0077486569219036025\n",
      "Training Loss: 0.007587181630078703\n",
      "Training Loss: 0.007417952949181199\n",
      "Validation Loss: 0.004704737925922938\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006051753469510004\n",
      "Training Loss: 0.005906215101713314\n",
      "Training Loss: 0.005764246858889237\n",
      "Training Loss: 0.00673421879822854\n",
      "Training Loss: 0.007731833070283756\n",
      "Training Loss: 0.00757449263939634\n",
      "Training Loss: 0.007404151684604585\n",
      "Validation Loss: 0.004692037260658649\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006039389738580212\n",
      "Training Loss: 0.0058940573211293664\n",
      "Training Loss: 0.005752368478570133\n",
      "Training Loss: 0.006730332400766201\n",
      "Training Loss: 0.007722114970092662\n",
      "Training Loss: 0.007561601739143953\n",
      "Training Loss: 0.007390462731709704\n",
      "Validation Loss: 0.0046781157679709326\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006026120209135115\n",
      "Training Loss: 0.005881846369011328\n",
      "Training Loss: 0.0057404423342086374\n",
      "Training Loss: 0.0067075983923859895\n",
      "Training Loss: 0.007705068890936672\n",
      "Training Loss: 0.00754942151485011\n",
      "Training Loss: 0.007377054456155747\n",
      "Validation Loss: 0.004665691058032828\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006014102178160102\n",
      "Training Loss: 0.0058696199598489325\n",
      "Training Loss: 0.005728930657496676\n",
      "Training Loss: 0.006700640940107405\n",
      "Training Loss: 0.007692518396070227\n",
      "Training Loss: 0.0075371162593364715\n",
      "Training Loss: 0.007362823126604781\n",
      "Validation Loss: 0.004652278172549851\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.00600174002465792\n",
      "Training Loss: 0.0058577471738681195\n",
      "Training Loss: 0.005717070926912129\n",
      "Training Loss: 0.0066921447002096105\n",
      "Training Loss: 0.007681056344881654\n",
      "Training Loss: 0.007524003529688343\n",
      "Training Loss: 0.007349163260078058\n",
      "Validation Loss: 0.004638371850043005\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.0059889597911387685\n",
      "Training Loss: 0.005846114730811678\n",
      "Training Loss: 0.005705272310297005\n",
      "Training Loss: 0.006678778466884978\n",
      "Training Loss: 0.007669256167137064\n",
      "Training Loss: 0.007511071776971221\n",
      "Training Loss: 0.007335960210766644\n",
      "Validation Loss: 0.004624806778341504\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.005976132068317383\n",
      "Training Loss: 0.005834168584551662\n",
      "Training Loss: 0.005693976917536929\n",
      "Training Loss: 0.006656893862527795\n",
      "Training Loss: 0.007653131131082774\n",
      "Training Loss: 0.0074990691046696156\n",
      "Training Loss: 0.0073231588862836365\n",
      "Validation Loss: 0.004612488595835083\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.005964518702821806\n",
      "Training Loss: 0.0058221256115939465\n",
      "Training Loss: 0.005682833110331558\n",
      "Training Loss: 0.006648613007273525\n",
      "Training Loss: 0.007640545406611636\n",
      "Training Loss: 0.007487094590906054\n",
      "Training Loss: 0.007309530079364777\n",
      "Validation Loss: 0.004599560404018489\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.0059527515969239175\n",
      "Training Loss: 0.005810474315076135\n",
      "Training Loss: 0.005671631512232125\n",
      "Training Loss: 0.006633183779194951\n",
      "Training Loss: 0.007627851790748537\n",
      "Training Loss: 0.007474665228510275\n",
      "Training Loss: 0.007295505388174206\n",
      "Validation Loss: 0.004586659531433428\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.005940802050754428\n",
      "Training Loss: 0.005798942675464786\n",
      "Training Loss: 0.005660395087907091\n",
      "Training Loss: 0.0066211753088282425\n",
      "Training Loss: 0.007615383594529703\n",
      "Training Loss: 0.007461954868631437\n",
      "Training Loss: 0.007281864091055468\n",
      "Validation Loss: 0.004573352075938819\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.00592866568476893\n",
      "Training Loss: 0.0057873776176711545\n",
      "Training Loss: 0.0056489941122708845\n",
      "Training Loss: 0.006609122672234662\n",
      "Training Loss: 0.007602794763515703\n",
      "Training Loss: 0.007449232075596228\n",
      "Training Loss: 0.007268514671595767\n",
      "Validation Loss: 0.004559819261745763\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.0059165404771920295\n",
      "Training Loss: 0.005775786381564103\n",
      "Training Loss: 0.0056376435980200765\n",
      "Training Loss: 0.006597071507712826\n",
      "Training Loss: 0.0075902083260007205\n",
      "Training Loss: 0.007436440252931788\n",
      "Training Loss: 0.007255164039088413\n",
      "Validation Loss: 0.004546276502544932\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.005904422359308228\n",
      "Training Loss: 0.005764255734393373\n",
      "Training Loss: 0.0056263898080214855\n",
      "Training Loss: 0.006584945081267506\n",
      "Training Loss: 0.007577575135510415\n",
      "Training Loss: 0.007423623473150656\n",
      "Training Loss: 0.0072418939869385215\n",
      "Validation Loss: 0.004532735067949899\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.005892366502084769\n",
      "Training Loss: 0.005752766492078081\n",
      "Training Loss: 0.005615231603733264\n",
      "Training Loss: 0.006572872273391112\n",
      "Training Loss: 0.007564968432416208\n",
      "Training Loss: 0.00741075097117573\n",
      "Training Loss: 0.007228622464463114\n",
      "Validation Loss: 0.004519220378794027\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.005880349760409445\n",
      "Training Loss: 0.005741323932888918\n",
      "Training Loss: 0.005604154238826595\n",
      "Training Loss: 0.006560781374573708\n",
      "Training Loss: 0.007552368799224496\n",
      "Training Loss: 0.007397851912537589\n",
      "Training Loss: 0.007215396417304874\n",
      "Validation Loss: 0.004505711160286256\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0058683903777273375\n",
      "Training Loss: 0.005729913213872351\n",
      "Training Loss: 0.005593152081710287\n",
      "Training Loss: 0.006548758281278424\n",
      "Training Loss: 0.00753982005699072\n",
      "Training Loss: 0.007384904205100611\n",
      "Training Loss: 0.007202167673967779\n",
      "Validation Loss: 0.004492222498893068\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.005856469437712803\n",
      "Training Loss: 0.00571853629196994\n",
      "Training Loss: 0.005582223024102859\n",
      "Training Loss: 0.006536740288720466\n",
      "Training Loss: 0.00752730555832386\n",
      "Training Loss: 0.007371929463697597\n",
      "Training Loss: 0.0071889766620006415\n",
      "Validation Loss: 0.00447873669686947\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.00584459709469229\n",
      "Training Loss: 0.0057071840716525915\n",
      "Training Loss: 0.00557136294664815\n",
      "Training Loss: 0.006524787766975351\n",
      "Training Loss: 0.007514855318586342\n",
      "Training Loss: 0.007358914010692388\n",
      "Training Loss: 0.007175797931849957\n",
      "Validation Loss: 0.004465261822625688\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.005832760225748643\n",
      "Training Loss: 0.005695860270643607\n",
      "Training Loss: 0.005560576532152481\n",
      "Training Loss: 0.006512853971798904\n",
      "Training Loss: 0.007502459943061694\n",
      "Training Loss: 0.007345876096514985\n",
      "Training Loss: 0.007162659789901227\n",
      "Validation Loss: 0.0044517974881106495\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.005820968796033412\n",
      "Training Loss: 0.0056845606630668045\n",
      "Training Loss: 0.005549866832443513\n",
      "Training Loss: 0.006500980291166343\n",
      "Training Loss: 0.007490141356829554\n",
      "Training Loss: 0.007332809581421316\n",
      "Training Loss: 0.007149547419976443\n",
      "Validation Loss: 0.004438357763445835\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.005809217242058367\n",
      "Training Loss: 0.00567329196841456\n",
      "Training Loss: 0.00553923953382764\n",
      "Training Loss: 0.0064891372324200345\n",
      "Training Loss: 0.007477898662327789\n",
      "Training Loss: 0.007319730439921841\n",
      "Training Loss: 0.007136484298389405\n",
      "Validation Loss: 0.004424946221317189\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.005797515929443762\n",
      "Training Loss: 0.0056620556116104125\n",
      "Training Loss: 0.005528702324954793\n",
      "Training Loss: 0.006477354148519226\n",
      "Training Loss: 0.007465751566924155\n",
      "Training Loss: 0.007306641977047548\n",
      "Training Loss: 0.007123465993208811\n",
      "Validation Loss: 0.004411580223740124\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.005785867825616151\n",
      "Training Loss: 0.005650857979780995\n",
      "Training Loss: 0.005518264562706463\n",
      "Training Loss: 0.006465616602217779\n",
      "Training Loss: 0.0074537050997605546\n",
      "Training Loss: 0.007293557795928791\n",
      "Training Loss: 0.0071105101145803925\n",
      "Validation Loss: 0.004398269778944217\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.0057742817187681796\n",
      "Training Loss: 0.005639703733031638\n",
      "Training Loss: 0.005507933261687867\n",
      "Training Loss: 0.006453943970263935\n",
      "Training Loss: 0.007441776252235286\n",
      "Training Loss: 0.007280487451935187\n",
      "Training Loss: 0.00709761846694164\n",
      "Validation Loss: 0.004385033914948419\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.005762765223626048\n",
      "Training Loss: 0.005628601799835451\n",
      "Training Loss: 0.005497719857958145\n",
      "Training Loss: 0.006442333121667616\n",
      "Training Loss: 0.0074299753131344915\n",
      "Training Loss: 0.007267446239711717\n",
      "Training Loss: 0.007084807129576803\n",
      "Validation Loss: 0.004371882382734968\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.005751328483456746\n",
      "Training Loss: 0.005617558023659513\n",
      "Training Loss: 0.005487630875431932\n",
      "Training Loss: 0.006430796113563702\n",
      "Training Loss: 0.007418318268610165\n",
      "Training Loss: 0.007254447448067367\n",
      "Training Loss: 0.007072082923259586\n",
      "Validation Loss: 0.0043588326977956554\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.005739981783553958\n",
      "Training Loss: 0.0056065799668431286\n",
      "Training Loss: 0.005477674781577662\n",
      "Training Loss: 0.006419337625266053\n",
      "Training Loss: 0.0074068164336495105\n",
      "Training Loss: 0.007241508376318961\n",
      "Training Loss: 0.0070594583544880156\n",
      "Validation Loss: 0.004345898578389921\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005728735664160922\n",
      "Training Loss: 0.005595675523509271\n",
      "Training Loss: 0.005467859284253791\n",
      "Training Loss: 0.006407968545681797\n",
      "Training Loss: 0.0073954847222194075\n",
      "Training Loss: 0.007228644135175273\n",
      "Training Loss: 0.007046941950684413\n",
      "Validation Loss: 0.004333094455657548\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.005717602712102234\n",
      "Training Loss: 0.005584853937034495\n",
      "Training Loss: 0.005458192140213214\n",
      "Training Loss: 0.00639669721480459\n",
      "Training Loss: 0.0073843355954159055\n",
      "Training Loss: 0.007215873864479363\n",
      "Training Loss: 0.007034548205556348\n",
      "Validation Loss: 0.004320432201345288\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.005706591551424936\n",
      "Training Loss: 0.005574121120152995\n",
      "Training Loss: 0.005448677645181306\n",
      "Training Loss: 0.006385531806736253\n",
      "Training Loss: 0.007373380740173161\n",
      "Training Loss: 0.007203215828631073\n",
      "Training Loss: 0.007022284935228527\n",
      "Validation Loss: 0.004307925622223105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005695717777125537\n",
      "Training Loss: 0.005563488057814539\n",
      "Training Loss: 0.005439323319587856\n",
      "Training Loss: 0.006374482793617063\n",
      "Training Loss: 0.007362631586147472\n",
      "Training Loss: 0.007190685198875144\n",
      "Training Loss: 0.0070101628068368884\n",
      "Validation Loss: 0.004295585919886632\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.00568498887354508\n",
      "Training Loss: 0.0055529598996508864\n",
      "Training Loss: 0.005430129559827037\n",
      "Training Loss: 0.006363555951975286\n",
      "Training Loss: 0.007352095657261089\n",
      "Training Loss: 0.0071783005504403264\n",
      "Training Loss: 0.0069981929566711185\n",
      "Validation Loss: 0.004283419995104776\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005674415382090956\n",
      "Training Loss: 0.005542544911731966\n",
      "Training Loss: 0.0054210998828057196\n",
      "Training Loss: 0.006352763170725666\n",
      "Training Loss: 0.007341781439608894\n",
      "Training Loss: 0.007166077273432166\n",
      "Training Loss: 0.006986381686292589\n",
      "Validation Loss: 0.004271438974043013\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.005664005593862384\n",
      "Training Loss: 0.005532249243115075\n",
      "Training Loss: 0.005412236768752337\n",
      "Training Loss: 0.006342107531381771\n",
      "Training Loss: 0.0073316927783889695\n",
      "Training Loss: 0.007154029660159722\n",
      "Training Loss: 0.0069747366930823776\n",
      "Validation Loss: 0.004259647729140375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.005653767843032256\n",
      "Training Loss: 0.0055220790475141255\n",
      "Training Loss: 0.005403540770639665\n",
      "Training Loss: 0.0063315982243511825\n",
      "Training Loss: 0.007321834312169813\n",
      "Training Loss: 0.007142170360311866\n",
      "Training Loss: 0.006963263895595446\n",
      "Validation Loss: 0.004248053185582468\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005643707475392148\n",
      "Training Loss: 0.005512037869775668\n",
      "Training Loss: 0.005395008446648717\n",
      "Training Loss: 0.006321240272955038\n",
      "Training Loss: 0.007312206516508013\n",
      "Training Loss: 0.00713051063939929\n",
      "Training Loss: 0.006951970766531304\n",
      "Validation Loss: 0.004236656210162024\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005633827432757243\n",
      "Training Loss: 0.005502129865344613\n",
      "Training Loss: 0.005386639759526588\n",
      "Training Loss: 0.006311036365223117\n",
      "Training Loss: 0.007302808179520071\n",
      "Training Loss: 0.007119059233227745\n",
      "Training Loss: 0.006940857816953212\n",
      "Validation Loss: 0.00422546224562235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005624132330995053\n",
      "Training Loss: 0.0054923562851035964\n",
      "Training Loss: 0.005378430268610828\n",
      "Training Loss: 0.006300991720054299\n",
      "Training Loss: 0.007293637366383336\n",
      "Training Loss: 0.007107822665711865\n",
      "Training Loss: 0.006929929185425863\n",
      "Validation Loss: 0.004214471745416382\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005614621500717476\n",
      "Training Loss: 0.005482719922438264\n",
      "Training Loss: 0.005370377267245203\n",
      "Training Loss: 0.006291105762938969\n",
      "Training Loss: 0.007284690052620136\n",
      "Training Loss: 0.007096805509645492\n",
      "Training Loss: 0.006919185986043885\n",
      "Validation Loss: 0.004203682847334986\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005605293815024197\n",
      "Training Loss: 0.005473219047416933\n",
      "Training Loss: 0.005362475352594629\n",
      "Training Loss: 0.006281379589345306\n",
      "Training Loss: 0.007275958247482776\n",
      "Training Loss: 0.007086009751074016\n",
      "Training Loss: 0.006908627009252087\n",
      "Validation Loss: 0.00419309920559843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005596148151671514\n",
      "Training Loss: 0.005463855885900557\n",
      "Training Loss: 0.005354720510658808\n",
      "Training Loss: 0.006271814501960762\n",
      "Training Loss: 0.007267437180853448\n",
      "Training Loss: 0.007075436777668074\n",
      "Training Loss: 0.006898251360980794\n",
      "Validation Loss: 0.004182712367233517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.0055871789093362165\n",
      "Training Loss: 0.0054546244244556875\n",
      "Training Loss: 0.005347105505643412\n",
      "Training Loss: 0.006262408221373334\n",
      "Training Loss: 0.007259118039510213\n",
      "Training Loss: 0.007065084538189694\n",
      "Training Loss: 0.006888056683819741\n",
      "Validation Loss: 0.0041725270525109345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005578384331311099\n",
      "Training Loss: 0.005445525487884879\n",
      "Training Loss: 0.005339625939377583\n",
      "Training Loss: 0.00625315863697324\n",
      "Training Loss: 0.007250990535831079\n",
      "Training Loss: 0.00705494906171225\n",
      "Training Loss: 0.006878040295559913\n",
      "Validation Loss: 0.004162535807797999\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005569755950127728\n",
      "Training Loss: 0.005436554186162539\n",
      "Training Loss: 0.00533227744337637\n",
      "Training Loss: 0.006244063705089502\n",
      "Training Loss: 0.007243047142401338\n",
      "Training Loss: 0.0070450278010684994\n",
      "Training Loss: 0.006868197593139485\n",
      "Validation Loss: 0.004152739498579407\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005561288883909583\n",
      "Training Loss: 0.005427707800990902\n",
      "Training Loss: 0.005325050498358905\n",
      "Training Loss: 0.006235119858756661\n",
      "Training Loss: 0.007235276467399672\n",
      "Training Loss: 0.007035315938992426\n",
      "Training Loss: 0.006858526192372665\n",
      "Validation Loss: 0.004143131716780309\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005552975591272115\n",
      "Training Loss: 0.005418983103591017\n",
      "Training Loss: 0.0053179424372501675\n",
      "Training Loss: 0.006226322795264423\n",
      "Training Loss: 0.007227668062550947\n",
      "Training Loss: 0.00702580651268363\n",
      "Training Loss: 0.006849018201464787\n",
      "Validation Loss: 0.004133712049932934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.0055448103818343955\n",
      "Training Loss: 0.005410374515340663\n",
      "Training Loss: 0.00531094576290343\n",
      "Training Loss: 0.0062176711088977755\n",
      "Training Loss: 0.0072202139219734816\n",
      "Training Loss: 0.007016493573319167\n",
      "Training Loss: 0.006839672507485375\n",
      "Validation Loss: 0.004124473540150745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005536781434202566\n",
      "Training Loss: 0.005401877581025474\n",
      "Training Loss: 0.005304053835570813\n",
      "Training Loss: 0.006209155834512785\n",
      "Training Loss: 0.007212900829035789\n",
      "Training Loss: 0.007007368721533566\n",
      "Training Loss: 0.00683048099395819\n",
      "Validation Loss: 0.004115409128392317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005528882291982882\n",
      "Training Loss: 0.005393486720276996\n",
      "Training Loss: 0.0052972618787316605\n",
      "Training Loss: 0.006200776245095767\n",
      "Training Loss: 0.007205719732446596\n",
      "Training Loss: 0.0069984248781111095\n",
      "Training Loss: 0.006821438724873587\n",
      "Validation Loss: 0.004106521309084827\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005521106900414452\n",
      "Training Loss: 0.005385198598378338\n",
      "Training Loss: 0.005290562543086708\n",
      "Training Loss: 0.006192525360966102\n",
      "Training Loss: 0.007198660804424435\n",
      "Training Loss: 0.006989655138459057\n",
      "Training Loss: 0.006812541645485908\n",
      "Validation Loss: 0.004097793386439873\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005513443039963022\n",
      "Training Loss: 0.005377006357884966\n",
      "Training Loss: 0.005283949722070247\n",
      "Training Loss: 0.006184397325851024\n",
      "Training Loss: 0.00719171303207986\n",
      "Training Loss: 0.006981050738831982\n",
      "Training Loss: 0.0068037832889240235\n",
      "Validation Loss: 0.004089225387054213\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005505882920115255\n",
      "Training Loss: 0.005368904391652904\n",
      "Training Loss: 0.005277415166492574\n",
      "Training Loss: 0.006176386556471698\n",
      "Training Loss: 0.007184868750628084\n",
      "Training Loss: 0.006972603952744976\n",
      "Training Loss: 0.0067951580090448264\n",
      "Validation Loss: 0.004080813566453002\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005498418635106646\n",
      "Training Loss: 0.0053608892054762694\n",
      "Training Loss: 0.005270955950836651\n",
      "Training Loss: 0.006168487567920238\n",
      "Training Loss: 0.007178115748101846\n",
      "Training Loss: 0.006964305797591806\n",
      "Training Loss: 0.0067866613843943924\n",
      "Validation Loss: 0.004072546022368616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005491040411870926\n",
      "Training Loss: 0.0053529544529737905\n",
      "Training Loss: 0.0052645634365035224\n",
      "Training Loss: 0.006160693921847269\n",
      "Training Loss: 0.00717144652386196\n",
      "Training Loss: 0.006956149252364412\n",
      "Training Loss: 0.006778286084299907\n",
      "Validation Loss: 0.004064418459206485\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005483741200296208\n",
      "Training Loss: 0.005345094426884316\n",
      "Training Loss: 0.005258232392952778\n",
      "Training Loss: 0.006153000861522742\n",
      "Training Loss: 0.007164852615678683\n",
      "Training Loss: 0.006948125980561599\n",
      "Training Loss: 0.006770028285682201\n",
      "Validation Loss: 0.004056422702169346\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005476511416491121\n",
      "Training Loss: 0.005337306030560285\n",
      "Training Loss: 0.005251954583218321\n",
      "Training Loss: 0.0061453994188923385\n",
      "Training Loss: 0.00715832355665043\n",
      "Training Loss: 0.006940227141603828\n",
      "Training Loss: 0.006761882572900504\n",
      "Validation Loss: 0.004048552852008868\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005469344937009737\n",
      "Training Loss: 0.005329583837883547\n",
      "Training Loss: 0.005245728107402101\n",
      "Training Loss: 0.006137886695214547\n",
      "Training Loss: 0.007151852671522647\n",
      "Training Loss: 0.006932447561994195\n",
      "Training Loss: 0.0067538440378848465\n",
      "Validation Loss: 0.00404080073741589\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005462233497528359\n",
      "Training Loss: 0.005321924083982594\n",
      "Training Loss: 0.005239543843781575\n",
      "Training Loss: 0.006130453979130835\n",
      "Training Loss: 0.00714543359237723\n",
      "Training Loss: 0.006924777909880504\n",
      "Training Loss: 0.006745909033343196\n",
      "Validation Loss: 0.004033157840451359\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005455170723726041\n",
      "Training Loss: 0.005314321837504394\n",
      "Training Loss: 0.005233397429110482\n",
      "Training Loss: 0.006123096916126087\n",
      "Training Loss: 0.007139057639287785\n",
      "Training Loss: 0.0069172118382994086\n",
      "Training Loss: 0.006738071085419506\n",
      "Validation Loss: 0.004025617989795178\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005448149068979546\n",
      "Training Loss: 0.005306773884803988\n",
      "Training Loss: 0.00522728304611519\n",
      "Training Loss: 0.006115807555615902\n",
      "Training Loss: 0.007132718492066488\n",
      "Training Loss: 0.006909743112046272\n",
      "Training Loss: 0.006730328145204112\n",
      "Validation Loss: 0.004018170878985262\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005441163204959593\n",
      "Training Loss: 0.005299277516896837\n",
      "Training Loss: 0.005221196039929055\n",
      "Training Loss: 0.006108581364387646\n",
      "Training Loss: 0.007126412154175341\n",
      "Training Loss: 0.0069023652805481105\n",
      "Training Loss: 0.006722676432691515\n",
      "Validation Loss: 0.004010811006852639\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005434207938378677\n",
      "Training Loss: 0.005291828215704299\n",
      "Training Loss: 0.005215132183511741\n",
      "Training Loss: 0.006101412922143936\n",
      "Training Loss: 0.00712013119715266\n",
      "Training Loss: 0.006895071110920981\n",
      "Training Loss: 0.006715113451937213\n",
      "Validation Loss: 0.004003528942884214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005427278626011684\n",
      "Training Loss: 0.005284426260041073\n",
      "Training Loss: 0.005209086185786873\n",
      "Training Loss: 0.006094295876682736\n",
      "Training Loss: 0.007113872522022575\n",
      "Training Loss: 0.00688785646460019\n",
      "Training Loss: 0.0067076350178103894\n",
      "Validation Loss: 0.003996321810705915\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005420372785301879\n",
      "Training Loss: 0.005277069167350419\n",
      "Training Loss: 0.005203056633472442\n",
      "Training Loss: 0.006087226212257519\n",
      "Training Loss: 0.007107632624683902\n",
      "Training Loss: 0.006880717265885323\n",
      "Training Loss: 0.006700240381760523\n",
      "Validation Loss: 0.00398918099645595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005413487897021696\n",
      "Training Loss: 0.005269757813657634\n",
      "Training Loss: 0.005197041955543682\n",
      "Training Loss: 0.006080198729760014\n",
      "Training Loss: 0.007101407902082428\n",
      "Training Loss: 0.006873646787134931\n",
      "Training Loss: 0.006692926718387753\n",
      "Validation Loss: 0.003982100711197136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005406620560679584\n",
      "Training Loss: 0.0052624918380752206\n",
      "Training Loss: 0.0051910396671155466\n",
      "Training Loss: 0.006073211396578699\n",
      "Training Loss: 0.007095198959577828\n",
      "Training Loss: 0.006866644365945831\n",
      "Training Loss: 0.006685694493353367\n",
      "Validation Loss: 0.003975077875809611\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005399772538221441\n",
      "Training Loss: 0.005255271652713418\n",
      "Training Loss: 0.005185049097053707\n",
      "Training Loss: 0.0060662617487832905\n",
      "Training Loss: 0.007089004259323701\n",
      "Training Loss: 0.006859707318944856\n",
      "Training Loss: 0.006678542854497209\n",
      "Validation Loss: 0.0039681086115267005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.0053929419460473586\n",
      "Training Loss: 0.005248098726733588\n",
      "Training Loss: 0.005179069298319519\n",
      "Training Loss: 0.006059345681569539\n",
      "Training Loss: 0.007082822176162153\n",
      "Training Loss: 0.006852832045406103\n",
      "Training Loss: 0.006671470454894007\n",
      "Validation Loss: 0.003961194008815517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005386133312131278\n",
      "Training Loss: 0.00524097747169435\n",
      "Training Loss: 0.005173104670830071\n",
      "Training Loss: 0.006052464579697698\n",
      "Training Loss: 0.007076653629774228\n",
      "Training Loss: 0.0068460178619716315\n",
      "Training Loss: 0.006664477043086663\n",
      "Validation Loss: 0.003954329261946935\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005379347219131887\n",
      "Training Loss: 0.005233907438232563\n",
      "Training Loss: 0.005167151962523348\n",
      "Training Loss: 0.006045617456547916\n",
      "Training Loss: 0.007070500642294064\n",
      "Training Loss: 0.0068392648070584984\n",
      "Training Loss: 0.006657564835622906\n",
      "Validation Loss: 0.0039475156602097414\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005372585995355621\n",
      "Training Loss: 0.005226893780636601\n",
      "Training Loss: 0.00516121580847539\n",
      "Training Loss: 0.00603880463517271\n",
      "Training Loss: 0.007064366214908659\n",
      "Training Loss: 0.006832572940038517\n",
      "Training Loss: 0.006650732394773513\n",
      "Validation Loss: 0.003940750433616898\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005365850258385763\n",
      "Training Loss: 0.0052199356129858645\n",
      "Training Loss: 0.005155295461881906\n",
      "Training Loss: 0.006032026753528044\n",
      "Training Loss: 0.007058249887777492\n",
      "Training Loss: 0.006825940993148833\n",
      "Training Loss: 0.006643980070948601\n",
      "Validation Loss: 0.003934037529389036\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.00535914596519433\n",
      "Training Loss: 0.005213038170477375\n",
      "Training Loss: 0.005149396830238402\n",
      "Training Loss: 0.006025284146307968\n",
      "Training Loss: 0.00705215418478474\n",
      "Training Loss: 0.006819367978023365\n",
      "Training Loss: 0.00663730721687898\n",
      "Validation Loss: 0.003927377324676916\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.00535247347143013\n",
      "Training Loss: 0.005206201141118072\n",
      "Training Loss: 0.005143517356482334\n",
      "Training Loss: 0.006018578392686322\n",
      "Training Loss: 0.007046082373708486\n",
      "Training Loss: 0.006812855770112947\n",
      "Training Loss: 0.006630714405328035\n",
      "Validation Loss: 0.003920769979061091\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005345835676998831\n",
      "Training Loss: 0.005199425825849175\n",
      "Training Loss: 0.0051376603246899325\n",
      "Training Loss: 0.0060119086428312585\n",
      "Training Loss: 0.007040032958611846\n",
      "Training Loss: 0.00680640266276896\n",
      "Training Loss: 0.006624197986675426\n",
      "Validation Loss: 0.003914215140243511\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005339233948616311\n",
      "Training Loss: 0.005192713135620579\n",
      "Training Loss: 0.0051318230869947\n",
      "Training Loss: 0.006005278234370053\n",
      "Training Loss: 0.007034010310890153\n",
      "Training Loss: 0.006800009859725833\n",
      "Training Loss: 0.006617760396329686\n",
      "Validation Loss: 0.003907713940548484\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005332668993505649\n",
      "Training Loss: 0.005186062407447025\n",
      "Training Loss: 0.005126010011299514\n",
      "Training Loss: 0.005998686482198537\n",
      "Training Loss: 0.007028013271046803\n",
      "Training Loss: 0.006793674514628947\n",
      "Training Loss: 0.006611394757637754\n",
      "Validation Loss: 0.003901273470846343\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005326144195860252\n",
      "Training Loss: 0.005179475178010762\n",
      "Training Loss: 0.005120221003890038\n",
      "Training Loss: 0.005992132671526634\n",
      "Training Loss: 0.007022041765740141\n",
      "Training Loss: 0.006787395732244477\n",
      "Training Loss: 0.006605102872708812\n",
      "Validation Loss: 0.00389488588552922\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005319657000945881\n",
      "Training Loss: 0.005172947343089618\n",
      "Training Loss: 0.005114453211426735\n",
      "Training Loss: 0.0059856168797705325\n",
      "Training Loss: 0.00701609596144408\n",
      "Training Loss: 0.0067811730701942\n",
      "Training Loss: 0.006598879840457812\n",
      "Validation Loss: 0.0038885561311582614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005313207469880581\n",
      "Training Loss: 0.005166478127939626\n",
      "Training Loss: 0.005108705498860218\n",
      "Training Loss: 0.005979136740788818\n",
      "Training Loss: 0.0070101753470953555\n",
      "Training Loss: 0.006775002655340359\n",
      "Training Loss: 0.006592723432695493\n",
      "Validation Loss: 0.0038822803939326427\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005306794309290126\n",
      "Training Loss: 0.005160064848023466\n",
      "Training Loss: 0.005102978501236066\n",
      "Training Loss: 0.005972693392541259\n",
      "Training Loss: 0.007004278971580788\n",
      "Training Loss: 0.006768882597098127\n",
      "Training Loss: 0.0065866307995747775\n",
      "Validation Loss: 0.003876056939830885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005300416648387909\n",
      "Training Loss: 0.005153704036492854\n",
      "Training Loss: 0.005097266620141454\n",
      "Training Loss: 0.00596628354396671\n",
      "Training Loss: 0.006998404088662937\n",
      "Training Loss: 0.006762810761574656\n",
      "Training Loss: 0.006580597825814038\n",
      "Validation Loss: 0.003869885527302096\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.0052940729114925485\n",
      "Training Loss: 0.005147393986117094\n",
      "Training Loss: 0.0050915704562794414\n",
      "Training Loss: 0.005959904213086702\n",
      "Training Loss: 0.006992547940462827\n",
      "Training Loss: 0.006756782219745219\n",
      "Training Loss: 0.006574620750034228\n",
      "Validation Loss: 0.003863763378075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.0052877594286110255\n",
      "Training Loss: 0.005141130595002323\n",
      "Training Loss: 0.005085886658052914\n",
      "Training Loss: 0.005953554231673479\n",
      "Training Loss: 0.006986711007775739\n",
      "Training Loss: 0.006750795633997768\n",
      "Training Loss: 0.006568696984322742\n",
      "Validation Loss: 0.003857688113265716\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.00528147478820756\n",
      "Training Loss: 0.005134910071501508\n",
      "Training Loss: 0.005080213142791763\n",
      "Training Loss: 0.005947230943129398\n",
      "Training Loss: 0.006980888773687184\n",
      "Training Loss: 0.006744846362853423\n",
      "Training Loss: 0.0065628227300476285\n",
      "Validation Loss: 0.003851655343536999\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005275215956498869\n",
      "Training Loss: 0.005128728948184289\n",
      "Training Loss: 0.005074545738752931\n",
      "Training Loss: 0.005940929907956161\n",
      "Training Loss: 0.00697507816250436\n",
      "Training Loss: 0.006738931214204058\n",
      "Training Loss: 0.006556993015110492\n",
      "Validation Loss: 0.0038456597996784953\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005268978229141794\n",
      "Training Loss: 0.005122582215699367\n",
      "Training Loss: 0.00506888039293699\n",
      "Training Loss: 0.0059346484043635425\n",
      "Training Loss: 0.006969276464078575\n",
      "Training Loss: 0.006733045403379947\n",
      "Training Loss: 0.0065512054064311084\n",
      "Validation Loss: 0.0038396984884647954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005262759344768711\n",
      "Training Loss: 0.005116467087646015\n",
      "Training Loss: 0.005063215535483323\n",
      "Training Loss: 0.005928381169796921\n",
      "Training Loss: 0.006963478873949498\n",
      "Training Loss: 0.0067271847778465595\n",
      "Training Loss: 0.006545454190345481\n",
      "Validation Loss: 0.0038337693248844044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.0052565555309411136\n",
      "Training Loss: 0.005110378641984425\n",
      "Training Loss: 0.005057546216994524\n",
      "Training Loss: 0.005922126623918302\n",
      "Training Loss: 0.006957683364162221\n",
      "Training Loss: 0.006721345758996904\n",
      "Training Loss: 0.00653973642271012\n",
      "Validation Loss: 0.0038278653030894166\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005250363767845556\n",
      "Training Loss: 0.005104313133633695\n",
      "Training Loss: 0.005051869381568395\n",
      "Training Loss: 0.0059158814459806305\n",
      "Training Loss: 0.006951886173337698\n",
      "Training Loss: 0.006715523297898472\n",
      "Training Loss: 0.006534048749599606\n",
      "Validation Loss: 0.00382198078979909\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.0052441780141089115\n",
      "Training Loss: 0.005098265712149441\n",
      "Training Loss: 0.00504618075909093\n",
      "Training Loss: 0.005909636564319953\n",
      "Training Loss: 0.006946081938222051\n",
      "Training Loss: 0.006709712543524802\n",
      "Training Loss: 0.006528385524870828\n",
      "Validation Loss: 0.0038161101591448936\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005237995287170633\n",
      "Training Loss: 0.00509223228902556\n",
      "Training Loss: 0.005040477108559571\n",
      "Training Loss: 0.005903390223393217\n",
      "Training Loss: 0.006940264417789876\n",
      "Training Loss: 0.006703909478383139\n",
      "Training Loss: 0.006522741422522813\n",
      "Validation Loss: 0.003810250212975572\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0052318110404303295\n",
      "Training Loss: 0.005086209400324151\n",
      "Training Loss: 0.00503475360921584\n",
      "Training Loss: 0.005897137619904243\n",
      "Training Loss: 0.006934430554974824\n",
      "Training Loss: 0.006698107118718326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [14:32<33:55, 290.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.006517111884895712\n",
      "Validation Loss: 0.0038043923714383043\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.3496429241448641\n",
      "Training Loss: 0.23628470659255982\n",
      "Training Loss: 0.15472014095634223\n",
      "Training Loss: 0.10280798483639955\n",
      "Training Loss: 0.07711144067347049\n",
      "Training Loss: 0.06644655827432872\n",
      "Training Loss: 0.06228389635682106\n",
      "Validation Loss: 0.05912454692081789\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.060105521082878116\n",
      "Training Loss: 0.057653724793344734\n",
      "Training Loss: 0.05646635100245476\n",
      "Training Loss: 0.05569142369553447\n",
      "Training Loss: 0.05307673780247569\n",
      "Training Loss: 0.05256976637989283\n",
      "Training Loss: 0.04983686039224267\n",
      "Validation Loss: 0.04760685220556089\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.04750325556844473\n",
      "Training Loss: 0.04473854023963213\n",
      "Training Loss: 0.04284152553416789\n",
      "Training Loss: 0.04172790175303817\n",
      "Training Loss: 0.03911690212786198\n",
      "Training Loss: 0.038143388498574494\n",
      "Training Loss: 0.03549833364784718\n",
      "Validation Loss: 0.03417054177279329\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.03336429776623845\n",
      "Training Loss: 0.031380216851830484\n",
      "Training Loss: 0.029882222567684948\n",
      "Training Loss: 0.029612965835258364\n",
      "Training Loss: 0.028106521526351572\n",
      "Training Loss: 0.02760578827932477\n",
      "Training Loss: 0.02588992225471884\n",
      "Validation Loss: 0.024990322884548916\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.024438131456263362\n",
      "Training Loss: 0.02332132083363831\n",
      "Training Loss: 0.022307867691852154\n",
      "Training Loss: 0.02253323420882225\n",
      "Training Loss: 0.021751259793527423\n",
      "Training Loss: 0.021353640290908516\n",
      "Training Loss: 0.020156623856164516\n",
      "Validation Loss: 0.019064670974521268\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.01892326251603663\n",
      "Training Loss: 0.018131664483807982\n",
      "Training Loss: 0.017398715284653008\n",
      "Training Loss: 0.017843747141305356\n",
      "Training Loss: 0.017591118775308133\n",
      "Training Loss: 0.01720379040343687\n",
      "Training Loss: 0.016333375107496977\n",
      "Validation Loss: 0.014957602186414521\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.015215890540275723\n",
      "Training Loss: 0.014566408661194146\n",
      "Training Loss: 0.014047872906085104\n",
      "Training Loss: 0.0146397347364109\n",
      "Training Loss: 0.014796755395364016\n",
      "Training Loss: 0.014405190744437277\n",
      "Training Loss: 0.013743621893227101\n",
      "Validation Loss: 0.012050740816717625\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.012669022674672306\n",
      "Training Loss: 0.012081674037035554\n",
      "Training Loss: 0.01172616231138818\n",
      "Training Loss: 0.012432198202004656\n",
      "Training Loss: 0.01290246898541227\n",
      "Training Loss: 0.012508912917692214\n",
      "Training Loss: 0.011991398525424302\n",
      "Validation Loss: 0.009958686363754257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.01089032772812061\n",
      "Training Loss: 0.010333425414282829\n",
      "Training Loss: 0.010099272114457563\n",
      "Training Loss: 0.01090481532504782\n",
      "Training Loss: 0.01161465711891651\n",
      "Training Loss: 0.011229116407921538\n",
      "Training Loss: 0.010824046701891347\n",
      "Validation Loss: 0.008457038726845986\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.009648664722917602\n",
      "Training Loss: 0.00911702299490571\n",
      "Training Loss: 0.008970014044316486\n",
      "Training Loss: 0.009860774029511958\n",
      "Training Loss: 0.01073164602741599\n",
      "Training Loss: 0.010361852961359545\n",
      "Training Loss: 0.0100406736030709\n",
      "Validation Loss: 0.007384421451240499\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.008771407313179224\n",
      "Training Loss: 0.008260388103080914\n",
      "Training Loss: 0.00815738650271669\n",
      "Training Loss: 0.009086930081248284\n",
      "Training Loss: 0.01002434552880004\n",
      "Training Loss: 0.009680475366767496\n",
      "Training Loss: 0.009438130019698293\n",
      "Validation Loss: 0.006602571529862977\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.00811671172035858\n",
      "Training Loss: 0.007643346778349951\n",
      "Training Loss: 0.007574852132238448\n",
      "Training Loss: 0.008531209717039018\n",
      "Training Loss: 0.009513123010983691\n",
      "Training Loss: 0.009234007517807185\n",
      "Training Loss: 0.009080172625835985\n",
      "Validation Loss: 0.006176971613293451\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.007741824956610799\n",
      "Training Loss: 0.007317992830649018\n",
      "Training Loss: 0.007266851153690368\n",
      "Training Loss: 0.00824379847443197\n",
      "Training Loss: 0.00925978180137463\n",
      "Training Loss: 0.009021345657529309\n",
      "Training Loss: 0.00891526272520423\n",
      "Validation Loss: 0.005986491902748799\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.007557574865641073\n",
      "Training Loss: 0.007164263925515116\n",
      "Training Loss: 0.0071132314461283385\n",
      "Training Loss: 0.008100854904623702\n",
      "Training Loss: 0.009137492818990723\n",
      "Training Loss: 0.0089142599736806\n",
      "Training Loss: 0.008829544732579962\n",
      "Validation Loss: 0.0058881159373018405\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.0074559521221090105\n",
      "Training Loss: 0.007077560430625454\n",
      "Training Loss: 0.007024309736443683\n",
      "Training Loss: 0.008018277666997165\n",
      "Training Loss: 0.009066714474465698\n",
      "Training Loss: 0.008848098131129517\n",
      "Training Loss: 0.008774251841241493\n",
      "Validation Loss: 0.005822884681903189\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.007388305563945323\n",
      "Training Loss: 0.007017564055277034\n",
      "Training Loss: 0.0069633484247606245\n",
      "Training Loss: 0.007961490100133233\n",
      "Training Loss: 0.009016626892844215\n",
      "Training Loss: 0.00879881274420768\n",
      "Training Loss: 0.008731949704233556\n",
      "Validation Loss: 0.005771303906939114\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.007336161320563406\n",
      "Training Loss: 0.006970352848293259\n",
      "Training Loss: 0.006916355441790074\n",
      "Training Loss: 0.007917255130014383\n",
      "Training Loss: 0.008976210694527254\n",
      "Training Loss: 0.008757772708777338\n",
      "Training Loss: 0.008696311669191346\n",
      "Validation Loss: 0.005727153185106228\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.007292517852038145\n",
      "Training Loss: 0.006930769975297153\n",
      "Training Loss: 0.006877618802245706\n",
      "Training Loss: 0.0078802122437628\n",
      "Training Loss: 0.008941275605466217\n",
      "Training Loss: 0.008721601808210834\n",
      "Training Loss: 0.008664725024718791\n",
      "Validation Loss: 0.005688167502405994\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.007254400866804645\n",
      "Training Loss: 0.00689649804495275\n",
      "Training Loss: 0.00684441962162964\n",
      "Training Loss: 0.007847880871850066\n",
      "Training Loss: 0.008909962475299835\n",
      "Training Loss: 0.008688743367092683\n",
      "Training Loss: 0.00863590259803459\n",
      "Validation Loss: 0.005653235270408042\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.007220295541919768\n",
      "Training Loss: 0.0068662275583483276\n",
      "Training Loss: 0.006815237009432167\n",
      "Training Loss: 0.007818917697295546\n",
      "Training Loss: 0.008881281652720644\n",
      "Training Loss: 0.008658351292833686\n",
      "Training Loss: 0.008609105235664175\n",
      "Validation Loss: 0.005621666329379162\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.0071893123467452824\n",
      "Training Loss: 0.006839103483362124\n",
      "Training Loss: 0.006789117485750467\n",
      "Training Loss: 0.007792512251180597\n",
      "Training Loss: 0.008854630020214244\n",
      "Training Loss: 0.008629908204311506\n",
      "Training Loss: 0.008583873390452936\n",
      "Validation Loss: 0.005592933569054646\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0071608553524129094\n",
      "Training Loss: 0.006814513572026044\n",
      "Training Loss: 0.0067654168209992345\n",
      "Training Loss: 0.007768132742494344\n",
      "Training Loss: 0.008829607501393184\n",
      "Training Loss: 0.008603077006991953\n",
      "Training Loss: 0.008559908090392128\n",
      "Validation Loss: 0.005566622016337537\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.007134513427736237\n",
      "Training Loss: 0.006792005868628621\n",
      "Training Loss: 0.006743679380742833\n",
      "Training Loss: 0.007745417187688872\n",
      "Training Loss: 0.008805943843908608\n",
      "Training Loss: 0.00857763733365573\n",
      "Training Loss: 0.0085370162257459\n",
      "Validation Loss: 0.005542408935690808\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.007109985915012658\n",
      "Training Loss: 0.006771242727991194\n",
      "Training Loss: 0.006723578765522689\n",
      "Training Loss: 0.00772411294397898\n",
      "Training Loss: 0.008783452278003097\n",
      "Training Loss: 0.00855343782575801\n",
      "Training Loss: 0.008515069803688675\n",
      "Validation Loss: 0.00552003381229686\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.007087049102410674\n",
      "Training Loss: 0.0067519661539699885\n",
      "Training Loss: 0.006704872787231579\n",
      "Training Loss: 0.0077040416968520735\n",
      "Training Loss: 0.008762004990130664\n",
      "Training Loss: 0.00853037576074712\n",
      "Training Loss: 0.008493985272943974\n",
      "Validation Loss: 0.005499270373139154\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007065525489160791\n",
      "Training Loss: 0.006733971096109599\n",
      "Training Loss: 0.00668737008003518\n",
      "Training Loss: 0.007685067749698646\n",
      "Training Loss: 0.008741505313664674\n",
      "Training Loss: 0.008508377100806683\n",
      "Training Loss: 0.008473706275690347\n",
      "Validation Loss: 0.0054799463653985815\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.007045280298916623\n",
      "Training Loss: 0.0067170981073286385\n",
      "Training Loss: 0.006670929233077914\n",
      "Training Loss: 0.007667091783951037\n",
      "Training Loss: 0.008721885790582746\n",
      "Training Loss: 0.008487385247135534\n",
      "Training Loss: 0.008454192832577973\n",
      "Validation Loss: 0.00546190623640456\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007026198470266536\n",
      "Training Loss: 0.006701220353133977\n",
      "Training Loss: 0.006655433520209044\n",
      "Training Loss: 0.007650030822260305\n",
      "Training Loss: 0.008703086944296955\n",
      "Training Loss: 0.008467348900157958\n",
      "Training Loss: 0.008435405659256504\n",
      "Validation Loss: 0.005445017247401485\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.007008180405246094\n",
      "Training Loss: 0.006686223131837323\n",
      "Training Loss: 0.006640780897578224\n",
      "Training Loss: 0.007633815216831863\n",
      "Training Loss: 0.008685062839649618\n",
      "Training Loss: 0.008448224866297096\n",
      "Training Loss: 0.00841731672291644\n",
      "Validation Loss: 0.005429151420889611\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006991134113632142\n",
      "Training Loss: 0.006672010299516842\n",
      "Training Loss: 0.006626884731231257\n",
      "Training Loss: 0.007618383584776893\n",
      "Training Loss: 0.008667766320286319\n",
      "Training Loss: 0.008429963995004073\n",
      "Training Loss: 0.008399888849817216\n",
      "Validation Loss: 0.005414205213506403\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006974975967314095\n",
      "Training Loss: 0.006658494910225272\n",
      "Training Loss: 0.006613666956545785\n",
      "Training Loss: 0.007603675185819157\n",
      "Training Loss: 0.008651147406781093\n",
      "Training Loss: 0.008412514593219384\n",
      "Training Loss: 0.008383084046654403\n",
      "Validation Loss: 0.00540007594344991\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006959623481379822\n",
      "Training Loss: 0.006645597057649865\n",
      "Training Loss: 0.006601055329665542\n",
      "Training Loss: 0.007589631369337439\n",
      "Training Loss: 0.008635157402604818\n",
      "Training Loss: 0.008395821187878027\n",
      "Training Loss: 0.008366858176887036\n",
      "Validation Loss: 0.005386661445156911\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.006944994285004213\n",
      "Training Loss: 0.006633239174261689\n",
      "Training Loss: 0.006588976500788704\n",
      "Training Loss: 0.007576191177940927\n",
      "Training Loss: 0.008619741883594543\n",
      "Training Loss: 0.008379827443277463\n",
      "Training Loss: 0.0083511632157024\n",
      "Validation Loss: 0.0053738579572586535\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006931001240154729\n",
      "Training Loss: 0.006621345247840509\n",
      "Training Loss: 0.0065773624426219614\n",
      "Training Loss: 0.007563294082647189\n",
      "Training Loss: 0.0086048428225331\n",
      "Training Loss: 0.008364467474166304\n",
      "Training Loss: 0.008335945970611647\n",
      "Validation Loss: 0.0053615856118681615\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006917566865449771\n",
      "Training Loss: 0.0066098468203563245\n",
      "Training Loss: 0.006566149527207017\n",
      "Training Loss: 0.007550876768655144\n",
      "Training Loss: 0.008590393785852938\n",
      "Training Loss: 0.008349668502341956\n",
      "Training Loss: 0.008321143513312564\n",
      "Validation Loss: 0.0053497495501187075\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.0069046052102930845\n",
      "Training Loss: 0.006598669283557683\n",
      "Training Loss: 0.00655526828370057\n",
      "Training Loss: 0.007538872482255102\n",
      "Training Loss: 0.008576329769566656\n",
      "Training Loss: 0.008335362046491355\n",
      "Training Loss: 0.008306692368350924\n",
      "Validation Loss: 0.005338260072914793\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006892027461435646\n",
      "Training Loss: 0.006587740486720577\n",
      "Training Loss: 0.0065446508943568914\n",
      "Training Loss: 0.007527213735738769\n",
      "Training Loss: 0.008562574137467891\n",
      "Training Loss: 0.008321465837070718\n",
      "Training Loss: 0.00829252255964093\n",
      "Validation Loss: 0.005327024763931384\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006879742771852762\n",
      "Training Loss: 0.006576990556204691\n",
      "Training Loss: 0.0065342338779009876\n",
      "Training Loss: 0.007515828468604014\n",
      "Training Loss: 0.008549048730637878\n",
      "Training Loss: 0.008307898663915694\n",
      "Training Loss: 0.008278556774603202\n",
      "Validation Loss: 0.005315962820470835\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006867663728771731\n",
      "Training Loss: 0.0065663413295987995\n",
      "Training Loss: 0.0065239428961649535\n",
      "Training Loss: 0.007504641505656764\n",
      "Training Loss: 0.00853566949488595\n",
      "Training Loss: 0.008294575359905139\n",
      "Training Loss: 0.008264713917160407\n",
      "Validation Loss: 0.005304977901224117\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006855689871590584\n",
      "Training Loss: 0.00655571905663237\n",
      "Training Loss: 0.006513712093001231\n",
      "Training Loss: 0.007493576728156768\n",
      "Training Loss: 0.00852234410122037\n",
      "Training Loss: 0.008281402541324496\n",
      "Training Loss: 0.008250907016918062\n",
      "Validation Loss: 0.005293976893990339\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006843723068013788\n",
      "Training Loss: 0.00654504154692404\n",
      "Training Loss: 0.0065034660650417206\n",
      "Training Loss: 0.0074825521482853215\n",
      "Training Loss: 0.008508977410383523\n",
      "Training Loss: 0.00826828709919937\n",
      "Training Loss: 0.008237044143024832\n",
      "Validation Loss: 0.005282870128450825\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006831661508185789\n",
      "Training Loss: 0.006534228416858241\n",
      "Training Loss: 0.006493133410112932\n",
      "Training Loss: 0.00747148617519997\n",
      "Training Loss: 0.00849547096178867\n",
      "Training Loss: 0.008255130350589753\n",
      "Training Loss: 0.008223029744112865\n",
      "Validation Loss: 0.005271549726407347\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006819399029482156\n",
      "Training Loss: 0.00652319363434799\n",
      "Training Loss: 0.006482637508306652\n",
      "Training Loss: 0.007460291034658439\n",
      "Training Loss: 0.008481719723204151\n",
      "Training Loss: 0.00824183071963489\n",
      "Training Loss: 0.008208764141891152\n",
      "Validation Loss: 0.005259929235313046\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.00680683393147774\n",
      "Training Loss: 0.0065118558006361125\n",
      "Training Loss: 0.006471910674590618\n",
      "Training Loss: 0.0074488840339472515\n",
      "Training Loss: 0.008467620346928016\n",
      "Training Loss: 0.00822828691219911\n",
      "Training Loss: 0.008194147957256063\n",
      "Validation Loss: 0.0052479102384396705\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006793872866546735\n",
      "Training Loss: 0.0065001367498189215\n",
      "Training Loss: 0.006460887042339891\n",
      "Training Loss: 0.007437183959991671\n",
      "Training Loss: 0.008453078663442283\n",
      "Training Loss: 0.008214404640020802\n",
      "Training Loss: 0.008179087800672278\n",
      "Validation Loss: 0.005235411162219579\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006780431272927672\n",
      "Training Loss: 0.0064879669807851315\n",
      "Training Loss: 0.006449508107034489\n",
      "Training Loss: 0.0074251161882421006\n",
      "Training Loss: 0.008438007179647685\n",
      "Training Loss: 0.008200091572944075\n",
      "Training Loss: 0.00816349679022096\n",
      "Validation Loss: 0.005222361356940832\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0067664480092935264\n",
      "Training Loss: 0.006475290792295709\n",
      "Training Loss: 0.006437733359634876\n",
      "Training Loss: 0.0074126187217189\n",
      "Training Loss: 0.00842233810108155\n",
      "Training Loss: 0.008185273949056864\n",
      "Training Loss: 0.008147305040620268\n",
      "Validation Loss: 0.005208705176079597\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006751882219687104\n",
      "Training Loss: 0.006462071312125772\n",
      "Training Loss: 0.006425535348244011\n",
      "Training Loss: 0.007399639015202411\n",
      "Training Loss: 0.00840601913165301\n",
      "Training Loss: 0.008169885536190122\n",
      "Training Loss: 0.008130457201041282\n",
      "Validation Loss: 0.005194413497492894\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.00673672778881155\n",
      "Training Loss: 0.006448293811408803\n",
      "Training Loss: 0.006412902229931206\n",
      "Training Loss: 0.00738613756839186\n",
      "Training Loss: 0.008389019325841218\n",
      "Training Loss: 0.008153872605180367\n",
      "Training Loss: 0.008112910888157784\n",
      "Validation Loss: 0.005179470428527238\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006720991774927825\n",
      "Training Loss: 0.006433953393716365\n",
      "Training Loss: 0.006399831069866195\n",
      "Training Loss: 0.007372079513734206\n",
      "Training Loss: 0.008371320617152378\n",
      "Training Loss: 0.008137192210415378\n",
      "Training Loss: 0.008094638054026292\n",
      "Validation Loss: 0.005163880493247721\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006704704456496984\n",
      "Training Loss: 0.006419062194181606\n",
      "Training Loss: 0.006386327011277899\n",
      "Training Loss: 0.007357430241536349\n",
      "Training Loss: 0.00835290617775172\n",
      "Training Loss: 0.008119795435341075\n",
      "Training Loss: 0.008075609523802995\n",
      "Validation Loss: 0.005147642485643929\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006687888944288716\n",
      "Training Loss: 0.0064036262058652935\n",
      "Training Loss: 0.006372387070441619\n",
      "Training Loss: 0.00734214621130377\n",
      "Training Loss: 0.008333755126222968\n",
      "Training Loss: 0.008101628215517848\n",
      "Training Loss: 0.008055794758256525\n",
      "Validation Loss: 0.0051307526368243195\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006670555263990536\n",
      "Training Loss: 0.006387639599852264\n",
      "Training Loss: 0.006357991950353608\n",
      "Training Loss: 0.007326170394080691\n",
      "Training Loss: 0.008313829562393949\n",
      "Training Loss: 0.008082624287344515\n",
      "Training Loss: 0.00803515426814556\n",
      "Validation Loss: 0.005113177094782951\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006652685373555869\n",
      "Training Loss: 0.006371082189725712\n",
      "Training Loss: 0.0063431157695595175\n",
      "Training Loss: 0.007309429999440909\n",
      "Training Loss: 0.008293073223903775\n",
      "Training Loss: 0.008062703389441594\n",
      "Training Loss: 0.008013637503609061\n",
      "Validation Loss: 0.005094872937583075\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.00663423583493568\n",
      "Training Loss: 0.0063539139734348285\n",
      "Training Loss: 0.006327710716286674\n",
      "Training Loss: 0.007291839224053547\n",
      "Training Loss: 0.008271413068287074\n",
      "Training Loss: 0.008041775827296078\n",
      "Training Loss: 0.00799118390888907\n",
      "Validation Loss: 0.005075765315840921\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.006615131692960859\n",
      "Training Loss: 0.006336076104780659\n",
      "Training Loss: 0.006311715793563053\n",
      "Training Loss: 0.007273303992114961\n",
      "Training Loss: 0.008248764827148989\n",
      "Training Loss: 0.00801974952337332\n",
      "Training Loss: 0.00796772955567576\n",
      "Validation Loss: 0.005055761410355958\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006595277435844764\n",
      "Training Loss: 0.006317497676936909\n",
      "Training Loss: 0.006295063232537359\n",
      "Training Loss: 0.007253725110786036\n",
      "Training Loss: 0.008225038055097684\n",
      "Training Loss: 0.00799653169233352\n",
      "Training Loss: 0.007943204443436117\n",
      "Validation Loss: 0.005034762145149127\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.0065745702001731845\n",
      "Training Loss: 0.006298102225409821\n",
      "Training Loss: 0.006277680795174092\n",
      "Training Loss: 0.007233003638102673\n",
      "Training Loss: 0.008200137024978175\n",
      "Training Loss: 0.007972028186777606\n",
      "Training Loss: 0.007917536029126496\n",
      "Validation Loss: 0.0050126574017902775\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.0065528967930004005\n",
      "Training Loss: 0.006277808439917863\n",
      "Training Loss: 0.006259490940719843\n",
      "Training Loss: 0.007211040976690128\n",
      "Training Loss: 0.008173967307666317\n",
      "Training Loss: 0.007946151585783809\n",
      "Training Loss: 0.007890650950139388\n",
      "Validation Loss: 0.004989347337790475\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006530148846795782\n",
      "Training Loss: 0.0062565385713242\n",
      "Training Loss: 0.006240424466086552\n",
      "Training Loss: 0.007187742278911173\n",
      "Training Loss: 0.008146438765106723\n",
      "Training Loss: 0.007918818403268232\n",
      "Training Loss: 0.007862477687885984\n",
      "Validation Loss: 0.004964731647615576\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006506223143078387\n",
      "Training Loss: 0.0062342189584160225\n",
      "Training Loss: 0.006220414356794208\n",
      "Training Loss: 0.007163020100560971\n",
      "Training Loss: 0.008117468523560092\n",
      "Training Loss: 0.007889956317376346\n",
      "Training Loss: 0.007832952730823309\n",
      "Validation Loss: 0.004938724961773621\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.00648103155544959\n",
      "Training Loss: 0.006210787973832339\n",
      "Training Loss: 0.006199403534410521\n",
      "Training Loss: 0.007136796489357948\n",
      "Training Loss: 0.008086982731474563\n",
      "Training Loss: 0.007859503856161609\n",
      "Training Loss: 0.007802017643116415\n",
      "Validation Loss: 0.004911253062226855\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006454495812067762\n",
      "Training Loss: 0.006186194277252071\n",
      "Training Loss: 0.006177343974122777\n",
      "Training Loss: 0.007109007409308105\n",
      "Training Loss: 0.008054928233614192\n",
      "Training Loss: 0.007827419969253242\n",
      "Training Loss: 0.007769633143907413\n",
      "Validation Loss: 0.004882259292646629\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006426563542336225\n",
      "Training Loss: 0.006160408511059359\n",
      "Training Loss: 0.006154210155364126\n",
      "Training Loss: 0.007079606836196035\n",
      "Training Loss: 0.008021270539611578\n",
      "Training Loss: 0.00779368327348493\n",
      "Training Loss: 0.007735777321504429\n",
      "Validation Loss: 0.0048517185993220534\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006397207754198462\n",
      "Training Loss: 0.006133426985470578\n",
      "Training Loss: 0.006129997364478185\n",
      "Training Loss: 0.0070485829096287485\n",
      "Training Loss: 0.007986013641348108\n",
      "Training Loss: 0.007758317366242409\n",
      "Training Loss: 0.007700470003765076\n",
      "Validation Loss: 0.004819638993981737\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006366439872654155\n",
      "Training Loss: 0.006105282662319951\n",
      "Training Loss: 0.006104733740212396\n",
      "Training Loss: 0.007015958752017468\n",
      "Training Loss: 0.007949203641619534\n",
      "Training Loss: 0.007721387536730617\n",
      "Training Loss: 0.00766376901883632\n",
      "Validation Loss: 0.004786063146111224\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.006334315894637257\n",
      "Training Loss: 0.006076049237744883\n",
      "Training Loss: 0.006078483539167792\n",
      "Training Loss: 0.006981809601420537\n",
      "Training Loss: 0.007910944430623203\n",
      "Training Loss: 0.007683019314426929\n",
      "Training Loss: 0.00762579110218212\n",
      "Validation Loss: 0.004751090506660274\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.006300945926923305\n",
      "Training Loss: 0.0060458533931523565\n",
      "Training Loss: 0.006051360780838877\n",
      "Training Loss: 0.006946274715010077\n",
      "Training Loss: 0.007871410713996739\n",
      "Training Loss: 0.007643416415667161\n",
      "Training Loss: 0.007586722173728049\n",
      "Validation Loss: 0.004714886255890186\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006266513314330951\n",
      "Training Loss: 0.006014889515936375\n",
      "Training Loss: 0.00602353734895587\n",
      "Training Loss: 0.0069095709407702086\n",
      "Training Loss: 0.007830863243434578\n",
      "Training Loss: 0.007602867115056142\n",
      "Training Loss: 0.007546829429920763\n",
      "Validation Loss: 0.004677688733897825\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006231273406883702\n",
      "Training Loss: 0.005983415169757791\n",
      "Training Loss: 0.00599524580873549\n",
      "Training Loss: 0.006871997605194338\n",
      "Training Loss: 0.0077896538039203735\n",
      "Training Loss: 0.007561749855522066\n",
      "Training Loss: 0.007506467545172199\n",
      "Validation Loss: 0.004639807376968726\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0061955588997807355\n",
      "Training Loss: 0.005951760016614571\n",
      "Training Loss: 0.005966781149618327\n",
      "Training Loss: 0.006833943928941153\n",
      "Training Loss: 0.007748223994858563\n",
      "Training Loss: 0.00752053611446172\n",
      "Training Loss: 0.007466076771961525\n",
      "Validation Loss: 0.0046016363678186125\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006159779067384079\n",
      "Training Loss: 0.005920312316156924\n",
      "Training Loss: 0.005938488726969809\n",
      "Training Loss: 0.006795874551753514\n",
      "Training Loss: 0.007707094898214564\n",
      "Training Loss: 0.007479764365125447\n",
      "Training Loss: 0.007426160387694836\n",
      "Validation Loss: 0.004563629574906291\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006124394169310108\n",
      "Training Loss: 0.0058894983684876934\n",
      "Training Loss: 0.005910753424977884\n",
      "Training Loss: 0.006758308317512274\n",
      "Training Loss: 0.007666839114390314\n",
      "Training Loss: 0.007440016568871215\n",
      "Training Loss: 0.007387267197482288\n",
      "Validation Loss: 0.004526284933159972\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.0060898943094071\n",
      "Training Loss: 0.005859753997647203\n",
      "Training Loss: 0.005883962649386376\n",
      "Training Loss: 0.00672178223147057\n",
      "Training Loss: 0.007628032212378458\n",
      "Training Loss: 0.007401860776590183\n",
      "Training Loss: 0.0073499311483465135\n",
      "Validation Loss: 0.004490090534090996\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006056734804296866\n",
      "Training Loss: 0.0058314706111559644\n",
      "Training Loss: 0.005858463575132191\n",
      "Training Loss: 0.006686795053537935\n",
      "Training Loss: 0.007591202318435535\n",
      "Training Loss: 0.007365798783721403\n",
      "Training Loss: 0.007314625699073076\n",
      "Validation Loss: 0.004455492805169847\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.0060253106732852755\n",
      "Training Loss: 0.00580495563976001\n",
      "Training Loss: 0.005834527284605429\n",
      "Training Loss: 0.006653756609302945\n",
      "Training Loss: 0.007556765205226839\n",
      "Training Loss: 0.007332202264806255\n",
      "Training Loss: 0.00728170415153727\n",
      "Validation Loss: 0.004422847320210565\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.005995895648375153\n",
      "Training Loss: 0.005780395985930227\n",
      "Training Loss: 0.005812314852373674\n",
      "Training Loss: 0.006622945275739767\n",
      "Training Loss: 0.007524991671089083\n",
      "Training Loss: 0.007301285647554323\n",
      "Training Loss: 0.007251373877516017\n",
      "Validation Loss: 0.00439237349244595\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.005968624943634495\n",
      "Training Loss: 0.005757842636667192\n",
      "Training Loss: 0.005791866722283885\n",
      "Training Loss: 0.006594485268578865\n",
      "Training Loss: 0.007495984222041443\n",
      "Training Loss: 0.007273091516690328\n",
      "Training Loss: 0.007223678092705086\n",
      "Validation Loss: 0.004364181393216807\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00594350227736868\n",
      "Training Loss: 0.005737221404560841\n",
      "Training Loss: 0.005773112418828532\n",
      "Training Loss: 0.006568357108626514\n",
      "Training Loss: 0.007469690898433328\n",
      "Training Loss: 0.00724751093192026\n",
      "Training Loss: 0.007198518605437129\n",
      "Validation Loss: 0.004338244207321984\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.005920416137669236\n",
      "Training Loss: 0.005718364798231051\n",
      "Training Loss: 0.005755898481002077\n",
      "Training Loss: 0.006544421510188841\n",
      "Training Loss: 0.007445938921300695\n",
      "Training Loss: 0.007224327804287896\n",
      "Training Loss: 0.007175693013705313\n",
      "Validation Loss: 0.004314458535719063\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.005899182093562558\n",
      "Training Loss: 0.005701052626245655\n",
      "Training Loss: 0.005740030107554048\n",
      "Training Loss: 0.00652247053629253\n",
      "Training Loss: 0.007424480573972687\n",
      "Training Loss: 0.0072032680432312195\n",
      "Training Loss: 0.007154944189824164\n",
      "Validation Loss: 0.004292661928456653\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0058795866509899495\n",
      "Training Loss: 0.0056850539677543565\n",
      "Training Loss: 0.00572530415491201\n",
      "Training Loss: 0.006502266939496622\n",
      "Training Loss: 0.007405043335165828\n",
      "Training Loss: 0.00718404924729839\n",
      "Training Loss: 0.0071359978604596105\n",
      "Validation Loss: 0.0042726681249439435\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.005861415729159489\n",
      "Training Loss: 0.005670155512634664\n",
      "Training Loss: 0.0057115358836017545\n",
      "Training Loss: 0.006483576751779765\n",
      "Training Loss: 0.007387358682462946\n",
      "Training Loss: 0.0071664084354415535\n",
      "Training Loss: 0.007118600107496605\n",
      "Validation Loss: 0.004254301374234008\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.005844488819129765\n",
      "Training Loss: 0.005656186531414278\n",
      "Training Loss: 0.005698578077135608\n",
      "Training Loss: 0.006466195218381472\n",
      "Training Loss: 0.007371187639655545\n",
      "Training Loss: 0.007150118564022705\n",
      "Training Loss: 0.007102527775568888\n",
      "Validation Loss: 0.004237386531437214\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.005828641486587003\n",
      "Training Loss: 0.005643006274476647\n",
      "Training Loss: 0.005686310210730881\n",
      "Training Loss: 0.006449943674961105\n",
      "Training Loss: 0.00735632091993466\n",
      "Training Loss: 0.007134995608357713\n",
      "Training Loss: 0.007087594467448071\n",
      "Validation Loss: 0.004221768975641565\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.005813750067027286\n",
      "Training Loss: 0.005630511758499779\n",
      "Training Loss: 0.005674645780818537\n",
      "Training Loss: 0.006434683736297302\n",
      "Training Loss: 0.007342590428888798\n",
      "Training Loss: 0.00712089401204139\n",
      "Training Loss: 0.007073648734949529\n",
      "Validation Loss: 0.004207326835701556\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.005799717564368621\n",
      "Training Loss: 0.005618631789693609\n",
      "Training Loss: 0.005663525671698153\n",
      "Training Loss: 0.006420304470811971\n",
      "Training Loss: 0.007329854407580569\n",
      "Training Loss: 0.007107698685722426\n",
      "Training Loss: 0.007060573740163818\n",
      "Validation Loss: 0.00419394698949733\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.005786466866265982\n",
      "Training Loss: 0.005607314974768087\n",
      "Training Loss: 0.0056529062136542055\n",
      "Training Loss: 0.006406715710181743\n",
      "Training Loss: 0.007318002196843736\n",
      "Training Loss: 0.007095321865053847\n",
      "Training Loss: 0.007048272973624989\n",
      "Validation Loss: 0.004181530658410311\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.005773936125915497\n",
      "Training Loss: 0.005596521654515527\n",
      "Training Loss: 0.00564275277662091\n",
      "Training Loss: 0.006393847631989047\n",
      "Training Loss: 0.007306938519468531\n",
      "Training Loss: 0.007083690795116127\n",
      "Training Loss: 0.007036670602392405\n",
      "Validation Loss: 0.0041699997399679\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.005762075970415026\n",
      "Training Loss: 0.005586222785059363\n",
      "Training Loss: 0.0056330423266626895\n",
      "Training Loss: 0.006381643213098869\n",
      "Training Loss: 0.007296587579185143\n",
      "Training Loss: 0.007072746506892145\n",
      "Training Loss: 0.007025702823884785\n",
      "Validation Loss: 0.0041592698592225206\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.005750839039683342\n",
      "Training Loss: 0.005576390237547457\n",
      "Training Loss: 0.005623751225648448\n",
      "Training Loss: 0.006370052638230845\n",
      "Training Loss: 0.007286883245105855\n",
      "Training Loss: 0.007062439864967018\n",
      "Training Loss: 0.007015316893812269\n",
      "Validation Loss: 0.004149279011280907\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.005740188097115606\n",
      "Training Loss: 0.0055670027219457555\n",
      "Training Loss: 0.005614857755135745\n",
      "Training Loss: 0.006359033457702026\n",
      "Training Loss: 0.007277767220512032\n",
      "Training Loss: 0.007052725834073499\n",
      "Training Loss: 0.007005466758273542\n",
      "Validation Loss: 0.004139967039070522\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.005730086144758388\n",
      "Training Loss: 0.005558038376620971\n",
      "Training Loss: 0.0056063457066193225\n",
      "Training Loss: 0.006348548530950211\n",
      "Training Loss: 0.007269189114449546\n",
      "Training Loss: 0.007043565490748733\n",
      "Training Loss: 0.006996110039763153\n",
      "Validation Loss: 0.0041312761857673136\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.005720498772570863\n",
      "Training Loss: 0.00554947379976511\n",
      "Training Loss: 0.005598190013552084\n",
      "Training Loss: 0.006338560010190122\n",
      "Training Loss: 0.007261099037714302\n",
      "Training Loss: 0.007034918832359836\n",
      "Training Loss: 0.006987209470244124\n",
      "Validation Loss: 0.004123156202245462\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0057113928033504634\n",
      "Training Loss: 0.005541288261883892\n",
      "Training Loss: 0.005590375657193363\n",
      "Training Loss: 0.006329036265378818\n",
      "Training Loss: 0.007253458629129454\n",
      "Training Loss: 0.007026752457022667\n",
      "Training Loss: 0.006978728874819353\n",
      "Validation Loss: 0.0041155628519907094\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.005702739343978465\n",
      "Training Loss: 0.00553346125932876\n",
      "Training Loss: 0.005582881814334541\n",
      "Training Loss: 0.006319946072180755\n",
      "Training Loss: 0.007246225092094391\n",
      "Training Loss: 0.007019030445953831\n",
      "Training Loss: 0.006970637803897261\n",
      "Validation Loss: 0.004108445270663679\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.005694506586878561\n",
      "Training Loss: 0.00552596797409933\n",
      "Training Loss: 0.005575686007505283\n",
      "Training Loss: 0.00631125902000349\n",
      "Training Loss: 0.007239362552645616\n",
      "Training Loss: 0.007011720585869625\n",
      "Training Loss: 0.006962902559898794\n",
      "Validation Loss: 0.004101766068453479\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.005686664709355682\n",
      "Training Loss: 0.005518787556211464\n",
      "Training Loss: 0.0055687686218880116\n",
      "Training Loss: 0.006302945688948966\n",
      "Training Loss: 0.007232837018673308\n",
      "Training Loss: 0.007004790925420821\n",
      "Training Loss: 0.006955494148423895\n",
      "Validation Loss: 0.004095487295273231\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.005679186047054827\n",
      "Training Loss: 0.005511899240664206\n",
      "Training Loss: 0.0055621088808402415\n",
      "Training Loss: 0.006294980002567172\n",
      "Training Loss: 0.00722661723033525\n",
      "Training Loss: 0.006998212317703292\n",
      "Training Loss: 0.006948383878916502\n",
      "Validation Loss: 0.004089570586388226\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.005672042643418536\n",
      "Training Loss: 0.005505279682110995\n",
      "Training Loss: 0.005555686979787425\n",
      "Training Loss: 0.006287333156797104\n",
      "Training Loss: 0.0072206710919272155\n",
      "Training Loss: 0.006991954039549455\n",
      "Training Loss: 0.006941547045717016\n",
      "Validation Loss: 0.004083983822941975\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.005665210048318841\n",
      "Training Loss: 0.005498909425223246\n",
      "Training Loss: 0.0055494838254526255\n",
      "Training Loss: 0.006279983058921062\n",
      "Training Loss: 0.007214972454821691\n",
      "Training Loss: 0.006985990187386051\n",
      "Training Loss: 0.006934958652127534\n",
      "Validation Loss: 0.004078699127171007\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.005658665976952761\n",
      "Training Loss: 0.005492771388962865\n",
      "Training Loss: 0.005543483236106112\n",
      "Training Loss: 0.00627290497184731\n",
      "Training Loss: 0.007209495176794007\n",
      "Training Loss: 0.006980293189408258\n",
      "Training Loss: 0.006928594023920595\n",
      "Validation Loss: 0.004073688263345635\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.005652385629946366\n",
      "Training Loss: 0.0054868438211269676\n",
      "Training Loss: 0.005537664382718504\n",
      "Training Loss: 0.0062660755997058\n",
      "Training Loss: 0.007204216670943424\n",
      "Training Loss: 0.006974839463364333\n",
      "Training Loss: 0.006922433716244996\n",
      "Validation Loss: 0.00406891736027537\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0056463449902366845\n",
      "Training Loss: 0.005481108539970592\n",
      "Training Loss: 0.005532010702881962\n",
      "Training Loss: 0.006259474660037085\n",
      "Training Loss: 0.007199113221140578\n",
      "Training Loss: 0.006969603683101014\n",
      "Training Loss: 0.006916455315658823\n",
      "Validation Loss: 0.004064367452727051\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.00564052555593662\n",
      "Training Loss: 0.005475548911490478\n",
      "Training Loss: 0.005526507248869166\n",
      "Training Loss: 0.006253081652102992\n",
      "Training Loss: 0.007194167017005384\n",
      "Training Loss: 0.006964566234964878\n",
      "Training Loss: 0.006910641569411382\n",
      "Validation Loss: 0.004060015186566288\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.005634908770443872\n",
      "Training Loss: 0.005470149824977852\n",
      "Training Loss: 0.005521138499025255\n",
      "Training Loss: 0.006246878797537647\n",
      "Training Loss: 0.007189359724288806\n",
      "Training Loss: 0.00695970673346892\n",
      "Training Loss: 0.006904976121149957\n",
      "Validation Loss: 0.004055835555607543\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.005629473392036744\n",
      "Training Loss: 0.005464895478799008\n",
      "Training Loss: 0.0055158905603457245\n",
      "Training Loss: 0.006240848285378888\n",
      "Training Loss: 0.007184675076277926\n",
      "Training Loss: 0.006955005117924884\n",
      "Training Loss: 0.006899442522553727\n",
      "Validation Loss: 0.004051816747401761\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.00562420800270047\n",
      "Training Loss: 0.0054597731254762034\n",
      "Training Loss: 0.005510752281406894\n",
      "Training Loss: 0.0062349748489214105\n",
      "Training Loss: 0.00718009996751789\n",
      "Training Loss: 0.006950446048285812\n",
      "Training Loss: 0.006894028098322451\n",
      "Validation Loss: 0.004047935972450657\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.005619093088898808\n",
      "Training Loss: 0.005454768118797801\n",
      "Training Loss: 0.005505709493299946\n",
      "Training Loss: 0.006229243280831725\n",
      "Training Loss: 0.007175619566114619\n",
      "Training Loss: 0.006946011643158272\n",
      "Training Loss: 0.006888720489805564\n",
      "Validation Loss: 0.00404417820314943\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.005614115648786538\n",
      "Training Loss: 0.005449869130388834\n",
      "Training Loss: 0.005500751772196963\n",
      "Training Loss: 0.006223640770185739\n",
      "Training Loss: 0.0071712235984159635\n",
      "Training Loss: 0.006941689909435809\n",
      "Training Loss: 0.006883506276644766\n",
      "Validation Loss: 0.004040529191919536\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.005609261671779677\n",
      "Training Loss: 0.005445065295207314\n",
      "Training Loss: 0.005495869267033413\n",
      "Training Loss: 0.006218154107918963\n",
      "Training Loss: 0.0071669023972935975\n",
      "Training Loss: 0.006937466281233356\n",
      "Training Loss: 0.006878378310939297\n",
      "Validation Loss: 0.004036973844010258\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.005604519569897093\n",
      "Training Loss: 0.005440345408278518\n",
      "Training Loss: 0.005491051007993519\n",
      "Training Loss: 0.006212772015715018\n",
      "Training Loss: 0.007162645902717486\n",
      "Training Loss: 0.006933328580344096\n",
      "Training Loss: 0.0068733258568681775\n",
      "Validation Loss: 0.004033504088831305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.005599879111396149\n",
      "Training Loss: 0.005435699847876094\n",
      "Training Loss: 0.005486290031112731\n",
      "Training Loss: 0.006207483626785688\n",
      "Training Loss: 0.007158447897527367\n",
      "Training Loss: 0.006929266578517854\n",
      "Training Loss: 0.006868342282250524\n",
      "Validation Loss: 0.004030104572204559\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0055953290255274625\n",
      "Training Loss: 0.00543112192011904\n",
      "Training Loss: 0.005481578415492549\n",
      "Training Loss: 0.006202279860735871\n",
      "Training Loss: 0.007154300620313734\n",
      "Training Loss: 0.006925270186038688\n",
      "Training Loss: 0.006863419258734211\n",
      "Validation Loss: 0.004026766298757343\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.005590858451323583\n",
      "Training Loss: 0.005426599007914774\n",
      "Training Loss: 0.005476907810661942\n",
      "Training Loss: 0.00619715248409193\n",
      "Training Loss: 0.007150199110037647\n",
      "Training Loss: 0.006921331314370036\n",
      "Training Loss: 0.00685855247429572\n",
      "Validation Loss: 0.004023479051410119\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.005586460314807482\n",
      "Training Loss: 0.005422127427882515\n",
      "Training Loss: 0.005472272564657032\n",
      "Training Loss: 0.006192091500852257\n",
      "Training Loss: 0.007146137398667634\n",
      "Training Loss: 0.006917439219541847\n",
      "Training Loss: 0.006853734068572521\n",
      "Validation Loss: 0.004020238647931552\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.0055821272020693865\n",
      "Training Loss: 0.005417699896497652\n",
      "Training Loss: 0.005467667281627655\n",
      "Training Loss: 0.006187091515166685\n",
      "Training Loss: 0.007142110901768319\n",
      "Training Loss: 0.006913589374162257\n",
      "Training Loss: 0.006848962062504143\n",
      "Validation Loss: 0.00401703558499447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.005577851324342191\n",
      "Training Loss: 0.005413309909054078\n",
      "Training Loss: 0.005463086209492758\n",
      "Training Loss: 0.0061821441608481105\n",
      "Training Loss: 0.007138117784634233\n",
      "Training Loss: 0.00690977347898297\n",
      "Training Loss: 0.0068442306329961865\n",
      "Validation Loss: 0.004013862229765075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.005573623552336358\n",
      "Training Loss: 0.005408950286800973\n",
      "Training Loss: 0.005458523445995524\n",
      "Training Loss: 0.006177242678240873\n",
      "Training Loss: 0.007134151675272733\n",
      "Training Loss: 0.006905985787743703\n",
      "Training Loss: 0.006839535827748477\n",
      "Validation Loss: 0.004010715124228697\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.005569441153202206\n",
      "Training Loss: 0.005404616310843266\n",
      "Training Loss: 0.005453975284472108\n",
      "Training Loss: 0.0061723820673068985\n",
      "Training Loss: 0.007130211724434048\n",
      "Training Loss: 0.006902220780029893\n",
      "Training Loss: 0.006834874171763658\n",
      "Validation Loss: 0.0040075936348592574\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.005565299115842208\n",
      "Training Loss: 0.005400305271032266\n",
      "Training Loss: 0.005449437887873501\n",
      "Training Loss: 0.00616755664057564\n",
      "Training Loss: 0.007126293678302318\n",
      "Training Loss: 0.006898472515167668\n",
      "Training Loss: 0.006830243653384968\n",
      "Validation Loss: 0.004004484169840924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.00556118882028386\n",
      "Training Loss: 0.0053960076009389015\n",
      "Training Loss: 0.0054449050454422835\n",
      "Training Loss: 0.006162759971921332\n",
      "Training Loss: 0.0071223958540940656\n",
      "Training Loss: 0.00689473748090677\n",
      "Training Loss: 0.0068256395671050995\n",
      "Validation Loss: 0.004001386167589673\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0055571055872133\n",
      "Training Loss: 0.005391722308122553\n",
      "Training Loss: 0.005440373675664887\n",
      "Training Loss: 0.006157986408798024\n",
      "Training Loss: 0.007118515139445662\n",
      "Training Loss: 0.006891008290695027\n",
      "Training Loss: 0.006821060255169868\n",
      "Validation Loss: 0.003998300623521907\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.005553046309505589\n",
      "Training Loss: 0.005387444125371985\n",
      "Training Loss: 0.005435841022990644\n",
      "Training Loss: 0.00615323236619588\n",
      "Training Loss: 0.007114648833521642\n",
      "Training Loss: 0.0068872837803792206\n",
      "Training Loss: 0.006816503118025139\n",
      "Validation Loss: 0.0039952185060759164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.005549005033681169\n",
      "Training Loss: 0.005383169224369339\n",
      "Training Loss: 0.00543130173580721\n",
      "Training Loss: 0.0061484911892330275\n",
      "Training Loss: 0.007110795192420483\n",
      "Training Loss: 0.006883557668188587\n",
      "Training Loss: 0.0068119640822988\n",
      "Validation Loss: 0.0039921412881017285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0055449786584358665\n",
      "Training Loss: 0.005378891607397236\n",
      "Training Loss: 0.005426753520732745\n",
      "Training Loss: 0.0061437582882354035\n",
      "Training Loss: 0.007106950974557549\n",
      "Training Loss: 0.006879826450021938\n",
      "Training Loss: 0.006807441621785984\n",
      "Validation Loss: 0.003989064203983361\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0055409626453183595\n",
      "Training Loss: 0.005374611335573718\n",
      "Training Loss: 0.005422191742109134\n",
      "Training Loss: 0.006139029473415576\n",
      "Training Loss: 0.007103114052442834\n",
      "Training Loss: 0.0068760854157153516\n",
      "Training Loss: 0.0068029328773263845\n",
      "Validation Loss: 0.003985987830565273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.00553695325506851\n",
      "Training Loss: 0.0053703223133925344\n",
      "Training Loss: 0.005417614239268005\n",
      "Training Loss: 0.0061342995706945656\n",
      "Training Loss: 0.007099282304989174\n",
      "Training Loss: 0.006872331647900865\n",
      "Training Loss: 0.006798434875672683\n",
      "Validation Loss: 0.0039829100523150985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.005532947020838037\n",
      "Training Loss: 0.005366019865614362\n",
      "Training Loss: 0.005413015349768102\n",
      "Training Loss: 0.006129563649883494\n",
      "Training Loss: 0.007095452889916487\n",
      "Training Loss: 0.006868561466690153\n",
      "Training Loss: 0.006793945220997557\n",
      "Validation Loss: 0.003979829028824323\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.005528939546784386\n",
      "Training Loss: 0.00536170200095512\n",
      "Training Loss: 0.005408392194658518\n",
      "Training Loss: 0.006124816315132193\n",
      "Training Loss: 0.007091622551088222\n",
      "Training Loss: 0.006864770440151915\n",
      "Training Loss: 0.006789460165891797\n",
      "Validation Loss: 0.003976743857111182\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.0055249269958585505\n",
      "Training Loss: 0.005357364064548164\n",
      "Training Loss: 0.005403741541085765\n",
      "Training Loss: 0.006120051420875825\n",
      "Training Loss: 0.007087788022472523\n",
      "Training Loss: 0.006860954257426784\n",
      "Training Loss: 0.006784977250499651\n",
      "Validation Loss: 0.003973657730530794\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.005520907587488182\n",
      "Training Loss: 0.0053530044283252205\n",
      "Training Loss: 0.005399059054907411\n",
      "Training Loss: 0.006115265270927921\n",
      "Training Loss: 0.00708394747634884\n",
      "Training Loss: 0.006857110142009333\n",
      "Training Loss: 0.006780493293190375\n",
      "Validation Loss: 0.003970567658037812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.005516875383909792\n",
      "Training Loss: 0.005348616432747803\n",
      "Training Loss: 0.005394341169157997\n",
      "Training Loss: 0.006110451341373846\n",
      "Training Loss: 0.007080095206620171\n",
      "Training Loss: 0.006853233595611527\n",
      "Training Loss: 0.006776004587300122\n",
      "Validation Loss: 0.0039674736431203365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.005512828367063776\n",
      "Training Loss: 0.005344199457904324\n",
      "Training Loss: 0.005389581953641027\n",
      "Training Loss: 0.006105605149641633\n",
      "Training Loss: 0.007076230487436988\n",
      "Training Loss: 0.006849321889458224\n",
      "Training Loss: 0.006771508366800845\n",
      "Validation Loss: 0.0039643763707539114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005508763124234975\n",
      "Training Loss: 0.005339748372789472\n",
      "Training Loss: 0.005384779359446839\n",
      "Training Loss: 0.006100721575785428\n",
      "Training Loss: 0.00707234994857572\n",
      "Training Loss: 0.006845371654490009\n",
      "Training Loss: 0.006767001086845994\n",
      "Validation Loss: 0.003961280469210602\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.0055046772229252385\n",
      "Training Loss: 0.005335261770524085\n",
      "Training Loss: 0.0053799288393929605\n",
      "Training Loss: 0.0060957951535237954\n",
      "Training Loss: 0.007068449333310127\n",
      "Training Loss: 0.006841379168909043\n",
      "Training Loss: 0.006762480334145948\n",
      "Validation Loss: 0.0039581808131641605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.005500564568210393\n",
      "Training Loss: 0.005330733958398923\n",
      "Training Loss: 0.005375023911474272\n",
      "Training Loss: 0.0060908203304279596\n",
      "Training Loss: 0.007064526013564319\n",
      "Training Loss: 0.0068373415316455065\n",
      "Training Loss: 0.006757943268166855\n",
      "Validation Loss: 0.003955085237322145\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005496424112352543\n",
      "Training Loss: 0.005326163610443473\n",
      "Training Loss: 0.0053700631938409064\n",
      "Training Loss: 0.0060857923666480926\n",
      "Training Loss: 0.00706057884846814\n",
      "Training Loss: 0.006833256622776389\n",
      "Training Loss: 0.006753386295167729\n",
      "Validation Loss: 0.003951994366054371\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.005492253447882831\n",
      "Training Loss: 0.005321548379724845\n",
      "Training Loss: 0.005365041742334142\n",
      "Training Loss: 0.006080707176006399\n",
      "Training Loss: 0.007056604159297422\n",
      "Training Loss: 0.006829122804338113\n",
      "Training Loss: 0.006748809061245992\n",
      "Validation Loss: 0.003948912515447995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005488052370492369\n",
      "Training Loss: 0.005316886993823573\n",
      "Training Loss: 0.005359958896879107\n",
      "Training Loss: 0.006075561853358522\n",
      "Training Loss: 0.007052599946036935\n",
      "Training Loss: 0.006824938869103789\n",
      "Training Loss: 0.006744208629243076\n",
      "Validation Loss: 0.003945845645357077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.005483817443018779\n",
      "Training Loss: 0.005312179061002098\n",
      "Training Loss: 0.005354811750585213\n",
      "Training Loss: 0.006070352531387471\n",
      "Training Loss: 0.0070485677081160245\n",
      "Training Loss: 0.00682070438284427\n",
      "Training Loss: 0.006739585702307522\n",
      "Validation Loss: 0.003942798220113003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.005479548980947584\n",
      "Training Loss: 0.005307425523060374\n",
      "Training Loss: 0.005349599387263879\n",
      "Training Loss: 0.00606507821183186\n",
      "Training Loss: 0.007044506317470223\n",
      "Training Loss: 0.00681642139214091\n",
      "Training Loss: 0.0067349415982607755\n",
      "Validation Loss: 0.003939775469685008\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005475247545400634\n",
      "Training Loss: 0.0053026262612547725\n",
      "Training Loss: 0.005344320889562369\n",
      "Training Loss: 0.006059736962779425\n",
      "Training Loss: 0.007040417164098471\n",
      "Training Loss: 0.006812091923784465\n",
      "Training Loss: 0.006730276932939887\n",
      "Validation Loss: 0.003936787769861156\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005470913774915971\n",
      "Training Loss: 0.005297785608563572\n",
      "Training Loss: 0.005338979230728001\n",
      "Training Loss: 0.006054331577615812\n",
      "Training Loss: 0.007036303981440142\n",
      "Training Loss: 0.006807720133801922\n",
      "Training Loss: 0.0067255961487535385\n",
      "Validation Loss: 0.003933842724698424\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005466551397694275\n",
      "Training Loss: 0.005292907311231829\n",
      "Training Loss: 0.00533357779844664\n",
      "Training Loss: 0.006048864615731873\n",
      "Training Loss: 0.007032171725295484\n",
      "Training Loss: 0.0068033117498271165\n",
      "Training Loss: 0.006720903511159122\n",
      "Validation Loss: 0.00393095515382368\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0054621666367165745\n",
      "Training Loss: 0.005287999150459655\n",
      "Training Loss: 0.0053281234751921145\n",
      "Training Loss: 0.006043340310570784\n",
      "Training Loss: 0.007028026091866195\n",
      "Training Loss: 0.006798874619416893\n",
      "Training Loss: 0.006716206724522635\n",
      "Validation Loss: 0.003928133075069706\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.0054577619314659384\n",
      "Training Loss: 0.005283068341086618\n",
      "Training Loss: 0.005322622489184141\n",
      "Training Loss: 0.0060377673752373085\n",
      "Training Loss: 0.007023877173196524\n",
      "Training Loss: 0.006794418876525015\n",
      "Training Loss: 0.006711514703929424\n",
      "Validation Loss: 0.003925390598235115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005453347378061153\n",
      "Training Loss: 0.005278127763303928\n",
      "Training Loss: 0.00531708664027974\n",
      "Training Loss: 0.006032153291162103\n",
      "Training Loss: 0.007019732847111299\n",
      "Training Loss: 0.0067899550730362536\n",
      "Training Loss: 0.006706837416859343\n",
      "Validation Loss: 0.0039227376914363395\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.005448929754202254\n",
      "Training Loss: 0.005273186535923742\n",
      "Training Loss: 0.005311527343001217\n",
      "Training Loss: 0.00602651058929041\n",
      "Training Loss: 0.00701560391811654\n",
      "Training Loss: 0.006785495871445164\n",
      "Training Loss: 0.006702185613103211\n",
      "Validation Loss: 0.003920194969341605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005444521153112874\n",
      "Training Loss: 0.005268259976292029\n",
      "Training Loss: 0.005305960181867703\n",
      "Training Loss: 0.006020852147485129\n",
      "Training Loss: 0.007011503882240504\n",
      "Training Loss: 0.006781056387117132\n",
      "Training Loss: 0.006697574720019475\n",
      "Validation Loss: 0.003917770826062432\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005440132687799633\n",
      "Training Loss: 0.005263365643331781\n",
      "Training Loss: 0.005300402590073645\n",
      "Training Loss: 0.006015193513012491\n",
      "Training Loss: 0.007007445086492226\n",
      "Training Loss: 0.006776650382671505\n",
      "Training Loss: 0.006693017239449546\n",
      "Validation Loss: 0.003915481247994979\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0054357755824457855\n",
      "Training Loss: 0.005258518890477717\n",
      "Training Loss: 0.00529487241175957\n",
      "Training Loss: 0.006009550305898301\n",
      "Training Loss: 0.007003442186396569\n",
      "Training Loss: 0.0067722938524093475\n",
      "Training Loss: 0.006688527230871841\n",
      "Validation Loss: 0.003913334598293884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005431461950647645\n",
      "Training Loss: 0.005253735070000403\n",
      "Training Loss: 0.005289389460813254\n",
      "Training Loss: 0.006003938183421269\n",
      "Training Loss: 0.006999506278662011\n",
      "Training Loss: 0.006768000804586336\n",
      "Training Loss: 0.006684118130942806\n",
      "Validation Loss: 0.0039113424094882185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005427202301216311\n",
      "Training Loss: 0.005249030672712251\n",
      "Training Loss: 0.005283970609307289\n",
      "Training Loss: 0.005998373786569573\n",
      "Training Loss: 0.006995648450683802\n",
      "Training Loss: 0.006763784126378596\n",
      "Training Loss: 0.006679801743011922\n",
      "Validation Loss: 0.003909510936176453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005423008798388764\n",
      "Training Loss: 0.005244422022369691\n",
      "Training Loss: 0.0052786350017413495\n",
      "Training Loss: 0.0059928729879902675\n",
      "Training Loss: 0.0069918790238443764\n",
      "Training Loss: 0.006759654001798481\n",
      "Training Loss: 0.006675588773796335\n",
      "Validation Loss: 0.0039078461260043\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.0054188890708610415\n",
      "Training Loss: 0.0052399203664390374\n",
      "Training Loss: 0.005273398238932714\n",
      "Training Loss: 0.005987447075312957\n",
      "Training Loss: 0.006988204518565908\n",
      "Training Loss: 0.006755621143383905\n",
      "Training Loss: 0.006671486339764669\n",
      "Validation Loss: 0.003906343312330599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005414851871901192\n",
      "Training Loss: 0.005235536958789453\n",
      "Training Loss: 0.0052682750334497545\n",
      "Training Loss: 0.0059821077506057915\n",
      "Training Loss: 0.0069846293260343376\n",
      "Training Loss: 0.006751688303193077\n",
      "Training Loss: 0.006667498138267547\n",
      "Validation Loss: 0.003905004869243095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005410901245777495\n",
      "Training Loss: 0.0052312795107718555\n",
      "Training Loss: 0.005263276339974254\n",
      "Training Loss: 0.005976861562230624\n",
      "Training Loss: 0.006981153284432367\n",
      "Training Loss: 0.006747860629111528\n",
      "Training Loss: 0.006663628232199698\n",
      "Validation Loss: 0.0039038253500743698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005407038310077042\n",
      "Training Loss: 0.005227151630679145\n",
      "Training Loss: 0.0052584092738106845\n",
      "Training Loss: 0.005971712751197629\n",
      "Training Loss: 0.006977775893174112\n",
      "Training Loss: 0.0067441358219366525\n",
      "Training Loss: 0.006659872174495831\n",
      "Validation Loss: 0.003902797677470476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00540326569287572\n",
      "Training Loss: 0.005223155957064591\n",
      "Training Loss: 0.005253680157475174\n",
      "Training Loss: 0.005966664233710617\n",
      "Training Loss: 0.00697449130937457\n",
      "Training Loss: 0.006740510676754638\n",
      "Training Loss: 0.006656225572805851\n",
      "Validation Loss: 0.0039019140546569385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005399581392994151\n",
      "Training Loss: 0.005219290787936188\n",
      "Training Loss: 0.005249089830322191\n",
      "Training Loss: 0.005961713453871198\n",
      "Training Loss: 0.006971291492227465\n",
      "Training Loss: 0.006736978430999443\n",
      "Training Loss: 0.00665268181823194\n",
      "Validation Loss: 0.00390116829723329\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005395981739857234\n",
      "Training Loss: 0.005215550814755261\n",
      "Training Loss: 0.005244638011790812\n",
      "Training Loss: 0.005956858002464287\n",
      "Training Loss: 0.006968167163431644\n",
      "Training Loss: 0.006733528634067625\n",
      "Training Loss: 0.006649228525348007\n",
      "Validation Loss: 0.0039005506293645848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005392464785254561\n",
      "Training Loss: 0.005211931469966657\n",
      "Training Loss: 0.005240321015007794\n",
      "Training Loss: 0.005952089347993024\n",
      "Training Loss: 0.00696510688518174\n",
      "Training Loss: 0.0067301527608651665\n",
      "Training Loss: 0.006645855759270489\n",
      "Validation Loss: 0.0039000520382376476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005389021767186932\n",
      "Training Loss: 0.005208424224983901\n",
      "Training Loss: 0.005236132488353178\n",
      "Training Loss: 0.00594739934487734\n",
      "Training Loss: 0.006962095665512607\n",
      "Training Loss: 0.006726835943991319\n",
      "Training Loss: 0.006642546623479575\n",
      "Validation Loss: 0.0038996651312615373\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005385647814837285\n",
      "Training Loss: 0.005205017258995213\n",
      "Training Loss: 0.005232063509756699\n",
      "Training Loss: 0.00594277759664692\n",
      "Training Loss: 0.00695912025636062\n",
      "Training Loss: 0.006723564203130081\n",
      "Training Loss: 0.006639285921119153\n",
      "Validation Loss: 0.0038993885084003042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005382336299517192\n",
      "Training Loss: 0.005201700743637048\n",
      "Training Loss: 0.005228105614660308\n",
      "Training Loss: 0.005938211144530215\n",
      "Training Loss: 0.006956163609866053\n",
      "Training Loss: 0.006720321075990796\n",
      "Training Loss: 0.0066360553994309155\n",
      "Validation Loss: 0.0038992190870459887\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005379079733393155\n",
      "Training Loss: 0.005198462552507408\n",
      "Training Loss: 0.0052242494805250315\n",
      "Training Loss: 0.005933685102500022\n",
      "Training Loss: 0.006953206710750237\n",
      "Training Loss: 0.00671708801528439\n",
      "Training Loss: 0.00663283504312858\n",
      "Validation Loss: 0.003899158322172107\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005375869222916663\n",
      "Training Loss: 0.005195287722744979\n",
      "Training Loss: 0.005220480541465804\n",
      "Training Loss: 0.005929183909320273\n",
      "Training Loss: 0.006950231875525788\n",
      "Training Loss: 0.006713845467893407\n",
      "Training Loss: 0.0066296027996577325\n",
      "Validation Loss: 0.0038992047208720603\n",
      "Validation Accuracy: 0.0\n",
      "INFO: Validation loss did not improve in epoch 168\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005372696144040674\n",
      "Training Loss: 0.00519216034153942\n",
      "Training Loss: 0.005216785194352269\n",
      "Training Loss: 0.0059246880863793195\n",
      "Training Loss: 0.006947216060943902\n",
      "Training Loss: 0.006710570537252352\n",
      "Training Loss: 0.00662633384228684\n",
      "Validation Loss: 0.003899362126159506\n",
      "Validation Accuracy: 0.0\n",
      "INFO: Validation loss did not improve in epoch 169\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005369547085720115\n",
      "Training Loss: 0.0051890628720866515\n",
      "Training Loss: 0.005213148144539445\n",
      "Training Loss: 0.005920177593361586\n",
      "Training Loss: 0.0069441334612201895\n",
      "Training Loss: 0.006707236230140552\n",
      "Training Loss: 0.006622999398969114\n",
      "Validation Loss: 0.0038996326213018894\n",
      "Validation Accuracy: 0.0\n",
      "INFO: Validation loss did not improve in epoch 170\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0053664120769826695\n",
      "Training Loss: 0.005185977107612416\n",
      "Training Loss: 0.005209550065919757\n",
      "Training Loss: 0.005915625880006701\n",
      "Training Loss: 0.006940953173907474\n",
      "Training Loss: 0.006703811129555106\n",
      "Training Loss: 0.006619566364679486\n",
      "Validation Loss: 0.003900025883595344\n",
      "Validation Accuracy: 0.0\n",
      "INFO: Validation loss did not improve in epoch 171\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005363272092072293\n",
      "Training Loss: 0.005182875950704329\n",
      "Training Loss: 0.005205970004899427\n",
      "Training Loss: 0.005911002391367219\n",
      "Training Loss: 0.006937643178971485\n",
      "Training Loss: 0.006700258993078023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [18:42<27:27, 274.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.006615993975428864\n",
      "Validation Loss: 0.00390054897295332\n",
      "Validation Accuracy: 0.0\n",
      "INFO: Validation loss did not improve in epoch 172\n",
      "Early stopping after 172 epochs\n",
      "Epoch: 1\n",
      "Training Loss: 0.07362146044149995\n",
      "Training Loss: 0.0653688457235694\n",
      "Training Loss: 0.0640603168681264\n",
      "Training Loss: 0.06375404492020607\n",
      "Training Loss: 0.06191727327182889\n",
      "Training Loss: 0.06207784572616219\n",
      "Training Loss: 0.05991902019828558\n",
      "Validation Loss: 0.058982931523957054\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.05855969338677824\n",
      "Training Loss: 0.05589630875736475\n",
      "Training Loss: 0.054467774219810965\n",
      "Training Loss: 0.05363731293007731\n",
      "Training Loss: 0.05094945041462779\n",
      "Training Loss: 0.049914721325039865\n",
      "Training Loss: 0.04653417774476111\n",
      "Validation Loss: 0.044564279228839536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.043331340234726665\n",
      "Training Loss: 0.0398154263291508\n",
      "Training Loss: 0.037109982082620264\n",
      "Training Loss: 0.03573515533469617\n",
      "Training Loss: 0.03307770639657974\n",
      "Training Loss: 0.03185602999292314\n",
      "Training Loss: 0.02940858408343047\n",
      "Validation Loss: 0.028018923263564314\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.02727314471732825\n",
      "Training Loss: 0.025530231669545173\n",
      "Training Loss: 0.023993972786702216\n",
      "Training Loss: 0.02395924648735672\n",
      "Training Loss: 0.02274334830697626\n",
      "Training Loss: 0.022087223599664866\n",
      "Training Loss: 0.020747899585403504\n",
      "Validation Loss: 0.019369395585793456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.01930080450605601\n",
      "Training Loss: 0.01828594849444926\n",
      "Training Loss: 0.017366503169760107\n",
      "Training Loss: 0.01784924304811284\n",
      "Training Loss: 0.017496885345317424\n",
      "Training Loss: 0.017011491460725666\n",
      "Training Loss: 0.016165048291441053\n",
      "Validation Loss: 0.014597126469364886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.015008513983339071\n",
      "Training Loss: 0.01426670781802386\n",
      "Training Loss: 0.013705806821817532\n",
      "Training Loss: 0.014437143211252987\n",
      "Training Loss: 0.014636896206066013\n",
      "Training Loss: 0.014211011317092926\n",
      "Training Loss: 0.013592931078746914\n",
      "Validation Loss: 0.01175815813515461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.012538740162272006\n",
      "Training Loss: 0.01191174611914903\n",
      "Training Loss: 0.011563239946262911\n",
      "Training Loss: 0.012453831329476089\n",
      "Training Loss: 0.012992489989846945\n",
      "Training Loss: 0.012600701677147299\n",
      "Training Loss: 0.01209781586425379\n",
      "Validation Loss: 0.00993437314500216\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.01101818933035247\n",
      "Training Loss: 0.010438126518856734\n",
      "Training Loss: 0.010207301432965324\n",
      "Training Loss: 0.011202428587712348\n",
      "Training Loss: 0.011938659496372565\n",
      "Training Loss: 0.01156426830100827\n",
      "Training Loss: 0.011126337512396276\n",
      "Validation Loss: 0.008630276079488438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.009955221015261487\n",
      "Training Loss: 0.009385570926824585\n",
      "Training Loss: 0.00923078095074743\n",
      "Training Loss: 0.010278999825241044\n",
      "Training Loss: 0.011135178976692259\n",
      "Training Loss: 0.010768809020519257\n",
      "Training Loss: 0.01039620779454708\n",
      "Validation Loss: 0.00766013004234556\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.009157516837585717\n",
      "Training Loss: 0.008603540741605685\n",
      "Training Loss: 0.008526031693909317\n",
      "Training Loss: 0.009591642518062144\n",
      "Training Loss: 0.010526677293237299\n",
      "Training Loss: 0.010191052899463102\n",
      "Training Loss: 0.009903621543198824\n",
      "Validation Loss: 0.007048626642234707\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.00863418503664434\n",
      "Training Loss: 0.008106515138642863\n",
      "Training Loss: 0.008073056373978033\n",
      "Training Loss: 0.00913621116313152\n",
      "Training Loss: 0.010099420569604263\n",
      "Training Loss: 0.009800606539938599\n",
      "Training Loss: 0.009579783485969528\n",
      "Validation Loss: 0.006674355174125301\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.008283335138112307\n",
      "Training Loss: 0.007779582641087472\n",
      "Training Loss: 0.007753946764860302\n",
      "Training Loss: 0.00881054564844817\n",
      "Training Loss: 0.009773905813926832\n",
      "Training Loss: 0.009506437911186367\n",
      "Training Loss: 0.009331034814240411\n",
      "Validation Loss: 0.006412412230909038\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.008014900342095644\n",
      "Training Loss: 0.007535264334874228\n",
      "Training Loss: 0.00750418623094447\n",
      "Training Loss: 0.008557717183139175\n",
      "Training Loss: 0.009515359326032922\n",
      "Training Loss: 0.009272650681668892\n",
      "Training Loss: 0.009128012512810528\n",
      "Validation Loss: 0.006213906285621589\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.007799270595423877\n",
      "Training Loss: 0.007343094018287957\n",
      "Training Loss: 0.0073046645103022455\n",
      "Training Loss: 0.008356677315896377\n",
      "Training Loss: 0.009310234532458708\n",
      "Training Loss: 0.009084620319772512\n",
      "Training Loss: 0.008960846780100838\n",
      "Validation Loss: 0.006057174712391605\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.007623582147061825\n",
      "Training Loss: 0.0071882267715409395\n",
      "Training Loss: 0.007143946330761537\n",
      "Training Loss: 0.008193027323577552\n",
      "Training Loss: 0.009145062938332558\n",
      "Training Loss: 0.008930216713342815\n",
      "Training Loss: 0.008820849697804079\n",
      "Validation Loss: 0.005928457426849041\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.007477146899327635\n",
      "Training Loss: 0.007059784467564896\n",
      "Training Loss: 0.007011390301631764\n",
      "Training Loss: 0.008055623213294894\n",
      "Training Loss: 0.009008116970071568\n",
      "Training Loss: 0.008800284956814721\n",
      "Training Loss: 0.008701166593236848\n",
      "Validation Loss: 0.005819877798839957\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.00735212517902255\n",
      "Training Loss: 0.006950814067386091\n",
      "Training Loss: 0.006899528371868655\n",
      "Training Loss: 0.007937860612291842\n",
      "Training Loss: 0.008892193066421895\n",
      "Training Loss: 0.008689430307131261\n",
      "Training Loss: 0.008597771810600533\n",
      "Validation Loss: 0.005727496026920971\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.007243893020786345\n",
      "Training Loss: 0.006857431201497093\n",
      "Training Loss: 0.0068040504772216084\n",
      "Training Loss: 0.007836298162583261\n",
      "Training Loss: 0.008793385198805482\n",
      "Training Loss: 0.008594596820184961\n",
      "Training Loss: 0.008508460086304695\n",
      "Validation Loss: 0.0056489015884055626\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.007149847701657564\n",
      "Training Loss: 0.006777368388138711\n",
      "Training Loss: 0.006722381735453382\n",
      "Training Loss: 0.007748901087325066\n",
      "Training Loss: 0.00870928404154256\n",
      "Training Loss: 0.008513738921610638\n",
      "Training Loss: 0.008431772848125548\n",
      "Validation Loss: 0.005582063161499435\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.00706829680246301\n",
      "Training Loss: 0.00670900380704552\n",
      "Training Loss: 0.006652648312738165\n",
      "Training Loss: 0.007674068519845605\n",
      "Training Loss: 0.008638010777067393\n",
      "Training Loss: 0.008445147268939763\n",
      "Training Loss: 0.008366386386333033\n",
      "Validation Loss: 0.0055251008358327075\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.006997831133194268\n",
      "Training Loss: 0.006650881285313517\n",
      "Training Loss: 0.006593189735431224\n",
      "Training Loss: 0.007610238523920998\n",
      "Training Loss: 0.00857782583218068\n",
      "Training Loss: 0.008387175885727629\n",
      "Training Loss: 0.008310892839217559\n",
      "Validation Loss: 0.0054762956009626615\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.006937069832347333\n",
      "Training Loss: 0.0066015436255838725\n",
      "Training Loss: 0.006542436300078407\n",
      "Training Loss: 0.007555833897786215\n",
      "Training Loss: 0.00852705102530308\n",
      "Training Loss: 0.008338205015752465\n",
      "Training Loss: 0.008263797495746985\n",
      "Validation Loss: 0.005434176711930653\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.00688464654609561\n",
      "Training Loss: 0.006559570864774287\n",
      "Training Loss: 0.006498941825702787\n",
      "Training Loss: 0.0075093314424157145\n",
      "Training Loss: 0.008484103016089648\n",
      "Training Loss: 0.008296692162984983\n",
      "Training Loss: 0.00822363155311905\n",
      "Validation Loss: 0.005397520900951622\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.006839257776737213\n",
      "Training Loss: 0.006523639076622203\n",
      "Training Loss: 0.006461416680831462\n",
      "Training Loss: 0.0074693315394688395\n",
      "Training Loss: 0.008447535925079137\n",
      "Training Loss: 0.00826123307342641\n",
      "Training Loss: 0.00818904489162378\n",
      "Validation Loss: 0.0053653199712254025\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.006799713913351297\n",
      "Training Loss: 0.006492589213885367\n",
      "Training Loss: 0.006428757878020406\n",
      "Training Loss: 0.007434610013151541\n",
      "Training Loss: 0.008416085160570219\n",
      "Training Loss: 0.008230603598058224\n",
      "Training Loss: 0.008158873295178637\n",
      "Validation Loss: 0.005336753450236405\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.006764980211155489\n",
      "Training Loss: 0.006465450006071478\n",
      "Training Loss: 0.006400046782800928\n",
      "Training Loss: 0.007404137453995645\n",
      "Training Loss: 0.008388679001946002\n",
      "Training Loss: 0.008203778221504763\n",
      "Training Loss: 0.008132155645871535\n",
      "Validation Loss: 0.00531117189658129\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.006734187464462593\n",
      "Training Loss: 0.006441438243491575\n",
      "Training Loss: 0.006374539708485827\n",
      "Training Loss: 0.007377074260730297\n",
      "Training Loss: 0.008364442067686469\n",
      "Training Loss: 0.008179926002630964\n",
      "Training Loss: 0.008108129734173417\n",
      "Validation Loss: 0.005288043580879544\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.006706617588642985\n",
      "Training Loss: 0.006419933036668226\n",
      "Training Loss: 0.0063516341219656165\n",
      "Training Loss: 0.007352747423574328\n",
      "Training Loss: 0.008342677582986653\n",
      "Training Loss: 0.008158389640739188\n",
      "Training Loss: 0.008086197798838839\n",
      "Validation Loss: 0.005266946837243237\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.0066816923266742375\n",
      "Training Loss: 0.006400454779504798\n",
      "Training Loss: 0.006330860601738095\n",
      "Training Loss: 0.007330627361079678\n",
      "Training Loss: 0.008322839970933273\n",
      "Training Loss: 0.008138661253033206\n",
      "Training Loss: 0.008065909082069993\n",
      "Validation Loss: 0.005247546904040187\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006658945906674489\n",
      "Training Loss: 0.00638262590218801\n",
      "Training Loss: 0.0063118376780767\n",
      "Training Loss: 0.0073102982714772224\n",
      "Training Loss: 0.008304512909380718\n",
      "Training Loss: 0.008120350643293932\n",
      "Training Loss: 0.008046916311141104\n",
      "Validation Loss: 0.005229563313651453\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006637999785598367\n",
      "Training Loss: 0.00636615036521107\n",
      "Training Loss: 0.00629426199477166\n",
      "Training Loss: 0.007291433601640165\n",
      "Training Loss: 0.008287376124644651\n",
      "Training Loss: 0.008103163859341294\n",
      "Training Loss: 0.00802896214532666\n",
      "Validation Loss: 0.005212772097822265\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006618555401219055\n",
      "Training Loss: 0.006350800614454783\n",
      "Training Loss: 0.006277897015679628\n",
      "Training Loss: 0.007273778667440638\n",
      "Training Loss: 0.008271188725484536\n",
      "Training Loss: 0.008086875809822232\n",
      "Training Loss: 0.008011847357265652\n",
      "Validation Loss: 0.0051969915998171775\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.006600369372172281\n",
      "Training Loss: 0.006336390934884548\n",
      "Training Loss: 0.00626254613744095\n",
      "Training Loss: 0.007257131539518014\n",
      "Training Loss: 0.008255766191286967\n",
      "Training Loss: 0.00807131868088618\n",
      "Training Loss: 0.007995426086708904\n",
      "Validation Loss: 0.00518206581525672\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006583242624765262\n",
      "Training Loss: 0.006322773991269059\n",
      "Training Loss: 0.006248054906027391\n",
      "Training Loss: 0.007241332846460864\n",
      "Training Loss: 0.008240969083271921\n",
      "Training Loss: 0.00805636142147705\n",
      "Training Loss: 0.007979579874081537\n",
      "Validation Loss: 0.005167871163390605\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.0065670180891174825\n",
      "Training Loss: 0.006309832209954038\n",
      "Training Loss: 0.006234294170280918\n",
      "Training Loss: 0.007226253267144784\n",
      "Training Loss: 0.00822668835055083\n",
      "Training Loss: 0.008041903246194124\n",
      "Training Loss: 0.00796422200743109\n",
      "Validation Loss: 0.005154299062296525\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.006551560497609898\n",
      "Training Loss: 0.006297466399846598\n",
      "Training Loss: 0.006221159090055153\n",
      "Training Loss: 0.007211789245484396\n",
      "Training Loss: 0.008212842614157126\n",
      "Training Loss: 0.008027867132332176\n",
      "Training Loss: 0.00794928040006198\n",
      "Validation Loss: 0.005141257890064116\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006536762327887118\n",
      "Training Loss: 0.006285597642417997\n",
      "Training Loss: 0.006208562939427793\n",
      "Training Loss: 0.007197855953127146\n",
      "Training Loss: 0.008199365519685671\n",
      "Training Loss: 0.00801419042982161\n",
      "Training Loss: 0.007934699315810576\n",
      "Validation Loss: 0.005128672149395117\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006522531323134899\n",
      "Training Loss: 0.006274155891733244\n",
      "Training Loss: 0.006196432356955483\n",
      "Training Loss: 0.007184381341794506\n",
      "Training Loss: 0.008186201773351059\n",
      "Training Loss: 0.008000820960151032\n",
      "Training Loss: 0.007920431835809722\n",
      "Validation Loss: 0.005116470689163282\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006508787670172751\n",
      "Training Loss: 0.006263083386584185\n",
      "Training Loss: 0.006184702294413\n",
      "Training Loss: 0.007171305005904287\n",
      "Training Loss: 0.008173310611164197\n",
      "Training Loss: 0.007987719163065777\n",
      "Training Loss: 0.007906439247308298\n",
      "Validation Loss: 0.00510460018853514\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006495467406930402\n",
      "Training Loss: 0.006252330709830858\n",
      "Training Loss: 0.006173321534879506\n",
      "Training Loss: 0.007158576103975065\n",
      "Training Loss: 0.008160655231913552\n",
      "Training Loss: 0.007974847317673266\n",
      "Training Loss: 0.007892689241562039\n",
      "Validation Loss: 0.005093011416473918\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.00648251359234564\n",
      "Training Loss: 0.006241856514825486\n",
      "Training Loss: 0.00616224514436908\n",
      "Training Loss: 0.007146150640910492\n",
      "Training Loss: 0.00814820548053831\n",
      "Training Loss: 0.007962176306173206\n",
      "Training Loss: 0.007879151507513598\n",
      "Validation Loss: 0.005081656572063652\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006469876521732658\n",
      "Training Loss: 0.0062316221103537826\n",
      "Training Loss: 0.0061514326778706165\n",
      "Training Loss: 0.007133989329449832\n",
      "Training Loss: 0.008135935319587588\n",
      "Training Loss: 0.007949678947916254\n",
      "Training Loss: 0.007865802476881073\n",
      "Validation Loss: 0.00507050214251608\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006457515825750306\n",
      "Training Loss: 0.006221597025287338\n",
      "Training Loss: 0.00614085178473033\n",
      "Training Loss: 0.007122059792163782\n",
      "Training Loss: 0.008123821304179728\n",
      "Training Loss: 0.007937331891153008\n",
      "Training Loss: 0.00785261731944047\n",
      "Validation Loss: 0.0050595213706197024\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006445396627532319\n",
      "Training Loss: 0.00621175485837739\n",
      "Training Loss: 0.006130474027013406\n",
      "Training Loss: 0.007110330000868998\n",
      "Training Loss: 0.00811184365535155\n",
      "Training Loss: 0.007925112711964175\n",
      "Training Loss: 0.007839577036211267\n",
      "Validation Loss: 0.005048674371700441\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006433482925640419\n",
      "Training Loss: 0.006202065646648407\n",
      "Training Loss: 0.00612026920192875\n",
      "Training Loss: 0.007098774880869314\n",
      "Training Loss: 0.00809998567099683\n",
      "Training Loss: 0.007913004788570106\n",
      "Training Loss: 0.007826662842417136\n",
      "Validation Loss: 0.005037938239408678\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006421745329862461\n",
      "Training Loss: 0.0061925089918076996\n",
      "Training Loss: 0.006110217028763145\n",
      "Training Loss: 0.007087370330700651\n",
      "Training Loss: 0.008088230331195519\n",
      "Training Loss: 0.007900988610927015\n",
      "Training Loss: 0.007813857120927423\n",
      "Validation Loss: 0.005027290177269933\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006410161022795364\n",
      "Training Loss: 0.0061830649297917265\n",
      "Training Loss: 0.006100295590003952\n",
      "Training Loss: 0.007076092279166914\n",
      "Training Loss: 0.008076562234200536\n",
      "Training Loss: 0.007889047138160094\n",
      "Training Loss: 0.007801142754033208\n",
      "Validation Loss: 0.005016709489517667\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006398704560706392\n",
      "Training Loss: 0.0061737127671949565\n",
      "Training Loss: 0.006090483200969174\n",
      "Training Loss: 0.007064922886784188\n",
      "Training Loss: 0.008064968262333423\n",
      "Training Loss: 0.007877166245598345\n",
      "Training Loss: 0.007788508081575856\n",
      "Validation Loss: 0.0050061717110577875\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006387354276375845\n",
      "Training Loss: 0.006164438400883227\n",
      "Training Loss: 0.006080767007078975\n",
      "Training Loss: 0.0070538417395437135\n",
      "Training Loss: 0.008053435793844982\n",
      "Training Loss: 0.007865331214852632\n",
      "Training Loss: 0.007775936691323295\n",
      "Validation Loss: 0.004995666327450271\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006376093561411836\n",
      "Training Loss: 0.00615522586798761\n",
      "Training Loss: 0.00607113093836233\n",
      "Training Loss: 0.007042832504375838\n",
      "Training Loss: 0.008041953201172873\n",
      "Training Loss: 0.007853528355481103\n",
      "Training Loss: 0.007763414571527392\n",
      "Validation Loss: 0.004985172316791273\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006364902788773179\n",
      "Training Loss: 0.006146059480379336\n",
      "Training Loss: 0.006061557193752378\n",
      "Training Loss: 0.007031880465801805\n",
      "Training Loss: 0.008030510282842442\n",
      "Training Loss: 0.007841747733764351\n",
      "Training Loss: 0.0077509317768272015\n",
      "Validation Loss: 0.004974673864473537\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006353765996755101\n",
      "Training Loss: 0.006136925765313208\n",
      "Training Loss: 0.006052034945460036\n",
      "Training Loss: 0.007020969127188437\n",
      "Training Loss: 0.008019094372866676\n",
      "Training Loss: 0.007829974690685049\n",
      "Training Loss: 0.007738476154627278\n",
      "Validation Loss: 0.004964156224556295\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006342667224816978\n",
      "Training Loss: 0.0061278134776512165\n",
      "Training Loss: 0.0060425514436792584\n",
      "Training Loss: 0.007010084975627251\n",
      "Training Loss: 0.008007697410648688\n",
      "Training Loss: 0.00781819887459278\n",
      "Training Loss: 0.00772603529971093\n",
      "Validation Loss: 0.004953610965866972\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006331594800576568\n",
      "Training Loss: 0.006118710136506706\n",
      "Training Loss: 0.006033095576567575\n",
      "Training Loss: 0.0069992136239307\n",
      "Training Loss: 0.007996307833818718\n",
      "Training Loss: 0.007806409086333588\n",
      "Training Loss: 0.007713599317939952\n",
      "Validation Loss: 0.004943020996717869\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006320535214035772\n",
      "Training Loss: 0.006109607898397371\n",
      "Training Loss: 0.006023657794576138\n",
      "Training Loss: 0.00698834421869833\n",
      "Training Loss: 0.007984918940346688\n",
      "Training Loss: 0.00779459499521181\n",
      "Training Loss: 0.007701157730771228\n",
      "Validation Loss: 0.004932374703917611\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.006309476302121766\n",
      "Training Loss: 0.006100494500133209\n",
      "Training Loss: 0.006014227125560865\n",
      "Training Loss: 0.006977465707459487\n",
      "Training Loss: 0.007973520327941514\n",
      "Training Loss: 0.007782748318277299\n",
      "Training Loss: 0.007688702379819006\n",
      "Validation Loss: 0.004921659783631117\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0062984054494882\n",
      "Training Loss: 0.006091359993442893\n",
      "Training Loss: 0.00600479232496582\n",
      "Training Loss: 0.006966565599432215\n",
      "Training Loss: 0.007962107190978713\n",
      "Training Loss: 0.0077708604815416036\n",
      "Training Loss: 0.007676224968163297\n",
      "Validation Loss: 0.004910866215856474\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006287313768989406\n",
      "Training Loss: 0.0060821962228510525\n",
      "Training Loss: 0.0059953469142783435\n",
      "Training Loss: 0.006955633537145332\n",
      "Training Loss: 0.007950668435078113\n",
      "Training Loss: 0.007758921480271966\n",
      "Training Loss: 0.007663715950911865\n",
      "Validation Loss: 0.004899983480485498\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006276190932840109\n",
      "Training Loss: 0.00607299470633734\n",
      "Training Loss: 0.005985879885265603\n",
      "Training Loss: 0.006944660228327848\n",
      "Training Loss: 0.007939197206869721\n",
      "Training Loss: 0.007746924239909276\n",
      "Training Loss: 0.00765116808237508\n",
      "Validation Loss: 0.004889001922458689\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.00626502834376879\n",
      "Training Loss: 0.006063746793661267\n",
      "Training Loss: 0.005976384350797162\n",
      "Training Loss: 0.006933634962770156\n",
      "Training Loss: 0.007927687068586238\n",
      "Training Loss: 0.007734860472846776\n",
      "Training Loss: 0.007638573200674728\n",
      "Validation Loss: 0.004877907596354125\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.00625381585676223\n",
      "Training Loss: 0.006054444730398245\n",
      "Training Loss: 0.005966851327102631\n",
      "Training Loss: 0.0069225487642688675\n",
      "Training Loss: 0.007916129708173686\n",
      "Training Loss: 0.007722724257037044\n",
      "Training Loss: 0.0076259242394007744\n",
      "Validation Loss: 0.004866701885203269\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006242550088791177\n",
      "Training Loss: 0.00604508443037048\n",
      "Training Loss: 0.005957279009744525\n",
      "Training Loss: 0.006911392376059667\n",
      "Training Loss: 0.007904518411378377\n",
      "Training Loss: 0.007710506990551948\n",
      "Training Loss: 0.007613216108875349\n",
      "Validation Loss: 0.00485536801996763\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006231220079353079\n",
      "Training Loss: 0.006035655133309774\n",
      "Training Loss: 0.005947654272895306\n",
      "Training Loss: 0.006900158099015243\n",
      "Training Loss: 0.007892848269548267\n",
      "Training Loss: 0.007698204879416153\n",
      "Training Loss: 0.007600442187394947\n",
      "Validation Loss: 0.004843901548524167\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006219822028651833\n",
      "Training Loss: 0.006026153895072639\n",
      "Training Loss: 0.005937974164262414\n",
      "Training Loss: 0.0068888376140967015\n",
      "Training Loss: 0.007881112323957495\n",
      "Training Loss: 0.007685811147093773\n",
      "Training Loss: 0.007587598056998104\n",
      "Validation Loss: 0.004832298181382486\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006208352142130025\n",
      "Training Loss: 0.006016575243556872\n",
      "Training Loss: 0.005928235112223774\n",
      "Training Loss: 0.006877424689009786\n",
      "Training Loss: 0.007869306174688973\n",
      "Training Loss: 0.007673322933260352\n",
      "Training Loss: 0.007574679725803435\n",
      "Validation Loss: 0.0048205559979077805\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006196808517561294\n",
      "Training Loss: 0.006006914448225871\n",
      "Training Loss: 0.005918431668542326\n",
      "Training Loss: 0.006865913082147017\n",
      "Training Loss: 0.007857426544069313\n",
      "Training Loss: 0.007660736620891839\n",
      "Training Loss: 0.007561685291584581\n",
      "Validation Loss: 0.004808666734792264\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.006185187806840986\n",
      "Training Loss: 0.005997169321635738\n",
      "Training Loss: 0.005908562013646587\n",
      "Training Loss: 0.006854298036196269\n",
      "Training Loss: 0.007845469246967695\n",
      "Training Loss: 0.007648052787408233\n",
      "Training Loss: 0.007548612876562401\n",
      "Validation Loss: 0.004796629989160441\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.00617348907107953\n",
      "Training Loss: 0.005987335204845294\n",
      "Training Loss: 0.005898622628301382\n",
      "Training Loss: 0.006842574625625275\n",
      "Training Loss: 0.007833431402686983\n",
      "Training Loss: 0.0076352692372165624\n",
      "Training Loss: 0.007535462863743305\n",
      "Validation Loss: 0.004784450215751424\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0061617159366142\n",
      "Training Loss: 0.005977415819652379\n",
      "Training Loss: 0.005888615780277178\n",
      "Training Loss: 0.006830739655997604\n",
      "Training Loss: 0.007821312894229777\n",
      "Training Loss: 0.0076223890634719285\n",
      "Training Loss: 0.007522237146040425\n",
      "Validation Loss: 0.004772131565839666\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006149872720125131\n",
      "Training Loss: 0.005967408989090472\n",
      "Training Loss: 0.005878542326390743\n",
      "Training Loss: 0.006818792773992754\n",
      "Training Loss: 0.007809114925912582\n",
      "Training Loss: 0.007609415535116568\n",
      "Training Loss: 0.007508937424281612\n",
      "Validation Loss: 0.004759682936259032\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006137965904781595\n",
      "Training Loss: 0.005957318338914774\n",
      "Training Loss: 0.005868403408676386\n",
      "Training Loss: 0.00680673319264315\n",
      "Training Loss: 0.007796840482042171\n",
      "Training Loss: 0.00759635483729653\n",
      "Training Loss: 0.007495571878971532\n",
      "Validation Loss: 0.004747108119009726\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006126001530792564\n",
      "Training Loss: 0.005947147376718931\n",
      "Training Loss: 0.005858205470722169\n",
      "Training Loss: 0.006794564097654074\n",
      "Training Loss: 0.007784492612117901\n",
      "Training Loss: 0.007583217875799164\n",
      "Training Loss: 0.007482146242400631\n",
      "Validation Loss: 0.004734423893594535\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006113991527818143\n",
      "Training Loss: 0.005936901930836029\n",
      "Training Loss: 0.005847953365882858\n",
      "Training Loss: 0.006782289604889229\n",
      "Training Loss: 0.007772080940776504\n",
      "Training Loss: 0.007570015653036535\n",
      "Training Loss: 0.007468673716066405\n",
      "Validation Loss: 0.0047216485568395495\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006101949966978282\n",
      "Training Loss: 0.005926590992021375\n",
      "Training Loss: 0.0058376588486135006\n",
      "Training Loss: 0.006769914476317354\n",
      "Training Loss: 0.007759610959910787\n",
      "Training Loss: 0.007556760845473036\n",
      "Training Loss: 0.007455162628320977\n",
      "Validation Loss: 0.004708800881260427\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006089891460142098\n",
      "Training Loss: 0.0059162241220474246\n",
      "Training Loss: 0.005827329691965133\n",
      "Training Loss: 0.006757450131117366\n",
      "Training Loss: 0.007747097047395073\n",
      "Training Loss: 0.007543471761746332\n",
      "Training Loss: 0.007441629836102947\n",
      "Validation Loss: 0.004695905593122715\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006077832876471802\n",
      "Training Loss: 0.005905811910633929\n",
      "Training Loss: 0.005816977168433368\n",
      "Training Loss: 0.006744906259700656\n",
      "Training Loss: 0.007734554526978172\n",
      "Training Loss: 0.007530169825768099\n",
      "Training Loss: 0.007428094416391104\n",
      "Validation Loss: 0.004682981140548817\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006065792620647699\n",
      "Training Loss: 0.005895367201883345\n",
      "Training Loss: 0.005806614407338202\n",
      "Training Loss: 0.006732296729460359\n",
      "Training Loss: 0.007721999683999456\n",
      "Training Loss: 0.007516874829307198\n",
      "Training Loss: 0.007414572876878083\n",
      "Validation Loss: 0.004670063129186714\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.006053791414597071\n",
      "Training Loss: 0.005884905099519528\n",
      "Training Loss: 0.005796258029295132\n",
      "Training Loss: 0.006719634655746631\n",
      "Training Loss: 0.007709450055845082\n",
      "Training Loss: 0.007503609780687839\n",
      "Training Loss: 0.007401084927842021\n",
      "Validation Loss: 0.004657175192218762\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00604184806230478\n",
      "Training Loss: 0.005874437060556375\n",
      "Training Loss: 0.0057859176408965145\n",
      "Training Loss: 0.0067069358722073955\n",
      "Training Loss: 0.007696923647890799\n",
      "Training Loss: 0.007490397110814229\n",
      "Training Loss: 0.0073876503913197665\n",
      "Validation Loss: 0.004644343151173761\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006029979800805449\n",
      "Training Loss: 0.005863978426204995\n",
      "Training Loss: 0.005775610158452764\n",
      "Training Loss: 0.006694218651973643\n",
      "Training Loss: 0.007684442063327879\n",
      "Training Loss: 0.007477260836167261\n",
      "Training Loss: 0.0073742896807380024\n",
      "Validation Loss: 0.00463159772007611\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006018207117449492\n",
      "Training Loss: 0.005853541735559702\n",
      "Training Loss: 0.005765347104752436\n",
      "Training Loss: 0.006681499643018469\n",
      "Training Loss: 0.007672024816274643\n",
      "Training Loss: 0.007464221846312284\n",
      "Training Loss: 0.007361022684490308\n",
      "Validation Loss: 0.004618961188753837\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006006546018179506\n",
      "Training Loss: 0.005843142417143099\n",
      "Training Loss: 0.005755143316928297\n",
      "Training Loss: 0.00666879492055159\n",
      "Training Loss: 0.007659689058782533\n",
      "Training Loss: 0.0074513004207983615\n",
      "Training Loss: 0.007347866422496736\n",
      "Validation Loss: 0.004606459639471178\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.005995010557817295\n",
      "Training Loss: 0.005832788216066547\n",
      "Training Loss: 0.005745006192009896\n",
      "Training Loss: 0.006656122170388698\n",
      "Training Loss: 0.007647455227561295\n",
      "Training Loss: 0.0074385159183293584\n",
      "Training Loss: 0.007334837709786371\n",
      "Validation Loss: 0.0045941031053104205\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.005983606876689009\n",
      "Training Loss: 0.005822488333797083\n",
      "Training Loss: 0.005734943528659641\n",
      "Training Loss: 0.006643494620220736\n",
      "Training Loss: 0.007635337905958295\n",
      "Training Loss: 0.007425881356466562\n",
      "Training Loss: 0.007321951194899157\n",
      "Validation Loss: 0.0045819110193460735\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.005972344182082452\n",
      "Training Loss: 0.005812249607406557\n",
      "Training Loss: 0.005724963587708771\n",
      "Training Loss: 0.006630924887722358\n",
      "Training Loss: 0.007623348253546282\n",
      "Training Loss: 0.007413406802807003\n",
      "Training Loss: 0.007309214319102466\n",
      "Validation Loss: 0.0045698887515073635\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.0059612230630591515\n",
      "Training Loss: 0.0058020763250533494\n",
      "Training Loss: 0.005715068083954975\n",
      "Training Loss: 0.006618423232575879\n",
      "Training Loss: 0.007611494459561072\n",
      "Training Loss: 0.007401099258568138\n",
      "Training Loss: 0.00729663469013758\n",
      "Validation Loss: 0.004558045662545393\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.005950245688436553\n",
      "Training Loss: 0.0057919709658017385\n",
      "Training Loss: 0.005705259726382792\n",
      "Training Loss: 0.006605998380109668\n",
      "Training Loss: 0.00759978385118302\n",
      "Training Loss: 0.0073889618925750255\n",
      "Training Loss: 0.007284216616535559\n",
      "Validation Loss: 0.004546377614266082\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.005939406570978463\n",
      "Training Loss: 0.005781932735117152\n",
      "Training Loss: 0.005695537189021707\n",
      "Training Loss: 0.006593655113247224\n",
      "Training Loss: 0.007588219003519043\n",
      "Training Loss: 0.0073769943788647655\n",
      "Training Loss: 0.007271960438229144\n",
      "Validation Loss: 0.004534885565460788\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.005928699841024354\n",
      "Training Loss: 0.005771959931589663\n",
      "Training Loss: 0.005685894435737282\n",
      "Training Loss: 0.006581398872076534\n",
      "Training Loss: 0.007576799600501545\n",
      "Training Loss: 0.007365193013101816\n",
      "Training Loss: 0.007259864422958344\n",
      "Validation Loss: 0.004523556921474134\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.005918112929794006\n",
      "Training Loss: 0.005762046974268742\n",
      "Training Loss: 0.0056763279833830895\n",
      "Training Loss: 0.006569227037834935\n",
      "Training Loss: 0.007565521752112545\n",
      "Training Loss: 0.00735354981618002\n",
      "Training Loss: 0.007247921541566029\n",
      "Validation Loss: 0.004512388539964452\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.005907637790078297\n",
      "Training Loss: 0.005752190044149757\n",
      "Training Loss: 0.005666832394199446\n",
      "Training Loss: 0.006557141526718624\n",
      "Training Loss: 0.0075543791707605125\n",
      "Training Loss: 0.007342057727510109\n",
      "Training Loss: 0.007236125404015184\n",
      "Validation Loss: 0.0045013616209473\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.00589725898229517\n",
      "Training Loss: 0.005742378954892047\n",
      "Training Loss: 0.005657395481830463\n",
      "Training Loss: 0.0065451388014480475\n",
      "Training Loss: 0.007543363720178604\n",
      "Training Loss: 0.007330701205646619\n",
      "Training Loss: 0.0072244657261762765\n",
      "Validation Loss: 0.004490465809999222\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.005886962183867581\n",
      "Training Loss: 0.0057326071709394456\n",
      "Training Loss: 0.005648010992445052\n",
      "Training Loss: 0.006533213573275134\n",
      "Training Loss: 0.007532464587129652\n",
      "Training Loss: 0.00731947026331909\n",
      "Training Loss: 0.007212930982932448\n",
      "Validation Loss: 0.004479687254132933\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.005876734788180329\n",
      "Training Loss: 0.005722867680015042\n",
      "Training Loss: 0.005638669326435775\n",
      "Training Loss: 0.006521359792095609\n",
      "Training Loss: 0.007521669482812286\n",
      "Training Loss: 0.007308348246151581\n",
      "Training Loss: 0.0072015089122578506\n",
      "Validation Loss: 0.0044690082342388315\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.005866560175782069\n",
      "Training Loss: 0.005713149294024333\n",
      "Training Loss: 0.00562935923691839\n",
      "Training Loss: 0.006509572632494382\n",
      "Training Loss: 0.007510967853013426\n",
      "Training Loss: 0.0072973227861803025\n",
      "Training Loss: 0.007190186610678211\n",
      "Validation Loss: 0.004458412109902094\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.005856424069497735\n",
      "Training Loss: 0.005703444475657306\n",
      "Training Loss: 0.005620073176687584\n",
      "Training Loss: 0.006497844076366163\n",
      "Training Loss: 0.007500346257002093\n",
      "Training Loss: 0.007286377532873303\n",
      "Training Loss: 0.007178952221293003\n",
      "Validation Loss: 0.004447888572916584\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.005846316915121861\n",
      "Training Loss: 0.005693746102042496\n",
      "Training Loss: 0.005610799216665328\n",
      "Training Loss: 0.006486168386181816\n",
      "Training Loss: 0.007489791007828899\n",
      "Training Loss: 0.007275501760886982\n",
      "Training Loss: 0.007167790895327926\n",
      "Validation Loss: 0.004437417114129088\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.0058362180355470625\n",
      "Training Loss: 0.005684042401262559\n",
      "Training Loss: 0.005601528283441439\n",
      "Training Loss: 0.006474536448949948\n",
      "Training Loss: 0.0074792890943354\n",
      "Training Loss: 0.007264676649356261\n",
      "Training Loss: 0.007156690793344751\n",
      "Validation Loss: 0.00442698879920867\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.005826123167644255\n",
      "Training Loss: 0.005674328861641697\n",
      "Training Loss: 0.005592252898495644\n",
      "Training Loss: 0.006462944375234656\n",
      "Training Loss: 0.007468829491408542\n",
      "Training Loss: 0.007253894711611792\n",
      "Training Loss: 0.007145641769748181\n",
      "Validation Loss: 0.004416584393030472\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.005816016303142533\n",
      "Training Loss: 0.005664597587310709\n",
      "Training Loss: 0.005582964116474614\n",
      "Training Loss: 0.006451382808154449\n",
      "Training Loss: 0.007458400706527754\n",
      "Training Loss: 0.007243141134968028\n",
      "Training Loss: 0.0071346304798498746\n",
      "Validation Loss: 0.0044061959575519855\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0058058903261553494\n",
      "Training Loss: 0.005654842157964595\n",
      "Training Loss: 0.005573655187617987\n",
      "Training Loss: 0.006439848041627556\n",
      "Training Loss: 0.007447990281507373\n",
      "Training Loss: 0.007232406928669661\n",
      "Training Loss: 0.00712364875129424\n",
      "Validation Loss: 0.004395814162106968\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.005795737929875031\n",
      "Training Loss: 0.005645059028174728\n",
      "Training Loss: 0.0055643204448278995\n",
      "Training Loss: 0.006428334891679697\n",
      "Training Loss: 0.007437591553898528\n",
      "Training Loss: 0.007221683713141829\n",
      "Training Loss: 0.007112688096240163\n",
      "Validation Loss: 0.004385428514056019\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.005785551372682675\n",
      "Training Loss: 0.005635242516873405\n",
      "Training Loss: 0.00555495243286714\n",
      "Training Loss: 0.006416838523000479\n",
      "Training Loss: 0.007427194108022377\n",
      "Training Loss: 0.007210964448750019\n",
      "Training Loss: 0.007101741364458576\n",
      "Validation Loss: 0.00437502836902741\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.005775323701673187\n",
      "Training Loss: 0.005625388610060327\n",
      "Training Loss: 0.005545546651119366\n",
      "Training Loss: 0.006405355270253495\n",
      "Training Loss: 0.007416792365256697\n",
      "Training Loss: 0.007200243955012411\n",
      "Training Loss: 0.007090801686281338\n",
      "Validation Loss: 0.0043646083492436885\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.005765051938942634\n",
      "Training Loss: 0.0056154947291361165\n",
      "Training Loss: 0.005536098197335377\n",
      "Training Loss: 0.006393882579868659\n",
      "Training Loss: 0.007406379325548187\n",
      "Training Loss: 0.007189517231890932\n",
      "Training Loss: 0.007079864415572956\n",
      "Validation Loss: 0.004354166711527115\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.005754733138019219\n",
      "Training Loss: 0.005605559647083282\n",
      "Training Loss: 0.005526608035434037\n",
      "Training Loss: 0.00638242055894807\n",
      "Training Loss: 0.007395951622747816\n",
      "Training Loss: 0.007178781756665558\n",
      "Training Loss: 0.007068927116924897\n",
      "Validation Loss: 0.004343697285865632\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.0057443663908634335\n",
      "Training Loss: 0.005595583195681684\n",
      "Training Loss: 0.00551707147853449\n",
      "Training Loss: 0.006370965681271628\n",
      "Training Loss: 0.007385505466954783\n",
      "Training Loss: 0.007168037286028266\n",
      "Training Loss: 0.007057986477157101\n",
      "Validation Loss: 0.004333198193481632\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.005733950090361759\n",
      "Training Loss: 0.005585564883658662\n",
      "Training Loss: 0.00550748887239024\n",
      "Training Loss: 0.006359519886900671\n",
      "Training Loss: 0.007375040091574192\n",
      "Training Loss: 0.007157283909618855\n",
      "Training Loss: 0.007047041890909895\n",
      "Validation Loss: 0.004322670198704922\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.0057234867796069015\n",
      "Training Loss: 0.005575506323948503\n",
      "Training Loss: 0.005497858065646142\n",
      "Training Loss: 0.006348083950579167\n",
      "Training Loss: 0.007364554557716474\n",
      "Training Loss: 0.007146525343414396\n",
      "Training Loss: 0.00703609534073621\n",
      "Validation Loss: 0.004312114425575577\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.005712978155934252\n",
      "Training Loss: 0.005565410762210376\n",
      "Training Loss: 0.005488184419227764\n",
      "Training Loss: 0.006336659950320609\n",
      "Training Loss: 0.007354051875881851\n",
      "Training Loss: 0.007135764353442937\n",
      "Training Loss: 0.007025149236433208\n",
      "Validation Loss: 0.00430152879039014\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.005702427624491975\n",
      "Training Loss: 0.005555280361440964\n",
      "Training Loss: 0.005478467696812004\n",
      "Training Loss: 0.00632525201595854\n",
      "Training Loss: 0.0073435347294434905\n",
      "Training Loss: 0.00712500684778206\n",
      "Training Loss: 0.00701420653029345\n",
      "Validation Loss: 0.004290922593729996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0056918391969520594\n",
      "Training Loss: 0.005545119221205823\n",
      "Training Loss: 0.0054687109647784385\n",
      "Training Loss: 0.006313863202231005\n",
      "Training Loss: 0.007333006546832621\n",
      "Training Loss: 0.007114260263042524\n",
      "Training Loss: 0.007003272584406659\n",
      "Validation Loss: 0.004280296287304035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.005681218322133645\n",
      "Training Loss: 0.005534933218732476\n",
      "Training Loss: 0.005458919369848445\n",
      "Training Loss: 0.0063024979445617645\n",
      "Training Loss: 0.007322474009124562\n",
      "Training Loss: 0.007103532146429643\n",
      "Training Loss: 0.006992351481458172\n",
      "Validation Loss: 0.004269659311653635\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.005670572313247249\n",
      "Training Loss: 0.005524728203308769\n",
      "Training Loss: 0.0054490967083256694\n",
      "Training Loss: 0.006291162694105878\n",
      "Training Loss: 0.007311943470267579\n",
      "Training Loss: 0.007092830634210259\n",
      "Training Loss: 0.006981450668536127\n",
      "Validation Loss: 0.004259016356528615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.005659908243687823\n",
      "Training Loss: 0.005514509107451886\n",
      "Training Loss: 0.005439250584458932\n",
      "Training Loss: 0.006279862546361983\n",
      "Training Loss: 0.007301424089819193\n",
      "Training Loss: 0.007082166771870107\n",
      "Training Loss: 0.006970580188790336\n",
      "Validation Loss: 0.004248377512564904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.005649234990705736\n",
      "Training Loss: 0.005504287074436434\n",
      "Training Loss: 0.005429388877237215\n",
      "Training Loss: 0.006268605962977744\n",
      "Training Loss: 0.007290924489498138\n",
      "Training Loss: 0.007071551623521372\n",
      "Training Loss: 0.006959746084176004\n",
      "Validation Loss: 0.004237754951485813\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.005638562191743404\n",
      "Training Loss: 0.005494069028063677\n",
      "Training Loss: 0.005419518244452775\n",
      "Training Loss: 0.00625740013201721\n",
      "Training Loss: 0.007280455575091764\n",
      "Training Loss: 0.0070609980658628045\n",
      "Training Loss: 0.006948960780864582\n",
      "Validation Loss: 0.004227153335572377\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.005627900183899328\n",
      "Training Loss: 0.005483864700654522\n",
      "Training Loss: 0.005409648501081392\n",
      "Training Loss: 0.006246253431309015\n",
      "Training Loss: 0.007270028539933264\n",
      "Training Loss: 0.007050517254974693\n",
      "Training Loss: 0.006938233081018552\n",
      "Validation Loss: 0.004216591971019485\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.005617259851424024\n",
      "Training Loss: 0.005473683900781907\n",
      "Training Loss: 0.0053997900325339285\n",
      "Training Loss: 0.006235174366738647\n",
      "Training Loss: 0.007259657478425652\n",
      "Training Loss: 0.007040122910402715\n",
      "Training Loss: 0.00692757451790385\n",
      "Validation Loss: 0.004206077075800562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.005606651169946417\n",
      "Training Loss: 0.005463536289171316\n",
      "Training Loss: 0.0053899522975552824\n",
      "Training Loss: 0.006224171222420409\n",
      "Training Loss: 0.007249352792277932\n",
      "Training Loss: 0.007029827912338078\n",
      "Training Loss: 0.006916996238287538\n",
      "Validation Loss: 0.0041956230400702205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.005596087376470677\n",
      "Training Loss: 0.005453432949725539\n",
      "Training Loss: 0.005380144156515598\n",
      "Training Loss: 0.006213254167232662\n",
      "Training Loss: 0.00723912978428416\n",
      "Training Loss: 0.00701964485575445\n",
      "Training Loss: 0.0069065095577389\n",
      "Validation Loss: 0.004185241754323746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.005585577944875695\n",
      "Training Loss: 0.005443383651436306\n",
      "Training Loss: 0.005370377596700564\n",
      "Training Loss: 0.006202432843274437\n",
      "Training Loss: 0.00722900236141868\n",
      "Training Loss: 0.007009588646469638\n",
      "Training Loss: 0.006896127237705513\n",
      "Validation Loss: 0.004174943555544099\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.005575136008556001\n",
      "Training Loss: 0.005433400609181262\n",
      "Training Loss: 0.005360664816107601\n",
      "Training Loss: 0.006191713055013679\n",
      "Training Loss: 0.007218982216436416\n",
      "Training Loss: 0.006999668779317289\n",
      "Training Loss: 0.006885858661262318\n",
      "Validation Loss: 0.004164745800514542\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.005564772236393765\n",
      "Training Loss: 0.005423492119880393\n",
      "Training Loss: 0.00535101363202557\n",
      "Training Loss: 0.0061811038822634145\n",
      "Training Loss: 0.0072090823540929706\n",
      "Training Loss: 0.006989896463928744\n",
      "Training Loss: 0.0068757130741141734\n",
      "Validation Loss: 0.004154659767568893\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.005554496917175129\n",
      "Training Loss: 0.005413668731926009\n",
      "Training Loss: 0.0053414363600313665\n",
      "Training Loss: 0.006170613975846209\n",
      "Training Loss: 0.007199316098121926\n",
      "Training Loss: 0.0069802848366089165\n",
      "Training Loss: 0.006865703500807285\n",
      "Validation Loss: 0.004144692203999682\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.005544320733752102\n",
      "Training Loss: 0.005403939699172042\n",
      "Training Loss: 0.00533194298739545\n",
      "Training Loss: 0.00616024934919551\n",
      "Training Loss: 0.007189693639520556\n",
      "Training Loss: 0.006970839165151119\n",
      "Training Loss: 0.006855836383765563\n",
      "Validation Loss: 0.004134854991145934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.005534251885255799\n",
      "Training Loss: 0.005394313670694828\n",
      "Training Loss: 0.005322541494388133\n",
      "Training Loss: 0.0061500151554355394\n",
      "Training Loss: 0.007180226071504876\n",
      "Training Loss: 0.0069615687953773885\n",
      "Training Loss: 0.006846119636902586\n",
      "Validation Loss: 0.004125161711667221\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.005524299567914568\n",
      "Training Loss: 0.00538479681184981\n",
      "Training Loss: 0.005313239448005333\n",
      "Training Loss: 0.006139918917906471\n",
      "Training Loss: 0.007170923189260065\n",
      "Training Loss: 0.0069524814619217066\n",
      "Training Loss: 0.0068365611066110435\n",
      "Validation Loss: 0.004115616713043595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.005514471742790192\n",
      "Training Loss: 0.0053753975522704426\n",
      "Training Loss: 0.005304046554956585\n",
      "Training Loss: 0.006129962033592165\n",
      "Training Loss: 0.007161792966071516\n",
      "Training Loss: 0.006943580835359171\n",
      "Training Loss: 0.006827166067669168\n",
      "Validation Loss: 0.004106228053220984\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.0055047734966501595\n",
      "Training Loss: 0.005366120991529897\n",
      "Training Loss: 0.005294968670932576\n",
      "Training Loss: 0.006120150273200124\n",
      "Training Loss: 0.007152842042269185\n",
      "Training Loss: 0.006934871011180803\n",
      "Training Loss: 0.006817939013708383\n",
      "Validation Loss: 0.00409700216948707\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.00549521253677085\n",
      "Training Loss: 0.005356973690795712\n",
      "Training Loss: 0.005286012888536789\n",
      "Training Loss: 0.006110485725221224\n",
      "Training Loss: 0.007144076048862189\n",
      "Training Loss: 0.006926356212934479\n",
      "Training Loss: 0.006808883659541607\n",
      "Validation Loss: 0.00408794410824985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00548579219088424\n",
      "Training Loss: 0.005347957970807329\n",
      "Training Loss: 0.005277181548299268\n",
      "Training Loss: 0.006100971446139738\n",
      "Training Loss: 0.0071355012559797615\n",
      "Training Loss: 0.006918037427822128\n",
      "Training Loss: 0.006800003664102406\n",
      "Validation Loss: 0.004079055965421128\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.00547651692177169\n",
      "Training Loss: 0.005339077963726595\n",
      "Training Loss: 0.0052684808010235425\n",
      "Training Loss: 0.006091607447597198\n",
      "Training Loss: 0.007127119997749105\n",
      "Training Loss: 0.006909914423013106\n",
      "Training Loss: 0.00679129931028001\n",
      "Validation Loss: 0.004070342664453393\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.005467388919205405\n",
      "Training Loss: 0.005330336427432485\n",
      "Training Loss: 0.005259912922629156\n",
      "Training Loss: 0.00608239491994027\n",
      "Training Loss: 0.0071189330087509\n",
      "Training Loss: 0.006901986681623384\n",
      "Training Loss: 0.006782771189464256\n",
      "Validation Loss: 0.004061806873742784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005458413377054967\n",
      "Training Loss: 0.005321735555771739\n",
      "Training Loss: 0.005251480869483203\n",
      "Training Loss: 0.0060733356239506976\n",
      "Training Loss: 0.007110944322776049\n",
      "Training Loss: 0.006894254242070019\n",
      "Training Loss: 0.006774421560112387\n",
      "Validation Loss: 0.0040534453471766764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.005449587811599485\n",
      "Training Loss: 0.005313276434317231\n",
      "Training Loss: 0.005243186915176921\n",
      "Training Loss: 0.006064428096287884\n",
      "Training Loss: 0.007103151910705492\n",
      "Training Loss: 0.006886714751599357\n",
      "Training Loss: 0.006766247141640634\n",
      "Validation Loss: 0.00404526465720661\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.005440918952808715\n",
      "Training Loss: 0.005304960758076049\n",
      "Training Loss: 0.005235031362390146\n",
      "Training Loss: 0.0060556745145004245\n",
      "Training Loss: 0.007095558069413528\n",
      "Training Loss: 0.006879367011133581\n",
      "Training Loss: 0.006758249326376245\n",
      "Validation Loss: 0.004037259686790574\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005432404563180171\n",
      "Training Loss: 0.005296789114945568\n",
      "Training Loss: 0.005227017421275377\n",
      "Training Loss: 0.00604707270336803\n",
      "Training Loss: 0.0070881590608041735\n",
      "Training Loss: 0.006872207339620217\n",
      "Training Loss: 0.0067504251468926664\n",
      "Validation Loss: 0.004029433777091906\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.005424047022243031\n",
      "Training Loss: 0.005288762206910178\n",
      "Training Loss: 0.005219144388101995\n",
      "Training Loss: 0.0060386228980496524\n",
      "Training Loss: 0.00708095433539711\n",
      "Training Loss: 0.006865233205026015\n",
      "Training Loss: 0.006742773416917771\n",
      "Validation Loss: 0.004021782037795065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00541584433754906\n",
      "Training Loss: 0.005280877157929353\n",
      "Training Loss: 0.005211408981704153\n",
      "Training Loss: 0.006030325727770105\n",
      "Training Loss: 0.007073942903662101\n",
      "Training Loss: 0.006858440727228299\n",
      "Training Loss: 0.006735292037483305\n",
      "Validation Loss: 0.00401430468865032\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.005407798495725729\n",
      "Training Loss: 0.005273136618197896\n",
      "Training Loss: 0.005203815550776199\n",
      "Training Loss: 0.006022180275758729\n",
      "Training Loss: 0.007067121622385457\n",
      "Training Loss: 0.006851828119251877\n",
      "Training Loss: 0.006727979392744601\n",
      "Validation Loss: 0.004006997206900617\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.0053999080852372575\n",
      "Training Loss: 0.005265539695392363\n",
      "Training Loss: 0.005196361765847542\n",
      "Training Loss: 0.0060141871368978176\n",
      "Training Loss: 0.007060486511327326\n",
      "Training Loss: 0.006845391447423026\n",
      "Training Loss: 0.00672083328361623\n",
      "Validation Loss: 0.00399986350052547\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005392175304004923\n",
      "Training Loss: 0.00525808610545937\n",
      "Training Loss: 0.005189048153115436\n",
      "Training Loss: 0.0060063444543629885\n",
      "Training Loss: 0.007054036451736465\n",
      "Training Loss: 0.006839127225102857\n",
      "Training Loss: 0.0067138505645561964\n",
      "Validation Loss: 0.003992896423716977\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005384597424417734\n",
      "Training Loss: 0.0052507756074192\n",
      "Training Loss: 0.005181872565299272\n",
      "Training Loss: 0.0059986527875298635\n",
      "Training Loss: 0.007047766844043508\n",
      "Training Loss: 0.0068330319668166335\n",
      "Training Loss: 0.00670703028794378\n",
      "Validation Loss: 0.003986096705051927\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005377175045432523\n",
      "Training Loss: 0.005243606177973561\n",
      "Training Loss: 0.005174834321369417\n",
      "Training Loss: 0.005991111826151609\n",
      "Training Loss: 0.007041674484498799\n",
      "Training Loss: 0.006827101989183575\n",
      "Training Loss: 0.0067003699310589585\n",
      "Validation Loss: 0.003979459057443457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005369907187414356\n",
      "Training Loss: 0.005236578223994002\n",
      "Training Loss: 0.005167933146003634\n",
      "Training Loss: 0.005983720928779804\n",
      "Training Loss: 0.007035756683908403\n",
      "Training Loss: 0.006821334413252771\n",
      "Training Loss: 0.006693864959524945\n",
      "Validation Loss: 0.003972987087586483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.0053627951664384456\n",
      "Training Loss: 0.0052296914026374\n",
      "Training Loss: 0.005161168162012473\n",
      "Training Loss: 0.00597647980262991\n",
      "Training Loss: 0.007030007713474333\n",
      "Training Loss: 0.00681572562083602\n",
      "Training Loss: 0.0066875141998752954\n",
      "Validation Loss: 0.003966673750391976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005355836804374121\n",
      "Training Loss: 0.005222944164997898\n",
      "Training Loss: 0.005154537479975261\n",
      "Training Loss: 0.005969388147932477\n",
      "Training Loss: 0.0070244260120671245\n",
      "Training Loss: 0.0068102723849006\n",
      "Training Loss: 0.006681316297035664\n",
      "Validation Loss: 0.0039605170454348575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.005349029932986014\n",
      "Training Loss: 0.005216334221768193\n",
      "Training Loss: 0.005148038069019094\n",
      "Training Loss: 0.005962445758050308\n",
      "Training Loss: 0.007019007311901078\n",
      "Training Loss: 0.006804970130324364\n",
      "Training Loss: 0.006675266708480194\n",
      "Validation Loss: 0.00395451641985135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005342374406172894\n",
      "Training Loss: 0.005209862756892107\n",
      "Training Loss: 0.005141671552555635\n",
      "Training Loss: 0.0059556508017703895\n",
      "Training Loss: 0.00701374676427804\n",
      "Training Loss: 0.0067998149280902\n",
      "Training Loss: 0.006669364565750584\n",
      "Validation Loss: 0.003948669595040436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005335868196561932\n",
      "Training Loss: 0.005203526411787607\n",
      "Training Loss: 0.005135434638359584\n",
      "Training Loss: 0.005949002467095852\n",
      "Training Loss: 0.00700864007580094\n",
      "Training Loss: 0.006794805343961343\n",
      "Training Loss: 0.006663606750080362\n",
      "Validation Loss: 0.003942970655621326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0053295111475745215\n",
      "Training Loss: 0.005197324167238548\n",
      "Training Loss: 0.005129325137822889\n",
      "Training Loss: 0.005942499811062589\n",
      "Training Loss: 0.007003685052040964\n",
      "Training Loss: 0.006789935814449564\n",
      "Training Loss: 0.006657990314997733\n",
      "Validation Loss: 0.003937420294417787\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.0053232989372918385\n",
      "Training Loss: 0.005191255479003303\n",
      "Training Loss: 0.005123342325678095\n",
      "Training Loss: 0.005936140056001023\n",
      "Training Loss: 0.006998874519485981\n",
      "Training Loss: 0.006785201217280701\n",
      "Training Loss: 0.0066525105386972426\n",
      "Validation Loss: 0.0039320169749219765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005317231778171845\n",
      "Training Loss: 0.005185317335417494\n",
      "Training Loss: 0.005117482568603009\n",
      "Training Loss: 0.005929923435323872\n",
      "Training Loss: 0.0069942061940673735\n",
      "Training Loss: 0.0067806002125144005\n",
      "Training Loss: 0.006647166374605149\n",
      "Validation Loss: 0.003926753443630922\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.00531130564166233\n",
      "Training Loss: 0.005179508615401574\n",
      "Training Loss: 0.005111744449241087\n",
      "Training Loss: 0.0059238446736708285\n",
      "Training Loss: 0.006989674414508045\n",
      "Training Loss: 0.006776127284392714\n",
      "Training Loss: 0.0066419544792734085\n",
      "Validation Loss: 0.0039216305669013424\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005305519390385598\n",
      "Training Loss: 0.005173825930105522\n",
      "Training Loss: 0.005106124976882711\n",
      "Training Loss: 0.005917904640664346\n",
      "Training Loss: 0.006985276804771274\n",
      "Training Loss: 0.006771778656402602\n",
      "Training Loss: 0.0066368717502336945\n",
      "Validation Loss: 0.003916639878243654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005299867824651301\n",
      "Training Loss: 0.005168266559485346\n",
      "Training Loss: 0.005100620777811855\n",
      "Training Loss: 0.00591209857200738\n",
      "Training Loss: 0.006981008035363629\n",
      "Training Loss: 0.006767551153898239\n",
      "Training Loss: 0.006631914988392964\n",
      "Validation Loss: 0.0039117832952405744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005294348955503665\n",
      "Training Loss: 0.005162828656611964\n",
      "Training Loss: 0.0050952294172020626\n",
      "Training Loss: 0.005906423716805875\n",
      "Training Loss: 0.006976863180752844\n",
      "Training Loss: 0.006763438576599583\n",
      "Training Loss: 0.006627080056350678\n",
      "Validation Loss: 0.003907054060135274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005288959259050898\n",
      "Training Loss: 0.005157509704586119\n",
      "Training Loss: 0.0050899484084220604\n",
      "Training Loss: 0.005900878371903673\n",
      "Training Loss: 0.006972838623914868\n",
      "Training Loss: 0.006759438385488465\n",
      "Training Loss: 0.006622363219503313\n",
      "Validation Loss: 0.0039024520521523075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00528369560779538\n",
      "Training Loss: 0.005152305992669426\n",
      "Training Loss: 0.005084774473216385\n",
      "Training Loss: 0.005895457037258894\n",
      "Training Loss: 0.006968927561538294\n",
      "Training Loss: 0.006755544944899157\n",
      "Training Loss: 0.006617761121597141\n",
      "Validation Loss: 0.003897971417388638\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005278555924305693\n",
      "Training Loss: 0.005147215923061594\n",
      "Training Loss: 0.005079704837989993\n",
      "Training Loss: 0.005890158553957008\n",
      "Training Loss: 0.006965128376614302\n",
      "Training Loss: 0.0067517548450268805\n",
      "Training Loss: 0.006613270645029843\n",
      "Validation Loss: 0.003893611349221249\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.0052735346392728385\n",
      "Training Loss: 0.005142236187239178\n",
      "Training Loss: 0.0050747359351953495\n",
      "Training Loss: 0.00588497830030974\n",
      "Training Loss: 0.006961436355486512\n",
      "Training Loss: 0.006748064635321498\n",
      "Training Loss: 0.006608887498732657\n",
      "Validation Loss: 0.003889364478167858\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005268627547775395\n",
      "Training Loss: 0.00513736286549829\n",
      "Training Loss: 0.005069864598917775\n",
      "Training Loss: 0.005879912335658446\n",
      "Training Loss: 0.006957847150042653\n",
      "Training Loss: 0.006744469123659656\n",
      "Training Loss: 0.006604609636124224\n",
      "Validation Loss: 0.003885228172893381\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.00526382997748442\n",
      "Training Loss: 0.005132591762812808\n",
      "Training Loss: 0.005065085637033917\n",
      "Training Loss: 0.005874956605839543\n",
      "Training Loss: 0.006954354472691193\n",
      "Training Loss: 0.006740963513730094\n",
      "Training Loss: 0.006600430301623419\n",
      "Validation Loss: 0.003881200810995525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005259141029091552\n",
      "Training Loss: 0.005127922439132817\n",
      "Training Loss: 0.0050603997084544975\n",
      "Training Loss: 0.005870107517694123\n",
      "Training Loss: 0.006950956680811942\n",
      "Training Loss: 0.006737544670468196\n",
      "Training Loss: 0.006596349347382784\n",
      "Validation Loss: 0.0038772772536316558\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005254556033760309\n",
      "Training Loss: 0.005123350596986711\n",
      "Training Loss: 0.0050558021134929736\n",
      "Training Loss: 0.005865360431489535\n",
      "Training Loss: 0.006947647369233891\n",
      "Training Loss: 0.006734208215493709\n",
      "Training Loss: 0.006592359201749787\n",
      "Validation Loss: 0.0038734559725781673\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.00525007021788042\n",
      "Training Loss: 0.0051188727369299155\n",
      "Training Loss: 0.005051287887617946\n",
      "Training Loss: 0.00586071299854666\n",
      "Training Loss: 0.0069444247521460055\n",
      "Training Loss: 0.00673095049452968\n",
      "Training Loss: 0.0065884589881170545\n",
      "Validation Loss: 0.003869731353738865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005245679522049613\n",
      "Training Loss: 0.005114485097583383\n",
      "Training Loss: 0.0050468552025267855\n",
      "Training Loss: 0.0058561603556154295\n",
      "Training Loss: 0.006941284091444686\n",
      "Training Loss: 0.0067277678661048414\n",
      "Training Loss: 0.0065846455178689214\n",
      "Validation Loss: 0.0038660995859289993\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005241379318758846\n",
      "Training Loss: 0.005110185746452771\n",
      "Training Loss: 0.005042500611161813\n",
      "Training Loss: 0.005851698007900268\n",
      "Training Loss: 0.006938223369652405\n",
      "Training Loss: 0.006724656304577366\n",
      "Training Loss: 0.006580913715297356\n",
      "Validation Loss: 0.0038625587131967593\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005237166626029648\n",
      "Training Loss: 0.005105970108998008\n",
      "Training Loss: 0.005038221275899559\n",
      "Training Loss: 0.0058473218424478545\n",
      "Training Loss: 0.0069352359010372315\n",
      "Training Loss: 0.006721613059053198\n",
      "Training Loss: 0.006577259856276214\n",
      "Validation Loss: 0.0038591067412212378\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0052330391120631245\n",
      "Training Loss: 0.005101836599642411\n",
      "Training Loss: 0.005034014789853245\n",
      "Training Loss: 0.005843029649695381\n",
      "Training Loss: 0.006932320665800944\n",
      "Training Loss: 0.0067186331038828935\n",
      "Training Loss: 0.006573681475128978\n",
      "Validation Loss: 0.0038557369894198098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005228990046307444\n",
      "Training Loss: 0.005097780704963952\n",
      "Training Loss: 0.0050298761902377006\n",
      "Training Loss: 0.00583881564438343\n",
      "Training Loss: 0.006929473221534863\n",
      "Training Loss: 0.006715714330784977\n",
      "Training Loss: 0.00657017485355027\n",
      "Validation Loss: 0.0038524494107031308\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005225019237259403\n",
      "Training Loss: 0.005093801291659475\n",
      "Training Loss: 0.005025805819313973\n",
      "Training Loss: 0.005834676102967933\n",
      "Training Loss: 0.006926689950050786\n",
      "Training Loss: 0.006712852502241731\n",
      "Training Loss: 0.006566736304666847\n",
      "Validation Loss: 0.0038492388897732404\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.00522112007427495\n",
      "Training Loss: 0.005089893631520681\n",
      "Training Loss: 0.005021797068184242\n",
      "Training Loss: 0.005830608729738742\n",
      "Training Loss: 0.006923969354247674\n",
      "Training Loss: 0.0067100459756329655\n",
      "Training Loss: 0.006563363599125296\n",
      "Validation Loss: 0.003846104724214095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.00521729037980549\n",
      "Training Loss: 0.005086055185529403\n",
      "Training Loss: 0.005017849437426776\n",
      "Training Loss: 0.005826609723735601\n",
      "Training Loss: 0.006921308466698975\n",
      "Training Loss: 0.00670729074976407\n",
      "Training Loss: 0.00656005343538709\n",
      "Validation Loss: 0.0038430421191968777\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005213527524610981\n",
      "Training Loss: 0.0050822844653157515\n",
      "Training Loss: 0.00501396152714733\n",
      "Training Loss: 0.005822673707152717\n",
      "Training Loss: 0.006918702605180443\n",
      "Training Loss: 0.006704584277467802\n",
      "Training Loss: 0.006556801863480359\n",
      "Validation Loss: 0.003840050486104793\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005209828026709147\n",
      "Training Loss: 0.005078578230459243\n",
      "Training Loss: 0.005010129158035852\n",
      "Training Loss: 0.005818799457629211\n",
      "Training Loss: 0.006916151901241392\n",
      "Training Loss: 0.0067019240686204285\n",
      "Training Loss: 0.0065536069322843105\n",
      "Validation Loss: 0.003837124648598138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005206188160227612\n",
      "Training Loss: 0.005074932422139682\n",
      "Training Loss: 0.00500634967465885\n",
      "Training Loss: 0.005814982759766281\n",
      "Training Loss: 0.006913652359507978\n",
      "Training Loss: 0.006699307597009465\n",
      "Training Loss: 0.006550465981708839\n",
      "Validation Loss: 0.0038342645251425857\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.0052026058692717925\n",
      "Training Loss: 0.00507134776446037\n",
      "Training Loss: 0.0050026225257897745\n",
      "Training Loss: 0.005811219736933708\n",
      "Training Loss: 0.006911199865862727\n",
      "Training Loss: 0.006696731337578967\n",
      "Training Loss: 0.006547375646186992\n",
      "Validation Loss: 0.003831467221051538\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005199079569429159\n",
      "Training Loss: 0.005067819955293089\n",
      "Training Loss: 0.004998945834231563\n",
      "Training Loss: 0.005807508155703545\n",
      "Training Loss: 0.006908794578630477\n",
      "Training Loss: 0.006694194616284221\n",
      "Training Loss: 0.0065443337103351954\n",
      "Validation Loss: 0.0038287311121786524\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.00519560475659091\n",
      "Training Loss: 0.005064347294392064\n",
      "Training Loss: 0.004995316489948891\n",
      "Training Loss: 0.0058038451662287115\n",
      "Training Loss: 0.006906434075208381\n",
      "Training Loss: 0.006691694068722427\n",
      "Training Loss: 0.006541337927337736\n",
      "Validation Loss: 0.0038260521802332683\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.00519217926543206\n",
      "Training Loss: 0.005060926942969672\n",
      "Training Loss: 0.004991731960326433\n",
      "Training Loss: 0.005800227831932716\n",
      "Training Loss: 0.0069041168666444715\n",
      "Training Loss: 0.0066892293328419325\n",
      "Training Loss: 0.006538386582396924\n",
      "Validation Loss: 0.003823429926417154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005188801904441789\n",
      "Training Loss: 0.0050575579464202745\n",
      "Training Loss: 0.00498819234315306\n",
      "Training Loss: 0.005796652815770358\n",
      "Training Loss: 0.006901838699122891\n",
      "Training Loss: 0.0066867971559986476\n",
      "Training Loss: 0.00653547634021379\n",
      "Validation Loss: 0.003820863147335971\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005185470535652712\n",
      "Training Loss: 0.0050542386638699095\n",
      "Training Loss: 0.004984695691964589\n",
      "Training Loss: 0.0057931196421850475\n",
      "Training Loss: 0.006899600903270766\n",
      "Training Loss: 0.006684395980555564\n",
      "Training Loss: 0.006532606867840514\n",
      "Validation Loss: 0.003818348675795346\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005182183856959455\n",
      "Training Loss: 0.005050967439892702\n",
      "Training Loss: 0.004981240262277424\n",
      "Training Loss: 0.005789624854805879\n",
      "Training Loss: 0.006897400442976504\n",
      "Training Loss: 0.0066820247285068034\n",
      "Training Loss: 0.006529775379458442\n",
      "Validation Loss: 0.003815887756174786\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.0051789382973220196\n",
      "Training Loss: 0.0050477417086949574\n",
      "Training Loss: 0.004977825048263185\n",
      "Training Loss: 0.005786166063044221\n",
      "Training Loss: 0.006895236049313098\n",
      "Training Loss: 0.006679681586101651\n",
      "Training Loss: 0.006526980140479282\n",
      "Validation Loss: 0.0038134766213267987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.00517573336372152\n",
      "Training Loss: 0.005044560405658558\n",
      "Training Loss: 0.004974448019638657\n",
      "Training Loss: 0.005782740366994403\n",
      "Training Loss: 0.006893105924827978\n",
      "Training Loss: 0.006677365181967616\n",
      "Training Loss: 0.00652422022423707\n",
      "Validation Loss: 0.00381111303101946\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005172567786066793\n",
      "Training Loss: 0.005041422859649174\n",
      "Training Loss: 0.0049711095885140826\n",
      "Training Loss: 0.005779348124633543\n",
      "Training Loss: 0.006891010847175494\n",
      "Training Loss: 0.006675074209924787\n",
      "Training Loss: 0.006521494642365724\n",
      "Validation Loss: 0.0038087989590810927\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005169440464815125\n",
      "Training Loss: 0.005038326901849359\n",
      "Training Loss: 0.004967806907370686\n",
      "Training Loss: 0.005775985375512391\n",
      "Training Loss: 0.006888946866383776\n",
      "Training Loss: 0.006672807724680751\n",
      "Training Loss: 0.0065188018348999325\n",
      "Validation Loss: 0.0038065290434306925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005166347558260895\n",
      "Training Loss: 0.005035270857042633\n",
      "Training Loss: 0.004964538876665756\n",
      "Training Loss: 0.0057726525643374774\n",
      "Training Loss: 0.006886915629729628\n",
      "Training Loss: 0.006670564844971523\n",
      "Training Loss: 0.006516140269814059\n",
      "Validation Loss: 0.00380430670414985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005163292631623335\n",
      "Training Loss: 0.00503225618507713\n",
      "Training Loss: 0.004961308030760847\n",
      "Training Loss: 0.005769346205051988\n",
      "Training Loss: 0.006884912366513163\n",
      "Training Loss: 0.006668345142388716\n",
      "Training Loss: 0.006513509720098227\n",
      "Validation Loss: 0.0038021282242325025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005160272890352644\n",
      "Training Loss: 0.00502928041794803\n",
      "Training Loss: 0.0049581113050226125\n",
      "Training Loss: 0.005766067254007794\n",
      "Training Loss: 0.006882939378265292\n",
      "Training Loss: 0.00666614634799771\n",
      "Training Loss: 0.006510908601339907\n",
      "Validation Loss: 0.0037999941064009953\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005157287286128849\n",
      "Training Loss: 0.005026342102792114\n",
      "Training Loss: 0.004954948065569624\n",
      "Training Loss: 0.005762813608162105\n",
      "Training Loss: 0.00688099475693889\n",
      "Training Loss: 0.006663969355868176\n",
      "Training Loss: 0.00650833711377345\n",
      "Validation Loss: 0.003797902355898409\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005154334578546696\n",
      "Training Loss: 0.005023441535886377\n",
      "Training Loss: 0.00495181912148837\n",
      "Training Loss: 0.005759583882754668\n",
      "Training Loss: 0.0068790761637501415\n",
      "Training Loss: 0.006661812603706494\n",
      "Training Loss: 0.006505794192198664\n",
      "Validation Loss: 0.003795853193347039\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.00515141585143283\n",
      "Training Loss: 0.005020577231771312\n",
      "Training Loss: 0.004948722121771425\n",
      "Training Loss: 0.005756377979414537\n",
      "Training Loss: 0.006877184982877225\n",
      "Training Loss: 0.0066596755990758535\n",
      "Training Loss: 0.0065032802138011905\n",
      "Validation Loss: 0.0037938454297409578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005148530656588264\n",
      "Training Loss: 0.005017748439568095\n",
      "Training Loss: 0.004945657404605299\n",
      "Training Loss: 0.005753196012810804\n",
      "Training Loss: 0.0068753206066321584\n",
      "Training Loss: 0.006657558649312705\n",
      "Training Loss: 0.0065007954812608655\n",
      "Validation Loss: 0.003791875721300753\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005145676939864643\n",
      "Training Loss: 0.00501495543459896\n",
      "Training Loss: 0.004942624897230416\n",
      "Training Loss: 0.005750034746015444\n",
      "Training Loss: 0.006873480788199231\n",
      "Training Loss: 0.006655461431946606\n",
      "Training Loss: 0.006498337757075205\n",
      "Validation Loss: 0.0037899482441535884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.0051428567920811475\n",
      "Training Loss: 0.005012196748284623\n",
      "Training Loss: 0.004939624951221049\n",
      "Training Loss: 0.005746896652271971\n",
      "Training Loss: 0.006871664534555748\n",
      "Training Loss: 0.006653382545337081\n",
      "Training Loss: 0.0064959075604565444\n",
      "Validation Loss: 0.003788059542464853\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005140069024637341\n",
      "Training Loss: 0.005009473711252213\n",
      "Training Loss: 0.004936656547361053\n",
      "Training Loss: 0.005743779526092112\n",
      "Training Loss: 0.006869872682727873\n",
      "Training Loss: 0.006651322854449973\n",
      "Training Loss: 0.006493505362886936\n",
      "Validation Loss: 0.0037862092613204197\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0051373137999325994\n",
      "Training Loss: 0.005006782652344555\n",
      "Training Loss: 0.004933718091924675\n",
      "Training Loss: 0.0057406843465287235\n",
      "Training Loss: 0.006868102917214855\n",
      "Training Loss: 0.00664928114740178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [23:33<23:22, 280.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.006491131437942385\n",
      "Validation Loss: 0.0037843976758441394\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.089188864082098\n",
      "Training Loss: 0.06611082479357719\n",
      "Training Loss: 0.05991122655570507\n",
      "Training Loss: 0.058817522656172516\n",
      "Training Loss: 0.05610646761953831\n",
      "Training Loss: 0.05545242195948959\n",
      "Training Loss: 0.05262578463181853\n",
      "Validation Loss: 0.05047118270330215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.049921884890645744\n",
      "Training Loss: 0.04699098696932197\n",
      "Training Loss: 0.044450144832953814\n",
      "Training Loss: 0.04289044418372214\n",
      "Training Loss: 0.03944903915748\n",
      "Training Loss: 0.03755229788832366\n",
      "Training Loss: 0.034050758816301825\n",
      "Validation Loss: 0.031139755998267217\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.030328139802441\n",
      "Training Loss: 0.02731298979371786\n",
      "Training Loss: 0.024442891008220613\n",
      "Training Loss: 0.023246886553242804\n",
      "Training Loss: 0.02086288542021066\n",
      "Training Loss: 0.019224433861672878\n",
      "Training Loss: 0.017816413887776435\n",
      "Validation Loss: 0.01563620168283191\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.01600262738298625\n",
      "Training Loss: 0.014946658513508737\n",
      "Training Loss: 0.014179001599550246\n",
      "Training Loss: 0.014639789722859859\n",
      "Training Loss: 0.014457075987011195\n",
      "Training Loss: 0.013630108095239847\n",
      "Training Loss: 0.01331338642630726\n",
      "Validation Loss: 0.011444971589755253\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.012226415786426515\n",
      "Training Loss: 0.01157058235257864\n",
      "Training Loss: 0.011154263387434185\n",
      "Training Loss: 0.011679065558128058\n",
      "Training Loss: 0.011959296697750688\n",
      "Training Loss: 0.011316947645973414\n",
      "Training Loss: 0.01112709429115057\n",
      "Validation Loss: 0.009132835996591867\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.01015188534744084\n",
      "Training Loss: 0.009609757454600186\n",
      "Training Loss: 0.009351509015541524\n",
      "Training Loss: 0.009907866336870939\n",
      "Training Loss: 0.010422073282534257\n",
      "Training Loss: 0.00993833753047511\n",
      "Training Loss: 0.009840246111853048\n",
      "Validation Loss: 0.007726996335678835\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.008880239917198196\n",
      "Training Loss: 0.008424842117819934\n",
      "Training Loss: 0.008262849929742515\n",
      "Training Loss: 0.008862148873740807\n",
      "Training Loss: 0.009517018764745444\n",
      "Training Loss: 0.009111462815199048\n",
      "Training Loss: 0.009075481183826924\n",
      "Validation Loss: 0.006831714894783017\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.008078238528687507\n",
      "Training Loss: 0.007684519110480324\n",
      "Training Loss: 0.00757858716417104\n",
      "Training Loss: 0.008229343689745292\n",
      "Training Loss: 0.00898035116144456\n",
      "Training Loss: 0.008608536254614592\n",
      "Training Loss: 0.008619749625213445\n",
      "Validation Loss: 0.006234161824350723\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.007563197893323377\n",
      "Training Loss: 0.007216166977304966\n",
      "Training Loss: 0.0071485816338099535\n",
      "Training Loss: 0.007853808597428725\n",
      "Training Loss: 0.008676476118853315\n",
      "Training Loss: 0.008319356940919533\n",
      "Training Loss: 0.00836826068116352\n",
      "Validation Loss: 0.005842339769808039\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.0072446846240200105\n",
      "Training Loss: 0.006932980705751106\n",
      "Training Loss: 0.006892255140701309\n",
      "Training Loss: 0.007644686525454744\n",
      "Training Loss: 0.008518932476872578\n",
      "Training Loss: 0.008166947305435315\n",
      "Training Loss: 0.008242144545074553\n",
      "Validation Loss: 0.005596057255054764\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.0070562472485471515\n",
      "Training Loss: 0.0067686245706863706\n",
      "Training Loss: 0.006745403679087758\n",
      "Training Loss: 0.007531469619134441\n",
      "Training Loss: 0.008439150977646932\n",
      "Training Loss: 0.008088187943212689\n",
      "Training Loss: 0.008178093862952665\n",
      "Validation Loss: 0.005443679260819006\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.006943832862889394\n",
      "Training Loss: 0.006670600797515362\n",
      "Training Loss: 0.006658547504339367\n",
      "Training Loss: 0.007465200047008693\n",
      "Training Loss: 0.008392533718142659\n",
      "Training Loss: 0.00804240400204435\n",
      "Training Loss: 0.008138602970866486\n",
      "Validation Loss: 0.005346308208562517\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.006871703178621829\n",
      "Training Loss: 0.006606223268900067\n",
      "Training Loss: 0.006601812286535278\n",
      "Training Loss: 0.007420006765169091\n",
      "Training Loss: 0.008358298529637978\n",
      "Training Loss: 0.008010294695850462\n",
      "Training Loss: 0.008107897113077342\n",
      "Validation Loss: 0.005279748603328365\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.006820587672991678\n",
      "Training Loss: 0.0065589964564424005\n",
      "Training Loss: 0.006560327605111525\n",
      "Training Loss: 0.007384785894537345\n",
      "Training Loss: 0.008329111422644928\n",
      "Training Loss: 0.007984368298202753\n",
      "Training Loss: 0.008080840619513766\n",
      "Validation Loss: 0.005230585930255078\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.006780987088568509\n",
      "Training Loss: 0.006521261663874611\n",
      "Training Loss: 0.006527169458568096\n",
      "Training Loss: 0.007355016033397987\n",
      "Training Loss: 0.00830270759179257\n",
      "Training Loss: 0.007961793325375766\n",
      "Training Loss: 0.008055922598578036\n",
      "Validation Loss: 0.005191668933902759\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.006748167893383652\n",
      "Training Loss: 0.006489348787581548\n",
      "Training Loss: 0.0064989925816189495\n",
      "Training Loss: 0.007328664268716239\n",
      "Training Loss: 0.00827829261426814\n",
      "Training Loss: 0.007941325034480542\n",
      "Training Loss: 0.008032605934422463\n",
      "Validation Loss: 0.005159137345421431\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.006719621490919963\n",
      "Training Loss: 0.0064613295195158574\n",
      "Training Loss: 0.00647403689334169\n",
      "Training Loss: 0.0073046502913348375\n",
      "Training Loss: 0.00825544721330516\n",
      "Training Loss: 0.007922277542529627\n",
      "Training Loss: 0.00801057495875284\n",
      "Validation Loss: 0.005130827967129815\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.006693923402344808\n",
      "Training Loss: 0.006436080328421667\n",
      "Training Loss: 0.006451290134573355\n",
      "Training Loss: 0.007282310512964614\n",
      "Training Loss: 0.008233853683341294\n",
      "Training Loss: 0.007904206013772636\n",
      "Training Loss: 0.007989571625366808\n",
      "Validation Loss: 0.005105460389196343\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.006670208886498585\n",
      "Training Loss: 0.0064128826436353845\n",
      "Training Loss: 0.006430120064178482\n",
      "Training Loss: 0.007261201326036826\n",
      "Training Loss: 0.008213247277308256\n",
      "Training Loss: 0.007886795426020398\n",
      "Training Loss: 0.007969368235208094\n",
      "Validation Loss: 0.005082235304457362\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.00664792816969566\n",
      "Training Loss: 0.006391250602900982\n",
      "Training Loss: 0.006410111840814352\n",
      "Training Loss: 0.007241008607670665\n",
      "Training Loss: 0.008193405378842727\n",
      "Training Loss: 0.00786981755751185\n",
      "Training Loss: 0.007949771350249648\n",
      "Validation Loss: 0.005060629382440018\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.006626708820695057\n",
      "Training Loss: 0.0063708324334584175\n",
      "Training Loss: 0.006390973867382854\n",
      "Training Loss: 0.007221503869513981\n",
      "Training Loss: 0.008174146141391248\n",
      "Training Loss: 0.007853101970395073\n",
      "Training Loss: 0.007930619037942961\n",
      "Validation Loss: 0.005040276580235541\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.006606296203099191\n",
      "Training Loss: 0.006351369954063557\n",
      "Training Loss: 0.006372500394936651\n",
      "Training Loss: 0.007202521928120404\n",
      "Training Loss: 0.00815532356966287\n",
      "Training Loss: 0.007836526283062995\n",
      "Training Loss: 0.007911780144786463\n",
      "Validation Loss: 0.005020911058006383\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.006586504246806726\n",
      "Training Loss: 0.006332666221424006\n",
      "Training Loss: 0.006354537331499159\n",
      "Training Loss: 0.007183935443754308\n",
      "Training Loss: 0.008136818276252598\n",
      "Training Loss: 0.007819997366750613\n",
      "Training Loss: 0.0078931505815126\n",
      "Validation Loss: 0.00500233081642064\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.00656719962018542\n",
      "Training Loss: 0.006314570598187856\n",
      "Training Loss: 0.006336971676209942\n",
      "Training Loss: 0.007165652824332938\n",
      "Training Loss: 0.008118540600407868\n",
      "Training Loss: 0.007803450402570888\n",
      "Training Loss: 0.007874650754965842\n",
      "Validation Loss: 0.004984385719826298\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.006548282573930919\n",
      "Training Loss: 0.0062969639082439246\n",
      "Training Loss: 0.006319716750876978\n",
      "Training Loss: 0.007147603052435443\n",
      "Training Loss: 0.008100419508991763\n",
      "Training Loss: 0.007786839982727543\n",
      "Training Loss: 0.007856219531968236\n",
      "Validation Loss: 0.0049669511929286946\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.006529676746577024\n",
      "Training Loss: 0.006279754847637378\n",
      "Training Loss: 0.0063027071801479905\n",
      "Training Loss: 0.007129735423950479\n",
      "Training Loss: 0.008082404675660656\n",
      "Training Loss: 0.007770136286271736\n",
      "Training Loss: 0.007837814097292721\n",
      "Validation Loss: 0.004949935582424566\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.006511324626626447\n",
      "Training Loss: 0.006262869842466898\n",
      "Training Loss: 0.006285892219748348\n",
      "Training Loss: 0.007112011866411194\n",
      "Training Loss: 0.008064455359708518\n",
      "Training Loss: 0.00775332010933198\n",
      "Training Loss: 0.007819403957109899\n",
      "Validation Loss: 0.004933253974454959\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.0064931843394879256\n",
      "Training Loss: 0.006246251350385137\n",
      "Training Loss: 0.006269236144144088\n",
      "Training Loss: 0.0070944050257094205\n",
      "Training Loss: 0.008046545785618945\n",
      "Training Loss: 0.007736386547330767\n",
      "Training Loss: 0.007800970894750208\n",
      "Validation Loss: 0.004916855070848116\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.006475225501926616\n",
      "Training Loss: 0.006229856592835858\n",
      "Training Loss: 0.006252711697015911\n",
      "Training Loss: 0.007076899709645658\n",
      "Training Loss: 0.00802866097074002\n",
      "Training Loss: 0.007719335932051763\n",
      "Training Loss: 0.0077825074922293425\n",
      "Validation Loss: 0.004900686143747486\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006457426457200199\n",
      "Training Loss: 0.006213652122532949\n",
      "Training Loss: 0.006236300354357809\n",
      "Training Loss: 0.007059485223144293\n",
      "Training Loss: 0.008010791757842526\n",
      "Training Loss: 0.007702177696628496\n",
      "Training Loss: 0.007764012922998518\n",
      "Validation Loss: 0.004884705628523052\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006439770989818499\n",
      "Training Loss: 0.006197614232660271\n",
      "Training Loss: 0.006219987971708179\n",
      "Training Loss: 0.007042157853138633\n",
      "Training Loss: 0.007992937656817958\n",
      "Training Loss: 0.007684923260239884\n",
      "Training Loss: 0.007745492334943265\n",
      "Validation Loss: 0.004868886765783255\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.0064222538366448135\n",
      "Training Loss: 0.006181725334608927\n",
      "Training Loss: 0.006203771086875349\n",
      "Training Loss: 0.007024916782975197\n",
      "Training Loss: 0.007975103887729347\n",
      "Training Loss: 0.007667591456556693\n",
      "Training Loss: 0.007726955978432671\n",
      "Validation Loss: 0.004853207577448966\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.006404869255493395\n",
      "Training Loss: 0.006165974967880175\n",
      "Training Loss: 0.00618764513055794\n",
      "Training Loss: 0.007007766329916194\n",
      "Training Loss: 0.00795729766599834\n",
      "Training Loss: 0.00765020365244709\n",
      "Training Loss: 0.007708417886169628\n",
      "Validation Loss: 0.004837656302956606\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006387619016459211\n",
      "Training Loss: 0.006150355346035212\n",
      "Training Loss: 0.006171609271550551\n",
      "Training Loss: 0.006990710614481941\n",
      "Training Loss: 0.007939527765847743\n",
      "Training Loss: 0.007632779165869579\n",
      "Training Loss: 0.007689892965136096\n",
      "Validation Loss: 0.004822218377746586\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006370503749931231\n",
      "Training Loss: 0.006134864548803307\n",
      "Training Loss: 0.0061556684714742\n",
      "Training Loss: 0.006973756116931327\n",
      "Training Loss: 0.007921808303799481\n",
      "Training Loss: 0.007615340780466795\n",
      "Training Loss: 0.00767139956005849\n",
      "Validation Loss: 0.004806880805160055\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.00635352790937759\n",
      "Training Loss: 0.006119501952198334\n",
      "Training Loss: 0.006139824955025688\n",
      "Training Loss: 0.006956910160370171\n",
      "Training Loss: 0.007904151841066777\n",
      "Training Loss: 0.007597911834018305\n",
      "Training Loss: 0.007652953062206507\n",
      "Validation Loss: 0.004791647467451791\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006336694358615205\n",
      "Training Loss: 0.006104265935136937\n",
      "Training Loss: 0.006124081395100802\n",
      "Training Loss: 0.006940177648793906\n",
      "Training Loss: 0.007886569210095331\n",
      "Training Loss: 0.0075805132009554655\n",
      "Training Loss: 0.0076345701864920556\n",
      "Validation Loss: 0.004776510007571331\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006320011117495596\n",
      "Training Loss: 0.006089163142605685\n",
      "Training Loss: 0.006108448372688144\n",
      "Training Loss: 0.006923568148631602\n",
      "Training Loss: 0.007869075580965728\n",
      "Training Loss: 0.007563163739396259\n",
      "Training Loss: 0.0076162671321071684\n",
      "Validation Loss: 0.004761472009976956\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006303484585369006\n",
      "Training Loss: 0.006074195393593982\n",
      "Training Loss: 0.006092927114805207\n",
      "Training Loss: 0.0069070836232276635\n",
      "Training Loss: 0.0078516790503636\n",
      "Training Loss: 0.007545880932593718\n",
      "Training Loss: 0.007598058766452595\n",
      "Validation Loss: 0.0047465248102294315\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006287114719743841\n",
      "Training Loss: 0.006059363829554059\n",
      "Training Loss: 0.006077522315317765\n",
      "Training Loss: 0.006890727083664387\n",
      "Training Loss: 0.007834391857031733\n",
      "Training Loss: 0.007528680198593065\n",
      "Training Loss: 0.007579956093104556\n",
      "Validation Loss: 0.0047316742164504886\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006270908775622957\n",
      "Training Loss: 0.006044673552969471\n",
      "Training Loss: 0.006062236648285761\n",
      "Training Loss: 0.006874503323924728\n",
      "Training Loss: 0.007817220367724077\n",
      "Training Loss: 0.007511575240641832\n",
      "Training Loss: 0.0075619710923638195\n",
      "Validation Loss: 0.0047169147675843\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006254869332187809\n",
      "Training Loss: 0.006030126777477563\n",
      "Training Loss: 0.006047075813403353\n",
      "Training Loss: 0.006858411791617982\n",
      "Training Loss: 0.007800171993440017\n",
      "Training Loss: 0.007494576344033704\n",
      "Training Loss: 0.007544113248586655\n",
      "Validation Loss: 0.004702245727641864\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006238995452295057\n",
      "Training Loss: 0.006015724423341453\n",
      "Training Loss: 0.006032039574347436\n",
      "Training Loss: 0.006842452120617963\n",
      "Training Loss: 0.007783249716740101\n",
      "Training Loss: 0.0074776911200024184\n",
      "Training Loss: 0.007526386447716504\n",
      "Validation Loss: 0.004687667153740811\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006223290839698165\n",
      "Training Loss: 0.006001468091271818\n",
      "Training Loss: 0.006017127949744463\n",
      "Training Loss: 0.006826622638036497\n",
      "Training Loss: 0.007766457272227853\n",
      "Training Loss: 0.007460926510393619\n",
      "Training Loss: 0.007508799190400168\n",
      "Validation Loss: 0.004673175635827838\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006207753805792891\n",
      "Training Loss: 0.005987357723643072\n",
      "Training Loss: 0.006002342447172851\n",
      "Training Loss: 0.006810917988186702\n",
      "Training Loss: 0.007749794780975208\n",
      "Training Loss: 0.007444284654920921\n",
      "Training Loss: 0.00749135282705538\n",
      "Validation Loss: 0.004658764934564909\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006192378707346506\n",
      "Training Loss: 0.0059733903082087635\n",
      "Training Loss: 0.005987676182994619\n",
      "Training Loss: 0.006795333226909861\n",
      "Training Loss: 0.007733261666726321\n",
      "Training Loss: 0.0074277657084167005\n",
      "Training Loss: 0.0074740487697999925\n",
      "Validation Loss: 0.004644431354310573\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006177163477404974\n",
      "Training Loss: 0.0059595645091030745\n",
      "Training Loss: 0.005973129936028272\n",
      "Training Loss: 0.006779861849499866\n",
      "Training Loss: 0.007716854789759964\n",
      "Training Loss: 0.0074113707651849835\n",
      "Training Loss: 0.00745688715018332\n",
      "Validation Loss: 0.0046301691676054\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006162103074602783\n",
      "Training Loss: 0.005945875240140595\n",
      "Training Loss: 0.005958697741152719\n",
      "Training Loss: 0.006764495740062557\n",
      "Training Loss: 0.007700568572618067\n",
      "Training Loss: 0.0073950943467207255\n",
      "Training Loss: 0.007439863532781601\n",
      "Validation Loss: 0.004615969153658281\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006147188768372871\n",
      "Training Loss: 0.005932317147380672\n",
      "Training Loss: 0.005944370982469991\n",
      "Training Loss: 0.006749225377570838\n",
      "Training Loss: 0.007684398192213848\n",
      "Training Loss: 0.007378933468135074\n",
      "Training Loss: 0.007422978095710278\n",
      "Validation Loss: 0.00460181733042225\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006132412509759888\n",
      "Training Loss: 0.005918883512495085\n",
      "Training Loss: 0.005930143547011539\n",
      "Training Loss: 0.006734041360905394\n",
      "Training Loss: 0.007668338258517906\n",
      "Training Loss: 0.007362882096786052\n",
      "Training Loss: 0.0074062238039914515\n",
      "Validation Loss: 0.004587709489505222\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006117767115938477\n",
      "Training Loss: 0.0059055681322934105\n",
      "Training Loss: 0.00591600670479238\n",
      "Training Loss: 0.0067189306911313905\n",
      "Training Loss: 0.007652377000194974\n",
      "Training Loss: 0.007346930184867233\n",
      "Training Loss: 0.007389594458509237\n",
      "Validation Loss: 0.004573638530008578\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006103240765514784\n",
      "Training Loss: 0.005892361994483508\n",
      "Training Loss: 0.005901950867846608\n",
      "Training Loss: 0.006703881602734327\n",
      "Training Loss: 0.007636505713453517\n",
      "Training Loss: 0.0073310703493189066\n",
      "Training Loss: 0.007373082631966099\n",
      "Validation Loss: 0.0045595863234070355\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006088822024175897\n",
      "Training Loss: 0.005879253410967067\n",
      "Training Loss: 0.005887964240973815\n",
      "Training Loss: 0.006688881213776767\n",
      "Training Loss: 0.007620714298100211\n",
      "Training Loss: 0.0073152897623367606\n",
      "Training Loss: 0.007356679217191413\n",
      "Validation Loss: 0.004545541090824491\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006074498399393633\n",
      "Training Loss: 0.0058662335982080545\n",
      "Training Loss: 0.0058740355283953246\n",
      "Training Loss: 0.006673914261627942\n",
      "Training Loss: 0.007604989735409617\n",
      "Training Loss: 0.007299579790560529\n",
      "Training Loss: 0.0073403750860597935\n",
      "Validation Loss: 0.004531489701310263\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006060257030185312\n",
      "Training Loss: 0.0058532902662409465\n",
      "Training Loss: 0.005860149939544499\n",
      "Training Loss: 0.006658966392278671\n",
      "Training Loss: 0.007589318519458175\n",
      "Training Loss: 0.007283924700459465\n",
      "Training Loss: 0.007324158522533253\n",
      "Validation Loss: 0.004517410530827326\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.006046082396642305\n",
      "Training Loss: 0.005840409476077184\n",
      "Training Loss: 0.005846293830545619\n",
      "Training Loss: 0.006644020980456844\n",
      "Training Loss: 0.007573687129188329\n",
      "Training Loss: 0.00726831145468168\n",
      "Training Loss: 0.0073080177931115035\n",
      "Validation Loss: 0.004503296378194198\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006031961509725079\n",
      "Training Loss: 0.00582757850352209\n",
      "Training Loss: 0.00583245322224684\n",
      "Training Loss: 0.006629061542334966\n",
      "Training Loss: 0.007558079573791474\n",
      "Training Loss: 0.007252724001882598\n",
      "Training Loss: 0.007291939478600398\n",
      "Validation Loss: 0.004489123832594868\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006017879121936858\n",
      "Training Loss: 0.005814784166868776\n",
      "Training Loss: 0.005818612826988101\n",
      "Training Loss: 0.006614070262876339\n",
      "Training Loss: 0.007542480168631301\n",
      "Training Loss: 0.007237147177802399\n",
      "Training Loss: 0.007275911866454408\n",
      "Validation Loss: 0.004474874719286735\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006003817057353445\n",
      "Training Loss: 0.005802007871679961\n",
      "Training Loss: 0.00580475281807594\n",
      "Training Loss: 0.006599028787459247\n",
      "Training Loss: 0.007526870935689658\n",
      "Training Loss: 0.007221563265193254\n",
      "Training Loss: 0.007259916026378051\n",
      "Validation Loss: 0.004460529984090101\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.005989758957293816\n",
      "Training Loss: 0.005789235198171809\n",
      "Training Loss: 0.005790855590021238\n",
      "Training Loss: 0.006583917609532364\n",
      "Training Loss: 0.007511234214762226\n",
      "Training Loss: 0.007205955490935594\n",
      "Training Loss: 0.007243938548490405\n",
      "Validation Loss: 0.004446069663739327\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.0059756875084713106\n",
      "Training Loss: 0.005776447934913449\n",
      "Training Loss: 0.0057769018004182725\n",
      "Training Loss: 0.006568715933244675\n",
      "Training Loss: 0.007495549625018611\n",
      "Training Loss: 0.007190303455572576\n",
      "Training Loss: 0.007227962071774527\n",
      "Validation Loss: 0.0044314663871174\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.005961585144978017\n",
      "Training Loss: 0.005763629176071845\n",
      "Training Loss: 0.005762872556224465\n",
      "Training Loss: 0.006553404207807034\n",
      "Training Loss: 0.0074797960207797585\n",
      "Training Loss: 0.007174587633926421\n",
      "Training Loss: 0.0072119678906165064\n",
      "Validation Loss: 0.004416705657682671\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.0059474333212710916\n",
      "Training Loss: 0.005750758430804126\n",
      "Training Loss: 0.005748745735036209\n",
      "Training Loss: 0.006537959609995596\n",
      "Training Loss: 0.007463953461847268\n",
      "Training Loss: 0.007158789857057855\n",
      "Training Loss: 0.00719593829009682\n",
      "Validation Loss: 0.004401759271098349\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.005933212254312821\n",
      "Training Loss: 0.005737817086628638\n",
      "Training Loss: 0.00573449790244922\n",
      "Training Loss: 0.006522359778755344\n",
      "Training Loss: 0.007447998275747523\n",
      "Training Loss: 0.007142887213267386\n",
      "Training Loss: 0.007179852973204106\n",
      "Validation Loss: 0.004386598660265294\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.005918902552803047\n",
      "Training Loss: 0.005724782744073309\n",
      "Training Loss: 0.005720105239888653\n",
      "Training Loss: 0.006506581004941836\n",
      "Training Loss: 0.007431905914563686\n",
      "Training Loss: 0.007126857661642134\n",
      "Training Loss: 0.007163690453162417\n",
      "Validation Loss: 0.004371199414154954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.005904484477359802\n",
      "Training Loss: 0.005711635218467563\n",
      "Training Loss: 0.005705543664516881\n",
      "Training Loss: 0.006490598273230717\n",
      "Training Loss: 0.0074156517791561785\n",
      "Training Loss: 0.007110679215984419\n",
      "Training Loss: 0.00714742922806181\n",
      "Validation Loss: 0.004355531328005458\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.00588993695913814\n",
      "Training Loss: 0.005698350280290469\n",
      "Training Loss: 0.005690784570761025\n",
      "Training Loss: 0.006474386446643621\n",
      "Training Loss: 0.007399209677823819\n",
      "Training Loss: 0.007094330124091357\n",
      "Training Loss: 0.007131047903094441\n",
      "Validation Loss: 0.004339565577034312\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0058752377080963925\n",
      "Training Loss: 0.005684904388035647\n",
      "Training Loss: 0.005675800334429369\n",
      "Training Loss: 0.006457919806707651\n",
      "Training Loss: 0.007382553288480267\n",
      "Training Loss: 0.007077787169255317\n",
      "Training Loss: 0.007114522111369297\n",
      "Validation Loss: 0.004323270702507863\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.005860365134431049\n",
      "Training Loss: 0.00567127228656318\n",
      "Training Loss: 0.005660562375560403\n",
      "Training Loss: 0.006441170781035907\n",
      "Training Loss: 0.007365654049790464\n",
      "Training Loss: 0.00706102648167871\n",
      "Training Loss: 0.007097827704856172\n",
      "Validation Loss: 0.004306615540872883\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.005845297946361825\n",
      "Training Loss: 0.005657429679995402\n",
      "Training Loss: 0.005645038206130266\n",
      "Training Loss: 0.006424114685505629\n",
      "Training Loss: 0.00734848398307804\n",
      "Training Loss: 0.007044028308009729\n",
      "Training Loss: 0.007080943047767505\n",
      "Validation Loss: 0.004289567568653336\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.005830015885294415\n",
      "Training Loss: 0.005643351724138483\n",
      "Training Loss: 0.005629199031973258\n",
      "Training Loss: 0.006406724380794913\n",
      "Training Loss: 0.007331015937379562\n",
      "Training Loss: 0.00702677083783783\n",
      "Training Loss: 0.007063841493800282\n",
      "Validation Loss: 0.004272097912669349\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.005814499107073061\n",
      "Training Loss: 0.005629012410645373\n",
      "Training Loss: 0.0056130130123347045\n",
      "Training Loss: 0.006388974890578538\n",
      "Training Loss: 0.007313220726209693\n",
      "Training Loss: 0.007009234954603016\n",
      "Training Loss: 0.00704650126514025\n",
      "Validation Loss: 0.004254172179387526\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.005798725120839663\n",
      "Training Loss: 0.005614387680543587\n",
      "Training Loss: 0.005596446275012568\n",
      "Training Loss: 0.006370842600590549\n",
      "Training Loss: 0.007295075869187713\n",
      "Training Loss: 0.006991406112210825\n",
      "Training Loss: 0.007028903659665957\n",
      "Validation Loss: 0.004235763573663288\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.0057826792425476016\n",
      "Training Loss: 0.005599454767652787\n",
      "Training Loss: 0.0055794715101365\n",
      "Training Loss: 0.006352310045040213\n",
      "Training Loss: 0.007276556573342532\n",
      "Training Loss: 0.006973273718031124\n",
      "Training Loss: 0.007011028797132894\n",
      "Validation Loss: 0.004216852364625628\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.005766347027383744\n",
      "Training Loss: 0.005584193470422178\n",
      "Training Loss: 0.005562058540526778\n",
      "Training Loss: 0.0063333620876073835\n",
      "Training Loss: 0.007257647737278603\n",
      "Training Loss: 0.006954834330826998\n",
      "Training Loss: 0.00699286357848905\n",
      "Validation Loss: 0.004197416405022591\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00574971909809392\n",
      "Training Loss: 0.005568589668837376\n",
      "Training Loss: 0.005544187319464981\n",
      "Training Loss: 0.006313991175265983\n",
      "Training Loss: 0.007238336221780628\n",
      "Training Loss: 0.006936089035589248\n",
      "Training Loss: 0.006974400852341205\n",
      "Validation Loss: 0.004177451283280295\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00573279433825519\n",
      "Training Loss: 0.005552633611951023\n",
      "Training Loss: 0.0055258389224763955\n",
      "Training Loss: 0.006294200762058608\n",
      "Training Loss: 0.007218622646178119\n",
      "Training Loss: 0.0069170541630592195\n",
      "Training Loss: 0.006955640757223591\n",
      "Validation Loss: 0.004156961113612136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.005715576785732992\n",
      "Training Loss: 0.005536325415014289\n",
      "Training Loss: 0.00550701062893495\n",
      "Training Loss: 0.006274001711863093\n",
      "Training Loss: 0.0071985113405389715\n",
      "Training Loss: 0.006897750347852707\n",
      "Training Loss: 0.00693659414886497\n",
      "Validation Loss: 0.00413596450677316\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.005698082414455712\n",
      "Training Loss: 0.005519674486713484\n",
      "Training Loss: 0.005487704868428409\n",
      "Training Loss: 0.006253425692557357\n",
      "Training Loss: 0.007178030294016935\n",
      "Training Loss: 0.00687822116073221\n",
      "Training Loss: 0.006917285198578611\n",
      "Validation Loss: 0.0041144987374669715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.005680340023245662\n",
      "Training Loss: 0.005502704490208998\n",
      "Training Loss: 0.005467943286057562\n",
      "Training Loss: 0.006232517776661553\n",
      "Training Loss: 0.00715721815940924\n",
      "Training Loss: 0.006858518944354728\n",
      "Training Loss: 0.006897750783246011\n",
      "Validation Loss: 0.00409262140511713\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.00566239143779967\n",
      "Training Loss: 0.005485452720895409\n",
      "Training Loss: 0.005447766641154885\n",
      "Training Loss: 0.006211342508322559\n",
      "Training Loss: 0.0071361330396030095\n",
      "Training Loss: 0.006838714658515528\n",
      "Training Loss: 0.006878045450430363\n",
      "Validation Loss: 0.004070411436695759\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.005644292632932774\n",
      "Training Loss: 0.005467973605846055\n",
      "Training Loss: 0.005427234710659832\n",
      "Training Loss: 0.006189982662326656\n",
      "Training Loss: 0.0071148508711485195\n",
      "Training Loss: 0.006818893593735993\n",
      "Training Loss: 0.006858237733831629\n",
      "Validation Loss: 0.004047977366163797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.005626117379288189\n",
      "Training Loss: 0.0054503401106921956\n",
      "Training Loss: 0.005406430929433555\n",
      "Training Loss: 0.006168542411760427\n",
      "Training Loss: 0.007093467906815931\n",
      "Training Loss: 0.006799154519103468\n",
      "Training Loss: 0.006838412532815709\n",
      "Validation Loss: 0.0040254440291153356\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.0056079474231228235\n",
      "Training Loss: 0.0054326356458477675\n",
      "Training Loss: 0.005385456334333866\n",
      "Training Loss: 0.0061471385409822684\n",
      "Training Loss: 0.007072097199852578\n",
      "Training Loss: 0.006779601421440021\n",
      "Training Loss: 0.006818665912142023\n",
      "Validation Loss: 0.004002945618843485\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.005589874325087294\n",
      "Training Loss: 0.00541495654033497\n",
      "Training Loss: 0.0053644298121798785\n",
      "Training Loss: 0.006125897625461221\n",
      "Training Loss: 0.007050857092253864\n",
      "Training Loss: 0.006760342206107452\n",
      "Training Loss: 0.006799099723575636\n",
      "Validation Loss: 0.003980629286495097\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.0055719937436515465\n",
      "Training Loss: 0.005397407346754335\n",
      "Training Loss: 0.005343482483876869\n",
      "Training Loss: 0.00610494893277064\n",
      "Training Loss: 0.007029869701946154\n",
      "Training Loss: 0.006741473962320015\n",
      "Training Loss: 0.0067798135266639295\n",
      "Validation Loss: 0.003958651583209244\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.005554400884429924\n",
      "Training Loss: 0.00538009041629266\n",
      "Training Loss: 0.0053227494558086615\n",
      "Training Loss: 0.006084414468496106\n",
      "Training Loss: 0.007009251193376258\n",
      "Training Loss: 0.006723083386896178\n",
      "Training Loss: 0.006760902851819992\n",
      "Validation Loss: 0.00393714570337453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.005537174697383307\n",
      "Training Loss: 0.0053630995092680675\n",
      "Training Loss: 0.005302356625325046\n",
      "Training Loss: 0.006064401871990413\n",
      "Training Loss: 0.006989108530106023\n",
      "Training Loss: 0.006705234774854034\n",
      "Training Loss: 0.0067424488661345094\n",
      "Validation Loss: 0.003916223047733697\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.005520383036346175\n",
      "Training Loss: 0.005346516472636722\n",
      "Training Loss: 0.005282420557341539\n",
      "Training Loss: 0.006044997299322858\n",
      "Training Loss: 0.006969521999708377\n",
      "Training Loss: 0.006687968926271424\n",
      "Training Loss: 0.0067245120985899125\n",
      "Validation Loss: 0.003895982732378951\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.005504073496558704\n",
      "Training Loss: 0.0053304043074604126\n",
      "Training Loss: 0.005263038796838373\n",
      "Training Loss: 0.006026260968646966\n",
      "Training Loss: 0.006950553201604635\n",
      "Training Loss: 0.006671306694624945\n",
      "Training Loss: 0.00670713386614807\n",
      "Validation Loss: 0.0038764857589389276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0054882724361959845\n",
      "Training Loss: 0.005314807193353772\n",
      "Training Loss: 0.005244278462487273\n",
      "Training Loss: 0.00600822608161252\n",
      "Training Loss: 0.006932235533604399\n",
      "Training Loss: 0.006655239096144214\n",
      "Training Loss: 0.006690336591564119\n",
      "Validation Loss: 0.003857760313873908\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0054729867551941425\n",
      "Training Loss: 0.005299748240504414\n",
      "Training Loss: 0.0052261875220574435\n",
      "Training Loss: 0.005990901040495373\n",
      "Training Loss: 0.006914578309515491\n",
      "Training Loss: 0.006639742802362889\n",
      "Training Loss: 0.006674122419208288\n",
      "Validation Loss: 0.0038398124507806274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.005458207754418254\n",
      "Training Loss: 0.005285231005400419\n",
      "Training Loss: 0.0052087849896634\n",
      "Training Loss: 0.0059742745681433005\n",
      "Training Loss: 0.0068975777993910015\n",
      "Training Loss: 0.006624787271721289\n",
      "Training Loss: 0.006658477855380624\n",
      "Validation Loss: 0.0038226257485585212\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.005443915947689675\n",
      "Training Loss: 0.005271248998469673\n",
      "Training Loss: 0.005192074727383442\n",
      "Training Loss: 0.005958320259233005\n",
      "Training Loss: 0.006881212365115062\n",
      "Training Loss: 0.006610329692484811\n",
      "Training Loss: 0.006643383343471214\n",
      "Validation Loss: 0.003806169146989308\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.005430084453546442\n",
      "Training Loss: 0.005257785592693836\n",
      "Training Loss: 0.0051760395814199\n",
      "Training Loss: 0.005943004010478034\n",
      "Training Loss: 0.006865451277699322\n",
      "Training Loss: 0.006596335833892226\n",
      "Training Loss: 0.006628814964788034\n",
      "Validation Loss: 0.003790399533598806\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.005416685298550874\n",
      "Training Loss: 0.005244815379846841\n",
      "Training Loss: 0.005160657839733176\n",
      "Training Loss: 0.005928288182476535\n",
      "Training Loss: 0.006850264659151435\n",
      "Training Loss: 0.006582766418578103\n",
      "Training Loss: 0.006614741038065404\n",
      "Validation Loss: 0.003775274508793172\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.005403691802639514\n",
      "Training Loss: 0.0052323157142382115\n",
      "Training Loss: 0.005145898059708998\n",
      "Training Loss: 0.005914133604965173\n",
      "Training Loss: 0.006835617545293644\n",
      "Training Loss: 0.006569591955048963\n",
      "Training Loss: 0.0066011366771999746\n",
      "Validation Loss: 0.0037607484996800174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.005391078973771073\n",
      "Training Loss: 0.005220258691115305\n",
      "Training Loss: 0.005131726239342243\n",
      "Training Loss: 0.005900506367324852\n",
      "Training Loss: 0.006821478457422927\n",
      "Training Loss: 0.00655678690993227\n",
      "Training Loss: 0.006587974898284301\n",
      "Validation Loss: 0.0037467817462848198\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.005378828617394902\n",
      "Training Loss: 0.005208622204372659\n",
      "Training Loss: 0.005118109241593629\n",
      "Training Loss: 0.005887373731820844\n",
      "Training Loss: 0.006807819543173537\n",
      "Training Loss: 0.006544333967613056\n",
      "Training Loss: 0.006575232049217447\n",
      "Validation Loss: 0.003733337801891599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.005366922311950475\n",
      "Training Loss: 0.0051973827934125435\n",
      "Training Loss: 0.005105015365406871\n",
      "Training Loss: 0.0058747093839338045\n",
      "Training Loss: 0.006794614314567298\n",
      "Training Loss: 0.006532219829969108\n",
      "Training Loss: 0.0065628875873517244\n",
      "Validation Loss: 0.0037203853189498744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.005355350215686485\n",
      "Training Loss: 0.005186522829462774\n",
      "Training Loss: 0.00509241538704373\n",
      "Training Loss: 0.005862488246057182\n",
      "Training Loss: 0.006781844922807067\n",
      "Training Loss: 0.006520434353733435\n",
      "Training Loss: 0.006550924611510709\n",
      "Validation Loss: 0.003707894137607057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.005344100428046659\n",
      "Training Loss: 0.005176021662191488\n",
      "Training Loss: 0.005080280409893021\n",
      "Training Loss: 0.005850691048544831\n",
      "Training Loss: 0.006769487375859171\n",
      "Training Loss: 0.0065089678147342055\n",
      "Training Loss: 0.006539324574405327\n",
      "Validation Loss: 0.0036958448832143057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.005333168737124651\n",
      "Training Loss: 0.005165864797309041\n",
      "Training Loss: 0.0050685900537064295\n",
      "Training Loss: 0.005839299317449332\n",
      "Training Loss: 0.006757526024011895\n",
      "Training Loss: 0.006497816026676446\n",
      "Training Loss: 0.0065280731045641\n",
      "Validation Loss: 0.003684214152803833\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.005322544336086139\n",
      "Training Loss: 0.005156035691034049\n",
      "Training Loss: 0.005057318694307469\n",
      "Training Loss: 0.0058282972936285656\n",
      "Training Loss: 0.006745947989402339\n",
      "Training Loss: 0.006486974296858534\n",
      "Training Loss: 0.006517156000481919\n",
      "Validation Loss: 0.003672985386187106\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.005312224521185271\n",
      "Training Loss: 0.005146520147100091\n",
      "Training Loss: 0.005046447567292489\n",
      "Training Loss: 0.0058176708722021435\n",
      "Training Loss: 0.006734736704966053\n",
      "Training Loss: 0.006476436357479543\n",
      "Training Loss: 0.006506561099085957\n",
      "Validation Loss: 0.0036621437855378796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.005302203276660294\n",
      "Training Loss: 0.005137305042881053\n",
      "Training Loss: 0.005035955893690698\n",
      "Training Loss: 0.005807404236984439\n",
      "Training Loss: 0.006723877241602168\n",
      "Training Loss: 0.0064661960920784625\n",
      "Training Loss: 0.006496274503879249\n",
      "Validation Loss: 0.0036516713448503657\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.0052924731728853655\n",
      "Training Loss: 0.005128377921064384\n",
      "Training Loss: 0.0050258307188050825\n",
      "Training Loss: 0.005797484357608482\n",
      "Training Loss: 0.0067133600718807425\n",
      "Training Loss: 0.006456248441245407\n",
      "Training Loss: 0.006486284015700221\n",
      "Validation Loss: 0.0036415528028170938\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.005283027940313332\n",
      "Training Loss: 0.005119723961979616\n",
      "Training Loss: 0.005016051028505899\n",
      "Training Loss: 0.005787898366106674\n",
      "Training Loss: 0.006703170019900426\n",
      "Training Loss: 0.006446584977675229\n",
      "Training Loss: 0.006476577167632058\n",
      "Validation Loss: 0.0036317777186841444\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.005273860896704718\n",
      "Training Loss: 0.005111333578242921\n",
      "Training Loss: 0.005006604249356315\n",
      "Training Loss: 0.0057786317361751575\n",
      "Training Loss: 0.0066932935651857406\n",
      "Training Loss: 0.006437197956256568\n",
      "Training Loss: 0.006467143765185028\n",
      "Validation Loss: 0.0036223307613820564\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.0052649640472373\n",
      "Training Loss: 0.005103193788672797\n",
      "Training Loss: 0.004997475752606988\n",
      "Training Loss: 0.005769670961308293\n",
      "Training Loss: 0.006683719587745145\n",
      "Training Loss: 0.006428079063771293\n",
      "Training Loss: 0.006457970975898206\n",
      "Validation Loss: 0.00361319917187876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.005256330308620818\n",
      "Training Loss: 0.005095292152836919\n",
      "Training Loss: 0.004988650939194486\n",
      "Training Loss: 0.005761004923260771\n",
      "Training Loss: 0.00667443671147339\n",
      "Training Loss: 0.00641921921283938\n",
      "Training Loss: 0.006449047481874004\n",
      "Validation Loss: 0.003604368332421149\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.005247948021278716\n",
      "Training Loss: 0.00508761500910623\n",
      "Training Loss: 0.004980114863719791\n",
      "Training Loss: 0.005752617121906951\n",
      "Training Loss: 0.0066654307127464564\n",
      "Training Loss: 0.006410609923768788\n",
      "Training Loss: 0.006440361475106329\n",
      "Validation Loss: 0.0035958268813970273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.005239809529157356\n",
      "Training Loss: 0.005080153958697337\n",
      "Training Loss: 0.0049718579184263945\n",
      "Training Loss: 0.005744494738173671\n",
      "Training Loss: 0.006656690331874415\n",
      "Training Loss: 0.006402239558519795\n",
      "Training Loss: 0.006431904879864305\n",
      "Validation Loss: 0.003587557893239436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0052319052023813125\n",
      "Training Loss: 0.005072896477940958\n",
      "Training Loss: 0.004963861900614574\n",
      "Training Loss: 0.005736626628204249\n",
      "Training Loss: 0.006648203121731058\n",
      "Training Loss: 0.006394098300952464\n",
      "Training Loss: 0.006423663336317986\n",
      "Validation Loss: 0.003579550246143023\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.005224223652039655\n",
      "Training Loss: 0.005065832354302984\n",
      "Training Loss: 0.004956118019181304\n",
      "Training Loss: 0.005728997879195959\n",
      "Training Loss: 0.006639958509476855\n",
      "Training Loss: 0.006386174883227795\n",
      "Training Loss: 0.00641562960925512\n",
      "Validation Loss: 0.0035717926138126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.005216757762245834\n",
      "Training Loss: 0.005058951114478987\n",
      "Training Loss: 0.004948612020816654\n",
      "Training Loss: 0.005721596023649908\n",
      "Training Loss: 0.006631942627718672\n",
      "Training Loss: 0.0063784596382174645\n",
      "Training Loss: 0.006407791684614495\n",
      "Validation Loss: 0.00356427192677524\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.005209495314629748\n",
      "Training Loss: 0.00505224185064435\n",
      "Training Loss: 0.004941334803588688\n",
      "Training Loss: 0.005714409638894722\n",
      "Training Loss: 0.006624144779052586\n",
      "Training Loss: 0.006370941897621378\n",
      "Training Loss: 0.006400140053592622\n",
      "Validation Loss: 0.0035569743830571126\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.005202428125194274\n",
      "Training Loss: 0.0050456954384571874\n",
      "Training Loss: 0.004934272289974615\n",
      "Training Loss: 0.005707425262080505\n",
      "Training Loss: 0.0066165551624726504\n",
      "Training Loss: 0.006363612002460286\n",
      "Training Loss: 0.006392665957100689\n",
      "Validation Loss: 0.00354989027614925\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.005195545745664276\n",
      "Training Loss: 0.005039303379599005\n",
      "Training Loss: 0.00492741537746042\n",
      "Training Loss: 0.0057006325776455926\n",
      "Training Loss: 0.006609163670800626\n",
      "Training Loss: 0.006356459830421954\n",
      "Training Loss: 0.006385362094733864\n",
      "Validation Loss: 0.003543007440868528\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.005188838641624898\n",
      "Training Loss: 0.0050330580677837135\n",
      "Training Loss: 0.004920754226623103\n",
      "Training Loss: 0.0056940227368613705\n",
      "Training Loss: 0.00660195904201828\n",
      "Training Loss: 0.006349475762108341\n",
      "Training Loss: 0.006378217847086489\n",
      "Validation Loss: 0.003536315870729418\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.005182301518507302\n",
      "Training Loss: 0.005026949307066388\n",
      "Training Loss: 0.004914277643547393\n",
      "Training Loss: 0.005687580790836364\n",
      "Training Loss: 0.006594933130545541\n",
      "Training Loss: 0.006342652866151184\n",
      "Training Loss: 0.006371227245545015\n",
      "Validation Loss: 0.003529805594127388\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.005175922795315273\n",
      "Training Loss: 0.005020971306366846\n",
      "Training Loss: 0.004907976462855004\n",
      "Training Loss: 0.005681302084121853\n",
      "Training Loss: 0.006588077566120774\n",
      "Training Loss: 0.006335979595314711\n",
      "Training Loss: 0.006364382249303162\n",
      "Validation Loss: 0.003523465702467169\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.005169693978969008\n",
      "Training Loss: 0.0050151158712105825\n",
      "Training Loss: 0.004901843395200558\n",
      "Training Loss: 0.005675173908821307\n",
      "Training Loss: 0.0065813808690290895\n",
      "Training Loss: 0.006329449876211583\n",
      "Training Loss: 0.006357676309999078\n",
      "Validation Loss: 0.003517286060639563\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.005163608119473793\n",
      "Training Loss: 0.005009378188406117\n",
      "Training Loss: 0.004895868417806923\n",
      "Training Loss: 0.005669189587351866\n",
      "Training Loss: 0.006574838006636128\n",
      "Training Loss: 0.0063230558484792705\n",
      "Training Loss: 0.006351102474145592\n",
      "Validation Loss: 0.0035112620011313856\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.00515765996708069\n",
      "Training Loss: 0.005003749860916287\n",
      "Training Loss: 0.0048900438169948756\n",
      "Training Loss: 0.005663341374020092\n",
      "Training Loss: 0.006568441959097981\n",
      "Training Loss: 0.006316791308345273\n",
      "Training Loss: 0.00634465612238273\n",
      "Validation Loss: 0.003505381562377323\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0051518417627085\n",
      "Training Loss: 0.004998227099422365\n",
      "Training Loss: 0.004884361699223519\n",
      "Training Loss: 0.0056576197425602\n",
      "Training Loss: 0.006562182970810682\n",
      "Training Loss: 0.006310647598002106\n",
      "Training Loss: 0.006338329802965745\n",
      "Validation Loss: 0.0034996386846764646\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.005146145350881853\n",
      "Training Loss: 0.004992803858767729\n",
      "Training Loss: 0.004878816359560005\n",
      "Training Loss: 0.005652020205161534\n",
      "Training Loss: 0.006556055668042972\n",
      "Training Loss: 0.0063046188396401704\n",
      "Training Loss: 0.006332119349390268\n",
      "Validation Loss: 0.0034940262213488866\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.00514056786079891\n",
      "Training Loss: 0.00498747599311173\n",
      "Training Loss: 0.0048734008823521436\n",
      "Training Loss: 0.005646535496343859\n",
      "Training Loss: 0.006550054083345458\n",
      "Training Loss: 0.006298701288178563\n",
      "Training Loss: 0.0063260181562509385\n",
      "Validation Loss: 0.00348853858969129\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.005135100948391483\n",
      "Training Loss: 0.004982237381336745\n",
      "Training Loss: 0.004868108108639717\n",
      "Training Loss: 0.005641157357022166\n",
      "Training Loss: 0.006544171685818583\n",
      "Training Loss: 0.00629288726602681\n",
      "Training Loss: 0.006320023722946644\n",
      "Validation Loss: 0.00348316688938171\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.005129739288240671\n",
      "Training Loss: 0.004977084595593624\n",
      "Training Loss: 0.0048629322898341345\n",
      "Training Loss: 0.005635884031653405\n",
      "Training Loss: 0.006538404760649427\n",
      "Training Loss: 0.006287173802265897\n",
      "Training Loss: 0.00631413007969968\n",
      "Validation Loss: 0.003477907176687616\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.005124478713260033\n",
      "Training Loss: 0.004972012723446824\n",
      "Training Loss: 0.0048578678152989595\n",
      "Training Loss: 0.0056307086936431\n",
      "Training Loss: 0.006532747618621215\n",
      "Training Loss: 0.006281554813031107\n",
      "Training Loss: 0.006308334907516837\n",
      "Validation Loss: 0.00347275287784943\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.005119316559867002\n",
      "Training Loss: 0.004967020876647439\n",
      "Training Loss: 0.00485291009536013\n",
      "Training Loss: 0.005625626517576166\n",
      "Training Loss: 0.0065271952492184935\n",
      "Training Loss: 0.006276027370477095\n",
      "Training Loss: 0.006302634128369391\n",
      "Validation Loss: 0.0034677014788194152\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.005114245240110904\n",
      "Training Loss: 0.0049621037812903526\n",
      "Training Loss: 0.004848054858739488\n",
      "Training Loss: 0.005620633640210144\n",
      "Training Loss: 0.006521743314806372\n",
      "Training Loss: 0.0062705885851755735\n",
      "Training Loss: 0.00629702360718511\n",
      "Validation Loss: 0.003462747379453776\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.0051092647184850645\n",
      "Training Loss: 0.004957259316870477\n",
      "Training Loss: 0.004843295984901488\n",
      "Training Loss: 0.005615727000404149\n",
      "Training Loss: 0.0065163877652958035\n",
      "Training Loss: 0.006265231972793117\n",
      "Training Loss: 0.006291500503430143\n",
      "Validation Loss: 0.0034578885536026976\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005104369716718793\n",
      "Training Loss: 0.004952484634704888\n",
      "Training Loss: 0.0048386311659123745\n",
      "Training Loss: 0.005610901832114905\n",
      "Training Loss: 0.006511125357355922\n",
      "Training Loss: 0.006259958099108189\n",
      "Training Loss: 0.006286062181461602\n",
      "Validation Loss: 0.00345311636778383\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.00509955518762581\n",
      "Training Loss: 0.004947775884065777\n",
      "Training Loss: 0.004834055007086136\n",
      "Training Loss: 0.005606155646382831\n",
      "Training Loss: 0.006505953224841505\n",
      "Training Loss: 0.0062547614716459065\n",
      "Training Loss: 0.006280703998636454\n",
      "Validation Loss: 0.003448431404073782\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.0050948185072047635\n",
      "Training Loss: 0.004943131443869788\n",
      "Training Loss: 0.004829563227249309\n",
      "Training Loss: 0.005601484986254945\n",
      "Training Loss: 0.006500868110451847\n",
      "Training Loss: 0.006249640048481524\n",
      "Training Loss: 0.006275426753563807\n",
      "Validation Loss: 0.0034438281534553577\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005090157351805829\n",
      "Training Loss: 0.004938549373764545\n",
      "Training Loss: 0.004825154129648581\n",
      "Training Loss: 0.005596886738203466\n",
      "Training Loss: 0.006495865753386169\n",
      "Training Loss: 0.006244591476861388\n",
      "Training Loss: 0.006270225219195708\n",
      "Validation Loss: 0.0034393037103416487\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.00508556958520785\n",
      "Training Loss: 0.004934027890849393\n",
      "Training Loss: 0.004820823697373271\n",
      "Training Loss: 0.0055923590622842315\n",
      "Training Loss: 0.006490944311954081\n",
      "Training Loss: 0.006239614640362561\n",
      "Training Loss: 0.00626509880530648\n",
      "Validation Loss: 0.003434856045094741\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00508105298911687\n",
      "Training Loss: 0.004929563938057982\n",
      "Training Loss: 0.004816567274974659\n",
      "Training Loss: 0.0055879006098257375\n",
      "Training Loss: 0.0064861028280574825\n",
      "Training Loss: 0.00623470802907832\n",
      "Training Loss: 0.006260045469971374\n",
      "Validation Loss: 0.0034304830120974097\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.005076603589695878\n",
      "Training Loss: 0.004925158337282482\n",
      "Training Loss: 0.004812384939868935\n",
      "Training Loss: 0.0055835062911501155\n",
      "Training Loss: 0.006481335409916938\n",
      "Training Loss: 0.006229864758206531\n",
      "Training Loss: 0.006255061505362392\n",
      "Validation Loss: 0.0034261820096634188\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.0050722196337301285\n",
      "Training Loss: 0.004920807061134838\n",
      "Training Loss: 0.004808272104710341\n",
      "Training Loss: 0.005579175730235875\n",
      "Training Loss: 0.006476642781635746\n",
      "Training Loss: 0.006225089770741761\n",
      "Training Loss: 0.00625014629564248\n",
      "Validation Loss: 0.0034219504082017346\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005067900097928941\n",
      "Training Loss: 0.0049165089687448925\n",
      "Training Loss: 0.0048042262456147004\n",
      "Training Loss: 0.005574907861300744\n",
      "Training Loss: 0.0064720193250104785\n",
      "Training Loss: 0.006220377533463761\n",
      "Training Loss: 0.00624529822729528\n",
      "Validation Loss: 0.0034177886738096562\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005063643357716501\n",
      "Training Loss: 0.004912264084559866\n",
      "Training Loss: 0.004800245388178154\n",
      "Training Loss: 0.005570700314710848\n",
      "Training Loss: 0.006467466503381729\n",
      "Training Loss: 0.006215727793751284\n",
      "Training Loss: 0.00624051459482871\n",
      "Validation Loss: 0.0034136918969870952\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.0050594447838375346\n",
      "Training Loss: 0.004908070070378016\n",
      "Training Loss: 0.004796326977084391\n",
      "Training Loss: 0.0055665508704259995\n",
      "Training Loss: 0.006462982320226729\n",
      "Training Loss: 0.006211138335056603\n",
      "Training Loss: 0.006235794994281605\n",
      "Validation Loss: 0.0034096594825770692\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005055306464200839\n",
      "Training Loss: 0.00490392640727805\n",
      "Training Loss: 0.004792469838866964\n",
      "Training Loss: 0.005562459428911097\n",
      "Training Loss: 0.006458562271436676\n",
      "Training Loss: 0.0062066090363077815\n",
      "Training Loss: 0.006231136475689709\n",
      "Validation Loss: 0.0034056907682766795\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005051224352791905\n",
      "Training Loss: 0.0048998319549718875\n",
      "Training Loss: 0.004788670449052006\n",
      "Training Loss: 0.005558422752073966\n",
      "Training Loss: 0.006454206437338144\n",
      "Training Loss: 0.0062021376832854\n",
      "Training Loss: 0.006226538448827341\n",
      "Validation Loss: 0.003401783164172025\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005047195708029904\n",
      "Training Loss: 0.00489578341395827\n",
      "Training Loss: 0.004784927170840092\n",
      "Training Loss: 0.005554441175772809\n",
      "Training Loss: 0.006449912711977959\n",
      "Training Loss: 0.006197724108351395\n",
      "Training Loss: 0.006221998578403145\n",
      "Validation Loss: 0.003397935066609347\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.005043222574167885\n",
      "Training Loss: 0.00489178255724255\n",
      "Training Loss: 0.004781239358708262\n",
      "Training Loss: 0.005550511326291598\n",
      "Training Loss: 0.0064456810511183\n",
      "Training Loss: 0.0061933658132329586\n",
      "Training Loss: 0.006217516228789463\n",
      "Validation Loss: 0.003394146151631401\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005039301335345954\n",
      "Training Loss: 0.004887827945931349\n",
      "Training Loss: 0.004777603388647549\n",
      "Training Loss: 0.005546633964986541\n",
      "Training Loss: 0.006441508756252006\n",
      "Training Loss: 0.006189063627971336\n",
      "Training Loss: 0.006213091444224119\n",
      "Validation Loss: 0.003390413160045188\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.0050354298565071075\n",
      "Training Loss: 0.004883918181876652\n",
      "Training Loss: 0.0047740181925473735\n",
      "Training Loss: 0.005542806950397789\n",
      "Training Loss: 0.006437393437372521\n",
      "Training Loss: 0.0061848141031805425\n",
      "Training Loss: 0.006208720026770607\n",
      "Validation Loss: 0.0033867350301604146\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.00503160894731991\n",
      "Training Loss: 0.004880051831423771\n",
      "Training Loss: 0.0047704830387374384\n",
      "Training Loss: 0.005539029738865792\n",
      "Training Loss: 0.006433334733592346\n",
      "Training Loss: 0.006180618746438995\n",
      "Training Loss: 0.006204404139425606\n",
      "Validation Loss: 0.0033831136756356846\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005027836904046126\n",
      "Training Loss: 0.004876228900393471\n",
      "Training Loss: 0.004766996199614368\n",
      "Training Loss: 0.005535300687770359\n",
      "Training Loss: 0.006429330960381776\n",
      "Training Loss: 0.006176474811509252\n",
      "Training Loss: 0.0062001393688842655\n",
      "Validation Loss: 0.003379545820709527\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.0050241122668376196\n",
      "Training Loss: 0.004872450418479275\n",
      "Training Loss: 0.004763555905083194\n",
      "Training Loss: 0.00553161894553341\n",
      "Training Loss: 0.0064253817405551675\n",
      "Training Loss: 0.006172382866498083\n",
      "Training Loss: 0.0061959256429690865\n",
      "Validation Loss: 0.0033760302314667333\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005020435834303498\n",
      "Training Loss: 0.004868713514297269\n",
      "Training Loss: 0.004760161298327148\n",
      "Training Loss: 0.005527983167557977\n",
      "Training Loss: 0.006421483322046697\n",
      "Training Loss: 0.006168340935837477\n",
      "Training Loss: 0.00619176248088479\n",
      "Validation Loss: 0.003372567568466151\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005016802485915833\n",
      "Training Loss: 0.0048650162780540995\n",
      "Training Loss: 0.004756810379913077\n",
      "Training Loss: 0.005524394034873694\n",
      "Training Loss: 0.006417637695558369\n",
      "Training Loss: 0.006164349967148155\n",
      "Training Loss: 0.006187648157356307\n",
      "Validation Loss: 0.0033691541992881325\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005013215319486335\n",
      "Training Loss: 0.004861360715003684\n",
      "Training Loss: 0.004753502295934595\n",
      "Training Loss: 0.0055208482022862885\n",
      "Training Loss: 0.006413842486217618\n",
      "Training Loss: 0.006160407322458923\n",
      "Training Loss: 0.0061835829564370216\n",
      "Validation Loss: 0.0033657938692795483\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005009672322194092\n",
      "Training Loss: 0.004857745443587191\n",
      "Training Loss: 0.004750235669198446\n",
      "Training Loss: 0.005517346039414406\n",
      "Training Loss: 0.006410096931504085\n",
      "Training Loss: 0.006156513489549979\n",
      "Training Loss: 0.006179564056219533\n",
      "Validation Loss: 0.0033624790716141966\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005006171982386149\n",
      "Training Loss: 0.004854169364261906\n",
      "Training Loss: 0.004747009442071431\n",
      "Training Loss: 0.0055138870631344615\n",
      "Training Loss: 0.0064063981536310165\n",
      "Training Loss: 0.00615266635431908\n",
      "Training Loss: 0.006175591294886544\n",
      "Validation Loss: 0.0033592129695623387\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005002712528221309\n",
      "Training Loss: 0.004850632148445584\n",
      "Training Loss: 0.004743822536547668\n",
      "Training Loss: 0.005510468303109519\n",
      "Training Loss: 0.0064027459989301864\n",
      "Training Loss: 0.00614886520896107\n",
      "Training Loss: 0.006171664197463542\n",
      "Validation Loss: 0.003355991892771244\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.004999294106382877\n",
      "Training Loss: 0.004847133785078768\n",
      "Training Loss: 0.004740673708729446\n",
      "Training Loss: 0.005507092766929417\n",
      "Training Loss: 0.006399141993606463\n",
      "Training Loss: 0.006145111256046221\n",
      "Training Loss: 0.00616778114810586\n",
      "Validation Loss: 0.003352823003181274\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.004995916494517587\n",
      "Training Loss: 0.004843672680435702\n",
      "Training Loss: 0.0047375626099528745\n",
      "Training Loss: 0.00550375725782942\n",
      "Training Loss: 0.006395582294790075\n",
      "Training Loss: 0.006141403128858656\n",
      "Training Loss: 0.006163940485566855\n",
      "Validation Loss: 0.003349698596016577\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.004992579957470298\n",
      "Training Loss: 0.004840249803673941\n",
      "Training Loss: 0.004734488335088827\n",
      "Training Loss: 0.005500459846807644\n",
      "Training Loss: 0.006392066215630621\n",
      "Training Loss: 0.006137740495614707\n",
      "Training Loss: 0.006160142977023497\n",
      "Validation Loss: 0.003346617092912117\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.004989280980080366\n",
      "Training Loss: 0.004836863790114876\n",
      "Training Loss: 0.004731449887040071\n",
      "Training Loss: 0.005497203548438847\n",
      "Training Loss: 0.00638859485508874\n",
      "Training Loss: 0.006134119618218392\n",
      "Training Loss: 0.006156388511881232\n",
      "Validation Loss: 0.003343578458550885\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.004986020140349865\n",
      "Training Loss: 0.004833513121120632\n",
      "Training Loss: 0.004728445356013253\n",
      "Training Loss: 0.00549398350645788\n",
      "Training Loss: 0.00638516598730348\n",
      "Training Loss: 0.006130544082261622\n",
      "Training Loss: 0.0061526734801009295\n",
      "Validation Loss: 0.0033405860497926674\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.004982797781703994\n",
      "Training Loss: 0.004830199089483358\n",
      "Training Loss: 0.004725475490558893\n",
      "Training Loss: 0.005490800946718082\n",
      "Training Loss: 0.006381777868373319\n",
      "Training Loss: 0.0061270109901670365\n",
      "Training Loss: 0.006148999594151974\n",
      "Validation Loss: 0.0033376361753561356\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.004979611258604564\n",
      "Training Loss: 0.004826919962943066\n",
      "Training Loss: 0.004722537706256844\n",
      "Training Loss: 0.0054876555898226795\n",
      "Training Loss: 0.006378431000048295\n",
      "Training Loss: 0.006123520415276289\n",
      "Training Loss: 0.006145364519907162\n",
      "Validation Loss: 0.003334727037125982\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.004976462272461504\n",
      "Training Loss: 0.004823678085813299\n",
      "Training Loss: 0.0047196330293081705\n",
      "Training Loss: 0.005484546310035512\n",
      "Training Loss: 0.006375124305486679\n",
      "Training Loss: 0.00612007137038745\n",
      "Training Loss: 0.006141768982633949\n",
      "Validation Loss: 0.0033318594142564394\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.004973347958293743\n",
      "Training Loss: 0.0048204687924589965\n",
      "Training Loss: 0.0047167590737808495\n",
      "Training Loss: 0.0054814728774363174\n",
      "Training Loss: 0.006371857319027186\n",
      "Training Loss: 0.006116664024302736\n",
      "Training Loss: 0.006138211040524766\n",
      "Validation Loss: 0.0033290316407439693\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.004970269292243756\n",
      "Training Loss: 0.0048172935351612975\n",
      "Training Loss: 0.004713916384498589\n",
      "Training Loss: 0.0054784331540577115\n",
      "Training Loss: 0.006368629273492843\n",
      "Training Loss: 0.006113296202383936\n",
      "Training Loss: 0.0061346894537564365\n",
      "Validation Loss: 0.003326243958139464\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.004967223916319199\n",
      "Training Loss: 0.004814152083126828\n",
      "Training Loss: 0.004711103940499015\n",
      "Training Loss: 0.005475427411147394\n",
      "Training Loss: 0.006365437848726288\n",
      "Training Loss: 0.006109969506505877\n",
      "Training Loss: 0.006131205334095284\n",
      "Validation Loss: 0.003323494343781209\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.00496421268384438\n",
      "Training Loss: 0.004811043357185554\n",
      "Training Loss: 0.004708320663776249\n",
      "Training Loss: 0.005472455596900545\n",
      "Training Loss: 0.006362285433569923\n",
      "Training Loss: 0.006106681034434586\n",
      "Training Loss: 0.006127757541835308\n",
      "Validation Loss: 0.003320785193121556\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.004961235453956761\n",
      "Training Loss: 0.004807968791283201\n",
      "Training Loss: 0.004705567075870931\n",
      "Training Loss: 0.0054695162281859664\n",
      "Training Loss: 0.006359168137423694\n",
      "Training Loss: 0.006103431156370789\n",
      "Training Loss: 0.006124344752170146\n",
      "Validation Loss: 0.0033181118573958565\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.004958288819179871\n",
      "Training Loss: 0.004804925162752625\n",
      "Training Loss: 0.004702840627869591\n",
      "Training Loss: 0.005466610371368006\n",
      "Training Loss: 0.006356088829925284\n",
      "Training Loss: 0.006100220225052908\n",
      "Training Loss: 0.006120966641465202\n",
      "Validation Loss: 0.003315477843452175\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.004955374842975289\n",
      "Training Loss: 0.0048019131241017025\n",
      "Training Loss: 0.0047001428448129445\n",
      "Training Loss: 0.005463734023505822\n",
      "Training Loss: 0.0063530431035906075\n",
      "Training Loss: 0.006097046557115391\n",
      "Training Loss: 0.006117622837191448\n",
      "Validation Loss: 0.003312879223254429\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.004952493326272816\n",
      "Training Loss: 0.004798934178834316\n",
      "Training Loss: 0.004697473003179766\n",
      "Training Loss: 0.00546089050185401\n",
      "Training Loss: 0.006350032822228968\n",
      "Training Loss: 0.006093910564668476\n",
      "Training Loss: 0.006114312879508361\n",
      "Validation Loss: 0.0033103176266171233\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.004949643848813139\n",
      "Training Loss: 0.0047959860955597835\n",
      "Training Loss: 0.004694827974308282\n",
      "Training Loss: 0.00545807782560587\n",
      "Training Loss: 0.006347056324593723\n",
      "Training Loss: 0.0060908126924186946\n",
      "Training Loss: 0.006111036209622398\n",
      "Validation Loss: 0.003307792082976564\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.004946823018835858\n",
      "Training Loss: 0.004793067962746136\n",
      "Training Loss: 0.004692210095818154\n",
      "Training Loss: 0.005455295303254388\n",
      "Training Loss: 0.006344113496597856\n",
      "Training Loss: 0.006087749043945223\n",
      "Training Loss: 0.006107792697148398\n",
      "Validation Loss: 0.003305297677600512\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.004944032868370414\n",
      "Training Loss: 0.004790180815325585\n",
      "Training Loss: 0.004689617527765222\n",
      "Training Loss: 0.005452541440026834\n",
      "Training Loss: 0.006341205181088299\n",
      "Training Loss: 0.006084722090745345\n",
      "Training Loss: 0.006104580193059519\n",
      "Validation Loss: 0.0033028397211205033\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.004941272591822781\n",
      "Training Loss: 0.004787323445198126\n",
      "Training Loss: 0.004687050833017565\n",
      "Training Loss: 0.005449817929184064\n",
      "Training Loss: 0.0063383283850271255\n",
      "Training Loss: 0.006081729501020163\n",
      "Training Loss: 0.006101399327162653\n",
      "Validation Loss: 0.0033004153899199675\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.004938541076262482\n",
      "Training Loss: 0.004784495590429288\n",
      "Training Loss: 0.0046845090703573075\n",
      "Training Loss: 0.00544712279457599\n",
      "Training Loss: 0.006335482598515228\n",
      "Training Loss: 0.006078772146720439\n",
      "Training Loss: 0.006098249105270952\n",
      "Validation Loss: 0.0032980244271875766\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.00493583815463353\n",
      "Training Loss: 0.00478169656911632\n",
      "Training Loss: 0.0046819913177751004\n",
      "Training Loss: 0.0054444556834641845\n",
      "Training Loss: 0.006332669417606667\n",
      "Training Loss: 0.00607584863435477\n",
      "Training Loss: 0.006095130048925057\n",
      "Validation Loss: 0.0032956646646334605\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.004933162471279502\n",
      "Training Loss: 0.004778927089937497\n",
      "Training Loss: 0.004679497145116329\n",
      "Training Loss: 0.005441815798403696\n",
      "Training Loss: 0.006329886399907991\n",
      "Training Loss: 0.006072960235178471\n",
      "Training Loss: 0.006092041423544288\n",
      "Validation Loss: 0.003293337555051073\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.004930516654858366\n",
      "Training Loss: 0.004776185380178504\n",
      "Training Loss: 0.004677027292782441\n",
      "Training Loss: 0.005439205782604404\n",
      "Training Loss: 0.0063271367165725675\n",
      "Training Loss: 0.006070104197133333\n",
      "Training Loss: 0.006088983054505661\n",
      "Validation Loss: 0.003291041661343408\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0049278970109298825\n",
      "Training Loss: 0.004773472569359001\n",
      "Training Loss: 0.004674580242135562\n",
      "Training Loss: 0.005436622311826796\n",
      "Training Loss: 0.006324414573609829\n",
      "Training Loss: 0.006067282368894667\n",
      "Training Loss: 0.006085953109432012\n",
      "Validation Loss: 0.0032887786974753726\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.004925305554643274\n",
      "Training Loss: 0.004770787186571397\n",
      "Training Loss: 0.004672156558954157\n",
      "Training Loss: 0.0054340637312270705\n",
      "Training Loss: 0.006321722571738064\n",
      "Training Loss: 0.0060644923779182135\n",
      "Training Loss: 0.006082952750148252\n",
      "Validation Loss: 0.00328654328130939\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.004922737890155986\n",
      "Training Loss: 0.004768128637806512\n",
      "Training Loss: 0.0046697544871130954\n",
      "Training Loss: 0.005431531763169914\n",
      "Training Loss: 0.0063190605235286055\n",
      "Training Loss: 0.006061734786489978\n",
      "Training Loss: 0.006079981675138697\n",
      "Validation Loss: 0.0032843383123281967\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.004920198617619462\n",
      "Training Loss: 0.0047654972586315125\n",
      "Training Loss: 0.004667374661075883\n",
      "Training Loss: 0.005429026305209845\n",
      "Training Loss: 0.006316426723496988\n",
      "Training Loss: 0.006059007857693359\n",
      "Training Loss: 0.006077038390794769\n",
      "Validation Loss: 0.003282162577981192\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.004917684209649451\n",
      "Training Loss: 0.004762892280123197\n",
      "Training Loss: 0.004665016870712861\n",
      "Training Loss: 0.005426545789232478\n",
      "Training Loss: 0.006313820799114182\n",
      "Training Loss: 0.006056312697473913\n",
      "Training Loss: 0.006074122200952842\n",
      "Validation Loss: 0.003280016625900003\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.004915195388603025\n",
      "Training Loss: 0.0047603149362839756\n",
      "Training Loss: 0.004662681666668504\n",
      "Training Loss: 0.005424089933512732\n",
      "Training Loss: 0.006311243163654581\n",
      "Training Loss: 0.0060536492709070445\n",
      "Training Loss: 0.0060712337028235195\n",
      "Validation Loss: 0.003277897062163786\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.0049127297796076165\n",
      "Training Loss: 0.004757761640939861\n",
      "Training Loss: 0.004660366021562367\n",
      "Training Loss: 0.00542166001512669\n",
      "Training Loss: 0.006308693843893706\n",
      "Training Loss: 0.006051015822449699\n",
      "Training Loss: 0.0060683726274874065\n",
      "Validation Loss: 0.0032758051267919247\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.004910289024701342\n",
      "Training Loss: 0.00475523489934858\n",
      "Training Loss: 0.004658071880694479\n",
      "Training Loss: 0.005419253219151869\n",
      "Training Loss: 0.006306171952746809\n",
      "Training Loss: 0.006048411893425509\n",
      "Training Loss: 0.006065537498798222\n",
      "Validation Loss: 0.0032737411223770527\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.004907872151816264\n",
      "Training Loss: 0.004752731438493356\n",
      "Training Loss: 0.0046557967102853585\n",
      "Training Loss: 0.005416869758628309\n",
      "Training Loss: 0.006303677778923884\n",
      "Training Loss: 0.00604583642212674\n",
      "Training Loss: 0.006062727774260566\n",
      "Validation Loss: 0.0032717021463843487\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.0049054763501044365\n",
      "Training Loss: 0.004750252588419244\n",
      "Training Loss: 0.00465354137995746\n",
      "Training Loss: 0.005414510101545602\n",
      "Training Loss: 0.006301209325902164\n",
      "Training Loss: 0.006043291377136484\n",
      "Training Loss: 0.006059945555170998\n",
      "Validation Loss: 0.0032696909291237202\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.004903106585261412\n",
      "Training Loss: 0.004747799472243059\n",
      "Training Loss: 0.004651307541062124\n",
      "Training Loss: 0.005412174671655521\n",
      "Training Loss: 0.006298766526160762\n",
      "Training Loss: 0.006040774898137897\n",
      "Training Loss: 0.006057188976556062\n",
      "Validation Loss: 0.003267704637822336\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.0049007578159216796\n",
      "Training Loss: 0.004745370112068485\n",
      "Training Loss: 0.004649092442705296\n",
      "Training Loss: 0.005409861101652496\n",
      "Training Loss: 0.006296350366901606\n",
      "Training Loss: 0.006038286201655865\n",
      "Training Loss: 0.006054457108257338\n",
      "Validation Loss: 0.00326574494851202\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.00489843430405017\n",
      "Training Loss: 0.004742963846365455\n",
      "Training Loss: 0.004646897334023379\n",
      "Training Loss: 0.0054075705009745435\n",
      "Training Loss: 0.006293959055328742\n",
      "Training Loss: 0.006035826449515298\n",
      "Training Loss: 0.006051748428726569\n",
      "Validation Loss: 0.0032638074356664934\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.004896130368579179\n",
      "Training Loss: 0.004740580860816408\n",
      "Training Loss: 0.0046447199652902785\n",
      "Training Loss: 0.00540530284109991\n",
      "Training Loss: 0.006291591273620725\n",
      "Training Loss: 0.006033393793040886\n",
      "Training Loss: 0.006049065883271396\n",
      "Validation Loss: 0.0032618959226788542\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.004893848770298064\n",
      "Training Loss: 0.0047382212287629955\n",
      "Training Loss: 0.004642562092631124\n",
      "Training Loss: 0.005403055713977665\n",
      "Training Loss: 0.0062892501789610835\n",
      "Training Loss: 0.006030988043639809\n",
      "Training Loss: 0.006046408272814005\n",
      "Validation Loss: 0.003260008147952569\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.004891588181490079\n",
      "Training Loss: 0.004735884243564215\n",
      "Training Loss: 0.0046404226415324955\n",
      "Training Loss: 0.005400830371654592\n",
      "Training Loss: 0.006286932373186573\n",
      "Training Loss: 0.006028609159402549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [28:23<18:55, 283.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.006043774218996986\n",
      "Validation Loss: 0.003258145217215189\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.11438537370413542\n",
      "Training Loss: 0.07451612452045082\n",
      "Training Loss: 0.05466147415339947\n",
      "Training Loss: 0.04893398767337203\n",
      "Training Loss: 0.046335135279223325\n",
      "Training Loss: 0.04584996216930449\n",
      "Training Loss: 0.043287460366263984\n",
      "Validation Loss: 0.04084259106983406\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.040942498100921515\n",
      "Training Loss: 0.03874189724214375\n",
      "Training Loss: 0.03685160119086504\n",
      "Training Loss: 0.03617169683799148\n",
      "Training Loss: 0.03358718964271248\n",
      "Training Loss: 0.03269900935702026\n",
      "Training Loss: 0.030384849552065135\n",
      "Validation Loss: 0.0281410018568126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.028140679351054132\n",
      "Training Loss: 0.026412950088270007\n",
      "Training Loss: 0.02491334993392229\n",
      "Training Loss: 0.024915479333139957\n",
      "Training Loss: 0.02312548411078751\n",
      "Training Loss: 0.022310881488956512\n",
      "Training Loss: 0.02106242858339101\n",
      "Validation Loss: 0.01934457067679265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.019524581688456236\n",
      "Training Loss: 0.01842541106278077\n",
      "Training Loss: 0.017773480797186495\n",
      "Training Loss: 0.018427566806785763\n",
      "Training Loss: 0.01774344498757273\n",
      "Training Loss: 0.017116586773190647\n",
      "Training Loss: 0.01665933559415862\n",
      "Validation Loss: 0.015364047157323538\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.015591260716319085\n",
      "Training Loss: 0.014799189891200513\n",
      "Training Loss: 0.014494871546048671\n",
      "Training Loss: 0.015275859104003758\n",
      "Training Loss: 0.015071995018515735\n",
      "Training Loss: 0.014525332828052342\n",
      "Training Loss: 0.014217392052523791\n",
      "Validation Loss: 0.012719209457674388\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.013233031511772424\n",
      "Training Loss: 0.012550615014042706\n",
      "Training Loss: 0.01233026670757681\n",
      "Training Loss: 0.013096900908276438\n",
      "Training Loss: 0.01319464144296944\n",
      "Training Loss: 0.012733690126333385\n",
      "Training Loss: 0.01248381930636242\n",
      "Validation Loss: 0.010724175727511111\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.011537704486399888\n",
      "Training Loss: 0.010936536707449705\n",
      "Training Loss: 0.010776219123508781\n",
      "Training Loss: 0.011535477999132127\n",
      "Training Loss: 0.011860079218167811\n",
      "Training Loss: 0.0114681064884644\n",
      "Training Loss: 0.011264351622667163\n",
      "Validation Loss: 0.009307266974277544\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.010325163227971643\n",
      "Training Loss: 0.009785331727471203\n",
      "Training Loss: 0.00967217401135713\n",
      "Training Loss: 0.010432713190093637\n",
      "Training Loss: 0.01092037777416408\n",
      "Training Loss: 0.010577838593162596\n",
      "Training Loss: 0.010412420553620905\n",
      "Validation Loss: 0.008312898697610595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.009454878477845341\n",
      "Training Loss: 0.008965567665873095\n",
      "Training Loss: 0.008886265159817412\n",
      "Training Loss: 0.009653988117352129\n",
      "Training Loss: 0.010257576970616356\n",
      "Training Loss: 0.009951226350385695\n",
      "Training Loss: 0.009816807832103223\n",
      "Validation Loss: 0.007609123675403803\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.00882550167152658\n",
      "Training Loss: 0.008379616766469554\n",
      "Training Loss: 0.008322656146483497\n",
      "Training Loss: 0.009101612258236856\n",
      "Training Loss: 0.009789064497454092\n",
      "Training Loss: 0.009509673360735178\n",
      "Training Loss: 0.00939938013558276\n",
      "Validation Loss: 0.007103456894096252\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.008366609929362312\n",
      "Training Loss: 0.007957340178545564\n",
      "Training Loss: 0.007914323265431449\n",
      "Training Loss: 0.008706985842436551\n",
      "Training Loss: 0.009457232545828446\n",
      "Training Loss: 0.009197478026617318\n",
      "Training Loss: 0.009105426829773933\n",
      "Validation Loss: 0.006733138416806578\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.008028445874806493\n",
      "Training Loss: 0.007649016353534535\n",
      "Training Loss: 0.007614606600254774\n",
      "Training Loss: 0.008422140539623797\n",
      "Training Loss: 0.009221411519683897\n",
      "Training Loss: 0.008975256037665531\n",
      "Training Loss: 0.008896967276232317\n",
      "Validation Loss: 0.006456278976966882\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.007775919294217602\n",
      "Training Loss: 0.007420297616627067\n",
      "Training Loss: 0.007391344244824722\n",
      "Training Loss: 0.008213839860400186\n",
      "Training Loss: 0.009052872494794428\n",
      "Training Loss: 0.008815457476302982\n",
      "Training Loss: 0.008747820167336613\n",
      "Validation Loss: 0.006245132932467351\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.007584466622211039\n",
      "Training Loss: 0.007247775454306975\n",
      "Training Loss: 0.007222398989833892\n",
      "Training Loss: 0.008059102415572852\n",
      "Training Loss: 0.008931335003580898\n",
      "Training Loss: 0.008698935630964116\n",
      "Training Loss: 0.00863986614975147\n",
      "Validation Loss: 0.006081133292576347\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.007436879202723503\n",
      "Training Loss: 0.007115395362488925\n",
      "Training Loss: 0.007092404123395682\n",
      "Training Loss: 0.00794197249400895\n",
      "Training Loss: 0.008842481175670401\n",
      "Training Loss: 0.008612376871751622\n",
      "Training Loss: 0.008560449577635154\n",
      "Validation Loss: 0.005951588673426418\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.007321035943459719\n",
      "Training Loss: 0.007011979008093476\n",
      "Training Loss: 0.006990550369955599\n",
      "Training Loss: 0.007851307278033346\n",
      "Training Loss: 0.008776206377660856\n",
      "Training Loss: 0.008546505485428497\n",
      "Training Loss: 0.008500668274937197\n",
      "Validation Loss: 0.005847583285947975\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.0072283078776672486\n",
      "Training Loss: 0.006929599958239123\n",
      "Training Loss: 0.006909133294830099\n",
      "Training Loss: 0.007779290078324266\n",
      "Training Loss: 0.008725386908045038\n",
      "Training Loss: 0.008494838853366674\n",
      "Training Loss: 0.008454250881914049\n",
      "Validation Loss: 0.005762721891497517\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.007152500916272402\n",
      "Training Loss: 0.006862562741152942\n",
      "Training Loss: 0.006842601613607257\n",
      "Training Loss: 0.007720419744728133\n",
      "Training Loss: 0.008685019438853487\n",
      "Training Loss: 0.008452845526626333\n",
      "Training Loss: 0.008416802901774646\n",
      "Validation Loss: 0.005692349925588412\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.00708914605434984\n",
      "Training Loss: 0.006806749374372884\n",
      "Training Loss: 0.006786935540148988\n",
      "Training Loss: 0.007670834584860131\n",
      "Training Loss: 0.008651629337109626\n",
      "Training Loss: 0.008417385207721964\n",
      "Training Loss: 0.008385279575595633\n",
      "Validation Loss: 0.005633039073844192\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.007035004354547709\n",
      "Training Loss: 0.006759169981814921\n",
      "Training Loss: 0.006739220721647144\n",
      "Training Loss: 0.007627831061254256\n",
      "Training Loss: 0.008622837942093611\n",
      "Training Loss: 0.008386309299385174\n",
      "Training Loss: 0.008357606406789272\n",
      "Validation Loss: 0.005582253448665142\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.006987734596477822\n",
      "Training Loss: 0.0067176636587828395\n",
      "Training Loss: 0.006697355955839157\n",
      "Training Loss: 0.007589532632846385\n",
      "Training Loss: 0.008597057700389996\n",
      "Training Loss: 0.008358178230701015\n",
      "Training Loss: 0.008332406540866942\n",
      "Validation Loss: 0.005538096092466948\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.006945644153747707\n",
      "Training Loss: 0.0066806631739018485\n",
      "Training Loss: 0.0066598282032646235\n",
      "Training Loss: 0.007554647135548293\n",
      "Training Loss: 0.008573257814859972\n",
      "Training Loss: 0.008332055244827642\n",
      "Training Loss: 0.008308792957104743\n",
      "Validation Loss: 0.0054991367501503186\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.006907506065908819\n",
      "Training Loss: 0.006647037499933503\n",
      "Training Loss: 0.00662555375136435\n",
      "Training Loss: 0.007522290695342235\n",
      "Training Loss: 0.00855079066590406\n",
      "Training Loss: 0.00830734268995002\n",
      "Training Loss: 0.00828620493877679\n",
      "Validation Loss: 0.0054642835846484715\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.006872426836052909\n",
      "Training Loss: 0.00661596684367396\n",
      "Training Loss: 0.006593756171641871\n",
      "Training Loss: 0.007491858662688173\n",
      "Training Loss: 0.008529252170119435\n",
      "Training Loss: 0.008283669139491395\n",
      "Training Loss: 0.008264303975738584\n",
      "Validation Loss: 0.0054326968909370315\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.006839753871317953\n",
      "Training Loss: 0.006586859273957088\n",
      "Training Loss: 0.006563873662380502\n",
      "Training Loss: 0.007462939576944336\n",
      "Training Loss: 0.008508399911224842\n",
      "Training Loss: 0.008260812012013048\n",
      "Training Loss: 0.008242890891851857\n",
      "Validation Loss: 0.005403732849557078\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.006809011389268562\n",
      "Training Loss: 0.006559290539007634\n",
      "Training Loss: 0.00653551000286825\n",
      "Training Loss: 0.007435257145552896\n",
      "Training Loss: 0.008488094710046426\n",
      "Training Loss: 0.008238639933988452\n",
      "Training Loss: 0.00822185524040833\n",
      "Validation Loss: 0.005376896281179027\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.0067798487551044674\n",
      "Training Loss: 0.006532958485768177\n",
      "Training Loss: 0.006508381854509935\n",
      "Training Loss: 0.007408626083633863\n",
      "Training Loss: 0.008468252710299566\n",
      "Training Loss: 0.008217080794274807\n",
      "Training Loss: 0.008201141260797159\n",
      "Validation Loss: 0.0053518163031560095\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.006752018831903115\n",
      "Training Loss: 0.006507656137691811\n",
      "Training Loss: 0.006482292762957514\n",
      "Training Loss: 0.007382928185397759\n",
      "Training Loss: 0.008448827486718073\n",
      "Training Loss: 0.008196099057095126\n",
      "Training Loss: 0.008180728070437909\n",
      "Validation Loss: 0.0053282136608556975\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.006725348348263651\n",
      "Training Loss: 0.006483250326709822\n",
      "Training Loss: 0.0064571129472460595\n",
      "Training Loss: 0.007358090038760565\n",
      "Training Loss: 0.008429794107796625\n",
      "Training Loss: 0.008175678797997535\n",
      "Training Loss: 0.00816061392542906\n",
      "Validation Loss: 0.005305881732342283\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006699719942407683\n",
      "Training Loss: 0.0064596566860564055\n",
      "Training Loss: 0.006432757629081607\n",
      "Training Loss: 0.007334073592792265\n",
      "Training Loss: 0.008411136725917458\n",
      "Training Loss: 0.008155817857477814\n",
      "Training Loss: 0.008140809605829418\n",
      "Validation Loss: 0.0052846777697585605\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006675064497394487\n",
      "Training Loss: 0.00643683918111492\n",
      "Training Loss: 0.006409185255179182\n",
      "Training Loss: 0.00731086143408902\n",
      "Training Loss: 0.008392849924275652\n",
      "Training Loss: 0.008136521474225446\n",
      "Training Loss: 0.00812133345520124\n",
      "Validation Loss: 0.005264496152578724\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006651344393612817\n",
      "Training Loss: 0.0064147859235527\n",
      "Training Loss: 0.006386373206041753\n",
      "Training Loss: 0.007288452018401586\n",
      "Training Loss: 0.008374931080034002\n",
      "Training Loss: 0.00811779547133483\n",
      "Training Loss: 0.008102209354983642\n",
      "Validation Loss: 0.005245264462624373\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.006628541458630934\n",
      "Training Loss: 0.006393508172477596\n",
      "Training Loss: 0.006364318926353007\n",
      "Training Loss: 0.007266851631575264\n",
      "Training Loss: 0.008357380777597427\n",
      "Training Loss: 0.008099646347109229\n",
      "Training Loss: 0.008083458649925887\n",
      "Validation Loss: 0.005226928257638037\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006606653641210869\n",
      "Training Loss: 0.00637302655086387\n",
      "Training Loss: 0.006343028031988069\n",
      "Training Loss: 0.0072460678638890386\n",
      "Training Loss: 0.008340201315004379\n",
      "Training Loss: 0.008082077250583098\n",
      "Training Loss: 0.008065104690613225\n",
      "Validation Loss: 0.0052094487377445904\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006585681749274954\n",
      "Training Loss: 0.00635336589999497\n",
      "Training Loss: 0.006322507348377257\n",
      "Training Loss: 0.007226105802110396\n",
      "Training Loss: 0.008323394723702223\n",
      "Training Loss: 0.008065086120041087\n",
      "Training Loss: 0.008047167032491415\n",
      "Validation Loss: 0.005192783938490608\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.006565623806091025\n",
      "Training Loss: 0.006334547963924706\n",
      "Training Loss: 0.006302764305146411\n",
      "Training Loss: 0.007206964522483758\n",
      "Training Loss: 0.008306963017676026\n",
      "Training Loss: 0.008048665152164175\n",
      "Training Loss: 0.008029660384636372\n",
      "Validation Loss: 0.005176904151732388\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006546476643998176\n",
      "Training Loss: 0.006316589889465831\n",
      "Training Loss: 0.006283800378441811\n",
      "Training Loss: 0.007188636893988587\n",
      "Training Loss: 0.008290906999027356\n",
      "Training Loss: 0.00803280342835933\n",
      "Training Loss: 0.008012594203464687\n",
      "Validation Loss: 0.005161762697105941\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006528223803034052\n",
      "Training Loss: 0.006299494341365061\n",
      "Training Loss: 0.006265608126996085\n",
      "Training Loss: 0.007171109511400573\n",
      "Training Loss: 0.008275224918033928\n",
      "Training Loss: 0.008017480780836195\n",
      "Training Loss: 0.007995970463380218\n",
      "Validation Loss: 0.00514731565769413\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006510839945403859\n",
      "Training Loss: 0.00628325403900817\n",
      "Training Loss: 0.006248170170001685\n",
      "Training Loss: 0.00715435620280914\n",
      "Training Loss: 0.008259909006301313\n",
      "Training Loss: 0.008002670975401997\n",
      "Training Loss: 0.007979784382041544\n",
      "Validation Loss: 0.005133518207493876\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.0064942939998582005\n",
      "Training Loss: 0.006267849311698228\n",
      "Training Loss: 0.006231465879827738\n",
      "Training Loss: 0.007138345954590477\n",
      "Training Loss: 0.008244950304506346\n",
      "Training Loss: 0.007988344727782532\n",
      "Training Loss: 0.00796402561245486\n",
      "Validation Loss: 0.005120316264393987\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006478544833371415\n",
      "Training Loss: 0.006253254451439716\n",
      "Training Loss: 0.006215466738212854\n",
      "Training Loss: 0.007123042339226231\n",
      "Training Loss: 0.008230335649568588\n",
      "Training Loss: 0.007974468000466004\n",
      "Training Loss: 0.007948675969382748\n",
      "Validation Loss: 0.00510766239939711\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006463547698222101\n",
      "Training Loss: 0.00623943232698366\n",
      "Training Loss: 0.006200138321146369\n",
      "Training Loss: 0.007108399847056717\n",
      "Training Loss: 0.008216045489534736\n",
      "Training Loss: 0.007961000687209889\n",
      "Training Loss: 0.007933711372315884\n",
      "Validation Loss: 0.005095495362783527\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006449246729025617\n",
      "Training Loss: 0.006226334756356664\n",
      "Training Loss: 0.006185440899571404\n",
      "Training Loss: 0.007094372595311142\n",
      "Training Loss: 0.00820206100703217\n",
      "Training Loss: 0.0079479050997179\n",
      "Training Loss: 0.007919106226181612\n",
      "Validation Loss: 0.00508376229284087\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006435584819409996\n",
      "Training Loss: 0.0062139124970417466\n",
      "Training Loss: 0.006171328780474141\n",
      "Training Loss: 0.007080909190699458\n",
      "Training Loss: 0.008188358625629917\n",
      "Training Loss: 0.00793513948796317\n",
      "Training Loss: 0.00790483082528226\n",
      "Validation Loss: 0.00507241188747327\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006422509342082776\n",
      "Training Loss: 0.00620211401314009\n",
      "Training Loss: 0.006157763403607532\n",
      "Training Loss: 0.007067961983266286\n",
      "Training Loss: 0.008174914916744457\n",
      "Training Loss: 0.007922666643280537\n",
      "Training Loss: 0.007890854792203755\n",
      "Validation Loss: 0.005061392986724913\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.0064099621254717935\n",
      "Training Loss: 0.0061908838001545514\n",
      "Training Loss: 0.006144699484575539\n",
      "Training Loss: 0.007055479002883658\n",
      "Training Loss: 0.008161704755621032\n",
      "Training Loss: 0.00791044665616937\n",
      "Training Loss: 0.00787714657606557\n",
      "Validation Loss: 0.005050663773078569\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006397892244858667\n",
      "Training Loss: 0.0061801694007590415\n",
      "Training Loss: 0.006132095338543877\n",
      "Training Loss: 0.007043413369683549\n",
      "Training Loss: 0.008148704435443506\n",
      "Training Loss: 0.007898445319151506\n",
      "Training Loss: 0.007863675619009882\n",
      "Validation Loss: 0.005040178464355875\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006386247160844505\n",
      "Training Loss: 0.006169918630039319\n",
      "Training Loss: 0.0061199112574104216\n",
      "Training Loss: 0.007031718945363537\n",
      "Training Loss: 0.008135891270358115\n",
      "Training Loss: 0.007886628831038252\n",
      "Training Loss: 0.007850414689164609\n",
      "Validation Loss: 0.005029902406287997\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006374982617562637\n",
      "Training Loss: 0.006160084961447865\n",
      "Training Loss: 0.006108112413203344\n",
      "Training Loss: 0.007020354162668809\n",
      "Training Loss: 0.008123242085566744\n",
      "Training Loss: 0.007874967461684718\n",
      "Training Loss: 0.007837333724601193\n",
      "Validation Loss: 0.005019805508348323\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0063640574621967975\n",
      "Training Loss: 0.0061506233061663805\n",
      "Training Loss: 0.006096662327181548\n",
      "Training Loss: 0.0070092795469099654\n",
      "Training Loss: 0.008110738865798339\n",
      "Training Loss: 0.007863433888414874\n",
      "Training Loss: 0.007824410911416635\n",
      "Validation Loss: 0.005009856174895295\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006353429820737801\n",
      "Training Loss: 0.006141489747678861\n",
      "Training Loss: 0.00608552974765189\n",
      "Training Loss: 0.006998459416208789\n",
      "Training Loss: 0.008098363315220923\n",
      "Training Loss: 0.00785200558253564\n",
      "Training Loss: 0.007811622220324352\n",
      "Validation Loss: 0.00500002834415726\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.00634306633961387\n",
      "Training Loss: 0.006132646392215974\n",
      "Training Loss: 0.006074685475323349\n",
      "Training Loss: 0.006987860575318337\n",
      "Training Loss: 0.008086098927306011\n",
      "Training Loss: 0.007840661040972918\n",
      "Training Loss: 0.007798950389260426\n",
      "Validation Loss: 0.004990300866478726\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006332935526734218\n",
      "Training Loss: 0.006124058622517623\n",
      "Training Loss: 0.006064102789387107\n",
      "Training Loss: 0.006977456669555977\n",
      "Training Loss: 0.008073934009298683\n",
      "Training Loss: 0.007829384293872864\n",
      "Training Loss: 0.007786375171272084\n",
      "Validation Loss: 0.004980656049346851\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006323010411579162\n",
      "Training Loss: 0.00611569587723352\n",
      "Training Loss: 0.006053759494097904\n",
      "Training Loss: 0.006967219725484028\n",
      "Training Loss: 0.00806185569963418\n",
      "Training Loss: 0.007818159031448885\n",
      "Training Loss: 0.007773884397465736\n",
      "Validation Loss: 0.004971076465780369\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006313265999779105\n",
      "Training Loss: 0.006107529228902422\n",
      "Training Loss: 0.006043632869841531\n",
      "Training Loss: 0.006957128368667327\n",
      "Training Loss: 0.008049856843426824\n",
      "Training Loss: 0.007806975714629516\n",
      "Training Loss: 0.007761466498486698\n",
      "Validation Loss: 0.004961552811709794\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0063036843383451925\n",
      "Training Loss: 0.00609953680250328\n",
      "Training Loss: 0.006033705568406731\n",
      "Training Loss: 0.006947164954035543\n",
      "Training Loss: 0.008037929487181827\n",
      "Training Loss: 0.007795824729837478\n",
      "Training Loss: 0.007749110976001248\n",
      "Validation Loss: 0.004952071696757042\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006294245070894249\n",
      "Training Loss: 0.006091695690411143\n",
      "Training Loss: 0.006023961687460542\n",
      "Training Loss: 0.006937313462258316\n",
      "Training Loss: 0.008026070827618241\n",
      "Training Loss: 0.007784699400654063\n",
      "Training Loss: 0.007736810941714793\n",
      "Validation Loss: 0.0049426254680317435\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006284933340502903\n",
      "Training Loss: 0.006083987807505764\n",
      "Training Loss: 0.0060143863048870115\n",
      "Training Loss: 0.006927558423485607\n",
      "Training Loss: 0.008014276413014158\n",
      "Training Loss: 0.007773595446487889\n",
      "Training Loss: 0.007724560850765556\n",
      "Validation Loss: 0.004933206689341712\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00627573647594545\n",
      "Training Loss: 0.006076398331206292\n",
      "Training Loss: 0.00600496681407094\n",
      "Training Loss: 0.006917889650212601\n",
      "Training Loss: 0.008002544350456447\n",
      "Training Loss: 0.007762507775332779\n",
      "Training Loss: 0.007712356150150299\n",
      "Validation Loss: 0.004923812203084848\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006266643818235025\n",
      "Training Loss: 0.006068913365597836\n",
      "Training Loss: 0.005995693046133965\n",
      "Training Loss: 0.006908297794288956\n",
      "Training Loss: 0.007990875252289698\n",
      "Training Loss: 0.007751436590915546\n",
      "Training Loss: 0.007700195775832981\n",
      "Validation Loss: 0.004914437746803524\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006257645042496734\n",
      "Training Loss: 0.006061523821554147\n",
      "Training Loss: 0.005986555905546993\n",
      "Training Loss: 0.0068987747747451065\n",
      "Training Loss: 0.00797927079256624\n",
      "Training Loss: 0.00774038095260039\n",
      "Training Loss: 0.007688079464714974\n",
      "Validation Loss: 0.004905080963850496\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006248733976390213\n",
      "Training Loss: 0.0060542180843185635\n",
      "Training Loss: 0.005977547689108178\n",
      "Training Loss: 0.006889316950691864\n",
      "Training Loss: 0.007967732438119128\n",
      "Training Loss: 0.007729344242252409\n",
      "Training Loss: 0.007676007181871682\n",
      "Validation Loss: 0.004895737448320044\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006239902845700271\n",
      "Training Loss: 0.00604698975046631\n",
      "Training Loss: 0.005968661366496235\n",
      "Training Loss: 0.006879918885533698\n",
      "Training Loss: 0.007956265223911033\n",
      "Training Loss: 0.007718327934853732\n",
      "Training Loss: 0.007663982787635177\n",
      "Validation Loss: 0.0048864086255368385\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006231146415811964\n",
      "Training Loss: 0.0060398329392774034\n",
      "Training Loss: 0.005959891921374947\n",
      "Training Loss: 0.00687057719042059\n",
      "Training Loss: 0.007944870403734967\n",
      "Training Loss: 0.0077073344506789\n",
      "Training Loss: 0.007652006197022274\n",
      "Validation Loss: 0.0048770976007814874\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.00622246261627879\n",
      "Training Loss: 0.00603274482593406\n",
      "Training Loss: 0.005951236574910581\n",
      "Training Loss: 0.006861290836241096\n",
      "Training Loss: 0.007933554094051942\n",
      "Training Loss: 0.007696368484757841\n",
      "Training Loss: 0.0076400839863345026\n",
      "Validation Loss: 0.0048678014017571785\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006213847836479544\n",
      "Training Loss: 0.006025720129837282\n",
      "Training Loss: 0.005942690939409658\n",
      "Training Loss: 0.006852059434168041\n",
      "Training Loss: 0.007922322186641396\n",
      "Training Loss: 0.007685435391031206\n",
      "Training Loss: 0.007628218062454835\n",
      "Validation Loss: 0.004858523514819391\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.006205300315632485\n",
      "Training Loss: 0.0060187588399276135\n",
      "Training Loss: 0.005934251744765789\n",
      "Training Loss: 0.0068428826943272725\n",
      "Training Loss: 0.007911178675713018\n",
      "Training Loss: 0.007674540186999365\n",
      "Training Loss: 0.007616415764205158\n",
      "Validation Loss: 0.004849260167152471\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.00619681529875379\n",
      "Training Loss: 0.006011855425313115\n",
      "Training Loss: 0.005925915184197947\n",
      "Training Loss: 0.0068337612634059046\n",
      "Training Loss: 0.007900131401838735\n",
      "Training Loss: 0.007663689833134413\n",
      "Training Loss: 0.00760468089603819\n",
      "Validation Loss: 0.004840020542922482\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006188396057114005\n",
      "Training Loss: 0.0060050117917126045\n",
      "Training Loss: 0.005917682305444032\n",
      "Training Loss: 0.006824696719413623\n",
      "Training Loss: 0.007889183143852278\n",
      "Training Loss: 0.007652887700824067\n",
      "Training Loss: 0.007593020404456184\n",
      "Validation Loss: 0.004830806215262312\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006180042274063453\n",
      "Training Loss: 0.005998228688840754\n",
      "Training Loss: 0.005909553947858513\n",
      "Training Loss: 0.006815691429655999\n",
      "Training Loss: 0.007878342163749039\n",
      "Training Loss: 0.007642141128890216\n",
      "Training Loss: 0.007581439142813906\n",
      "Validation Loss: 0.004821617764068509\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006171752834925428\n",
      "Training Loss: 0.005991505905403756\n",
      "Training Loss: 0.00590152787626721\n",
      "Training Loss: 0.006806746865622699\n",
      "Training Loss: 0.007867613239213825\n",
      "Training Loss: 0.007631456231465563\n",
      "Training Loss: 0.007569943437119946\n",
      "Validation Loss: 0.004812463102094857\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006163530304911546\n",
      "Training Loss: 0.005984843917540275\n",
      "Training Loss: 0.005893604804296047\n",
      "Training Loss: 0.006797867539571598\n",
      "Training Loss: 0.007857003349345177\n",
      "Training Loss: 0.007620840597664938\n",
      "Training Loss: 0.007558541093021631\n",
      "Validation Loss: 0.004803339927631678\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006155374164227396\n",
      "Training Loss: 0.005978243437130004\n",
      "Training Loss: 0.005885783095145598\n",
      "Training Loss: 0.006789054165710695\n",
      "Training Loss: 0.00784651592024602\n",
      "Training Loss: 0.00761029933928512\n",
      "Training Loss: 0.007547236663522199\n",
      "Validation Loss: 0.004794255433140204\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006147286054911092\n",
      "Training Loss: 0.005971704158582725\n",
      "Training Loss: 0.00587806329363957\n",
      "Training Loss: 0.006780309572350234\n",
      "Training Loss: 0.007836156781995668\n",
      "Training Loss: 0.007599837125744671\n",
      "Training Loss: 0.007536036173114553\n",
      "Validation Loss: 0.004785207843177774\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006139264956582338\n",
      "Training Loss: 0.00596522718493361\n",
      "Training Loss: 0.005870444917818531\n",
      "Training Loss: 0.006771636757766828\n",
      "Training Loss: 0.007825931358383968\n",
      "Training Loss: 0.00758946102228947\n",
      "Training Loss: 0.007524945763871074\n",
      "Validation Loss: 0.004776204631433644\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006131314458325505\n",
      "Training Loss: 0.0059588140115374695\n",
      "Training Loss: 0.005862928978167474\n",
      "Training Loss: 0.006763039613724686\n",
      "Training Loss: 0.00781584263429977\n",
      "Training Loss: 0.007579176161671058\n",
      "Training Loss: 0.007513970676809549\n",
      "Validation Loss: 0.0047672465282286546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006123435538611375\n",
      "Training Loss: 0.005952464861911722\n",
      "Training Loss: 0.005855514910072088\n",
      "Training Loss: 0.006754520252579823\n",
      "Training Loss: 0.007805895196506754\n",
      "Training Loss: 0.007568987804697827\n",
      "Training Loss: 0.007503114929422736\n",
      "Validation Loss: 0.0047583386881018845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.006115629267296754\n",
      "Training Loss: 0.005946181297767908\n",
      "Training Loss: 0.005848203341010958\n",
      "Training Loss: 0.006746080101584085\n",
      "Training Loss: 0.007796090726042166\n",
      "Training Loss: 0.007558898375136778\n",
      "Training Loss: 0.007492383974604309\n",
      "Validation Loss: 0.004749481116721497\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006107896178145893\n",
      "Training Loss: 0.005939964171266183\n",
      "Training Loss: 0.005840994816971943\n",
      "Training Loss: 0.006737722848192788\n",
      "Training Loss: 0.0077864335873164235\n",
      "Training Loss: 0.007548913442296907\n",
      "Training Loss: 0.007481781790265814\n",
      "Validation Loss: 0.004740678423611022\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.0061002382182050495\n",
      "Training Loss: 0.00593381188577041\n",
      "Training Loss: 0.005833887862972915\n",
      "Training Loss: 0.006729449852718972\n",
      "Training Loss: 0.007776926229707897\n",
      "Training Loss: 0.007539037864189595\n",
      "Training Loss: 0.0074713128386065366\n",
      "Validation Loss: 0.00473193077707126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006092655580141581\n",
      "Training Loss: 0.0059277253015898165\n",
      "Training Loss: 0.005826880892273039\n",
      "Training Loss: 0.006721264168736525\n",
      "Training Loss: 0.007767568045528606\n",
      "Training Loss: 0.007529273399850354\n",
      "Training Loss: 0.00746097820228897\n",
      "Validation Loss: 0.004723244538087719\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006085149884456769\n",
      "Training Loss: 0.005921705416985787\n",
      "Training Loss: 0.005819975445047021\n",
      "Training Loss: 0.006713166981935501\n",
      "Training Loss: 0.007758362381719053\n",
      "Training Loss: 0.007519622989930213\n",
      "Training Loss: 0.007450782579835505\n",
      "Validation Loss: 0.004714615665261079\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006077721110195853\n",
      "Training Loss: 0.005915752016589977\n",
      "Training Loss: 0.005813171139452606\n",
      "Training Loss: 0.006705159601406194\n",
      "Training Loss: 0.007749308926286176\n",
      "Training Loss: 0.007510087812552229\n",
      "Training Loss: 0.007440728102810681\n",
      "Validation Loss: 0.004706048751546398\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006070368843502365\n",
      "Training Loss: 0.005909862558473833\n",
      "Training Loss: 0.005806464914930984\n",
      "Training Loss: 0.0066972436115611345\n",
      "Training Loss: 0.00774040813324973\n",
      "Training Loss: 0.007500672283349559\n",
      "Training Loss: 0.007430816175183281\n",
      "Validation Loss: 0.004697544593102393\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006063093857956119\n",
      "Training Loss: 0.005904038387234323\n",
      "Training Loss: 0.005799855928635225\n",
      "Training Loss: 0.006689419725444168\n",
      "Training Loss: 0.007731659337878227\n",
      "Training Loss: 0.0074913763860240574\n",
      "Training Loss: 0.0074210483534261585\n",
      "Validation Loss: 0.0046891050682706615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006055897818878293\n",
      "Training Loss: 0.005898278561071493\n",
      "Training Loss: 0.005793345593847334\n",
      "Training Loss: 0.00668168968288228\n",
      "Training Loss: 0.007723062054719776\n",
      "Training Loss: 0.007482201224192977\n",
      "Training Loss: 0.00741142489365302\n",
      "Validation Loss: 0.004680728524128038\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006048777110408992\n",
      "Training Loss: 0.0058925802377052605\n",
      "Training Loss: 0.005786927676526829\n",
      "Training Loss: 0.006674051440786571\n",
      "Training Loss: 0.007714615665609017\n",
      "Training Loss: 0.007473146903794259\n",
      "Training Loss: 0.007401945916935802\n",
      "Validation Loss: 0.004672416477997819\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006041731974110007\n",
      "Training Loss: 0.005886943205841817\n",
      "Training Loss: 0.005780603783205151\n",
      "Training Loss: 0.006666508353082463\n",
      "Training Loss: 0.007706316624535248\n",
      "Training Loss: 0.007464214491192251\n",
      "Training Loss: 0.007392612947151065\n",
      "Validation Loss: 0.004664168090556054\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.0060347619390813635\n",
      "Training Loss: 0.005881366063258611\n",
      "Training Loss: 0.005774370641447603\n",
      "Training Loss: 0.006659058569348417\n",
      "Training Loss: 0.0076981647731736305\n",
      "Training Loss: 0.007455403778003529\n",
      "Training Loss: 0.007383424425497651\n",
      "Validation Loss: 0.004655983923386769\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006027868093224242\n",
      "Training Loss: 0.0058758473303169014\n",
      "Training Loss: 0.005768228556262329\n",
      "Training Loss: 0.0066517030901741235\n",
      "Training Loss: 0.007690157104516402\n",
      "Training Loss: 0.007446715175174177\n",
      "Training Loss: 0.007374381077243015\n",
      "Validation Loss: 0.004647867029013064\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006021048597758636\n",
      "Training Loss: 0.005870386899332516\n",
      "Training Loss: 0.005762173945549875\n",
      "Training Loss: 0.006644441862590611\n",
      "Training Loss: 0.007682292503304779\n",
      "Training Loss: 0.007438148789806292\n",
      "Training Loss: 0.007365481610177085\n",
      "Validation Loss: 0.00463981396598874\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006014302020776086\n",
      "Training Loss: 0.005864980947226286\n",
      "Training Loss: 0.005756205932702869\n",
      "Training Loss: 0.006637275085668079\n",
      "Training Loss: 0.00767456796951592\n",
      "Training Loss: 0.007429703706875444\n",
      "Training Loss: 0.007356724116252735\n",
      "Validation Loss: 0.0046318236608511665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006007627568324097\n",
      "Training Loss: 0.005859628689358942\n",
      "Training Loss: 0.005750320928636938\n",
      "Training Loss: 0.006630200234358199\n",
      "Training Loss: 0.0076669808849692345\n",
      "Training Loss: 0.007421378138242289\n",
      "Training Loss: 0.0073481079924386\n",
      "Validation Loss: 0.0046238988609147375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006001023428398184\n",
      "Training Loss: 0.0058543291274691\n",
      "Training Loss: 0.0057445188635028895\n",
      "Training Loss: 0.006623218387248926\n",
      "Training Loss: 0.007659526054048911\n",
      "Training Loss: 0.007413172204978764\n",
      "Training Loss: 0.007339631153736263\n",
      "Validation Loss: 0.004616035701365172\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.005994488944998011\n",
      "Training Loss: 0.005849078510073014\n",
      "Training Loss: 0.005738794181961566\n",
      "Training Loss: 0.006616328172385693\n",
      "Training Loss: 0.0076522035943344235\n",
      "Training Loss: 0.007405084924539551\n",
      "Training Loss: 0.007331291509326547\n",
      "Validation Loss: 0.0046082343439630856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.005988021447556093\n",
      "Training Loss: 0.005843875805148855\n",
      "Training Loss: 0.005733147646533325\n",
      "Training Loss: 0.006609527849941514\n",
      "Training Loss: 0.007645007598912343\n",
      "Training Loss: 0.0073971145553514365\n",
      "Training Loss: 0.007323087070835755\n",
      "Validation Loss: 0.004600495201176249\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.005981621043756605\n",
      "Training Loss: 0.005838718506274745\n",
      "Training Loss: 0.005727575469063595\n",
      "Training Loss: 0.006602818765095435\n",
      "Training Loss: 0.007637937642866746\n",
      "Training Loss: 0.0073892607633024455\n",
      "Training Loss: 0.007315015207277611\n",
      "Validation Loss: 0.0045928196547280874\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.005975287681212649\n",
      "Training Loss: 0.005833608112880029\n",
      "Training Loss: 0.005722078803228214\n",
      "Training Loss: 0.006596198694314808\n",
      "Training Loss: 0.0076309888646937905\n",
      "Training Loss: 0.007381522044306621\n",
      "Training Loss: 0.007307074145646766\n",
      "Validation Loss: 0.004585205097700214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.005969017652678304\n",
      "Training Loss: 0.005828539201756939\n",
      "Training Loss: 0.005716652816627175\n",
      "Training Loss: 0.006589666623622179\n",
      "Training Loss: 0.007624157577520236\n",
      "Training Loss: 0.007373897809302434\n",
      "Training Loss: 0.007299261452862993\n",
      "Validation Loss: 0.004577649559752409\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.005962810885393992\n",
      "Training Loss: 0.005823511595954187\n",
      "Training Loss: 0.005711295048240572\n",
      "Training Loss: 0.006583221409819089\n",
      "Training Loss: 0.007617441458860412\n",
      "Training Loss: 0.007366385031491518\n",
      "Training Loss: 0.007291574587579817\n",
      "Validation Loss: 0.004570157377900423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0059566664521116764\n",
      "Training Loss: 0.005818525080103427\n",
      "Training Loss: 0.005706007166299969\n",
      "Training Loss: 0.00657686305406969\n",
      "Training Loss: 0.007610836956882849\n",
      "Training Loss: 0.0073589841614011674\n",
      "Training Loss: 0.007284010691801086\n",
      "Validation Loss: 0.0045627243363399635\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.005950582212535665\n",
      "Training Loss: 0.005813575186184607\n",
      "Training Loss: 0.005700784006621689\n",
      "Training Loss: 0.00657058916578535\n",
      "Training Loss: 0.007604342016857117\n",
      "Training Loss: 0.007351694609969855\n",
      "Training Loss: 0.007276567457010969\n",
      "Validation Loss: 0.004555352420235674\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.005944558916380629\n",
      "Training Loss: 0.005808663872303441\n",
      "Training Loss: 0.005695627185050398\n",
      "Training Loss: 0.006564400198403746\n",
      "Training Loss: 0.007597951492061839\n",
      "Training Loss: 0.007344513164134696\n",
      "Training Loss: 0.0072692420776002105\n",
      "Validation Loss: 0.004548040047298405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.005938592843012885\n",
      "Training Loss: 0.0058037865348160265\n",
      "Training Loss: 0.005690531260333955\n",
      "Training Loss: 0.0065582925966009495\n",
      "Training Loss: 0.007591663907514885\n",
      "Training Loss: 0.007337439919356256\n",
      "Training Loss: 0.007262031539576128\n",
      "Validation Loss: 0.004540786241296302\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0059326840168796476\n",
      "Training Loss: 0.00579894277441781\n",
      "Training Loss: 0.005685496341902763\n",
      "Training Loss: 0.006552267163642682\n",
      "Training Loss: 0.007585476161912083\n",
      "Training Loss: 0.007330472898902371\n",
      "Training Loss: 0.0072549327998422085\n",
      "Validation Loss: 0.00453359233206731\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.005926831606775523\n",
      "Training Loss: 0.005794131741276942\n",
      "Training Loss: 0.005680520520545542\n",
      "Training Loss: 0.006546321468777023\n",
      "Training Loss: 0.007579383993288502\n",
      "Training Loss: 0.007323612092295662\n",
      "Training Loss: 0.007247943329857663\n",
      "Validation Loss: 0.004526454225890617\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.005921032832702622\n",
      "Training Loss: 0.0057893516222247855\n",
      "Training Loss: 0.005675603320123628\n",
      "Training Loss: 0.006540454798960127\n",
      "Training Loss: 0.007573385662399232\n",
      "Training Loss: 0.00731685422710143\n",
      "Training Loss: 0.007241060577798635\n",
      "Validation Loss: 0.004519378257590984\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.00591528928198386\n",
      "Training Loss: 0.005784602830535732\n",
      "Training Loss: 0.005670742802321911\n",
      "Training Loss: 0.006534665686776861\n",
      "Training Loss: 0.007567478002747521\n",
      "Training Loss: 0.007310198982013389\n",
      "Training Loss: 0.007234281109413132\n",
      "Validation Loss: 0.004512359467962903\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.005909599456936121\n",
      "Training Loss: 0.0057798825157806275\n",
      "Training Loss: 0.005665937418816611\n",
      "Training Loss: 0.0065289526421111075\n",
      "Training Loss: 0.007561657521873713\n",
      "Training Loss: 0.007303643947234377\n",
      "Training Loss: 0.007227602781495079\n",
      "Validation Loss: 0.00450539783476974\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.005903959849965759\n",
      "Training Loss: 0.005775189165142365\n",
      "Training Loss: 0.00566118465969339\n",
      "Training Loss: 0.006523313676589169\n",
      "Training Loss: 0.007555921447928995\n",
      "Training Loss: 0.0072971891169436275\n",
      "Training Loss: 0.00722102074418217\n",
      "Validation Loss: 0.004498494363456183\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.0058983715809881685\n",
      "Training Loss: 0.005770523903192952\n",
      "Training Loss: 0.005656485635554418\n",
      "Training Loss: 0.006517747901380062\n",
      "Training Loss: 0.007550267596961931\n",
      "Training Loss: 0.007290832278085873\n",
      "Training Loss: 0.007214534310624004\n",
      "Validation Loss: 0.004491649120732137\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.005892833573161624\n",
      "Training Loss: 0.0057658838381757956\n",
      "Training Loss: 0.005651837558252737\n",
      "Training Loss: 0.0065122535673435775\n",
      "Training Loss: 0.007544691835064441\n",
      "Training Loss: 0.007284571290947497\n",
      "Training Loss: 0.007208139608846978\n",
      "Validation Loss: 0.004484860533900726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.0058873443963238965\n",
      "Training Loss: 0.005761269131908193\n",
      "Training Loss: 0.005647239915560931\n",
      "Training Loss: 0.0065068297216203065\n",
      "Training Loss: 0.00753919365699403\n",
      "Training Loss: 0.007278405349934474\n",
      "Training Loss: 0.007201834272127599\n",
      "Validation Loss: 0.004478128820968171\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.005881902479450219\n",
      "Training Loss: 0.005756678425823339\n",
      "Training Loss: 0.005642690134700388\n",
      "Training Loss: 0.00650147351087071\n",
      "Training Loss: 0.0075337693304754795\n",
      "Training Loss: 0.007272333818254992\n",
      "Training Loss: 0.007195615804521367\n",
      "Validation Loss: 0.0044714517412665356\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.005876506974454969\n",
      "Training Loss: 0.005752110026078299\n",
      "Training Loss: 0.005638186101568863\n",
      "Training Loss: 0.0064961850276449695\n",
      "Training Loss: 0.007528416190762072\n",
      "Training Loss: 0.00726635420229286\n",
      "Training Loss: 0.0071894800185691565\n",
      "Validation Loss: 0.004464833657536307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.005871157530928031\n",
      "Training Loss: 0.0057475653546862305\n",
      "Training Loss: 0.005633730298141018\n",
      "Training Loss: 0.006490961255622096\n",
      "Training Loss: 0.007523131597554311\n",
      "Training Loss: 0.0072604640817735345\n",
      "Training Loss: 0.007183425115654245\n",
      "Validation Loss: 0.00445827117542063\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.005865854602307081\n",
      "Training Loss: 0.005743042713729665\n",
      "Training Loss: 0.005629321018932387\n",
      "Training Loss: 0.006485801554517821\n",
      "Training Loss: 0.007517912397161126\n",
      "Training Loss: 0.007254661975894123\n",
      "Training Loss: 0.007177447944413871\n",
      "Validation Loss: 0.004451765069277601\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.005860596132115461\n",
      "Training Loss: 0.005738541584578343\n",
      "Training Loss: 0.005624954655067995\n",
      "Training Loss: 0.006480703867273405\n",
      "Training Loss: 0.0075127573893405495\n",
      "Training Loss: 0.007248947614571079\n",
      "Training Loss: 0.00717154631507583\n",
      "Validation Loss: 0.004445312169296772\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.005855379001004622\n",
      "Training Loss: 0.005734059463720769\n",
      "Training Loss: 0.005620629119221121\n",
      "Training Loss: 0.006475665115285665\n",
      "Training Loss: 0.007507664996664971\n",
      "Training Loss: 0.0072433189046569165\n",
      "Training Loss: 0.007165718245087192\n",
      "Validation Loss: 0.0044389122823246295\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.005850204299786128\n",
      "Training Loss: 0.0057295977917965505\n",
      "Training Loss: 0.005616346547612921\n",
      "Training Loss: 0.006470685716485605\n",
      "Training Loss: 0.007502629862865433\n",
      "Training Loss: 0.0072377721569500866\n",
      "Training Loss: 0.007159959679702297\n",
      "Validation Loss: 0.004432569472433141\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.005845071544172242\n",
      "Training Loss: 0.005725155517575331\n",
      "Training Loss: 0.005612104308092967\n",
      "Training Loss: 0.006465762761654332\n",
      "Training Loss: 0.007497652287129313\n",
      "Training Loss: 0.007232308067614213\n",
      "Training Loss: 0.007154270391911268\n",
      "Validation Loss: 0.004426276696713341\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.005839977659634315\n",
      "Training Loss: 0.00572073096933309\n",
      "Training Loss: 0.005607901798794046\n",
      "Training Loss: 0.006460894600604661\n",
      "Training Loss: 0.007492728962097317\n",
      "Training Loss: 0.007226923648267984\n",
      "Training Loss: 0.00714864534093067\n",
      "Validation Loss: 0.0044200392509723425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0058349235251080244\n",
      "Training Loss: 0.005716325363609939\n",
      "Training Loss: 0.00560373815940693\n",
      "Training Loss: 0.00645607927639503\n",
      "Training Loss: 0.007487858046079055\n",
      "Training Loss: 0.007221616629976779\n",
      "Training Loss: 0.007143083339324221\n",
      "Validation Loss: 0.004413852835691461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.005829908151063137\n",
      "Training Loss: 0.005711936706793494\n",
      "Training Loss: 0.00559961138991639\n",
      "Training Loss: 0.00645131579018198\n",
      "Training Loss: 0.007483037034980953\n",
      "Training Loss: 0.007216386110521853\n",
      "Training Loss: 0.007137582631548867\n",
      "Validation Loss: 0.004407718270138073\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.005824929295340553\n",
      "Training Loss: 0.005707565458142199\n",
      "Training Loss: 0.005595520972274243\n",
      "Training Loss: 0.006446601675124839\n",
      "Training Loss: 0.007478264347882941\n",
      "Training Loss: 0.007211229100357741\n",
      "Training Loss: 0.007132140061585233\n",
      "Validation Loss: 0.004401633942374185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.005819988118601032\n",
      "Training Loss: 0.005703211277723313\n",
      "Training Loss: 0.005591466360492632\n",
      "Training Loss: 0.0064419362094486135\n",
      "Training Loss: 0.007473537542391568\n",
      "Training Loss: 0.007206145433010533\n",
      "Training Loss: 0.007126754062483087\n",
      "Validation Loss: 0.004395601308245337\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.005815082204644568\n",
      "Training Loss: 0.0056988723942777144\n",
      "Training Loss: 0.005587445575511083\n",
      "Training Loss: 0.006437317518284544\n",
      "Training Loss: 0.007468854902544991\n",
      "Training Loss: 0.007201132017653436\n",
      "Training Loss: 0.00712142204050906\n",
      "Validation Loss: 0.004389619405036059\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.005810212126816623\n",
      "Training Loss: 0.005694550370099023\n",
      "Training Loss: 0.005583459311164916\n",
      "Training Loss: 0.006432742946781218\n",
      "Training Loss: 0.007464214649517089\n",
      "Training Loss: 0.007196186979999766\n",
      "Training Loss: 0.007116142747690901\n",
      "Validation Loss: 0.004383686718039131\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.0058053760684560985\n",
      "Training Loss: 0.005690243330900557\n",
      "Training Loss: 0.005579505664063618\n",
      "Training Loss: 0.006428212210885249\n",
      "Training Loss: 0.007459615358384326\n",
      "Training Loss: 0.007191309178015217\n",
      "Training Loss: 0.0071109133947175\n",
      "Validation Loss: 0.004377802731015821\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.005800573668675497\n",
      "Training Loss: 0.005685953017091378\n",
      "Training Loss: 0.0055755844572558996\n",
      "Training Loss: 0.006423723195912317\n",
      "Training Loss: 0.007455054812598973\n",
      "Training Loss: 0.007186496522044763\n",
      "Training Loss: 0.007105732805794105\n",
      "Validation Loss: 0.004371967766179323\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.0057958032796159385\n",
      "Training Loss: 0.005681677315733396\n",
      "Training Loss: 0.005571693979436532\n",
      "Training Loss: 0.006419273731880821\n",
      "Training Loss: 0.007450530843343585\n",
      "Training Loss: 0.00718174776644446\n",
      "Training Loss: 0.007100599521072582\n",
      "Validation Loss: 0.004366181087104625\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.0057910661329515275\n",
      "Training Loss: 0.005677415998070501\n",
      "Training Loss: 0.005567833638051525\n",
      "Training Loss: 0.006414863993995823\n",
      "Training Loss: 0.007446044619427994\n",
      "Training Loss: 0.0071770611824467775\n",
      "Training Loss: 0.007095513211097569\n",
      "Validation Loss: 0.004360439629060276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.005786359091871418\n",
      "Training Loss: 0.0056731696199858565\n",
      "Training Loss: 0.005564004306215793\n",
      "Training Loss: 0.0064104916923679416\n",
      "Training Loss: 0.007441591287497431\n",
      "Training Loss: 0.0071724342438392345\n",
      "Training Loss: 0.007090469298418611\n",
      "Validation Loss: 0.0043547476204949314\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.0057816839264705775\n",
      "Training Loss: 0.005668938429444097\n",
      "Training Loss: 0.005560203524073586\n",
      "Training Loss: 0.0064061544259311634\n",
      "Training Loss: 0.00743717092089355\n",
      "Training Loss: 0.007167864935472608\n",
      "Training Loss: 0.007085468267323449\n",
      "Validation Loss: 0.004349100067758103\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.0057770379597786815\n",
      "Training Loss: 0.005664720119675622\n",
      "Training Loss: 0.005556429249700159\n",
      "Training Loss: 0.006401852339040488\n",
      "Training Loss: 0.007432782833930105\n",
      "Training Loss: 0.00716335394885391\n",
      "Training Loss: 0.0070805086567997935\n",
      "Validation Loss: 0.004343500719684759\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.00577242220111657\n",
      "Training Loss: 0.0056605171848786995\n",
      "Training Loss: 0.005552684330614284\n",
      "Training Loss: 0.0063975831196876245\n",
      "Training Loss: 0.007428424925310537\n",
      "Training Loss: 0.007158897453919053\n",
      "Training Loss: 0.007075588376028463\n",
      "Validation Loss: 0.004337945858832825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.005767835788428783\n",
      "Training Loss: 0.0056563282565912235\n",
      "Training Loss: 0.0055489664955530316\n",
      "Training Loss: 0.006393346087425016\n",
      "Training Loss: 0.007424095632741228\n",
      "Training Loss: 0.007154493478592486\n",
      "Training Loss: 0.0070707072818186134\n",
      "Validation Loss: 0.0043324370718515765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005763277559308336\n",
      "Training Loss: 0.005652153462870046\n",
      "Training Loss: 0.005545275233453139\n",
      "Training Loss: 0.006389140512910671\n",
      "Training Loss: 0.00741979458136484\n",
      "Training Loss: 0.007150142686441541\n",
      "Training Loss: 0.0070658627932425585\n",
      "Validation Loss: 0.004326973028467054\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.0057587476714979855\n",
      "Training Loss: 0.005647992827580311\n",
      "Training Loss: 0.005541609833016992\n",
      "Training Loss: 0.0063849647232564165\n",
      "Training Loss: 0.007415521783987061\n",
      "Training Loss: 0.00714584264322184\n",
      "Training Loss: 0.007061055590165779\n",
      "Validation Loss: 0.004321552180399079\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005754244873533026\n",
      "Training Loss: 0.005643845394952223\n",
      "Training Loss: 0.005537969048600644\n",
      "Training Loss: 0.006380817300523632\n",
      "Training Loss: 0.007411276025231927\n",
      "Training Loss: 0.007141592007828876\n",
      "Training Loss: 0.00705628301948309\n",
      "Validation Loss: 0.00431617453113575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.005749768571695313\n",
      "Training Loss: 0.005639712564880028\n",
      "Training Loss: 0.005534353267867118\n",
      "Training Loss: 0.006376697623054497\n",
      "Training Loss: 0.007407054507639259\n",
      "Training Loss: 0.007137389130657539\n",
      "Training Loss: 0.007051546156872064\n",
      "Validation Loss: 0.004310840542414243\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.005745318756089546\n",
      "Training Loss: 0.005635592708713375\n",
      "Training Loss: 0.005530761240515858\n",
      "Training Loss: 0.006372603758936748\n",
      "Training Loss: 0.007402858248678967\n",
      "Training Loss: 0.00713323283358477\n",
      "Training Loss: 0.007046842811396345\n",
      "Validation Loss: 0.004305550323237668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.0057408962398767475\n",
      "Training Loss: 0.00563148754707072\n",
      "Training Loss: 0.005527194768656045\n",
      "Training Loss: 0.006368537085945718\n",
      "Training Loss: 0.0073986854776740075\n",
      "Training Loss: 0.007129121870966628\n",
      "Training Loss: 0.007042171530192718\n",
      "Validation Loss: 0.004300303242260011\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005736499569611624\n",
      "Training Loss: 0.005627396474010311\n",
      "Training Loss: 0.005523651113035158\n",
      "Training Loss: 0.006364494673907757\n",
      "Training Loss: 0.0073945367266424\n",
      "Training Loss: 0.0071250540867913515\n",
      "Training Loss: 0.007037533597322181\n",
      "Validation Loss: 0.0042950977259123705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005732127630035393\n",
      "Training Loss: 0.0056233182514552026\n",
      "Training Loss: 0.005520131195662543\n",
      "Training Loss: 0.006360476339468732\n",
      "Training Loss: 0.0073904103063978255\n",
      "Training Loss: 0.007121028862893581\n",
      "Training Loss: 0.007032926125684753\n",
      "Validation Loss: 0.004289938101618226\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0057277827872894704\n",
      "Training Loss: 0.005619255514466204\n",
      "Training Loss: 0.0055166340991854666\n",
      "Training Loss: 0.006356481367256492\n",
      "Training Loss: 0.0073863062995951625\n",
      "Training Loss: 0.0071170465182513\n",
      "Training Loss: 0.0070283497357741\n",
      "Validation Loss: 0.004284819189985789\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005723462490714155\n",
      "Training Loss: 0.005615205382346175\n",
      "Training Loss: 0.005513158099493012\n",
      "Training Loss: 0.006352509571006522\n",
      "Training Loss: 0.007382223963504657\n",
      "Training Loss: 0.007113103449810297\n",
      "Training Loss: 0.007023804881609977\n",
      "Validation Loss: 0.004279742245422851\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005719167603529058\n",
      "Training Loss: 0.005611170587944798\n",
      "Training Loss: 0.005509706635493785\n",
      "Training Loss: 0.006348559097968973\n",
      "Training Loss: 0.0073781631607562305\n",
      "Training Loss: 0.007109200287377462\n",
      "Training Loss: 0.007019288265146315\n",
      "Validation Loss: 0.004274708329619707\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.005714897874277085\n",
      "Training Loss: 0.005607149731367826\n",
      "Training Loss: 0.005506276654778049\n",
      "Training Loss: 0.006344629432423972\n",
      "Training Loss: 0.0073741219891235235\n",
      "Training Loss: 0.00710533423232846\n",
      "Training Loss: 0.007014801069162786\n",
      "Validation Loss: 0.004269714356480299\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005710651720291935\n",
      "Training Loss: 0.005603142676409334\n",
      "Training Loss: 0.0055028666858561335\n",
      "Training Loss: 0.0063407209550496195\n",
      "Training Loss: 0.007370102655841038\n",
      "Training Loss: 0.0071015059843193735\n",
      "Training Loss: 0.007010343343717977\n",
      "Validation Loss: 0.004264763165317643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.00570643090759404\n",
      "Training Loss: 0.005599150072666817\n",
      "Training Loss: 0.005499480036087334\n",
      "Training Loss: 0.006336831612861716\n",
      "Training Loss: 0.007366102458909154\n",
      "Training Loss: 0.00709771387046203\n",
      "Training Loss: 0.007005912670865655\n",
      "Validation Loss: 0.004259854122605663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005702233756310306\n",
      "Training Loss: 0.005595172002795152\n",
      "Training Loss: 0.005496113116387278\n",
      "Training Loss: 0.006332961319130846\n",
      "Training Loss: 0.007362122536869719\n",
      "Training Loss: 0.0070939555112272505\n",
      "Training Loss: 0.0070015104475896804\n",
      "Validation Loss: 0.0042549852457957876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.00569806101731956\n",
      "Training Loss: 0.005591206814278849\n",
      "Training Loss: 0.005492767783580348\n",
      "Training Loss: 0.006329109988291748\n",
      "Training Loss: 0.007358161023585125\n",
      "Training Loss: 0.007090232351329178\n",
      "Training Loss: 0.006997135359561071\n",
      "Validation Loss: 0.0042501582619332975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005693912175484001\n",
      "Training Loss: 0.005587257735896856\n",
      "Training Loss: 0.005489442200632766\n",
      "Training Loss: 0.006325276770512573\n",
      "Training Loss: 0.007354220323031768\n",
      "Training Loss: 0.0070865426701493565\n",
      "Training Loss: 0.0069927872705738995\n",
      "Validation Loss: 0.00424537034696561\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005689786729053594\n",
      "Training Loss: 0.0055833218741463495\n",
      "Training Loss: 0.00548613672493957\n",
      "Training Loss: 0.00632146141724661\n",
      "Training Loss: 0.007350297275697812\n",
      "Training Loss: 0.007082884007832036\n",
      "Training Loss: 0.0069884650176391\n",
      "Validation Loss: 0.004240625278504517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.00568568505696021\n",
      "Training Loss: 0.005579400300048292\n",
      "Training Loss: 0.005482852440327406\n",
      "Training Loss: 0.006317663579247892\n",
      "Training Loss: 0.00734639311558567\n",
      "Training Loss: 0.0070792565983720124\n",
      "Training Loss: 0.0069841688021551815\n",
      "Validation Loss: 0.004235922785786291\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005681607697624713\n",
      "Training Loss: 0.005575493323849514\n",
      "Training Loss: 0.005479587541194633\n",
      "Training Loss: 0.006313881531823427\n",
      "Training Loss: 0.0073425074829719965\n",
      "Training Loss: 0.007075659577967599\n",
      "Training Loss: 0.0069798994995653625\n",
      "Validation Loss: 0.004231256023851618\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005677553032292053\n",
      "Training Loss: 0.005571599222603254\n",
      "Training Loss: 0.005476342394249514\n",
      "Training Loss: 0.0063101156306220214\n",
      "Training Loss: 0.007338638975052163\n",
      "Training Loss: 0.0070720918208826335\n",
      "Training Loss: 0.006975653400877491\n",
      "Validation Loss: 0.004226635395085321\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005673522437573411\n",
      "Training Loss: 0.005567720374674536\n",
      "Training Loss: 0.005473116320790723\n",
      "Training Loss: 0.00630636663059704\n",
      "Training Loss: 0.007334788977168501\n",
      "Training Loss: 0.007068552526179701\n",
      "Training Loss: 0.006971432127756998\n",
      "Validation Loss: 0.004222053457191039\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00566951600892935\n",
      "Training Loss: 0.005563856404623948\n",
      "Training Loss: 0.005469911074033007\n",
      "Training Loss: 0.006302632494480349\n",
      "Training Loss: 0.007330954528879374\n",
      "Training Loss: 0.007065040224697441\n",
      "Training Loss: 0.006967234757030383\n",
      "Validation Loss: 0.004217512032264767\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005665531479171477\n",
      "Training Loss: 0.0055600053927628325\n",
      "Training Loss: 0.005466723354766145\n",
      "Training Loss: 0.0062989124172599985\n",
      "Training Loss: 0.007327138939872384\n",
      "Training Loss: 0.00706155554857105\n",
      "Training Loss: 0.006963061817223206\n",
      "Validation Loss: 0.004213008017205958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.0056615690788021315\n",
      "Training Loss: 0.005556166536989622\n",
      "Training Loss: 0.005463553303852678\n",
      "Training Loss: 0.006295207663206384\n",
      "Training Loss: 0.007323340841103345\n",
      "Training Loss: 0.007058096583932638\n",
      "Training Loss: 0.006958911989349872\n",
      "Validation Loss: 0.00420854567839635\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005657629552297294\n",
      "Training Loss: 0.005552342477603816\n",
      "Training Loss: 0.0054604011529590935\n",
      "Training Loss: 0.006291516580386087\n",
      "Training Loss: 0.0073195587808731945\n",
      "Training Loss: 0.007054662507725879\n",
      "Training Loss: 0.006954783947439864\n",
      "Validation Loss: 0.004204123018026938\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005653712874045595\n",
      "Training Loss: 0.005548531450331211\n",
      "Training Loss: 0.005457268449245021\n",
      "Training Loss: 0.006287838929565624\n",
      "Training Loss: 0.0073157920816447584\n",
      "Training Loss: 0.007051251876400783\n",
      "Training Loss: 0.0069506781280506405\n",
      "Validation Loss: 0.004199740156437155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005649817509111017\n",
      "Training Loss: 0.00554473212978337\n",
      "Training Loss: 0.005454152129823342\n",
      "Training Loss: 0.006284173415624536\n",
      "Training Loss: 0.007312041583936661\n",
      "Training Loss: 0.007047865819185972\n",
      "Training Loss: 0.006946594645269215\n",
      "Validation Loss: 0.0041953930670521064\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005645943480194546\n",
      "Training Loss: 0.0055409456533379855\n",
      "Training Loss: 0.0054510528512764726\n",
      "Training Loss: 0.006280522146844305\n",
      "Training Loss: 0.007308306416962296\n",
      "Training Loss: 0.007044501220807433\n",
      "Training Loss: 0.006942529827356338\n",
      "Validation Loss: 0.004191089275882541\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005642092190682888\n",
      "Training Loss: 0.005537171210162342\n",
      "Training Loss: 0.005447969792876393\n",
      "Training Loss: 0.006276882275706157\n",
      "Training Loss: 0.007304586779791861\n",
      "Training Loss: 0.0070411594316828995\n",
      "Training Loss: 0.006938486796570942\n",
      "Validation Loss: 0.0041868191705981776\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005638260947307572\n",
      "Training Loss: 0.005533408341580071\n",
      "Training Loss: 0.005444903080351651\n",
      "Training Loss: 0.006273253203835339\n",
      "Training Loss: 0.007300881166011095\n",
      "Training Loss: 0.0070378358673769984\n",
      "Training Loss: 0.006934461846249178\n",
      "Validation Loss: 0.0041825907140942324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005634451708756387\n",
      "Training Loss: 0.00552965781185776\n",
      "Training Loss: 0.00544185285572894\n",
      "Training Loss: 0.006269635249045677\n",
      "Training Loss: 0.007297188413795084\n",
      "Training Loss: 0.007034533108817413\n",
      "Training Loss: 0.006930454991525039\n",
      "Validation Loss: 0.0041784003114481265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.00563066296512261\n",
      "Training Loss: 0.005525917683844455\n",
      "Training Loss: 0.005438817486865446\n",
      "Training Loss: 0.0062660283141303804\n",
      "Training Loss: 0.007293509254232049\n",
      "Training Loss: 0.007031248327111825\n",
      "Training Loss: 0.006926466509466991\n",
      "Validation Loss: 0.004174244674254027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005626892750733532\n",
      "Training Loss: 0.005522187046590261\n",
      "Training Loss: 0.005435796116944403\n",
      "Training Loss: 0.00626243139617145\n",
      "Training Loss: 0.007289843043545261\n",
      "Training Loss: 0.007027981971623376\n",
      "Training Loss: 0.006922494291793555\n",
      "Validation Loss: 0.004170125910196077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005623142583644949\n",
      "Training Loss: 0.005518466416979209\n",
      "Training Loss: 0.0054327886074315756\n",
      "Training Loss: 0.006258843988762237\n",
      "Training Loss: 0.0072861906338948755\n",
      "Training Loss: 0.0070247336907777935\n",
      "Training Loss: 0.006918539309408516\n",
      "Validation Loss: 0.0041660406890112476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005619408907950856\n",
      "Training Loss: 0.005514752733288333\n",
      "Training Loss: 0.0054297930910252035\n",
      "Training Loss: 0.006255264668725431\n",
      "Training Loss: 0.007282549206865952\n",
      "Training Loss: 0.007021500037517399\n",
      "Training Loss: 0.006914599045412615\n",
      "Validation Loss: 0.004161991884021528\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005615695207961835\n",
      "Training Loss: 0.0055110479582799595\n",
      "Training Loss: 0.005426810382632538\n",
      "Training Loss: 0.00625169412465766\n",
      "Training Loss: 0.0072789173654746265\n",
      "Training Loss: 0.007018281657947227\n",
      "Training Loss: 0.006910671651130542\n",
      "Validation Loss: 0.004157978076006422\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005611998348613269\n",
      "Training Loss: 0.005507350384723395\n",
      "Training Loss: 0.005423840756993741\n",
      "Training Loss: 0.006248130919411779\n",
      "Training Loss: 0.007275294916471466\n",
      "Training Loss: 0.007015077010728419\n",
      "Training Loss: 0.006906757018296048\n",
      "Validation Loss: 0.004153998126101432\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005608319424791262\n",
      "Training Loss: 0.005503660219837911\n",
      "Training Loss: 0.005420882375910878\n",
      "Training Loss: 0.006244573238654993\n",
      "Training Loss: 0.007271681734127924\n",
      "Training Loss: 0.0070118840667419136\n",
      "Training Loss: 0.006902853599749506\n",
      "Validation Loss: 0.004150051477954685\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005604655200149864\n",
      "Training Loss: 0.005499975137063302\n",
      "Training Loss: 0.005417933050775901\n",
      "Training Loss: 0.006241023408947512\n",
      "Training Loss: 0.0072680769755970685\n",
      "Training Loss: 0.007008704007603228\n",
      "Training Loss: 0.006898959953105077\n",
      "Validation Loss: 0.0041461374936799765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005601008321973495\n",
      "Training Loss: 0.005496294106123969\n",
      "Training Loss: 0.005414994566235691\n",
      "Training Loss: 0.006237478763796389\n",
      "Training Loss: 0.00726447795634158\n",
      "Training Loss: 0.007005533709889278\n",
      "Training Loss: 0.006895075683714822\n",
      "Validation Loss: 0.004142257765594745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005597375244251452\n",
      "Training Loss: 0.005492618001298979\n",
      "Training Loss: 0.005412064614938572\n",
      "Training Loss: 0.0062339386623352765\n",
      "Training Loss: 0.007260885908035562\n",
      "Training Loss: 0.007002373833674937\n",
      "Training Loss: 0.0068911996181122955\n",
      "Validation Loss: 0.00413840391789999\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005593754620640538\n",
      "Training Loss: 0.00548894293315243\n",
      "Training Loss: 0.00540914018638432\n",
      "Training Loss: 0.006230401792563498\n",
      "Training Loss: 0.007257298725889995\n",
      "Training Loss: 0.006999221746809781\n",
      "Training Loss: 0.006887329660821706\n",
      "Validation Loss: 0.00413458295077546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005590148393530399\n",
      "Training Loss: 0.005485270303324796\n",
      "Training Loss: 0.005406224383041263\n",
      "Training Loss: 0.006226868610247039\n",
      "Training Loss: 0.007253715171245858\n",
      "Training Loss: 0.006996077306102961\n",
      "Training Loss: 0.00688346449402161\n",
      "Validation Loss: 0.004130789355203956\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005586552395834587\n",
      "Training Loss: 0.005481596926110797\n",
      "Training Loss: 0.00540331317926757\n",
      "Training Loss: 0.006223337458213791\n",
      "Training Loss: 0.007250132750486955\n",
      "Training Loss: 0.0069929376884829255\n",
      "Training Loss: 0.006879601429682225\n",
      "Validation Loss: 0.004127025392346001\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005582967929076404\n",
      "Training Loss: 0.0054779231973225255\n",
      "Training Loss: 0.005400406755506992\n",
      "Training Loss: 0.00621980817290023\n",
      "Training Loss: 0.007246552685974166\n",
      "Training Loss: 0.006989803791511804\n",
      "Training Loss: 0.006875740919495002\n",
      "Validation Loss: 0.00412328616621789\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.0055793921969598155\n",
      "Training Loss: 0.005474246779340319\n",
      "Training Loss: 0.005397502604173496\n",
      "Training Loss: 0.0062162787030683835\n",
      "Training Loss: 0.007242971191881224\n",
      "Training Loss: 0.006986674077343196\n",
      "Training Loss: 0.0068718794430606064\n",
      "Validation Loss: 0.004119571884797558\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005575824133702553\n",
      "Training Loss: 0.005470566739095375\n",
      "Training Loss: 0.005394600260769948\n",
      "Training Loss: 0.006212747661047615\n",
      "Training Loss: 0.007239387098234146\n",
      "Training Loss: 0.0069835452223196625\n",
      "Training Loss: 0.006868015732616186\n",
      "Validation Loss: 0.00411588285242169\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005572263301583007\n",
      "Training Loss: 0.005466881419415585\n",
      "Training Loss: 0.005391698947641998\n",
      "Training Loss: 0.006209215999115258\n",
      "Training Loss: 0.0072358007554430516\n",
      "Training Loss: 0.006980416540754959\n",
      "Training Loss: 0.0068641486810520295\n",
      "Validation Loss: 0.004112215529105274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.0055687064822996036\n",
      "Training Loss: 0.0054631879995577035\n",
      "Training Loss: 0.005388796612387523\n",
      "Training Loss: 0.006205681008286774\n",
      "Training Loss: 0.007232208342757076\n",
      "Training Loss: 0.006977286202600226\n",
      "Training Loss: 0.0068602739961352195\n",
      "Validation Loss: 0.004108573027541128\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005565157275996171\n",
      "Training Loss: 0.005459488524356857\n",
      "Training Loss: 0.005385894231731072\n",
      "Training Loss: 0.006202141476096585\n",
      "Training Loss: 0.007228607797296718\n",
      "Training Loss: 0.0069741524185519665\n",
      "Training Loss: 0.006856392114423215\n",
      "Validation Loss: 0.00410494750299341\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005561608214047737\n",
      "Training Loss: 0.005455777734168805\n",
      "Training Loss: 0.005382985670585185\n",
      "Training Loss: 0.006198596709291451\n",
      "Training Loss: 0.0072249992715660485\n",
      "Training Loss: 0.0069710163841955365\n",
      "Training Loss: 0.0068524990696460005\n",
      "Validation Loss: 0.004101342453153922\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.0055580599023960535\n",
      "Training Loss: 0.005452054199995473\n",
      "Training Loss: 0.005380071784602478\n",
      "Training Loss: 0.0061950459459330885\n",
      "Training Loss: 0.00722137933364138\n",
      "Training Loss: 0.006967875235714019\n",
      "Training Loss: 0.006848594652255997\n",
      "Validation Loss: 0.0040977502783258145\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005554509442881681\n",
      "Training Loss: 0.0054483151709428055\n",
      "Training Loss: 0.005377150072017684\n",
      "Training Loss: 0.006191486901952885\n",
      "Training Loss: 0.00721774640143849\n",
      "Training Loss: 0.006964724384015426\n",
      "Training Loss: 0.006844674104359001\n",
      "Validation Loss: 0.004094178681104873\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005550958752282895\n",
      "Training Loss: 0.005444563908386044\n",
      "Training Loss: 0.005374221527017653\n",
      "Training Loss: 0.0061879180010873825\n",
      "Training Loss: 0.0072140976833179596\n",
      "Training Loss: 0.0069615642516873775\n",
      "Training Loss: 0.006840735499281436\n",
      "Validation Loss: 0.004090615951504963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005547400224022567\n",
      "Training Loss: 0.005440791550790891\n",
      "Training Loss: 0.005371279094833881\n",
      "Training Loss: 0.006184338303864933\n",
      "Training Loss: 0.007210430735722184\n",
      "Training Loss: 0.006958392834058032\n",
      "Training Loss: 0.006836776876589283\n",
      "Validation Loss: 0.004087067826577787\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.0055438370537012815\n",
      "Training Loss: 0.00543700071692001\n",
      "Training Loss: 0.005368325194576755\n",
      "Training Loss: 0.0061807456449605525\n",
      "Training Loss: 0.007206743096467108\n",
      "Training Loss: 0.006955207587452605\n",
      "Training Loss: 0.006832794313086196\n",
      "Validation Loss: 0.0040835293165969655\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005540263772709295\n",
      "Training Loss: 0.00543318722571712\n",
      "Training Loss: 0.005365356021793559\n",
      "Training Loss: 0.006177140712388791\n",
      "Training Loss: 0.007203033863333985\n",
      "Training Loss: 0.006952006828505546\n",
      "Training Loss: 0.006828786988044158\n",
      "Validation Loss: 0.004079996027865129\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005536679070792161\n",
      "Training Loss: 0.005429348793695681\n",
      "Training Loss: 0.005362368535716086\n",
      "Training Loss: 0.006173518201103434\n",
      "Training Loss: 0.007199298369232565\n",
      "Training Loss: 0.006948788909940049\n",
      "Training Loss: 0.006824750375235453\n",
      "Validation Loss: 0.0040764701347763225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005533081441535614\n",
      "Training Loss: 0.005425484164734371\n",
      "Training Loss: 0.00535936240456067\n",
      "Training Loss: 0.006169879183289595\n",
      "Training Loss: 0.007195534963393584\n",
      "Training Loss: 0.00694555114605464\n",
      "Training Loss: 0.006820681768003851\n",
      "Validation Loss: 0.004072947259329622\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.0055294678418431435\n",
      "Training Loss: 0.005421589236357249\n",
      "Training Loss: 0.005356333839008584\n",
      "Training Loss: 0.006166219884762541\n",
      "Training Loss: 0.007191740514244884\n",
      "Training Loss: 0.006942291120067239\n",
      "Training Loss: 0.006816578928846866\n",
      "Validation Loss: 0.004069422730087732\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005525835162261501\n",
      "Training Loss: 0.005417661545216106\n",
      "Training Loss: 0.005353278695838526\n",
      "Training Loss: 0.006162539644865319\n",
      "Training Loss: 0.007187913007801399\n",
      "Training Loss: 0.006939006034517661\n",
      "Training Loss: 0.006812437855405733\n",
      "Validation Loss: 0.004065897446108305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0055221816926496105\n",
      "Training Loss: 0.00541370093531441\n",
      "Training Loss: 0.005350197054212913\n",
      "Training Loss: 0.006158835561363958\n",
      "Training Loss: 0.007184046709444374\n",
      "Training Loss: 0.006935693945270032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [33:13<14:17, 285.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.006808256029617041\n",
      "Validation Loss: 0.004062368992318431\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.08415109494701029\n",
      "Training Loss: 0.0717841755785048\n",
      "Training Loss: 0.06833817457780242\n",
      "Training Loss: 0.0659710925258696\n",
      "Training Loss: 0.06249840592965484\n",
      "Training Loss: 0.060536219999194146\n",
      "Training Loss: 0.056807017792016266\n",
      "Validation Loss: 0.05517768081319466\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.053591168271377686\n",
      "Training Loss: 0.04888478844426572\n",
      "Training Loss: 0.04582475450821221\n",
      "Training Loss: 0.04360263870097697\n",
      "Training Loss: 0.039988239416852596\n",
      "Training Loss: 0.03755320060066879\n",
      "Training Loss: 0.03397266293875873\n",
      "Validation Loss: 0.032070519263099194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.0308980627451092\n",
      "Training Loss: 0.027987168463878335\n",
      "Training Loss: 0.025978748071938753\n",
      "Training Loss: 0.025271528465673328\n",
      "Training Loss: 0.023635147106833756\n",
      "Training Loss: 0.022492728410288692\n",
      "Training Loss: 0.02090803492348641\n",
      "Validation Loss: 0.019457575170958086\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.01922999108675867\n",
      "Training Loss: 0.017923094439320265\n",
      "Training Loss: 0.01700295096030459\n",
      "Training Loss: 0.017206407689955086\n",
      "Training Loss: 0.016810045074671508\n",
      "Training Loss: 0.016112495129927993\n",
      "Training Loss: 0.015388049918692559\n",
      "Validation Loss: 0.013794918910840923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.014131052393931896\n",
      "Training Loss: 0.013275603209622204\n",
      "Training Loss: 0.01277018345426768\n",
      "Training Loss: 0.01326828395947814\n",
      "Training Loss: 0.013487427062354982\n",
      "Training Loss: 0.012916146947536617\n",
      "Training Loss: 0.01250450402731076\n",
      "Validation Loss: 0.010654785631970967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.011405022031394764\n",
      "Training Loss: 0.01070731580024585\n",
      "Training Loss: 0.01040384131250903\n",
      "Training Loss: 0.011046611982164905\n",
      "Training Loss: 0.011584489280357957\n",
      "Training Loss: 0.011092084848787636\n",
      "Training Loss: 0.010816887563560158\n",
      "Validation Loss: 0.008742048657998052\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.009778325699735434\n",
      "Training Loss: 0.009174572137417271\n",
      "Training Loss: 0.008987635014345869\n",
      "Training Loss: 0.0097267808415927\n",
      "Training Loss: 0.010432853469392284\n",
      "Training Loss: 0.010017920193495228\n",
      "Training Loss: 0.009825190075207501\n",
      "Validation Loss: 0.0075588477644650785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.008783657698659226\n",
      "Training Loss: 0.008258466611150652\n",
      "Training Loss: 0.008139056154759601\n",
      "Training Loss: 0.008945667801890522\n",
      "Training Loss: 0.00974354925681837\n",
      "Training Loss: 0.009392407838022336\n",
      "Training Loss: 0.009255185758229345\n",
      "Validation Loss: 0.006825900586035926\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.008171540802577511\n",
      "Training Loss: 0.007711253723828122\n",
      "Training Loss: 0.007626200546510518\n",
      "Training Loss: 0.008480486159678549\n",
      "Training Loss: 0.009332175824092702\n",
      "Training Loss: 0.009024366615340113\n",
      "Training Loss: 0.008925934856524691\n",
      "Validation Loss: 0.006360418190272727\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.007785476403078064\n",
      "Training Loss: 0.007375848507508636\n",
      "Training Loss: 0.007306153726531193\n",
      "Training Loss: 0.008194992407225073\n",
      "Training Loss: 0.009082003589719534\n",
      "Training Loss: 0.008801198885776103\n",
      "Training Loss: 0.008730791151756422\n",
      "Validation Loss: 0.006054920314423991\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.007533997162245214\n",
      "Training Loss: 0.007163012758828699\n",
      "Training Loss: 0.00709912184625864\n",
      "Training Loss: 0.008012845360208303\n",
      "Training Loss: 0.008924611008260398\n",
      "Training Loss: 0.00866043388727121\n",
      "Training Loss: 0.008610227146418766\n",
      "Validation Loss: 0.005848708003584979\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.007364467227598652\n",
      "Training Loss: 0.007022863714955747\n",
      "Training Loss: 0.006960145912598819\n",
      "Training Loss: 0.007890967505518346\n",
      "Training Loss: 0.008820043192245067\n",
      "Training Loss: 0.0085665472317487\n",
      "Training Loss: 0.008530487517127767\n",
      "Validation Loss: 0.005705787395632892\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.007245416566729545\n",
      "Training Loss: 0.006926311709685251\n",
      "Training Loss: 0.006862654342548922\n",
      "Training Loss: 0.007804334922111593\n",
      "Training Loss: 0.008745078291976825\n",
      "Training Loss: 0.008499051999533548\n",
      "Training Loss: 0.008472532234154641\n",
      "Validation Loss: 0.0056037296434648136\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.007157661549281329\n",
      "Training Loss: 0.006856065369211137\n",
      "Training Loss: 0.006790676191449165\n",
      "Training Loss: 0.007738446213770658\n",
      "Training Loss: 0.008686651507159695\n",
      "Training Loss: 0.008446406986331567\n",
      "Training Loss: 0.008426048599649221\n",
      "Validation Loss: 0.005528308973518595\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.007089592006523162\n",
      "Training Loss: 0.006801936724223197\n",
      "Training Loss: 0.00673468183260411\n",
      "Training Loss: 0.007685049964347854\n",
      "Training Loss: 0.008637689281022176\n",
      "Training Loss: 0.008402323662303389\n",
      "Training Loss: 0.008385715392651037\n",
      "Validation Loss: 0.005470453149356534\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.0070342238072771575\n",
      "Training Loss: 0.006757965685101226\n",
      "Training Loss: 0.006688997781602666\n",
      "Training Loss: 0.007639497896889225\n",
      "Training Loss: 0.008594446616480128\n",
      "Training Loss: 0.008363446046132594\n",
      "Training Loss: 0.008348869835026562\n",
      "Validation Loss: 0.005424352245900627\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.006987323426874355\n",
      "Training Loss: 0.006720634694211185\n",
      "Training Loss: 0.006650195000693202\n",
      "Training Loss: 0.007599126324057579\n",
      "Training Loss: 0.008554904382908717\n",
      "Training Loss: 0.008327954333508388\n",
      "Training Loss: 0.008314171844394877\n",
      "Validation Loss: 0.005386226630005776\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.006946254192152992\n",
      "Training Loss: 0.0066878026409540325\n",
      "Training Loss: 0.006616124890279025\n",
      "Training Loss: 0.007562348427018151\n",
      "Training Loss: 0.00851792648085393\n",
      "Training Loss: 0.008294819972943514\n",
      "Training Loss: 0.008280905921710656\n",
      "Validation Loss: 0.005353562100023059\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.006909299420658499\n",
      "Training Loss: 0.006658096588216722\n",
      "Training Loss: 0.006585373888956383\n",
      "Training Loss: 0.007528156084008515\n",
      "Training Loss: 0.008482798602199183\n",
      "Training Loss: 0.008263408149359748\n",
      "Training Loss: 0.008248645516578107\n",
      "Validation Loss: 0.005324655703411957\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.006875297216465697\n",
      "Training Loss: 0.006630593414884061\n",
      "Training Loss: 0.0065569752280134706\n",
      "Training Loss: 0.0074958683119621125\n",
      "Training Loss: 0.008449044758453966\n",
      "Training Loss: 0.008233302590670065\n",
      "Training Loss: 0.008217117879539729\n",
      "Validation Loss: 0.00529833172782837\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.00684343452565372\n",
      "Training Loss: 0.006604652444366366\n",
      "Training Loss: 0.006530249828938395\n",
      "Training Loss: 0.007465014677727595\n",
      "Training Loss: 0.008416331016924232\n",
      "Training Loss: 0.008204221300547942\n",
      "Training Loss: 0.008186145891668275\n",
      "Validation Loss: 0.005273782097898452\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.006813144733896479\n",
      "Training Loss: 0.006579826482338831\n",
      "Training Loss: 0.006504728179425001\n",
      "Training Loss: 0.007435282504302449\n",
      "Training Loss: 0.008384448228171096\n",
      "Training Loss: 0.008175992999458686\n",
      "Training Loss: 0.008155644916696474\n",
      "Validation Loss: 0.005250475620100979\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.006784056073520332\n",
      "Training Loss: 0.0065558254392817615\n",
      "Training Loss: 0.006480110557749868\n",
      "Training Loss: 0.007406487608095631\n",
      "Training Loss: 0.008353293042164296\n",
      "Training Loss: 0.008148539852118119\n",
      "Training Loss: 0.008125618891790509\n",
      "Validation Loss: 0.005228094378580454\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.006755957003915683\n",
      "Training Loss: 0.006532489177770912\n",
      "Training Loss: 0.0064562370569910855\n",
      "Training Loss: 0.007378566411789507\n",
      "Training Loss: 0.008322876850143076\n",
      "Training Loss: 0.00812187502044253\n",
      "Training Loss: 0.008096164187882095\n",
      "Validation Loss: 0.005206495328555197\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.006728779312688857\n",
      "Training Loss: 0.006509771443670615\n",
      "Training Loss: 0.006433066895697266\n",
      "Training Loss: 0.007351556667126715\n",
      "Training Loss: 0.008293309445725754\n",
      "Training Loss: 0.00809608492651023\n",
      "Training Loss: 0.008067451544338838\n",
      "Validation Loss: 0.0051856616177083405\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.00670256151817739\n",
      "Training Loss: 0.006487706077750772\n",
      "Training Loss: 0.0064106452034320685\n",
      "Training Loss: 0.007325567335356027\n",
      "Training Loss: 0.008264769417000934\n",
      "Training Loss: 0.008071297329152002\n",
      "Training Loss: 0.008039697217755019\n",
      "Validation Loss: 0.005165660516219388\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.0066774152580183\n",
      "Training Loss: 0.006466382008511573\n",
      "Training Loss: 0.0063890691637061535\n",
      "Training Loss: 0.0073007389641134065\n",
      "Training Loss: 0.008237461076350882\n",
      "Training Loss: 0.008047636456321925\n",
      "Training Loss: 0.008013108522864059\n",
      "Validation Loss: 0.005146590262487503\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.006653465239796787\n",
      "Training Loss: 0.006445885013090446\n",
      "Training Loss: 0.006368433897150681\n",
      "Training Loss: 0.007277184088015929\n",
      "Training Loss: 0.008211552568245678\n",
      "Training Loss: 0.008025179594988004\n",
      "Training Loss: 0.007987836527172476\n",
      "Validation Loss: 0.005128518443673236\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.006630798749392852\n",
      "Training Loss: 0.006426264496985823\n",
      "Training Loss: 0.006348788788309321\n",
      "Training Loss: 0.007254948837216943\n",
      "Training Loss: 0.008187129712896422\n",
      "Training Loss: 0.008003916244488209\n",
      "Training Loss: 0.007963931406848132\n",
      "Validation Loss: 0.005111459561550383\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006609425751958042\n",
      "Training Loss: 0.00640750334598124\n",
      "Training Loss: 0.006330114034935832\n",
      "Training Loss: 0.007233989419764839\n",
      "Training Loss: 0.008164169519441202\n",
      "Training Loss: 0.007983745515812189\n",
      "Training Loss: 0.007941333848284557\n",
      "Validation Loss: 0.00509533761609834\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006589260279433802\n",
      "Training Loss: 0.006389507000567392\n",
      "Training Loss: 0.006312312290538103\n",
      "Training Loss: 0.007214170219376683\n",
      "Training Loss: 0.008142544295405969\n",
      "Training Loss: 0.007964493470499292\n",
      "Training Loss: 0.007919894034275786\n",
      "Validation Loss: 0.005080011881685603\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006570142286363989\n",
      "Training Loss: 0.006372134962584823\n",
      "Training Loss: 0.006295231375843286\n",
      "Training Loss: 0.007195298445294611\n",
      "Training Loss: 0.008122054936829954\n",
      "Training Loss: 0.00794594788691029\n",
      "Training Loss: 0.007899410196114332\n",
      "Validation Loss: 0.005065293158561493\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.006551866214722395\n",
      "Training Loss: 0.006355218354729004\n",
      "Training Loss: 0.00627869572956115\n",
      "Training Loss: 0.00717715804465115\n",
      "Training Loss: 0.008102476114872843\n",
      "Training Loss: 0.007927900792565197\n",
      "Training Loss: 0.00787966939387843\n",
      "Validation Loss: 0.005050993256780148\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006534223492490127\n",
      "Training Loss: 0.006338598055299372\n",
      "Training Loss: 0.006262535743881017\n",
      "Training Loss: 0.007159547652699985\n",
      "Training Loss: 0.008083593170158564\n",
      "Training Loss: 0.007910175858996808\n",
      "Training Loss: 0.007860477742506191\n",
      "Validation Loss: 0.005036933535068167\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006517024693312124\n",
      "Training Loss: 0.006322136480594054\n",
      "Training Loss: 0.00624660657136701\n",
      "Training Loss: 0.007142295107478276\n",
      "Training Loss: 0.008065216322429479\n",
      "Training Loss: 0.007892633995506913\n",
      "Training Loss: 0.00784167614299804\n",
      "Validation Loss: 0.005022967406042785\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.006500113703077659\n",
      "Training Loss: 0.006305727277649566\n",
      "Training Loss: 0.006230794889852405\n",
      "Training Loss: 0.0071252646989887584\n",
      "Training Loss: 0.008047196910483763\n",
      "Training Loss: 0.007875176863744855\n",
      "Training Loss: 0.007823142308043316\n",
      "Validation Loss: 0.005008984458273493\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006483374664094299\n",
      "Training Loss: 0.006289294142043218\n",
      "Training Loss: 0.006215019618393853\n",
      "Training Loss: 0.007108359510893934\n",
      "Training Loss: 0.008029422647086903\n",
      "Training Loss: 0.007857739278115332\n",
      "Training Loss: 0.007804784752661362\n",
      "Validation Loss: 0.004994907077621627\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006466719990130514\n",
      "Training Loss: 0.0062727832136442885\n",
      "Training Loss: 0.006199224954470992\n",
      "Training Loss: 0.007091510318568908\n",
      "Training Loss: 0.008011811759788543\n",
      "Training Loss: 0.007840279404772445\n",
      "Training Loss: 0.007786539571825415\n",
      "Validation Loss: 0.004980683423951966\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006450090805301443\n",
      "Training Loss: 0.006256159408367239\n",
      "Training Loss: 0.006183372972300276\n",
      "Training Loss: 0.007074670569272712\n",
      "Training Loss: 0.007994306235341355\n",
      "Training Loss: 0.007822772013023496\n",
      "Training Loss: 0.007768363263458014\n",
      "Validation Loss: 0.00496628870428906\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006433448661118746\n",
      "Training Loss: 0.006239403145737015\n",
      "Training Loss: 0.006167445846367628\n",
      "Training Loss: 0.007057811613776721\n",
      "Training Loss: 0.007976865420350804\n",
      "Training Loss: 0.007805202094605192\n",
      "Training Loss: 0.0077502235805150125\n",
      "Validation Loss: 0.004951710277324201\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.0064167714747600255\n",
      "Training Loss: 0.006222503657918424\n",
      "Training Loss: 0.006151435557985679\n",
      "Training Loss: 0.007040917164413258\n",
      "Training Loss: 0.007959459543926642\n",
      "Training Loss: 0.00778756252140738\n",
      "Training Loss: 0.00773209952749312\n",
      "Validation Loss: 0.004936949172046747\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006400043371831999\n",
      "Training Loss: 0.006205454678274691\n",
      "Training Loss: 0.006135337824234739\n",
      "Training Loss: 0.007023977729841135\n",
      "Training Loss: 0.007942071482539177\n",
      "Training Loss: 0.00776985299657099\n",
      "Training Loss: 0.007713976325467229\n",
      "Validation Loss: 0.004922011438129323\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006383257687557489\n",
      "Training Loss: 0.0061882560222875325\n",
      "Training Loss: 0.006119156200438738\n",
      "Training Loss: 0.0070069902256364005\n",
      "Training Loss: 0.007924687582999468\n",
      "Training Loss: 0.007752074836753308\n",
      "Training Loss: 0.0076958453585393724\n",
      "Validation Loss: 0.0049069138476625085\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006366413403302431\n",
      "Training Loss: 0.006170912790694274\n",
      "Training Loss: 0.0061028983350843195\n",
      "Training Loss: 0.00698995771468617\n",
      "Training Loss: 0.007907298028003424\n",
      "Training Loss: 0.007734231138601899\n",
      "Training Loss: 0.007677698985207826\n",
      "Validation Loss: 0.004891668996173707\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006349510924192146\n",
      "Training Loss: 0.006153429439873435\n",
      "Training Loss: 0.0060865741700399665\n",
      "Training Loss: 0.0069728844973724335\n",
      "Training Loss: 0.00788989766035229\n",
      "Training Loss: 0.007716330554103479\n",
      "Training Loss: 0.0076595332322176544\n",
      "Validation Loss: 0.004876293491812821\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006332553292158991\n",
      "Training Loss: 0.0061358130496228114\n",
      "Training Loss: 0.006070192327024415\n",
      "Training Loss: 0.006955777037655935\n",
      "Training Loss: 0.007872480965452268\n",
      "Training Loss: 0.007698378477944062\n",
      "Training Loss: 0.007641343588475138\n",
      "Validation Loss: 0.00486080242279038\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.00631554564111866\n",
      "Training Loss: 0.006118073141551577\n",
      "Training Loss: 0.006053766073891893\n",
      "Training Loss: 0.006938643088797107\n",
      "Training Loss: 0.007855042933369987\n",
      "Training Loss: 0.007680384375853464\n",
      "Training Loss: 0.007623127585975453\n",
      "Validation Loss: 0.004845209584540028\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006298490723129362\n",
      "Training Loss: 0.006100218091160059\n",
      "Training Loss: 0.006037308224476874\n",
      "Training Loss: 0.006921492791152559\n",
      "Training Loss: 0.007837582118809224\n",
      "Training Loss: 0.007662357853259891\n",
      "Training Loss: 0.007604881838196889\n",
      "Validation Loss: 0.004829534092283204\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006281396478880197\n",
      "Training Loss: 0.006082260850816965\n",
      "Training Loss: 0.006020833164220676\n",
      "Training Loss: 0.006904335214057937\n",
      "Training Loss: 0.007820093130576425\n",
      "Training Loss: 0.007644305133726448\n",
      "Training Loss: 0.007586602609371766\n",
      "Validation Loss: 0.004813781829135779\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006264263474149629\n",
      "Training Loss: 0.006064209530013614\n",
      "Training Loss: 0.006004350759321824\n",
      "Training Loss: 0.006887179819750599\n",
      "Training Loss: 0.00780257246689871\n",
      "Training Loss: 0.007626236238284036\n",
      "Training Loss: 0.007568287313915789\n",
      "Validation Loss: 0.004797962052513905\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.0062470976426266134\n",
      "Training Loss: 0.006046075091580861\n",
      "Training Loss: 0.005987877856241539\n",
      "Training Loss: 0.006870037789340131\n",
      "Training Loss: 0.007785018808208406\n",
      "Training Loss: 0.0076081597083248196\n",
      "Training Loss: 0.007549929685192183\n",
      "Validation Loss: 0.004782081704171661\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006229901362676173\n",
      "Training Loss: 0.006027868140372448\n",
      "Training Loss: 0.005971424218732864\n",
      "Training Loss: 0.006852916922653094\n",
      "Training Loss: 0.0077674260118510575\n",
      "Training Loss: 0.007590078617213294\n",
      "Training Loss: 0.007531525617232546\n",
      "Validation Loss: 0.0047661437101885395\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006212677095318213\n",
      "Training Loss: 0.006009598793461919\n",
      "Training Loss: 0.005955002903938293\n",
      "Training Loss: 0.006835825457819737\n",
      "Training Loss: 0.007749790421221405\n",
      "Training Loss: 0.007571998966159299\n",
      "Training Loss: 0.007513068742118776\n",
      "Validation Loss: 0.00475014730579872\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006195424704346805\n",
      "Training Loss: 0.0059912753634853285\n",
      "Training Loss: 0.005938625301932916\n",
      "Training Loss: 0.006818772024125792\n",
      "Training Loss: 0.007732106347102672\n",
      "Training Loss: 0.007553922042716294\n",
      "Training Loss: 0.0074945519620087\n",
      "Validation Loss: 0.004734089213932693\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0061781437054742126\n",
      "Training Loss: 0.00597290490870364\n",
      "Training Loss: 0.005922301966929808\n",
      "Training Loss: 0.006801761685637757\n",
      "Training Loss: 0.007714367841253988\n",
      "Training Loss: 0.007535844682715833\n",
      "Training Loss: 0.007475964827463031\n",
      "Validation Loss: 0.0047179681426855954\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0061608333280310035\n",
      "Training Loss: 0.005954495945479721\n",
      "Training Loss: 0.00590604302356951\n",
      "Training Loss: 0.006784797784639522\n",
      "Training Loss: 0.007696564422221854\n",
      "Training Loss: 0.0075177602248732\n",
      "Training Loss: 0.007457297330256551\n",
      "Validation Loss: 0.0047017755575499\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006143490591784939\n",
      "Training Loss: 0.005936051839380525\n",
      "Training Loss: 0.005889854108681902\n",
      "Training Loss: 0.0067678812996018675\n",
      "Training Loss: 0.007678689313470386\n",
      "Training Loss: 0.007499661451438442\n",
      "Training Loss: 0.007438535424880683\n",
      "Validation Loss: 0.004685492584855062\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006126105119474232\n",
      "Training Loss: 0.0059175696864258494\n",
      "Training Loss: 0.005873734973138198\n",
      "Training Loss: 0.006751006505219266\n",
      "Training Loss: 0.007660724383313209\n",
      "Training Loss: 0.00748152983491309\n",
      "Training Loss: 0.007419660820160061\n",
      "Validation Loss: 0.004669098788697956\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006108665335923433\n",
      "Training Loss: 0.005899045823607594\n",
      "Training Loss: 0.005857685687951744\n",
      "Training Loss: 0.00673416480072774\n",
      "Training Loss: 0.007642651950009167\n",
      "Training Loss: 0.007463342805858701\n",
      "Training Loss: 0.0074006512947380546\n",
      "Validation Loss: 0.004652572339243685\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006091156385373324\n",
      "Training Loss: 0.005880470547708683\n",
      "Training Loss: 0.005841696886345744\n",
      "Training Loss: 0.006717338941525668\n",
      "Training Loss: 0.007624447054113261\n",
      "Training Loss: 0.007445071097463369\n",
      "Training Loss: 0.007381480532931164\n",
      "Validation Loss: 0.004635882980280908\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.0060735578101594\n",
      "Training Loss: 0.005861827904591337\n",
      "Training Loss: 0.0058257561735808846\n",
      "Training Loss: 0.006700507897185162\n",
      "Training Loss: 0.007606076595839113\n",
      "Training Loss: 0.00742667609709315\n",
      "Training Loss: 0.007362113884883001\n",
      "Validation Loss: 0.004618994728138859\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006055842806817964\n",
      "Training Loss: 0.0058430939720710735\n",
      "Training Loss: 0.0058098416880238805\n",
      "Training Loss: 0.006683638335089199\n",
      "Training Loss: 0.007587500801892019\n",
      "Training Loss: 0.00740811076015234\n",
      "Training Loss: 0.007342510325834155\n",
      "Validation Loss: 0.00460186689326449\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006037978887325153\n",
      "Training Loss: 0.005824236947810277\n",
      "Training Loss: 0.0057939232827629895\n",
      "Training Loss: 0.0066666938632261\n",
      "Training Loss: 0.007568673377390951\n",
      "Training Loss: 0.007389324943069368\n",
      "Training Loss: 0.007322623904328794\n",
      "Validation Loss: 0.004584444416442726\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006019924508873373\n",
      "Training Loss: 0.005805216047447175\n",
      "Training Loss: 0.005777963428990915\n",
      "Training Loss: 0.00664962463080883\n",
      "Training Loss: 0.007549534761346876\n",
      "Training Loss: 0.007370253572007641\n",
      "Training Loss: 0.00730239857104607\n",
      "Validation Loss: 0.00456667574923067\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0060016342380549755\n",
      "Training Loss: 0.005785983480745926\n",
      "Training Loss: 0.005761915155453608\n",
      "Training Loss: 0.00663237671367824\n",
      "Training Loss: 0.007530022393912077\n",
      "Training Loss: 0.007350828463677317\n",
      "Training Loss: 0.00728177375975065\n",
      "Validation Loss: 0.004548502257983252\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.005983055202523247\n",
      "Training Loss: 0.005766484080813825\n",
      "Training Loss: 0.005745725382585078\n",
      "Training Loss: 0.00661488693789579\n",
      "Training Loss: 0.007510062812361867\n",
      "Training Loss: 0.007330979985417798\n",
      "Training Loss: 0.007260682608466596\n",
      "Validation Loss: 0.004529854674711963\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.005964126801118254\n",
      "Training Loss: 0.005746652999077923\n",
      "Training Loss: 0.005729329262394458\n",
      "Training Loss: 0.006597085372195579\n",
      "Training Loss: 0.007489576218649745\n",
      "Training Loss: 0.007310624545207247\n",
      "Training Loss: 0.007239050974603743\n",
      "Validation Loss: 0.004510665497278676\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.005944782900623977\n",
      "Training Loss: 0.0057264201203361155\n",
      "Training Loss: 0.005712657228577882\n",
      "Training Loss: 0.006578895482234657\n",
      "Training Loss: 0.007468474403722212\n",
      "Training Loss: 0.007289683020208031\n",
      "Training Loss: 0.007216800353489816\n",
      "Validation Loss: 0.004490861865388376\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0059249520720914\n",
      "Training Loss: 0.005705708659952506\n",
      "Training Loss: 0.005695632067508996\n",
      "Training Loss: 0.006560236676014028\n",
      "Training Loss: 0.0074466629815287885\n",
      "Training Loss: 0.007268065537791699\n",
      "Training Loss: 0.007193844323046506\n",
      "Validation Loss: 0.004470369507809787\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.005904556738678366\n",
      "Training Loss: 0.005684439341421239\n",
      "Training Loss: 0.005678169497405179\n",
      "Training Loss: 0.006541019630385563\n",
      "Training Loss: 0.007424040902988054\n",
      "Training Loss: 0.00724568510428071\n",
      "Training Loss: 0.007170099196955561\n",
      "Validation Loss: 0.0044491121655309135\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.005883522310759872\n",
      "Training Loss: 0.005662529945839196\n",
      "Training Loss: 0.005660181852290407\n",
      "Training Loss: 0.00652115580975078\n",
      "Training Loss: 0.007400507096317597\n",
      "Training Loss: 0.007222456879680976\n",
      "Training Loss: 0.007145472241099924\n",
      "Validation Loss: 0.004427005488456886\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.00586176126729697\n",
      "Training Loss: 0.005639892121544108\n",
      "Training Loss: 0.005641569416038692\n",
      "Training Loss: 0.006500550351338461\n",
      "Training Loss: 0.007375951811554842\n",
      "Training Loss: 0.007198289775988087\n",
      "Training Loss: 0.007119874235941097\n",
      "Validation Loss: 0.004403968694818656\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.0058391917508561165\n",
      "Training Loss: 0.005616441331221722\n",
      "Training Loss: 0.005622233953326941\n",
      "Training Loss: 0.00647910448431503\n",
      "Training Loss: 0.0073502666235435755\n",
      "Training Loss: 0.007173102130182087\n",
      "Training Loss: 0.007093213764019311\n",
      "Validation Loss: 0.0043799269051475916\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.0058157349436078224\n",
      "Training Loss: 0.005592097650514915\n",
      "Training Loss: 0.00560207444941625\n",
      "Training Loss: 0.0064567235158756375\n",
      "Training Loss: 0.0073233453813008964\n",
      "Training Loss: 0.007146815279265866\n",
      "Training Loss: 0.007065403747837991\n",
      "Validation Loss: 0.004354794211040359\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.005791313919471577\n",
      "Training Loss: 0.005566782093374059\n",
      "Training Loss: 0.00558098117064219\n",
      "Training Loss: 0.006433311511063949\n",
      "Training Loss: 0.007295092213898897\n",
      "Training Loss: 0.007119365289108828\n",
      "Training Loss: 0.007036368834087625\n",
      "Validation Loss: 0.004328502914153235\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.005765864988788962\n",
      "Training Loss: 0.005540431881090626\n",
      "Training Loss: 0.005558859774610027\n",
      "Training Loss: 0.006408781346399337\n",
      "Training Loss: 0.00726542164280545\n",
      "Training Loss: 0.007090703012654558\n",
      "Training Loss: 0.007006048826733604\n",
      "Validation Loss: 0.004300991883361887\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.005739341537700966\n",
      "Training Loss: 0.00551299836486578\n",
      "Training Loss: 0.005535617870045826\n",
      "Training Loss: 0.006383059560321271\n",
      "Training Loss: 0.0072342772834235805\n",
      "Training Loss: 0.007060807289090008\n",
      "Training Loss: 0.006974410732509568\n",
      "Validation Loss: 0.004272226269935931\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.005711720100371167\n",
      "Training Loss: 0.00548446131288074\n",
      "Training Loss: 0.005511182107729837\n",
      "Training Loss: 0.006356096699601039\n",
      "Training Loss: 0.007201643479638733\n",
      "Training Loss: 0.0070296963676810265\n",
      "Training Loss: 0.006941460333764553\n",
      "Validation Loss: 0.004242198818628345\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.0056830112414900215\n",
      "Training Loss: 0.005454834182164632\n",
      "Training Loss: 0.005485514842439443\n",
      "Training Loss: 0.006327876733266748\n",
      "Training Loss: 0.00716755281551741\n",
      "Training Loss: 0.00699743323144503\n",
      "Training Loss: 0.006907256740378216\n",
      "Validation Loss: 0.0042109498767224085\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.005653269590111449\n",
      "Training Loss: 0.005424176838714629\n",
      "Training Loss: 0.0054586200520861895\n",
      "Training Loss: 0.006298432154580951\n",
      "Training Loss: 0.007132108471705578\n",
      "Training Loss: 0.0069641412212513385\n",
      "Training Loss: 0.006871926595922559\n",
      "Validation Loss: 0.004178590983176806\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.005622603469528258\n",
      "Training Loss: 0.005392611913848668\n",
      "Training Loss: 0.005430561100365594\n",
      "Training Loss: 0.006267857509083114\n",
      "Training Loss: 0.00709549555904232\n",
      "Training Loss: 0.006930012638913467\n",
      "Training Loss: 0.006835669670253992\n",
      "Validation Loss: 0.004145286617580843\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.005591166036901995\n",
      "Training Loss: 0.00536031358060427\n",
      "Training Loss: 0.005401465796167031\n",
      "Training Loss: 0.006236311548273079\n",
      "Training Loss: 0.0070579786371672525\n",
      "Training Loss: 0.006895298239542171\n",
      "Training Loss: 0.006798757413635031\n",
      "Validation Loss: 0.00411128818601621\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.005559168980689719\n",
      "Training Loss: 0.00532752244384028\n",
      "Training Loss: 0.005371539570041932\n",
      "Training Loss: 0.00620402448810637\n",
      "Training Loss: 0.007019893976976163\n",
      "Training Loss: 0.006860299508552999\n",
      "Training Loss: 0.006761527506168932\n",
      "Validation Loss: 0.004076919338774034\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.005526864294661209\n",
      "Training Loss: 0.005294525605859235\n",
      "Training Loss: 0.005341050997376442\n",
      "Training Loss: 0.006171282914583571\n",
      "Training Loss: 0.00698163474095054\n",
      "Training Loss: 0.006825355120236054\n",
      "Training Loss: 0.006724356492049992\n",
      "Validation Loss: 0.004042545149943439\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.005494530338328332\n",
      "Training Loss: 0.005261641754186712\n",
      "Training Loss: 0.005310317075345666\n",
      "Training Loss: 0.00613841777027119\n",
      "Training Loss: 0.006943620362435467\n",
      "Training Loss: 0.006790810885140672\n",
      "Training Loss: 0.006687633921392262\n",
      "Validation Loss: 0.0040085598528175875\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.005462458659894765\n",
      "Training Loss: 0.005229196943109855\n",
      "Training Loss: 0.005279681129031814\n",
      "Training Loss: 0.006105778217315674\n",
      "Training Loss: 0.006906263930723071\n",
      "Training Loss: 0.006756998216733336\n",
      "Training Loss: 0.006651733889011666\n",
      "Validation Loss: 0.0039753566564964165\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.005430934551404789\n",
      "Training Loss: 0.005197506640106439\n",
      "Training Loss: 0.0052494887460488825\n",
      "Training Loss: 0.006073703799047508\n",
      "Training Loss: 0.006869938972522504\n",
      "Training Loss: 0.006724202906480059\n",
      "Training Loss: 0.0066169869108125565\n",
      "Validation Loss: 0.003943285831574644\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.005400217305868864\n",
      "Training Loss: 0.005166843576007523\n",
      "Training Loss: 0.005220052868826315\n",
      "Training Loss: 0.006042496789013967\n",
      "Training Loss: 0.006834960130508989\n",
      "Training Loss: 0.006692663398571313\n",
      "Training Loss: 0.0065836557431612165\n",
      "Validation Loss: 0.0039126410638243795\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.005370530349900946\n",
      "Training Loss: 0.005137433952186256\n",
      "Training Loss: 0.0051916465914109726\n",
      "Training Loss: 0.006012410030234605\n",
      "Training Loss: 0.006801562231266871\n",
      "Training Loss: 0.006662551038898528\n",
      "Training Loss: 0.00655193320242688\n",
      "Validation Loss: 0.003883633252027469\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.005342048857710324\n",
      "Training Loss: 0.005109435706981458\n",
      "Training Loss: 0.0051644707762170585\n",
      "Training Loss: 0.005983629216789268\n",
      "Training Loss: 0.006769897963386029\n",
      "Training Loss: 0.006633972298586741\n",
      "Training Loss: 0.0065219279855955395\n",
      "Validation Loss: 0.0038563888284555647\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.005314892939059064\n",
      "Training Loss: 0.005082948944182136\n",
      "Training Loss: 0.0051386668480699884\n",
      "Training Loss: 0.005956275428761728\n",
      "Training Loss: 0.006740044705802575\n",
      "Training Loss: 0.0066069752792827785\n",
      "Training Loss: 0.006493682746076956\n",
      "Validation Loss: 0.0038309546892988156\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.005289131707395427\n",
      "Training Loss: 0.0050580081366933885\n",
      "Training Loss: 0.00511430861603003\n",
      "Training Loss: 0.005930404404061847\n",
      "Training Loss: 0.006712007253081537\n",
      "Training Loss: 0.006581554393051192\n",
      "Training Loss: 0.006467172736302018\n",
      "Validation Loss: 0.003807304999512038\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.005264783484162763\n",
      "Training Loss: 0.005034599019563757\n",
      "Training Loss: 0.005091411764733493\n",
      "Training Loss: 0.005906019223039039\n",
      "Training Loss: 0.006685737257357687\n",
      "Training Loss: 0.006557662409031764\n",
      "Training Loss: 0.0064423320523928854\n",
      "Validation Loss: 0.003785365536261336\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.005241832034662366\n",
      "Training Loss: 0.005012667595874518\n",
      "Training Loss: 0.005069948090822436\n",
      "Training Loss: 0.005883080121711828\n",
      "Training Loss: 0.006661148350685835\n",
      "Training Loss: 0.006535225673578679\n",
      "Training Loss: 0.006419055990409106\n",
      "Validation Loss: 0.00376501382016385\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0052202224894426765\n",
      "Training Loss: 0.0049921302101574834\n",
      "Training Loss: 0.005049854179378599\n",
      "Training Loss: 0.005861517632729374\n",
      "Training Loss: 0.006638124415185302\n",
      "Training Loss: 0.006514149189461023\n",
      "Training Loss: 0.006397221311926842\n",
      "Validation Loss: 0.003746116035513245\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.005199882448650896\n",
      "Training Loss: 0.004972884868620895\n",
      "Training Loss: 0.0050310460443142805\n",
      "Training Loss: 0.005841241254238412\n",
      "Training Loss: 0.006616539306123741\n",
      "Training Loss: 0.006494325818493962\n",
      "Training Loss: 0.006376697433879599\n",
      "Validation Loss: 0.003728524313922204\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.005180726082762703\n",
      "Training Loss: 0.004954824169399217\n",
      "Training Loss: 0.005013426070800051\n",
      "Training Loss: 0.005822152130422182\n",
      "Training Loss: 0.006596259576035664\n",
      "Training Loss: 0.006475646725157276\n",
      "Training Loss: 0.006357353058410809\n",
      "Validation Loss: 0.003712088161336488\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.0051626599731389436\n",
      "Training Loss: 0.004937834857264534\n",
      "Training Loss: 0.00499688959098421\n",
      "Training Loss: 0.0058041458297520875\n",
      "Training Loss: 0.006577157872379757\n",
      "Training Loss: 0.006458008462795988\n",
      "Training Loss: 0.006339060413883999\n",
      "Validation Loss: 0.003696668633235416\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.00514558901602868\n",
      "Training Loss: 0.004921813428518362\n",
      "Training Loss: 0.004981340363738127\n",
      "Training Loss: 0.005787124712369405\n",
      "Training Loss: 0.006559114478295669\n",
      "Training Loss: 0.00644130906322971\n",
      "Training Loss: 0.006321704701986164\n",
      "Validation Loss: 0.0036821396858344197\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.005129425079794601\n",
      "Training Loss: 0.004906661735731177\n",
      "Training Loss: 0.004966679986100644\n",
      "Training Loss: 0.005770990936434827\n",
      "Training Loss: 0.006542017369065433\n",
      "Training Loss: 0.006425457214936614\n",
      "Training Loss: 0.006305179254850373\n",
      "Validation Loss: 0.00366838027248752\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.005114080610219389\n",
      "Training Loss: 0.004892285792739131\n",
      "Training Loss: 0.004952819101745263\n",
      "Training Loss: 0.005755659748683683\n",
      "Training Loss: 0.0065257705561816695\n",
      "Training Loss: 0.006410371537785977\n",
      "Training Loss: 0.006289392723701894\n",
      "Validation Loss: 0.0036552881975466392\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.005099476773175411\n",
      "Training Loss: 0.004878606678685174\n",
      "Training Loss: 0.004939675962668843\n",
      "Training Loss: 0.005741050798678771\n",
      "Training Loss: 0.006510284662945196\n",
      "Training Loss: 0.0063959769532084465\n",
      "Training Loss: 0.006274262574734167\n",
      "Validation Loss: 0.003642777784131049\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.005085542454035022\n",
      "Training Loss: 0.004865554090938531\n",
      "Training Loss: 0.004927179238875396\n",
      "Training Loss: 0.005727091815206222\n",
      "Training Loss: 0.006495483953040093\n",
      "Training Loss: 0.0063822056748904284\n",
      "Training Loss: 0.006259718897053972\n",
      "Validation Loss: 0.0036307727937213378\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.005072211867664009\n",
      "Training Loss: 0.0048530640109675\n",
      "Training Loss: 0.004915264175506309\n",
      "Training Loss: 0.005713721002684906\n",
      "Training Loss: 0.006481300342129543\n",
      "Training Loss: 0.006369003335712478\n",
      "Training Loss: 0.006245701796142384\n",
      "Validation Loss: 0.0036192093448266945\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.005059429371613078\n",
      "Training Loss: 0.004841082368511706\n",
      "Training Loss: 0.004903876092866994\n",
      "Training Loss: 0.005700884083635174\n",
      "Training Loss: 0.006467679120833054\n",
      "Training Loss: 0.0063563165103551\n",
      "Training Loss: 0.00623216284555383\n",
      "Validation Loss: 0.0036080338737547175\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.005047144182608463\n",
      "Training Loss: 0.0048295649158535525\n",
      "Training Loss: 0.004892964850878343\n",
      "Training Loss: 0.005688531234627589\n",
      "Training Loss: 0.006454569465131499\n",
      "Training Loss: 0.006344104144955054\n",
      "Training Loss: 0.006219056606059894\n",
      "Validation Loss: 0.0035971979722241114\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.005035310700186528\n",
      "Training Loss: 0.004818467983277515\n",
      "Training Loss: 0.004882486013229936\n",
      "Training Loss: 0.005676621835445986\n",
      "Training Loss: 0.006441929190186784\n",
      "Training Loss: 0.006332328136777505\n",
      "Training Loss: 0.006206348645500839\n",
      "Validation Loss: 0.0035866664588270003\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.005023891129530967\n",
      "Training Loss: 0.004807760978583247\n",
      "Training Loss: 0.004872405906207859\n",
      "Training Loss: 0.005665119762998074\n",
      "Training Loss: 0.0064297214243561026\n",
      "Training Loss: 0.006320954553084448\n",
      "Training Loss: 0.006194005634170025\n",
      "Validation Loss: 0.003576407909274715\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.005012851853389293\n",
      "Training Loss: 0.004797411285107955\n",
      "Training Loss: 0.004862689831643365\n",
      "Training Loss: 0.005653992680017836\n",
      "Training Loss: 0.006417916535865515\n",
      "Training Loss: 0.0063099535007495435\n",
      "Training Loss: 0.006182004966540262\n",
      "Validation Loss: 0.0035663941362349505\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.005002162426826544\n",
      "Training Loss: 0.004787396382889711\n",
      "Training Loss: 0.004853313324856572\n",
      "Training Loss: 0.005643211831338704\n",
      "Training Loss: 0.006406484744511545\n",
      "Training Loss: 0.006299300675746054\n",
      "Training Loss: 0.0061703217460308225\n",
      "Validation Loss: 0.003556606017074056\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.004991796399117447\n",
      "Training Loss: 0.004777691803174094\n",
      "Training Loss: 0.004844247445580549\n",
      "Training Loss: 0.005632755046244711\n",
      "Training Loss: 0.006395402618218213\n",
      "Training Loss: 0.006288972991751507\n",
      "Training Loss: 0.006158938908483833\n",
      "Validation Loss: 0.0035470275797842817\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.004981731970328838\n",
      "Training Loss: 0.004768281067954377\n",
      "Training Loss: 0.004835475228610449\n",
      "Training Loss: 0.0056225982680916785\n",
      "Training Loss: 0.006384649145184084\n",
      "Training Loss: 0.006278950024861843\n",
      "Training Loss: 0.006147839329205453\n",
      "Validation Loss: 0.0035376349705609766\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.004971945283468813\n",
      "Training Loss: 0.004759144993731752\n",
      "Training Loss: 0.004826974432216957\n",
      "Training Loss: 0.005612725334358402\n",
      "Training Loss: 0.006374206831678748\n",
      "Training Loss: 0.0062692180729936806\n",
      "Training Loss: 0.006137009934755042\n",
      "Validation Loss: 0.0035284204078901015\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.004962422006065026\n",
      "Training Loss: 0.004750269816140644\n",
      "Training Loss: 0.004818729817052372\n",
      "Training Loss: 0.005603118241997436\n",
      "Training Loss: 0.0063640580303035675\n",
      "Training Loss: 0.006259757386287674\n",
      "Training Loss: 0.006126438961364329\n",
      "Validation Loss: 0.003519374543446908\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.004953144268947654\n",
      "Training Loss: 0.00474164237442892\n",
      "Training Loss: 0.004810726477880962\n",
      "Training Loss: 0.005593763458309695\n",
      "Training Loss: 0.006354188225232065\n",
      "Training Loss: 0.006250557120656594\n",
      "Training Loss: 0.006116114484611899\n",
      "Validation Loss: 0.0035104822419315214\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.004944097623229027\n",
      "Training Loss: 0.004733250749413855\n",
      "Training Loss: 0.0048029511194908996\n",
      "Training Loss: 0.005584647700889036\n",
      "Training Loss: 0.006344584568869323\n",
      "Training Loss: 0.006241603145608679\n",
      "Training Loss: 0.006106029934016988\n",
      "Validation Loss: 0.003501740825355527\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.004935270593850873\n",
      "Training Loss: 0.004725086995749734\n",
      "Training Loss: 0.004795394001994282\n",
      "Training Loss: 0.005575757779297419\n",
      "Training Loss: 0.006335235956357792\n",
      "Training Loss: 0.0062328851839993146\n",
      "Training Loss: 0.006096174931153655\n",
      "Validation Loss: 0.0034931401937087292\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.004926650842535309\n",
      "Training Loss: 0.004717138673877344\n",
      "Training Loss: 0.00478804215148557\n",
      "Training Loss: 0.005567083810456097\n",
      "Training Loss: 0.006326128598884679\n",
      "Training Loss: 0.006224390474380925\n",
      "Training Loss: 0.0060865422210190446\n",
      "Validation Loss: 0.0034846759552558124\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.004918227919260971\n",
      "Training Loss: 0.004709399251732975\n",
      "Training Loss: 0.0047808875510236245\n",
      "Training Loss: 0.005558618232607841\n",
      "Training Loss: 0.006317255044123158\n",
      "Training Loss: 0.006216113930568099\n",
      "Training Loss: 0.006077126694144681\n",
      "Validation Loss: 0.003476339341786582\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.004909991812310182\n",
      "Training Loss: 0.004701859280467033\n",
      "Training Loss: 0.004773919323342852\n",
      "Training Loss: 0.005550348911201581\n",
      "Training Loss: 0.006308603002689778\n",
      "Training Loss: 0.0062080420833081\n",
      "Training Loss: 0.006067918970948085\n",
      "Validation Loss: 0.0034681305931078808\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.004901935082743876\n",
      "Training Loss: 0.004694512910209597\n",
      "Training Loss: 0.004767131098778918\n",
      "Training Loss: 0.005542269413126633\n",
      "Training Loss: 0.006300166281871498\n",
      "Training Loss: 0.006200169828953221\n",
      "Training Loss: 0.0060589152324246245\n",
      "Validation Loss: 0.0034600426148612765\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.004894049171707593\n",
      "Training Loss: 0.004687351171160117\n",
      "Training Loss: 0.004760515133384615\n",
      "Training Loss: 0.005534371706307866\n",
      "Training Loss: 0.006291932631283998\n",
      "Training Loss: 0.006192488025408238\n",
      "Training Loss: 0.006050110962241888\n",
      "Validation Loss: 0.0034520708707733295\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.004886326173436828\n",
      "Training Loss: 0.004680368299013935\n",
      "Training Loss: 0.004754063027212397\n",
      "Training Loss: 0.005526647452497855\n",
      "Training Loss: 0.0062838979228399695\n",
      "Training Loss: 0.006184989699395373\n",
      "Training Loss: 0.006041497961268761\n",
      "Validation Loss: 0.0034442144626584113\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.004878760972060263\n",
      "Training Loss: 0.004673560623195953\n",
      "Training Loss: 0.004747770575340838\n",
      "Training Loss: 0.005519092141767033\n",
      "Training Loss: 0.006276052535977215\n",
      "Training Loss: 0.006177665505092591\n",
      "Training Loss: 0.006033072234713472\n",
      "Validation Loss: 0.0034364732224337253\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.004871345380088314\n",
      "Training Loss: 0.004666918665170669\n",
      "Training Loss: 0.004741629959316924\n",
      "Training Loss: 0.005511697552865371\n",
      "Training Loss: 0.006268389226752333\n",
      "Training Loss: 0.006170512227108702\n",
      "Training Loss: 0.0060248293087352064\n",
      "Validation Loss: 0.003428838074500306\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.004864074236247688\n",
      "Training Loss: 0.004660438404534943\n",
      "Training Loss: 0.004735635060933418\n",
      "Training Loss: 0.00550445910601411\n",
      "Training Loss: 0.006260901890927925\n",
      "Training Loss: 0.00616352331941016\n",
      "Training Loss: 0.006016762360231951\n",
      "Validation Loss: 0.0034213095854563214\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.004856941616744734\n",
      "Training Loss: 0.004654114160803147\n",
      "Training Loss: 0.00472978072182741\n",
      "Training Loss: 0.005497370547964238\n",
      "Training Loss: 0.006253583500511013\n",
      "Training Loss: 0.006156690615462139\n",
      "Training Loss: 0.00600886877742596\n",
      "Validation Loss: 0.0034138835129007166\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.004849940600688569\n",
      "Training Loss: 0.0046479396772338075\n",
      "Training Loss: 0.0047240604436956345\n",
      "Training Loss: 0.005490427286131308\n",
      "Training Loss: 0.00624642898212187\n",
      "Training Loss: 0.006150010385317728\n",
      "Training Loss: 0.006001142496825196\n",
      "Validation Loss: 0.0034065636438195515\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.004843069560592994\n",
      "Training Loss: 0.004641913051600568\n",
      "Training Loss: 0.004718470892403275\n",
      "Training Loss: 0.005483623388572596\n",
      "Training Loss: 0.006239432554575614\n",
      "Training Loss: 0.00614347725873813\n",
      "Training Loss: 0.005993577444460243\n",
      "Validation Loss: 0.003399338376575781\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.004836317979497835\n",
      "Training Loss: 0.004636023368220776\n",
      "Training Loss: 0.004713001517229714\n",
      "Training Loss: 0.005476953241741285\n",
      "Training Loss: 0.006232585995458066\n",
      "Training Loss: 0.006137086076196283\n",
      "Training Loss: 0.005986169285606593\n",
      "Validation Loss: 0.0033922123370434014\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.0048296846292214465\n",
      "Training Loss: 0.004630270712077618\n",
      "Training Loss: 0.004707652826909907\n",
      "Training Loss: 0.005470414599985815\n",
      "Training Loss: 0.006225886446190998\n",
      "Training Loss: 0.006130829975008964\n",
      "Training Loss: 0.005978913865983486\n",
      "Validation Loss: 0.0033851828352436564\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.004823164630215615\n",
      "Training Loss: 0.004624648544122465\n",
      "Training Loss: 0.0047024157945998015\n",
      "Training Loss: 0.005464001517975703\n",
      "Training Loss: 0.006219329897430725\n",
      "Training Loss: 0.006124709076248109\n",
      "Training Loss: 0.00597180595563259\n",
      "Validation Loss: 0.0033782430229471363\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.004816751221660524\n",
      "Training Loss: 0.0046191509038908405\n",
      "Training Loss: 0.004697286555310712\n",
      "Training Loss: 0.005457710523623973\n",
      "Training Loss: 0.00621290753420908\n",
      "Training Loss: 0.006118714126059786\n",
      "Training Loss: 0.0059648406703490765\n",
      "Validation Loss: 0.0033713963535903823\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.004810441926820203\n",
      "Training Loss: 0.004613776017795317\n",
      "Training Loss: 0.004692260904703289\n",
      "Training Loss: 0.00545153685787227\n",
      "Training Loss: 0.006206618895521388\n",
      "Training Loss: 0.006112841251306236\n",
      "Training Loss: 0.005958013905328698\n",
      "Validation Loss: 0.003364642210869809\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.004804233153699897\n",
      "Training Loss: 0.004608519531902857\n",
      "Training Loss: 0.00468733505345881\n",
      "Training Loss: 0.0054454758955398575\n",
      "Training Loss: 0.006200455699581653\n",
      "Training Loss: 0.006107088784920052\n",
      "Training Loss: 0.00595132011862006\n",
      "Validation Loss: 0.0033579734332810683\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.004798119761398993\n",
      "Training Loss: 0.004603374606813304\n",
      "Training Loss: 0.004682503249496222\n",
      "Training Loss: 0.005439525073161349\n",
      "Training Loss: 0.006194416473153979\n",
      "Training Loss: 0.006101450592977926\n",
      "Training Loss: 0.00594475578633137\n",
      "Validation Loss: 0.0033513918315838283\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.004792097735917196\n",
      "Training Loss: 0.004598339271033183\n",
      "Training Loss: 0.0046777610509889204\n",
      "Training Loss: 0.005433680246351286\n",
      "Training Loss: 0.0061884953768458215\n",
      "Training Loss: 0.00609592393389903\n",
      "Training Loss: 0.005938314875820651\n",
      "Validation Loss: 0.003344896796668709\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.004786165044642985\n",
      "Training Loss: 0.004593409537919797\n",
      "Training Loss: 0.004673106098198332\n",
      "Training Loss: 0.005427936966298148\n",
      "Training Loss: 0.006182688539265655\n",
      "Training Loss: 0.0060905047343112525\n",
      "Training Loss: 0.0059319953009253365\n",
      "Validation Loss: 0.0033384855108053078\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.004780317364493385\n",
      "Training Loss: 0.004588581513962709\n",
      "Training Loss: 0.004668533035437576\n",
      "Training Loss: 0.005422293773735873\n",
      "Training Loss: 0.006176992404507473\n",
      "Training Loss: 0.0060851893248036505\n",
      "Training Loss: 0.005925792965572327\n",
      "Validation Loss: 0.0033321543313276844\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.004774552027229219\n",
      "Training Loss: 0.004583849646151066\n",
      "Training Loss: 0.004664037453476339\n",
      "Training Loss: 0.005416747749550268\n",
      "Training Loss: 0.006171403878252022\n",
      "Training Loss: 0.00607997631537728\n",
      "Training Loss: 0.005919703147374094\n",
      "Validation Loss: 0.0033259071605471887\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.004768866645754315\n",
      "Training Loss: 0.004579213794204407\n",
      "Training Loss: 0.004659618809237145\n",
      "Training Loss: 0.005411294387886301\n",
      "Training Loss: 0.006165917533799074\n",
      "Training Loss: 0.006074859100626781\n",
      "Training Loss: 0.005913722148980014\n",
      "Validation Loss: 0.0033197377765662828\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.004763256824226118\n",
      "Training Loss: 0.004574667204287834\n",
      "Training Loss: 0.0046552712551783774\n",
      "Training Loss: 0.005405929424450733\n",
      "Training Loss: 0.006160531170316972\n",
      "Training Loss: 0.006069835778325796\n",
      "Training Loss: 0.005907845504116267\n",
      "Validation Loss: 0.003313647475649955\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.004757722046342679\n",
      "Training Loss: 0.0045702086307574065\n",
      "Training Loss: 0.004650992639944888\n",
      "Training Loss: 0.00540065343840979\n",
      "Training Loss: 0.006155242045642808\n",
      "Training Loss: 0.00606490280595608\n",
      "Training Loss: 0.005902071767486632\n",
      "Validation Loss: 0.003307638864934598\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.004752259870292619\n",
      "Training Loss: 0.004565835082903504\n",
      "Training Loss: 0.004646781772607938\n",
      "Training Loss: 0.00539546103507746\n",
      "Training Loss: 0.006150045602116734\n",
      "Training Loss: 0.006060058032162487\n",
      "Training Loss: 0.005896396948373877\n",
      "Validation Loss: 0.0033017058546344438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.004746867963694968\n",
      "Training Loss: 0.004561542591545731\n",
      "Training Loss: 0.004642633732873947\n",
      "Training Loss: 0.005390352430986241\n",
      "Training Loss: 0.006144939291989431\n",
      "Training Loss: 0.006055298268329352\n",
      "Training Loss: 0.005890816691680812\n",
      "Validation Loss: 0.0032958499014670594\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0047415458463365215\n",
      "Training Loss: 0.004557330617681146\n",
      "Training Loss: 0.004638547317590565\n",
      "Training Loss: 0.005385322583606467\n",
      "Training Loss: 0.006139921193243936\n",
      "Training Loss: 0.006050620496971533\n",
      "Training Loss: 0.005885329664452001\n",
      "Validation Loss: 0.0032900700418449494\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.004736288421554491\n",
      "Training Loss: 0.00455319244065322\n",
      "Training Loss: 0.00463451820134651\n",
      "Training Loss: 0.005380370007478632\n",
      "Training Loss: 0.00613498835999053\n",
      "Training Loss: 0.006046023197704926\n",
      "Training Loss: 0.005879931976669468\n",
      "Validation Loss: 0.003284365989961958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.004731097220210359\n",
      "Training Loss: 0.004549128182698041\n",
      "Training Loss: 0.004630545498221181\n",
      "Training Loss: 0.0053754927712725475\n",
      "Training Loss: 0.006130138335283846\n",
      "Training Loss: 0.006041503212181851\n",
      "Training Loss: 0.005874622115516104\n",
      "Validation Loss: 0.0032787349932715437\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.0047259674995439125\n",
      "Training Loss: 0.004545133156934753\n",
      "Training Loss: 0.004626626914250665\n",
      "Training Loss: 0.005370687625254504\n",
      "Training Loss: 0.006125367737840861\n",
      "Training Loss: 0.0060370575392153115\n",
      "Training Loss: 0.005869395812042057\n",
      "Validation Loss: 0.0032731804812295632\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.004720901582040824\n",
      "Training Loss: 0.00454120690759737\n",
      "Training Loss: 0.00462275976780802\n",
      "Training Loss: 0.0053659543569665406\n",
      "Training Loss: 0.006120674966950901\n",
      "Training Loss: 0.006032683711964637\n",
      "Training Loss: 0.005864251519087702\n",
      "Validation Loss: 0.0032676969868940223\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.004715895485132932\n",
      "Training Loss: 0.004537344962009229\n",
      "Training Loss: 0.004618943029781803\n",
      "Training Loss: 0.0053612890833755955\n",
      "Training Loss: 0.0061160569754429165\n",
      "Training Loss: 0.006028379532508552\n",
      "Training Loss: 0.005859185884473845\n",
      "Validation Loss: 0.0032622918263356783\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0047109511744929475\n",
      "Training Loss: 0.0045335478452034296\n",
      "Training Loss: 0.00461517644347623\n",
      "Training Loss: 0.00535669035743922\n",
      "Training Loss: 0.006111511347116902\n",
      "Training Loss: 0.006024140595691278\n",
      "Training Loss: 0.005854196879663504\n",
      "Validation Loss: 0.0032569622860311037\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.004706065824721008\n",
      "Training Loss: 0.004529811711399816\n",
      "Training Loss: 0.004611456128768623\n",
      "Training Loss: 0.0053521572606405245\n",
      "Training Loss: 0.006107037356123328\n",
      "Training Loss: 0.0060199695429764685\n",
      "Training Loss: 0.005849283401621505\n",
      "Validation Loss: 0.0032517019378488763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.004701236590626649\n",
      "Training Loss: 0.004526133123436011\n",
      "Training Loss: 0.0046077811130089685\n",
      "Training Loss: 0.00534768576559145\n",
      "Training Loss: 0.006102632667170838\n",
      "Training Loss: 0.006015860114712268\n",
      "Training Loss: 0.005844443174428306\n",
      "Validation Loss: 0.003246516151282211\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.004696464478620328\n",
      "Training Loss: 0.004522511764080263\n",
      "Training Loss: 0.004604149954393506\n",
      "Training Loss: 0.005343276216299273\n",
      "Training Loss: 0.006098295842530205\n",
      "Training Loss: 0.006011812840588391\n",
      "Training Loss: 0.005839672303991393\n",
      "Validation Loss: 0.0032414025903944525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.00469174918893259\n",
      "Training Loss: 0.004518945557065308\n",
      "Training Loss: 0.004600559709942899\n",
      "Training Loss: 0.005338927197735757\n",
      "Training Loss: 0.006094023695331998\n",
      "Training Loss: 0.006007824444677681\n",
      "Training Loss: 0.005834968262352049\n",
      "Validation Loss: 0.0032363646172775366\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.004687089272192679\n",
      "Training Loss: 0.004515432274783962\n",
      "Training Loss: 0.004597011914011091\n",
      "Training Loss: 0.005334636454936117\n",
      "Training Loss: 0.00608981374767609\n",
      "Training Loss: 0.006003892212174833\n",
      "Training Loss: 0.005830332151381299\n",
      "Validation Loss: 0.0032313973653785998\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.0046824834117433056\n",
      "Training Loss: 0.004511969375889748\n",
      "Training Loss: 0.004593504362273962\n",
      "Training Loss: 0.0053304008959094065\n",
      "Training Loss: 0.006085666717845015\n",
      "Training Loss: 0.006000015960307792\n",
      "Training Loss: 0.005825760015868582\n",
      "Validation Loss: 0.0032265044758375385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.00467793250340037\n",
      "Training Loss: 0.00450855640869122\n",
      "Training Loss: 0.004590035002329387\n",
      "Training Loss: 0.005326220960705541\n",
      "Training Loss: 0.006081580315367319\n",
      "Training Loss: 0.005996193200116977\n",
      "Training Loss: 0.005821249457076192\n",
      "Validation Loss: 0.003221681230563726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.004673434220021591\n",
      "Training Loss: 0.004505190831841901\n",
      "Training Loss: 0.004586603710195049\n",
      "Training Loss: 0.005322095871670172\n",
      "Training Loss: 0.0060775514831766485\n",
      "Training Loss: 0.005992421979317442\n",
      "Training Loss: 0.005816801855689846\n",
      "Validation Loss: 0.0032169302308073873\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.004668988536577672\n",
      "Training Loss: 0.004501871334505267\n",
      "Training Loss: 0.004583209881093353\n",
      "Training Loss: 0.0053180211904691535\n",
      "Training Loss: 0.0060735806747106835\n",
      "Training Loss: 0.005988700205925852\n",
      "Training Loss: 0.005812411186052486\n",
      "Validation Loss: 0.0032122529808114447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.0046645947167417035\n",
      "Training Loss: 0.004498595758341253\n",
      "Training Loss: 0.004579850442823954\n",
      "Training Loss: 0.005313998205238022\n",
      "Training Loss: 0.006069664030219428\n",
      "Training Loss: 0.0059850279637612405\n",
      "Training Loss: 0.005808077587280422\n",
      "Validation Loss: 0.003207644143347604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.004660251175519079\n",
      "Training Loss: 0.004495361944427714\n",
      "Training Loss: 0.004576525940792635\n",
      "Training Loss: 0.005310024002683349\n",
      "Training Loss: 0.006065803078818135\n",
      "Training Loss: 0.005981402263278142\n",
      "Training Loss: 0.00580380049708765\n",
      "Validation Loss: 0.003203105107115491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.004655958529328927\n",
      "Training Loss: 0.0044921711907954884\n",
      "Training Loss: 0.004573236365104094\n",
      "Training Loss: 0.005306097390712239\n",
      "Training Loss: 0.006061993956682272\n",
      "Training Loss: 0.00597782070748508\n",
      "Training Loss: 0.005799577522557229\n",
      "Validation Loss: 0.003198638340817548\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.004651715907966718\n",
      "Training Loss: 0.004489019340835512\n",
      "Training Loss: 0.004569979834486731\n",
      "Training Loss: 0.005302217607968487\n",
      "Training Loss: 0.00605823572142981\n",
      "Training Loss: 0.0059742833185009656\n",
      "Training Loss: 0.0057954072282882406\n",
      "Validation Loss: 0.003194240111097247\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.004647522662999108\n",
      "Training Loss: 0.004485905499313958\n",
      "Training Loss: 0.004566754782572389\n",
      "Training Loss: 0.0052983838162617754\n",
      "Training Loss: 0.0060545283195097\n",
      "Training Loss: 0.005970787950791419\n",
      "Training Loss: 0.005791287634056061\n",
      "Validation Loss: 0.0031899096119855897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.0046433776302728805\n",
      "Training Loss: 0.00448282877157908\n",
      "Training Loss: 0.00456356160226278\n",
      "Training Loss: 0.005294594120350667\n",
      "Training Loss: 0.00605086941504851\n",
      "Training Loss: 0.0059673348325304685\n",
      "Training Loss: 0.005787218082696199\n",
      "Validation Loss: 0.0031856462093024806\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.004639280405826867\n",
      "Training Loss: 0.004479788085445762\n",
      "Training Loss: 0.004560399343026802\n",
      "Training Loss: 0.005290848775184713\n",
      "Training Loss: 0.006047258211183361\n",
      "Training Loss: 0.005963919865898788\n",
      "Training Loss: 0.005783197103883139\n",
      "Validation Loss: 0.0031814505653508175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.0046352299820864576\n",
      "Training Loss: 0.004476782055571675\n",
      "Training Loss: 0.004557268671924249\n",
      "Training Loss: 0.005287144554895349\n",
      "Training Loss: 0.0060436921793734655\n",
      "Training Loss: 0.005960543593391776\n",
      "Training Loss: 0.00577922223310452\n",
      "Validation Loss: 0.0031773228979188155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.004631227271747775\n",
      "Training Loss: 0.004473810524796135\n",
      "Training Loss: 0.00455416721815709\n",
      "Training Loss: 0.005283480838988908\n",
      "Training Loss: 0.006040171763743274\n",
      "Training Loss: 0.0059572036762256175\n",
      "Training Loss: 0.005775293081533164\n",
      "Validation Loss: 0.00317326062014466\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.004627269998891279\n",
      "Training Loss: 0.004470870439545252\n",
      "Training Loss: 0.004551093874615617\n",
      "Training Loss: 0.005279857420246117\n",
      "Training Loss: 0.006036694523063488\n",
      "Training Loss: 0.005953899869928137\n",
      "Training Loss: 0.005771407906431705\n",
      "Validation Loss: 0.003169260889881252\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.004623357409145683\n",
      "Training Loss: 0.004467962304479442\n",
      "Training Loss: 0.004548048891010694\n",
      "Training Loss: 0.005276273470371962\n",
      "Training Loss: 0.006033261165139265\n",
      "Training Loss: 0.005950631194282323\n",
      "Training Loss: 0.005767566471477039\n",
      "Validation Loss: 0.0031653253166685196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.004619489283650182\n",
      "Training Loss: 0.004465084750554524\n",
      "Training Loss: 0.004545033404720016\n",
      "Training Loss: 0.00527272597362753\n",
      "Training Loss: 0.006029869079357013\n",
      "Training Loss: 0.005947396972915158\n",
      "Training Loss: 0.0057637657527811826\n",
      "Validation Loss: 0.0031614523291176233\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.004615665293531492\n",
      "Training Loss: 0.004462235934915952\n",
      "Training Loss: 0.004542042857501656\n",
      "Training Loss: 0.005269217392778956\n",
      "Training Loss: 0.006026518043945543\n",
      "Training Loss: 0.005944193642353639\n",
      "Training Loss: 0.00576000749017112\n",
      "Validation Loss: 0.003157641307872891\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.004611883679754101\n",
      "Training Loss: 0.0044594160368433226\n",
      "Training Loss: 0.004539080473477952\n",
      "Training Loss: 0.005265743082854896\n",
      "Training Loss: 0.006023205452947877\n",
      "Training Loss: 0.005941021952312439\n",
      "Training Loss: 0.005756287935655564\n",
      "Validation Loss: 0.0031538916728197693\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.004608145942329429\n",
      "Training Loss: 0.004456623287405819\n",
      "Training Loss: 0.004536145112942904\n",
      "Training Loss: 0.005262304507778026\n",
      "Training Loss: 0.006019931734190322\n",
      "Training Loss: 0.005937881666468457\n",
      "Training Loss: 0.005752607416943647\n",
      "Validation Loss: 0.0031502034383466683\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.0046044502162840215\n",
      "Training Loss: 0.00445385848346632\n",
      "Training Loss: 0.004533235081471503\n",
      "Training Loss: 0.005258900549961254\n",
      "Training Loss: 0.006016696225851774\n",
      "Training Loss: 0.005934770540334284\n",
      "Training Loss: 0.005748964045778848\n",
      "Validation Loss: 0.003146574117874636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.004600795612786896\n",
      "Training Loss: 0.0044511200854321946\n",
      "Training Loss: 0.0045303508994402365\n",
      "Training Loss: 0.005255529538844712\n",
      "Training Loss: 0.006013497163075953\n",
      "Training Loss: 0.005931687839329243\n",
      "Training Loss: 0.005745358397834934\n",
      "Validation Loss: 0.003143001384841285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.004597181328572333\n",
      "Training Loss: 0.004448406269075349\n",
      "Training Loss: 0.004527491059852764\n",
      "Training Loss: 0.005252192197949626\n",
      "Training Loss: 0.006010333728627301\n",
      "Training Loss: 0.005928632635623217\n",
      "Training Loss: 0.005741788059240207\n",
      "Validation Loss: 0.0031394842331479078\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.004593606962589547\n",
      "Training Loss: 0.004445716901682317\n",
      "Training Loss: 0.004524655640707352\n",
      "Training Loss: 0.0052488863252801825\n",
      "Training Loss: 0.006007206566864625\n",
      "Training Loss: 0.005925606157397852\n",
      "Training Loss: 0.005738251730799675\n",
      "Validation Loss: 0.003136023534819386\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.004590072090504691\n",
      "Training Loss: 0.004443052463466301\n",
      "Training Loss: 0.0045218461833428595\n",
      "Training Loss: 0.005245610472047702\n",
      "Training Loss: 0.0060041129920864475\n",
      "Training Loss: 0.005922603834187612\n",
      "Training Loss: 0.005734750863630325\n",
      "Validation Loss: 0.0031326178004372207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.00458657541545108\n",
      "Training Loss: 0.004440410024835728\n",
      "Training Loss: 0.004519058809382841\n",
      "Training Loss: 0.00524236609635409\n",
      "Training Loss: 0.00600105227320455\n",
      "Training Loss: 0.005919627648545429\n",
      "Training Loss: 0.005731283182394691\n",
      "Validation Loss: 0.0031292676447789544\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.004583117761649191\n",
      "Training Loss: 0.004437791520031169\n",
      "Training Loss: 0.004516295501380227\n",
      "Training Loss: 0.005239151279674843\n",
      "Training Loss: 0.00599802409473341\n",
      "Training Loss: 0.0059166759182699025\n",
      "Training Loss: 0.0057278463389957325\n",
      "Validation Loss: 0.003125969801893395\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.00457969777577091\n",
      "Training Loss: 0.004435195063706487\n",
      "Training Loss: 0.004513555749435909\n",
      "Training Loss: 0.005235964668099768\n",
      "Training Loss: 0.005995027671451681\n",
      "Training Loss: 0.0059137475583702324\n",
      "Training Loss: 0.005724442460341379\n",
      "Validation Loss: 0.0031227241974404216\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.004576314160367474\n",
      "Training Loss: 0.004432620746665634\n",
      "Training Loss: 0.0045108381396858025\n",
      "Training Loss: 0.005232807417633012\n",
      "Training Loss: 0.00599206336773932\n",
      "Training Loss: 0.005910843847086653\n",
      "Training Loss: 0.00572106905921828\n",
      "Validation Loss: 0.003119527986220848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.004572967081330716\n",
      "Training Loss: 0.0044300674117403105\n",
      "Training Loss: 0.004508143062121234\n",
      "Training Loss: 0.005229678721516393\n",
      "Training Loss: 0.005989129117806442\n",
      "Training Loss: 0.00590796294156462\n",
      "Training Loss: 0.0057177261967444794\n",
      "Validation Loss: 0.00311638110217879\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.004569655765662901\n",
      "Training Loss: 0.004427534286514856\n",
      "Training Loss: 0.0045054705522488804\n",
      "Training Loss: 0.005226576285785996\n",
      "Training Loss: 0.005986224196967669\n",
      "Training Loss: 0.005905103648547083\n",
      "Training Loss: 0.005714412090019323\n",
      "Validation Loss: 0.003113287102957762\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.004566380103351548\n",
      "Training Loss: 0.004425021394272335\n",
      "Training Loss: 0.0045028200704837215\n",
      "Training Loss: 0.005223500055144541\n",
      "Training Loss: 0.005983347977744416\n",
      "Training Loss: 0.005902265122858808\n",
      "Training Loss: 0.00571112672041636\n",
      "Validation Loss: 0.003110236541258192\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.004563137079239823\n",
      "Training Loss: 0.004422528225695714\n",
      "Training Loss: 0.004500190773978829\n",
      "Training Loss: 0.005220449967891909\n",
      "Training Loss: 0.005980500843143091\n",
      "Training Loss: 0.0058994480082765225\n",
      "Training Loss: 0.005707870459300466\n",
      "Validation Loss: 0.003107233834758136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.004559929425013252\n",
      "Training Loss: 0.004420054427464492\n",
      "Training Loss: 0.004497582636540756\n",
      "Training Loss: 0.005217425462906249\n",
      "Training Loss: 0.0059776811097981405\n",
      "Training Loss: 0.005896651385119185\n",
      "Training Loss: 0.005704642787459306\n",
      "Validation Loss: 0.003104278488583472\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.004556754613295197\n",
      "Training Loss: 0.004417599702137522\n",
      "Training Loss: 0.004494994524284266\n",
      "Training Loss: 0.005214426068705507\n",
      "Training Loss: 0.005974889063509181\n",
      "Training Loss: 0.005893875285983086\n",
      "Training Loss: 0.005701440595439635\n",
      "Validation Loss: 0.0031013689490038666\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.004553613759344444\n",
      "Training Loss: 0.004415163459489122\n",
      "Training Loss: 0.004492428074590862\n",
      "Training Loss: 0.005211449424969032\n",
      "Training Loss: 0.005972121804952621\n",
      "Training Loss: 0.005891116439597681\n",
      "Training Loss: 0.005698265508981421\n",
      "Validation Loss: 0.00309850185763349\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.004550503162899986\n",
      "Training Loss: 0.004412745128502138\n",
      "Training Loss: 0.004489882241468877\n",
      "Training Loss: 0.005208498624851927\n",
      "Training Loss: 0.0059693826118018475\n",
      "Training Loss: 0.005888378209201619\n",
      "Training Loss: 0.005695117574650794\n",
      "Validation Loss: 0.003095678812239933\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.0045474263018695634\n",
      "Training Loss: 0.004410345774958841\n",
      "Training Loss: 0.0044873562885913995\n",
      "Training Loss: 0.005205569921527058\n",
      "Training Loss: 0.005966668645269238\n",
      "Training Loss: 0.005885657668113708\n",
      "Training Loss: 0.005691994493827224\n",
      "Validation Loss: 0.003092898787975953\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.004544380195438862\n",
      "Training Loss: 0.004407962849945761\n",
      "Training Loss: 0.004484849499422125\n",
      "Training Loss: 0.005202664327225648\n",
      "Training Loss: 0.00596397916553542\n",
      "Training Loss: 0.005882955626584589\n",
      "Training Loss: 0.00568889691028744\n",
      "Validation Loss: 0.003090157922861351\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.004541364664328284\n",
      "Training Loss: 0.0044055973878130315\n",
      "Training Loss: 0.004482363403658382\n",
      "Training Loss: 0.005199780527036637\n",
      "Training Loss: 0.005961314614396542\n",
      "Training Loss: 0.005880272116046399\n",
      "Training Loss: 0.005685823611565865\n",
      "Validation Loss: 0.0030874603653365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.004538379501318559\n",
      "Training Loss: 0.004403249272727407\n",
      "Training Loss: 0.004479895290569402\n",
      "Training Loss: 0.005196919330628589\n",
      "Training Loss: 0.0059586735692573715\n",
      "Training Loss: 0.0058776039129588755\n",
      "Training Loss: 0.005682774894521572\n",
      "Validation Loss: 0.003084802609425359\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.004535423828056082\n",
      "Training Loss: 0.004400917322491296\n",
      "Training Loss: 0.004477446842356585\n",
      "Training Loss: 0.005194080074434168\n",
      "Training Loss: 0.005956057418370619\n",
      "Training Loss: 0.0058749545086175204\n",
      "Training Loss: 0.00567974996753037\n",
      "Validation Loss: 0.003082183610442119\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.004532498079352081\n",
      "Training Loss: 0.004398602718138136\n",
      "Training Loss: 0.004475017056101933\n",
      "Training Loss: 0.005191260776482523\n",
      "Training Loss: 0.005953462352044881\n",
      "Training Loss: 0.005872319639893249\n",
      "Training Loss: 0.0056767494732048365\n",
      "Validation Loss: 0.0030796040191353484\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.004529600877431221\n",
      "Training Loss: 0.00439630362612661\n",
      "Training Loss: 0.004472606331692077\n",
      "Training Loss: 0.005188462931546383\n",
      "Training Loss: 0.005950891358079389\n",
      "Training Loss: 0.005869702299823984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [38:04<09:34, 287.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.005673770505236462\n",
      "Validation Loss: 0.003077060654576282\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.08186990931630135\n",
      "Training Loss: 0.07225333360955119\n",
      "Training Loss: 0.0695342435874045\n",
      "Training Loss: 0.06857612252235412\n",
      "Training Loss: 0.06638177376240492\n",
      "Training Loss: 0.06599279431626201\n",
      "Training Loss: 0.06382682843133808\n",
      "Validation Loss: 0.06238908129517505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06182435423135758\n",
      "Training Loss: 0.05850298222154379\n",
      "Training Loss: 0.056260238960385324\n",
      "Training Loss: 0.05460201658308506\n",
      "Training Loss: 0.05093302519991994\n",
      "Training Loss: 0.04882427457720041\n",
      "Training Loss: 0.04500012054108083\n",
      "Validation Loss: 0.04244404360093874\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.04126772405579686\n",
      "Training Loss: 0.037757033919915556\n",
      "Training Loss: 0.035019482327625154\n",
      "Training Loss: 0.03391664890572429\n",
      "Training Loss: 0.03135335032828152\n",
      "Training Loss: 0.030256445193663238\n",
      "Training Loss: 0.028111258768476546\n",
      "Validation Loss: 0.02644133164236943\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.026161775896325706\n",
      "Training Loss: 0.024775241678580643\n",
      "Training Loss: 0.023427160657010972\n",
      "Training Loss: 0.02366158685646951\n",
      "Training Loss: 0.022476434810087084\n",
      "Training Loss: 0.021945680733770134\n",
      "Training Loss: 0.020711439475417138\n",
      "Validation Loss: 0.01924513955862167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.01935863299295306\n",
      "Training Loss: 0.018534320476464927\n",
      "Training Loss: 0.01768477130215615\n",
      "Training Loss: 0.018264225274324416\n",
      "Training Loss: 0.017832703082822265\n",
      "Training Loss: 0.01737944473978132\n",
      "Training Loss: 0.0165597923938185\n",
      "Validation Loss: 0.015003900844185205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.015418721260502935\n",
      "Training Loss: 0.014760930743068456\n",
      "Training Loss: 0.014181691645644605\n",
      "Training Loss: 0.01489978295750916\n",
      "Training Loss: 0.014973092162981629\n",
      "Training Loss: 0.014531860179267823\n",
      "Training Loss: 0.013926155853550882\n",
      "Validation Loss: 0.012178018799030714\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.012875807990785687\n",
      "Training Loss: 0.012280886168591678\n",
      "Training Loss: 0.011885843272320926\n",
      "Training Loss: 0.012707977830432355\n",
      "Training Loss: 0.013141269432380795\n",
      "Training Loss: 0.012726494270609692\n",
      "Training Loss: 0.012249509401153774\n",
      "Validation Loss: 0.010269414265517996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.011224982852581888\n",
      "Training Loss: 0.010670014900388197\n",
      "Training Loss: 0.010409493740880862\n",
      "Training Loss: 0.011331318451557309\n",
      "Training Loss: 0.012014721215236931\n",
      "Training Loss: 0.011641362080117687\n",
      "Training Loss: 0.011249473694479093\n",
      "Validation Loss: 0.009027921462131574\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.010198112315265462\n",
      "Training Loss: 0.009675044286996125\n",
      "Training Loss: 0.00950625280267559\n",
      "Training Loss: 0.010513079477241262\n",
      "Training Loss: 0.0113543532602489\n",
      "Training Loss: 0.011021636690711602\n",
      "Training Loss: 0.010684778451686725\n",
      "Validation Loss: 0.008241982767811875\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.009573619026923552\n",
      "Training Loss: 0.009073769784299657\n",
      "Training Loss: 0.008961029929341748\n",
      "Training Loss: 0.010028205334674568\n",
      "Training Loss: 0.010957427965477108\n",
      "Training Loss: 0.010655353976180776\n",
      "Training Loss: 0.010353033290011809\n",
      "Validation Loss: 0.0077347767718959504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.009175740773789584\n",
      "Training Loss: 0.008690395099110902\n",
      "Training Loss: 0.00860953378956765\n",
      "Training Loss: 0.009711775988107548\n",
      "Training Loss: 0.01068274857942015\n",
      "Training Loss: 0.01040055584628135\n",
      "Training Loss: 0.010120988906128331\n",
      "Validation Loss: 0.0073830313650953975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.00888935052556917\n",
      "Training Loss: 0.008411582333501429\n",
      "Training Loss: 0.008348728842101992\n",
      "Training Loss: 0.009464877875288949\n",
      "Training Loss: 0.010449332732241601\n",
      "Training Loss: 0.01017853440484032\n",
      "Training Loss: 0.009916370359715074\n",
      "Validation Loss: 0.007108869866471501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.008647003689548\n",
      "Training Loss: 0.008172270397190005\n",
      "Training Loss: 0.008119884942425415\n",
      "Training Loss: 0.009233207931974902\n",
      "Training Loss: 0.01021396903670393\n",
      "Training Loss: 0.009948221490485593\n",
      "Training Loss: 0.00970303055481054\n",
      "Validation Loss: 0.006862527752741613\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.008408651697682217\n",
      "Training Loss: 0.00793513009790331\n",
      "Training Loss: 0.007888898974051699\n",
      "Training Loss: 0.008986547570675612\n",
      "Training Loss: 0.009953523857984693\n",
      "Training Loss: 0.009690215318696573\n",
      "Training Loss: 0.009468718686839565\n",
      "Validation Loss: 0.006618830219280909\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.008157038863864728\n",
      "Training Loss: 0.007687987614190206\n",
      "Training Loss: 0.007647321703843773\n",
      "Training Loss: 0.008724150599446147\n",
      "Training Loss: 0.009677321749040856\n",
      "Training Loss: 0.009423058410175145\n",
      "Training Loss: 0.009238924901001155\n",
      "Validation Loss: 0.006397790758615911\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.007917307481984609\n",
      "Training Loss: 0.00745983186759986\n",
      "Training Loss: 0.007426702580414712\n",
      "Training Loss: 0.008486899104900658\n",
      "Training Loss: 0.009436558271991089\n",
      "Training Loss: 0.009197467665653675\n",
      "Training Loss: 0.009054379895096644\n",
      "Validation Loss: 0.006229172737166732\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.0077261551312403755\n",
      "Training Loss: 0.0072814038884826005\n",
      "Training Loss: 0.007255559698678553\n",
      "Training Loss: 0.008303722807904706\n",
      "Training Loss: 0.009260079069063067\n",
      "Training Loss: 0.009030629246262833\n",
      "Training Loss: 0.008920335937291383\n",
      "Validation Loss: 0.006104610650189751\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.007583317010430619\n",
      "Training Loss: 0.007149241623701528\n",
      "Training Loss: 0.007130404952913522\n",
      "Training Loss: 0.00816784972907044\n",
      "Training Loss: 0.009136612876318395\n",
      "Training Loss: 0.008909210008569061\n",
      "Training Loss: 0.00882228916278109\n",
      "Validation Loss: 0.006006733605931147\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.007473933239234612\n",
      "Training Loss: 0.007049313738243654\n",
      "Training Loss: 0.007037380501860753\n",
      "Training Loss: 0.008063644773792476\n",
      "Training Loss: 0.009046784806996585\n",
      "Training Loss: 0.008817255134927109\n",
      "Training Loss: 0.008746894946089014\n",
      "Validation Loss: 0.00592476024627267\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.007386062820442021\n",
      "Training Loss: 0.0069702714879531415\n",
      "Training Loss: 0.006964635332114995\n",
      "Training Loss: 0.007979219087283128\n",
      "Training Loss: 0.008976484237937257\n",
      "Training Loss: 0.008743324516108259\n",
      "Training Loss: 0.008685300459619611\n",
      "Validation Loss: 0.0058530465148612576\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.007311964601976797\n",
      "Training Loss: 0.006904585669981316\n",
      "Training Loss: 0.006904304827330634\n",
      "Training Loss: 0.00790721089579165\n",
      "Training Loss: 0.008917297852458433\n",
      "Training Loss: 0.008680208732839674\n",
      "Training Loss: 0.008631992362206803\n",
      "Validation Loss: 0.005788530952956402\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.007246829499490559\n",
      "Training Loss: 0.00684756490169093\n",
      "Training Loss: 0.006851642691763118\n",
      "Training Loss: 0.00784320262260735\n",
      "Training Loss: 0.008864494049921632\n",
      "Training Loss: 0.008623574152588845\n",
      "Training Loss: 0.008583595334785058\n",
      "Validation Loss: 0.005729358593587962\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.00718768953694962\n",
      "Training Loss: 0.0067963376885745675\n",
      "Training Loss: 0.006803862346569076\n",
      "Training Loss: 0.007784502170979976\n",
      "Training Loss: 0.008815432485425845\n",
      "Training Loss: 0.008570878901518881\n",
      "Training Loss: 0.008538080535363406\n",
      "Validation Loss: 0.0056743094338320395\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.007132706204429269\n",
      "Training Loss: 0.006749147864757106\n",
      "Training Loss: 0.006759328658226878\n",
      "Training Loss: 0.007729453496285715\n",
      "Training Loss: 0.00876864793128334\n",
      "Training Loss: 0.008520676128100604\n",
      "Training Loss: 0.008494272460229695\n",
      "Validation Loss: 0.005622575026396174\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.007080753264017403\n",
      "Training Loss: 0.006704934432636946\n",
      "Training Loss: 0.006717092848848551\n",
      "Training Loss: 0.007677060645073652\n",
      "Training Loss: 0.008723365702899173\n",
      "Training Loss: 0.008472199731040747\n",
      "Training Loss: 0.008451563448179513\n",
      "Validation Loss: 0.005573644767574659\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007031167519744486\n",
      "Training Loss: 0.006663077676203102\n",
      "Training Loss: 0.006676636050688103\n",
      "Training Loss: 0.0076267769874539225\n",
      "Training Loss: 0.008679249567212537\n",
      "Training Loss: 0.008425141067709774\n",
      "Training Loss: 0.00840972493751906\n",
      "Validation Loss: 0.005527225126379801\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.006983596963109449\n",
      "Training Loss: 0.0066232456045690925\n",
      "Training Loss: 0.006637718005804345\n",
      "Training Loss: 0.007578351879492402\n",
      "Training Loss: 0.008636242781067267\n",
      "Training Loss: 0.008379480490693822\n",
      "Training Loss: 0.008368783189216628\n",
      "Validation Loss: 0.0054831882391111015\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.006937902217032388\n",
      "Training Loss: 0.006585294156102464\n",
      "Training Loss: 0.006600282688159495\n",
      "Training Loss: 0.007531738879624755\n",
      "Training Loss: 0.008594467031070963\n",
      "Training Loss: 0.008335370593704284\n",
      "Training Loss: 0.008328905330272392\n",
      "Validation Loss: 0.0054414963710718274\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.006894065907690674\n",
      "Training Loss: 0.006549187960335985\n",
      "Training Loss: 0.006564374194713309\n",
      "Training Loss: 0.0074869984353426846\n",
      "Training Loss: 0.008554132702993229\n",
      "Training Loss: 0.008293049680069088\n",
      "Training Loss: 0.008290331099415198\n",
      "Validation Loss: 0.005402153133753031\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006852133768843487\n",
      "Training Loss: 0.006514940785709768\n",
      "Training Loss: 0.00653007909655571\n",
      "Training Loss: 0.007444239676697179\n",
      "Training Loss: 0.008515464349184185\n",
      "Training Loss: 0.008252757714362815\n",
      "Training Loss: 0.0082532931456808\n",
      "Validation Loss: 0.005365160887290057\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.00681216357043013\n",
      "Training Loss: 0.006482576422858983\n",
      "Training Loss: 0.006497477353550493\n",
      "Training Loss: 0.0074035667616408314\n",
      "Training Loss: 0.008478660845430568\n",
      "Training Loss: 0.008214695658534766\n",
      "Training Loss: 0.00821798163233325\n",
      "Validation Loss: 0.005330496223918982\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.0067742068495135754\n",
      "Training Loss: 0.006452112586703152\n",
      "Training Loss: 0.0064666266739368435\n",
      "Training Loss: 0.007365060430602171\n",
      "Training Loss: 0.008443861227715389\n",
      "Training Loss: 0.008178991534514353\n",
      "Training Loss: 0.008184521292569115\n",
      "Validation Loss: 0.005298102660408097\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.006738283808808774\n",
      "Training Loss: 0.006423542571137659\n",
      "Training Loss: 0.006437544910004363\n",
      "Training Loss: 0.007328760075615719\n",
      "Training Loss: 0.008411135143833235\n",
      "Training Loss: 0.00814570435322821\n",
      "Training Loss: 0.00815297027816996\n",
      "Validation Loss: 0.005267904464884448\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006704398571746424\n",
      "Training Loss: 0.0063968419411685315\n",
      "Training Loss: 0.006410219013923779\n",
      "Training Loss: 0.007294671402778476\n",
      "Training Loss: 0.008380494305165485\n",
      "Training Loss: 0.008114827915560454\n",
      "Training Loss: 0.008123328830115496\n",
      "Validation Loss: 0.005239808914070244\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006672536877449602\n",
      "Training Loss: 0.00637197911797557\n",
      "Training Loss: 0.0063846159330569205\n",
      "Training Loss: 0.0072627696202835065\n",
      "Training Loss: 0.008351895288797095\n",
      "Training Loss: 0.008086296032415704\n",
      "Training Loss: 0.008095555024920032\n",
      "Validation Loss: 0.00521371410226568\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.006642667343840003\n",
      "Training Loss: 0.006348902892787009\n",
      "Training Loss: 0.0063606796937529\n",
      "Training Loss: 0.007233005947782658\n",
      "Training Loss: 0.008325266134925187\n",
      "Training Loss: 0.00806001330493018\n",
      "Training Loss: 0.008069579630391671\n",
      "Validation Loss: 0.005189506534560259\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006614746939158067\n",
      "Training Loss: 0.006327556310570799\n",
      "Training Loss: 0.006338348714634776\n",
      "Training Loss: 0.007205315231112763\n",
      "Training Loss: 0.00830050471937284\n",
      "Training Loss: 0.00803585465415381\n",
      "Training Loss: 0.008045309201115743\n",
      "Validation Loss: 0.005167080096862094\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006588719175197184\n",
      "Training Loss: 0.006307868994190358\n",
      "Training Loss: 0.006317549905506894\n",
      "Training Loss: 0.007179609971935861\n",
      "Training Loss: 0.008277491488261148\n",
      "Training Loss: 0.00801367123844102\n",
      "Training Loss: 0.00802264140569605\n",
      "Validation Loss: 0.005146311731199814\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006564511202159337\n",
      "Training Loss: 0.006289756404003128\n",
      "Training Loss: 0.006298199967714027\n",
      "Training Loss: 0.0071557904407382016\n",
      "Training Loss: 0.008256098928395659\n",
      "Training Loss: 0.007993303685216233\n",
      "Training Loss: 0.008001460147788748\n",
      "Validation Loss: 0.005127079938346527\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006542031796416267\n",
      "Training Loss: 0.0062731191236525775\n",
      "Training Loss: 0.006280206715455278\n",
      "Training Loss: 0.0071337406011298295\n",
      "Training Loss: 0.00823619004455395\n",
      "Training Loss: 0.00797458503046073\n",
      "Training Loss: 0.007981644627870993\n",
      "Validation Loss: 0.005109251033835643\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006521177195827477\n",
      "Training Loss: 0.006257844626670703\n",
      "Training Loss: 0.006263470243429765\n",
      "Training Loss: 0.007113331484724768\n",
      "Training Loss: 0.008217625226825476\n",
      "Training Loss: 0.007957342722220347\n",
      "Training Loss: 0.007963072396814824\n",
      "Validation Loss: 0.005092690741095678\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006501830560155213\n",
      "Training Loss: 0.006243815082707442\n",
      "Training Loss: 0.00624788909801282\n",
      "Training Loss: 0.0070944313146173955\n",
      "Training Loss: 0.008200269285589456\n",
      "Training Loss: 0.007941409245831891\n",
      "Training Loss: 0.007945618636440485\n",
      "Validation Loss: 0.005077269315468461\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006483869144576601\n",
      "Training Loss: 0.006230905054835602\n",
      "Training Loss: 0.006233356323791668\n",
      "Training Loss: 0.007076898775412701\n",
      "Training Loss: 0.008183986302465201\n",
      "Training Loss: 0.007926620525540783\n",
      "Training Loss: 0.007929161443607882\n",
      "Validation Loss: 0.005062858138079947\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006467165219364687\n",
      "Training Loss: 0.006218986257445067\n",
      "Training Loss: 0.006219763496192172\n",
      "Training Loss: 0.007060592823545448\n",
      "Training Loss: 0.008168648513965308\n",
      "Training Loss: 0.007912820319179446\n",
      "Training Loss: 0.007913583223707974\n",
      "Validation Loss: 0.005049325917735836\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006451587140909396\n",
      "Training Loss: 0.006207932727993466\n",
      "Training Loss: 0.0062070063117425885\n",
      "Training Loss: 0.007045379838673398\n",
      "Training Loss: 0.008154137193923817\n",
      "Training Loss: 0.00789986846735701\n",
      "Training Loss: 0.007898775818757713\n",
      "Validation Loss: 0.00503656155061786\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006437014822731726\n",
      "Training Loss: 0.0061976273672189565\n",
      "Training Loss: 0.006194988475181162\n",
      "Training Loss: 0.007031131663243286\n",
      "Training Loss: 0.008140342689584941\n",
      "Training Loss: 0.00788763258839026\n",
      "Training Loss: 0.00788463578792289\n",
      "Validation Loss: 0.0050244547765918275\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006423332916456275\n",
      "Training Loss: 0.00618796179362107\n",
      "Training Loss: 0.006183619217481464\n",
      "Training Loss: 0.007017725536134094\n",
      "Training Loss: 0.008127165343030356\n",
      "Training Loss: 0.007875999399693683\n",
      "Training Loss: 0.007871072164271027\n",
      "Validation Loss: 0.005012916764663009\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006410432263510302\n",
      "Training Loss: 0.006178835447644815\n",
      "Training Loss: 0.0061728138523176315\n",
      "Training Loss: 0.0070050549379084255\n",
      "Training Loss: 0.008114519161754288\n",
      "Training Loss: 0.007864866891177371\n",
      "Training Loss: 0.007858003280125559\n",
      "Validation Loss: 0.005001857302985276\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.00639821607794147\n",
      "Training Loss: 0.006170159813482314\n",
      "Training Loss: 0.0061624985234811904\n",
      "Training Loss: 0.0069930192525498565\n",
      "Training Loss: 0.00810232394898776\n",
      "Training Loss: 0.00785414696438238\n",
      "Training Loss: 0.007845355038298293\n",
      "Validation Loss: 0.004991204579437521\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006386595357907936\n",
      "Training Loss: 0.006161855302052572\n",
      "Training Loss: 0.00615260619786568\n",
      "Training Loss: 0.006981530266930349\n",
      "Training Loss: 0.008090512870112435\n",
      "Training Loss: 0.007843764642020688\n",
      "Training Loss: 0.007833062584977598\n",
      "Validation Loss: 0.0049808980484601206\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006375496211112477\n",
      "Training Loss: 0.006153858336037956\n",
      "Training Loss: 0.006143080210313201\n",
      "Training Loss: 0.006970510349492543\n",
      "Training Loss: 0.008079026398481801\n",
      "Training Loss: 0.007833654828136787\n",
      "Training Loss: 0.007821071898797527\n",
      "Validation Loss: 0.004970882108002511\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006364848494413309\n",
      "Training Loss: 0.006146109009278007\n",
      "Training Loss: 0.006133869389304891\n",
      "Training Loss: 0.006959892801241949\n",
      "Training Loss: 0.008067814365494997\n",
      "Training Loss: 0.007823763928608968\n",
      "Training Loss: 0.007809333959594369\n",
      "Validation Loss: 0.004961104590920138\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00635459334764164\n",
      "Training Loss: 0.006138557083322667\n",
      "Training Loss: 0.006124927880009636\n",
      "Training Loss: 0.006949616327765398\n",
      "Training Loss: 0.008056831604917533\n",
      "Training Loss: 0.007814047009451314\n",
      "Training Loss: 0.007797809132607653\n",
      "Validation Loss: 0.004951532316666222\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006344677824527026\n",
      "Training Loss: 0.006131163202226162\n",
      "Training Loss: 0.006116219003451988\n",
      "Training Loss: 0.006939629943226464\n",
      "Training Loss: 0.00804604145232588\n",
      "Training Loss: 0.007804465444060042\n",
      "Training Loss: 0.00778646110673435\n",
      "Validation Loss: 0.004942125544014774\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006335057077230886\n",
      "Training Loss: 0.006123890855815261\n",
      "Training Loss: 0.006107707713963464\n",
      "Training Loss: 0.006929886855650693\n",
      "Training Loss: 0.00803540951339528\n",
      "Training Loss: 0.007794984497595578\n",
      "Training Loss: 0.007775257070316002\n",
      "Validation Loss: 0.004932856122694547\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0063256915978854526\n",
      "Training Loss: 0.006116709693451412\n",
      "Training Loss: 0.006099364450201392\n",
      "Training Loss: 0.0069203501159790905\n",
      "Training Loss: 0.008024909179657697\n",
      "Training Loss: 0.007785580622730777\n",
      "Training Loss: 0.007764174280455336\n",
      "Validation Loss: 0.004923691849698651\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006316543360007927\n",
      "Training Loss: 0.006109593130531721\n",
      "Training Loss: 0.006091161471558735\n",
      "Training Loss: 0.006910983141860925\n",
      "Training Loss: 0.008014513430534862\n",
      "Training Loss: 0.007776228341972455\n",
      "Training Loss: 0.00775318780913949\n",
      "Validation Loss: 0.004914619267939098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006307585996109992\n",
      "Training Loss: 0.006102521831635386\n",
      "Training Loss: 0.006083079195814207\n",
      "Training Loss: 0.006901758061721921\n",
      "Training Loss: 0.00800420312618371\n",
      "Training Loss: 0.007766909800702706\n",
      "Training Loss: 0.007742278614314273\n",
      "Validation Loss: 0.004905608140389097\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006298789047868922\n",
      "Training Loss: 0.006095474530011416\n",
      "Training Loss: 0.006075094289844856\n",
      "Training Loss: 0.006892644698964432\n",
      "Training Loss: 0.007993956787395291\n",
      "Training Loss: 0.007757606762461364\n",
      "Training Loss: 0.0077314276737160985\n",
      "Validation Loss: 0.004896643155599746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006290127111133188\n",
      "Training Loss: 0.006088434230769053\n",
      "Training Loss: 0.0060671881935559216\n",
      "Training Loss: 0.006883621470769867\n",
      "Training Loss: 0.007983758191694506\n",
      "Training Loss: 0.007748304887209088\n",
      "Training Loss: 0.007720619548344985\n",
      "Validation Loss: 0.004887708685577111\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006281579727074132\n",
      "Training Loss: 0.006081389180617407\n",
      "Training Loss: 0.006059346239781007\n",
      "Training Loss: 0.006874667179072275\n",
      "Training Loss: 0.007973591199843214\n",
      "Training Loss: 0.007738991391379386\n",
      "Training Loss: 0.007709838589653373\n",
      "Validation Loss: 0.004878788454412772\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.00627312662487384\n",
      "Training Loss: 0.006074323189095594\n",
      "Training Loss: 0.006051551850978285\n",
      "Training Loss: 0.00686576072126627\n",
      "Training Loss: 0.007963440631283448\n",
      "Training Loss: 0.0077296538732480255\n",
      "Training Loss: 0.007699072629911825\n",
      "Validation Loss: 0.004869864714656318\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006264747093664483\n",
      "Training Loss: 0.006067223596037365\n",
      "Training Loss: 0.006043788002571091\n",
      "Training Loss: 0.006856884759035893\n",
      "Training Loss: 0.007953293012687936\n",
      "Training Loss: 0.0077202820766251535\n",
      "Training Loss: 0.007688307190546766\n",
      "Validation Loss: 0.004860924923101838\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006256424684543163\n",
      "Training Loss: 0.006060079536982812\n",
      "Training Loss: 0.0060360440390650185\n",
      "Training Loss: 0.006848021862679161\n",
      "Training Loss: 0.007943135002860799\n",
      "Training Loss: 0.007710864846594632\n",
      "Training Loss: 0.0076775297406129536\n",
      "Validation Loss: 0.004851954610676466\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.00624814058362972\n",
      "Training Loss: 0.006052875961177051\n",
      "Training Loss: 0.006028303483035415\n",
      "Training Loss: 0.006839154455810785\n",
      "Training Loss: 0.007932952560950071\n",
      "Training Loss: 0.007701391081791371\n",
      "Training Loss: 0.007666728812037036\n",
      "Validation Loss: 0.0048429362409598145\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006239876906620339\n",
      "Training Loss: 0.006045603051315993\n",
      "Training Loss: 0.006020554589340464\n",
      "Training Loss: 0.006830266628530808\n",
      "Training Loss: 0.0079227325017564\n",
      "Training Loss: 0.007691849714610725\n",
      "Training Loss: 0.007655888743465766\n",
      "Validation Loss: 0.0048338548360525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.006231616797158494\n",
      "Training Loss: 0.00603824517398607\n",
      "Training Loss: 0.006012782416073606\n",
      "Training Loss: 0.006821341926697641\n",
      "Training Loss: 0.007912459911894985\n",
      "Training Loss: 0.0076822307915426795\n",
      "Training Loss: 0.007644997740862891\n",
      "Validation Loss: 0.0048246960851541735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.006223342139855958\n",
      "Training Loss: 0.0060307884344365445\n",
      "Training Loss: 0.006004975172691047\n",
      "Training Loss: 0.006812361975316889\n",
      "Training Loss: 0.007902120928047224\n",
      "Training Loss: 0.007672519998159259\n",
      "Training Loss: 0.007634038681862876\n",
      "Validation Loss: 0.004815436084754765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006215030713938177\n",
      "Training Loss: 0.006023216473404318\n",
      "Training Loss: 0.0059971154178492725\n",
      "Training Loss: 0.006803307821392082\n",
      "Training Loss: 0.00789169767696876\n",
      "Training Loss: 0.007662704007234424\n",
      "Training Loss: 0.007622995773563162\n",
      "Validation Loss: 0.004806053762809\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006206660391180776\n",
      "Training Loss: 0.0060155070328619335\n",
      "Training Loss: 0.005989185742801055\n",
      "Training Loss: 0.006794158412958495\n",
      "Training Loss: 0.007881170687032863\n",
      "Training Loss: 0.0076527655415702615\n",
      "Training Loss: 0.007611849458189681\n",
      "Validation Loss: 0.004796521884235745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006198203843669034\n",
      "Training Loss: 0.006007639465969987\n",
      "Training Loss: 0.005981166947167367\n",
      "Training Loss: 0.006784888760303147\n",
      "Training Loss: 0.007870519326534122\n",
      "Training Loss: 0.007642685460159555\n",
      "Training Loss: 0.007600577105768025\n",
      "Validation Loss: 0.004786813004231883\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006189632783643901\n",
      "Training Loss: 0.005999586113030091\n",
      "Training Loss: 0.005973040297394618\n",
      "Training Loss: 0.006775473540183157\n",
      "Training Loss: 0.007859719044645316\n",
      "Training Loss: 0.0076324412017129365\n",
      "Training Loss: 0.007589152745204046\n",
      "Validation Loss: 0.004776888112764289\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006180909859249368\n",
      "Training Loss: 0.005991310449317098\n",
      "Training Loss: 0.005964777877088636\n",
      "Training Loss: 0.006765878441510722\n",
      "Training Loss: 0.00784873882192187\n",
      "Training Loss: 0.007622002302668989\n",
      "Training Loss: 0.007577544362284243\n",
      "Validation Loss: 0.0047666988220442545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.0061719901656033475\n",
      "Training Loss: 0.005982772753341124\n",
      "Training Loss: 0.005956351296044886\n",
      "Training Loss: 0.006756064809742383\n",
      "Training Loss: 0.007837541808257811\n",
      "Training Loss: 0.007611333804670721\n",
      "Training Loss: 0.007565713298972696\n",
      "Validation Loss: 0.00475618696801211\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006162819997989573\n",
      "Training Loss: 0.005973918032250367\n",
      "Training Loss: 0.005947723230347038\n",
      "Training Loss: 0.006745985270245\n",
      "Training Loss: 0.007826085726264864\n",
      "Training Loss: 0.007600392093881965\n",
      "Training Loss: 0.007553611948387697\n",
      "Validation Loss: 0.004745281108721309\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006153332527028397\n",
      "Training Loss: 0.005964681053301319\n",
      "Training Loss: 0.005938850595848635\n",
      "Training Loss: 0.006735581181128509\n",
      "Training Loss: 0.007814314757706598\n",
      "Training Loss: 0.007589118215255439\n",
      "Training Loss: 0.007541180182015523\n",
      "Validation Loss: 0.0047338941947239616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00614344690926373\n",
      "Training Loss: 0.00595497592817992\n",
      "Training Loss: 0.0059296795970294625\n",
      "Training Loss: 0.0067247838707407935\n",
      "Training Loss: 0.007802166229812429\n",
      "Training Loss: 0.00757744325324893\n",
      "Training Loss: 0.0075283485790714625\n",
      "Validation Loss: 0.004721914375748499\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0061330623709363865\n",
      "Training Loss: 0.005944702121778392\n",
      "Training Loss: 0.0059201520658098165\n",
      "Training Loss: 0.00671351017721463\n",
      "Training Loss: 0.007789560197852552\n",
      "Training Loss: 0.007565278556430712\n",
      "Training Loss: 0.007515028066700324\n",
      "Validation Loss: 0.004709221130543936\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006122067008400336\n",
      "Training Loss: 0.005933738945168443\n",
      "Training Loss: 0.005910200419602916\n",
      "Training Loss: 0.006701665410073474\n",
      "Training Loss: 0.007776407536584884\n",
      "Training Loss: 0.007552520002936944\n",
      "Training Loss: 0.007501117369392887\n",
      "Validation Loss: 0.0046956732481969205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006110332085518167\n",
      "Training Loss: 0.005921943107387051\n",
      "Training Loss: 0.005899750317912549\n",
      "Training Loss: 0.006689144662814215\n",
      "Training Loss: 0.007762608632910997\n",
      "Training Loss: 0.007539047069149092\n",
      "Training Loss: 0.0074865027773194015\n",
      "Validation Loss: 0.0046811133151357705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0060977145802462475\n",
      "Training Loss: 0.005909159189322963\n",
      "Training Loss: 0.005888728720601648\n",
      "Training Loss: 0.006675843035336584\n",
      "Training Loss: 0.0077480590861523525\n",
      "Training Loss: 0.007524729619035497\n",
      "Training Loss: 0.007471065368736163\n",
      "Validation Loss: 0.004665404361104139\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006084087432827801\n",
      "Training Loss: 0.0058952359243994576\n",
      "Training Loss: 0.005877076828619465\n",
      "Training Loss: 0.006661665674764663\n",
      "Training Loss: 0.007732659957837313\n",
      "Training Loss: 0.007509443201124668\n",
      "Training Loss: 0.007454695819178596\n",
      "Validation Loss: 0.004648424955765099\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006069339170353487\n",
      "Training Loss: 0.005880043266224675\n",
      "Training Loss: 0.005864753727801144\n",
      "Training Loss: 0.006646546342526562\n",
      "Training Loss: 0.007716334246797487\n",
      "Training Loss: 0.007493089246563614\n",
      "Training Loss: 0.0074373183608986435\n",
      "Validation Loss: 0.004630108503014379\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.00605341395130381\n",
      "Training Loss: 0.005863504785229452\n",
      "Training Loss: 0.005851755309849977\n",
      "Training Loss: 0.006630467188660987\n",
      "Training Loss: 0.0076990374957676974\n",
      "Training Loss: 0.007475614226423204\n",
      "Training Loss: 0.007418900250922888\n",
      "Validation Loss: 0.004610465376459983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006036323118023574\n",
      "Training Loss: 0.0058456248702714216\n",
      "Training Loss: 0.005838115916121751\n",
      "Training Loss: 0.006613470081356354\n",
      "Training Loss: 0.0076807775115594265\n",
      "Training Loss: 0.007457040869630873\n",
      "Training Loss: 0.007399475892307237\n",
      "Validation Loss: 0.0045895785566817\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006018159102532081\n",
      "Training Loss: 0.005826500621624291\n",
      "Training Loss: 0.005823902955162339\n",
      "Training Loss: 0.006595659604063258\n",
      "Training Loss: 0.007661606890615076\n",
      "Training Loss: 0.0074374621931929145\n",
      "Training Loss: 0.007379148720065132\n",
      "Validation Loss: 0.004567612948816981\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.0059990849142195655\n",
      "Training Loss: 0.005806322242715396\n",
      "Training Loss: 0.005809217033092864\n",
      "Training Loss: 0.006577188625815324\n",
      "Training Loss: 0.007641627220436931\n",
      "Training Loss: 0.007417042563902214\n",
      "Training Loss: 0.0073580720834434035\n",
      "Validation Loss: 0.004544774883545322\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.005979314098949544\n",
      "Training Loss: 0.005785339988651685\n",
      "Training Loss: 0.00579416255641263\n",
      "Training Loss: 0.006558237334829755\n",
      "Training Loss: 0.0076209688046947124\n",
      "Training Loss: 0.007395982439629733\n",
      "Training Loss: 0.007336421051295474\n",
      "Validation Loss: 0.004521303859012627\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.005959078233572655\n",
      "Training Loss: 0.005763833763776347\n",
      "Training Loss: 0.005778845525928773\n",
      "Training Loss: 0.006538981160265394\n",
      "Training Loss: 0.007599777777213604\n",
      "Training Loss: 0.007374492308590561\n",
      "Training Loss: 0.007314381817122921\n",
      "Validation Loss: 0.0044974158526736585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.005938588419812731\n",
      "Training Loss: 0.0057420656044268985\n",
      "Training Loss: 0.005763356472016312\n",
      "Training Loss: 0.0065195804479299116\n",
      "Training Loss: 0.00757819835213013\n",
      "Training Loss: 0.007352768182754516\n",
      "Training Loss: 0.0072921180026605725\n",
      "Validation Loss: 0.004473312396018972\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00591802789713256\n",
      "Training Loss: 0.005720268030418083\n",
      "Training Loss: 0.005747776424977928\n",
      "Training Loss: 0.00650016474945005\n",
      "Training Loss: 0.007556368727236986\n",
      "Training Loss: 0.007330976043595001\n",
      "Training Loss: 0.0072697740385774525\n",
      "Validation Loss: 0.004449158866823501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.005897541599115357\n",
      "Training Loss: 0.00569862607633695\n",
      "Training Loss: 0.005732173778233119\n",
      "Training Loss: 0.006480839432333596\n",
      "Training Loss: 0.007534417276037857\n",
      "Training Loss: 0.007309252782724798\n",
      "Training Loss: 0.0072474722459446635\n",
      "Validation Loss: 0.004425105943138736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.005877244805451483\n",
      "Training Loss: 0.0056772862077923494\n",
      "Training Loss: 0.005716616357676685\n",
      "Training Loss: 0.0064616926794406025\n",
      "Training Loss: 0.007512466821353882\n",
      "Training Loss: 0.00728771519032307\n",
      "Training Loss: 0.007225323710590601\n",
      "Validation Loss: 0.004401270921903659\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.005857228089007549\n",
      "Training Loss: 0.005656360239372589\n",
      "Training Loss: 0.005701163167832419\n",
      "Training Loss: 0.006442798031494021\n",
      "Training Loss: 0.0074906287540215995\n",
      "Training Loss: 0.007266457693185657\n",
      "Training Loss: 0.007203421681188047\n",
      "Validation Loss: 0.004377761118424435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0058375622658059\n",
      "Training Loss: 0.005635936392936855\n",
      "Training Loss: 0.005685878112562932\n",
      "Training Loss: 0.006424221736961044\n",
      "Training Loss: 0.007469009769847617\n",
      "Training Loss: 0.00724556218017824\n",
      "Training Loss: 0.007181855090893805\n",
      "Validation Loss: 0.004354669668010614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.005818313020281494\n",
      "Training Loss: 0.005616084837238304\n",
      "Training Loss: 0.0056708177097607405\n",
      "Training Loss: 0.006406025559990667\n",
      "Training Loss: 0.0074477083305828275\n",
      "Training Loss: 0.007225101633230225\n",
      "Training Loss: 0.007160706479335204\n",
      "Validation Loss: 0.004332084905038751\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.005799534429097548\n",
      "Training Loss: 0.005596864545950666\n",
      "Training Loss: 0.005656039659515955\n",
      "Training Loss: 0.006388263699482195\n",
      "Training Loss: 0.00742681521223858\n",
      "Training Loss: 0.007205141768790782\n",
      "Training Loss: 0.00714004771783948\n",
      "Validation Loss: 0.004310076807761628\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.005781272001913748\n",
      "Training Loss: 0.005578319095657207\n",
      "Training Loss: 0.00564159172703512\n",
      "Training Loss: 0.006370988308335654\n",
      "Training Loss: 0.007406413359567523\n",
      "Training Loss: 0.00718574095168151\n",
      "Training Loss: 0.00711995143792592\n",
      "Validation Loss: 0.0042887147631991505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0057635709998430686\n",
      "Training Loss: 0.005560488725313917\n",
      "Training Loss: 0.005627527973847464\n",
      "Training Loss: 0.006354246247792616\n",
      "Training Loss: 0.00738657753681764\n",
      "Training Loss: 0.007166948867961764\n",
      "Training Loss: 0.007100477581843734\n",
      "Validation Loss: 0.004268050861321668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.005746461754897609\n",
      "Training Loss: 0.005543398241861723\n",
      "Training Loss: 0.005613879275042564\n",
      "Training Loss: 0.006338072156650014\n",
      "Training Loss: 0.0073673671262804415\n",
      "Training Loss: 0.007148807952180505\n",
      "Training Loss: 0.007081675723893568\n",
      "Validation Loss: 0.004248129382680697\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0057299723045434806\n",
      "Training Loss: 0.005527068828814663\n",
      "Training Loss: 0.005600684660603292\n",
      "Training Loss: 0.006322497865767218\n",
      "Training Loss: 0.007348832639399916\n",
      "Training Loss: 0.007131347582908347\n",
      "Training Loss: 0.007063587872544304\n",
      "Validation Loss: 0.004228982746545295\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.005714122112840414\n",
      "Training Loss: 0.00551150933955796\n",
      "Training Loss: 0.005587965147569775\n",
      "Training Loss: 0.006307542805443518\n",
      "Training Loss: 0.007331010529305786\n",
      "Training Loss: 0.007114594368031249\n",
      "Training Loss: 0.007046243874356151\n",
      "Validation Loss: 0.004210629912044448\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.005698919076821767\n",
      "Training Loss: 0.005496717658243142\n",
      "Training Loss: 0.005575736201135442\n",
      "Training Loss: 0.006293219457147643\n",
      "Training Loss: 0.007313927072100341\n",
      "Training Loss: 0.007098558836150915\n",
      "Training Loss: 0.007029661493143067\n",
      "Validation Loss: 0.004193084411260285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.005684367330977693\n",
      "Training Loss: 0.005482687792391516\n",
      "Training Loss: 0.005564005327178165\n",
      "Training Loss: 0.006279529507737607\n",
      "Training Loss: 0.007297593741677702\n",
      "Training Loss: 0.007083243526285515\n",
      "Training Loss: 0.007013845710316673\n",
      "Validation Loss: 0.004176345413153091\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.005670460686087608\n",
      "Training Loss: 0.00546940291998908\n",
      "Training Loss: 0.005552773899398744\n",
      "Training Loss: 0.006266469504334964\n",
      "Training Loss: 0.007282015327364206\n",
      "Training Loss: 0.007068646505940706\n",
      "Training Loss: 0.00699879739433527\n",
      "Validation Loss: 0.004160397870497506\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.005657184277079068\n",
      "Training Loss: 0.005456838979152962\n",
      "Training Loss: 0.005542033386300318\n",
      "Training Loss: 0.00625402525998652\n",
      "Training Loss: 0.0072671823971904815\n",
      "Training Loss: 0.007054751178948208\n",
      "Training Loss: 0.0069845005427487195\n",
      "Validation Loss: 0.004145232809721475\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.005644521322683432\n",
      "Training Loss: 0.005444965838687494\n",
      "Training Loss: 0.0055317701568128545\n",
      "Training Loss: 0.006242177155800163\n",
      "Training Loss: 0.007253078212961554\n",
      "Training Loss: 0.007041538709308952\n",
      "Training Loss: 0.006970934709534049\n",
      "Validation Loss: 0.0041308224454961635\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.005632446078816428\n",
      "Training Loss: 0.0054337511130142955\n",
      "Training Loss: 0.005521969263791107\n",
      "Training Loss: 0.006230902697425335\n",
      "Training Loss: 0.007239682452054694\n",
      "Training Loss: 0.007028984016506002\n",
      "Training Loss: 0.006958075708243996\n",
      "Validation Loss: 0.00411714042522962\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.005620934325852431\n",
      "Training Loss: 0.005423158086487092\n",
      "Training Loss: 0.0055126106383977455\n",
      "Training Loss: 0.00622017519781366\n",
      "Training Loss: 0.007226965018780902\n",
      "Training Loss: 0.0070170584681909535\n",
      "Training Loss: 0.006945888204500079\n",
      "Validation Loss: 0.0041041544212153955\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.005609954475658014\n",
      "Training Loss: 0.0054131468007108195\n",
      "Training Loss: 0.005503670014441013\n",
      "Training Loss: 0.006209964347071946\n",
      "Training Loss: 0.007214895564829931\n",
      "Training Loss: 0.007005730397067964\n",
      "Training Loss: 0.006934339191066102\n",
      "Validation Loss: 0.004091825286948474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.005599474241607822\n",
      "Training Loss: 0.005403675868874416\n",
      "Training Loss: 0.005495120363193564\n",
      "Training Loss: 0.00620023897907231\n",
      "Training Loss: 0.007203438879223541\n",
      "Training Loss: 0.006994966729544103\n",
      "Training Loss: 0.006923389593139291\n",
      "Validation Loss: 0.004080124944099661\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0055894639051985\n",
      "Training Loss: 0.005394707511877641\n",
      "Training Loss: 0.005486939054098912\n",
      "Training Loss: 0.006190967296133749\n",
      "Training Loss: 0.007192560730036348\n",
      "Training Loss: 0.006984732479322702\n",
      "Training Loss: 0.006913002725923434\n",
      "Validation Loss: 0.004069012672595038\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.005579890452791006\n",
      "Training Loss: 0.005386202114750631\n",
      "Training Loss: 0.005479099435033277\n",
      "Training Loss: 0.006182119238656014\n",
      "Training Loss: 0.007182223415002227\n",
      "Training Loss: 0.006974994910415262\n",
      "Training Loss: 0.006903139195637778\n",
      "Validation Loss: 0.004058452936112602\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.005570723737473599\n",
      "Training Loss: 0.005378120579989627\n",
      "Training Loss: 0.005471578050055541\n",
      "Training Loss: 0.006173664464149624\n",
      "Training Loss: 0.007172392663778737\n",
      "Training Loss: 0.006965721178567037\n",
      "Training Loss: 0.006893760749371722\n",
      "Validation Loss: 0.004048413339799988\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.00556193501863163\n",
      "Training Loss: 0.005370428508031182\n",
      "Training Loss: 0.005464349227258936\n",
      "Training Loss: 0.0061655733542284\n",
      "Training Loss: 0.007163033190881834\n",
      "Training Loss: 0.006956879259087145\n",
      "Training Loss: 0.006884832284413278\n",
      "Validation Loss: 0.004038858480755003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.005553493757615797\n",
      "Training Loss: 0.005363091201870702\n",
      "Training Loss: 0.005457391875097528\n",
      "Training Loss: 0.006157820133375935\n",
      "Training Loss: 0.007154113528085873\n",
      "Training Loss: 0.006948440214619041\n",
      "Training Loss: 0.00687631965149194\n",
      "Validation Loss: 0.004029759424320014\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.005545376210357063\n",
      "Training Loss: 0.005356079441262409\n",
      "Training Loss: 0.00545068415813148\n",
      "Training Loss: 0.0061503796151373535\n",
      "Training Loss: 0.007145602172240615\n",
      "Training Loss: 0.006940374459372833\n",
      "Training Loss: 0.006868189839879051\n",
      "Validation Loss: 0.004021082790690554\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.005537556848721579\n",
      "Training Loss: 0.0053493617015192285\n",
      "Training Loss: 0.005444205765379593\n",
      "Training Loss: 0.006143228726577945\n",
      "Training Loss: 0.007137469459557906\n",
      "Training Loss: 0.00693265826208517\n",
      "Training Loss: 0.006860413241665811\n",
      "Validation Loss: 0.00401280636503274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.005530014934483915\n",
      "Training Loss: 0.005342912671621889\n",
      "Training Loss: 0.005437938761897385\n",
      "Training Loss: 0.006136345696286298\n",
      "Training Loss: 0.007129688907880336\n",
      "Training Loss: 0.006925265374593437\n",
      "Training Loss: 0.006852962719276548\n",
      "Validation Loss: 0.004004897674011966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.005522727449424565\n",
      "Training Loss: 0.005336709164548665\n",
      "Training Loss: 0.005431866133585572\n",
      "Training Loss: 0.006129711783141829\n",
      "Training Loss: 0.007122236003633589\n",
      "Training Loss: 0.006918176694307476\n",
      "Training Loss: 0.006845811248058453\n",
      "Validation Loss: 0.00399733837109673\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.005515675786882639\n",
      "Training Loss: 0.005330728145199828\n",
      "Training Loss: 0.005425974082900211\n",
      "Training Loss: 0.0061233091430040075\n",
      "Training Loss: 0.007115087218116969\n",
      "Training Loss: 0.0069113689567893745\n",
      "Training Loss: 0.006838938221335411\n",
      "Validation Loss: 0.0039901022179303375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.005508843971183524\n",
      "Training Loss: 0.005324949905625545\n",
      "Training Loss: 0.005420246672583744\n",
      "Training Loss: 0.00611712169717066\n",
      "Training Loss: 0.007108222531387582\n",
      "Training Loss: 0.00690482524340041\n",
      "Training Loss: 0.006832319585373625\n",
      "Validation Loss: 0.0039831700543820694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.005502215715823695\n",
      "Training Loss: 0.005319358061533421\n",
      "Training Loss: 0.005414672569604591\n",
      "Training Loss: 0.006111135269748047\n",
      "Training Loss: 0.007101622128393501\n",
      "Training Loss: 0.0068985287635587155\n",
      "Training Loss: 0.006825938391266391\n",
      "Validation Loss: 0.00397652514498639\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.005495776875759475\n",
      "Training Loss: 0.005313934829318896\n",
      "Training Loss: 0.005409239187138155\n",
      "Training Loss: 0.006105337613844313\n",
      "Training Loss: 0.007095269002020359\n",
      "Training Loss: 0.006892463610274718\n",
      "Training Loss: 0.006819776956690476\n",
      "Validation Loss: 0.003970148540642601\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.005489514378714375\n",
      "Training Loss: 0.005308668312500231\n",
      "Training Loss: 0.00540393864735961\n",
      "Training Loss: 0.006099716346361674\n",
      "Training Loss: 0.007089146209182218\n",
      "Training Loss: 0.00688661512453109\n",
      "Training Loss: 0.006813818918308243\n",
      "Validation Loss: 0.003964024633413341\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00548341725836508\n",
      "Training Loss: 0.005303544927155599\n",
      "Training Loss: 0.0053987581469118594\n",
      "Training Loss: 0.006094260664540343\n",
      "Training Loss: 0.007083241124637425\n",
      "Training Loss: 0.006880971912760287\n",
      "Training Loss: 0.006808050933759659\n",
      "Validation Loss: 0.0039581392289135845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.005477474661893212\n",
      "Training Loss: 0.0052985521854134274\n",
      "Training Loss: 0.005393692234065384\n",
      "Training Loss: 0.006088961787172593\n",
      "Training Loss: 0.007077538875164464\n",
      "Training Loss: 0.0068755206954665485\n",
      "Training Loss: 0.006802459044847637\n",
      "Validation Loss: 0.003952478932841133\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.005471675671869889\n",
      "Training Loss: 0.005293681513867341\n",
      "Training Loss: 0.005388731796992943\n",
      "Training Loss: 0.0060838090267498045\n",
      "Training Loss: 0.007072027398971841\n",
      "Training Loss: 0.006870251392247156\n",
      "Training Loss: 0.006797031683381647\n",
      "Validation Loss: 0.003947030310780647\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.005466012494289316\n",
      "Training Loss: 0.005288923199987039\n",
      "Training Loss: 0.005383870184887201\n",
      "Training Loss: 0.006078796475194394\n",
      "Training Loss: 0.007066696876427159\n",
      "Training Loss: 0.006865154800470919\n",
      "Training Loss: 0.006791760316118598\n",
      "Validation Loss: 0.003941782870528738\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.005460478125023656\n",
      "Training Loss: 0.005284268886898644\n",
      "Training Loss: 0.0053791020810604094\n",
      "Training Loss: 0.00607391582976561\n",
      "Training Loss: 0.007061536827823147\n",
      "Training Loss: 0.0068602214357815685\n",
      "Training Loss: 0.006786633946467191\n",
      "Validation Loss: 0.003936723466310302\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.005455063053523191\n",
      "Training Loss: 0.005279713118798099\n",
      "Training Loss: 0.00537442060187459\n",
      "Training Loss: 0.006069161038612947\n",
      "Training Loss: 0.007056539381155744\n",
      "Training Loss: 0.0068554426496848465\n",
      "Training Loss: 0.0067816464509814975\n",
      "Validation Loss: 0.003931843983061779\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.0054497628501849246\n",
      "Training Loss: 0.005275247956742533\n",
      "Training Loss: 0.0053698221233207735\n",
      "Training Loss: 0.006064526791451499\n",
      "Training Loss: 0.007051693509565666\n",
      "Training Loss: 0.0068508111871778965\n",
      "Training Loss: 0.006776787990238517\n",
      "Validation Loss: 0.0039271352265888215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.005444572527776473\n",
      "Training Loss: 0.005270868841325865\n",
      "Training Loss: 0.0053653033205773686\n",
      "Training Loss: 0.006060006577172317\n",
      "Training Loss: 0.007046993516851217\n",
      "Training Loss: 0.00684631965123117\n",
      "Training Loss: 0.006772051737643778\n",
      "Validation Loss: 0.003922590489712045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.005439485514070839\n",
      "Training Loss: 0.005266571899992414\n",
      "Training Loss: 0.005360858497442678\n",
      "Training Loss: 0.006055596182704903\n",
      "Training Loss: 0.00704243196058087\n",
      "Training Loss: 0.006841962429462001\n",
      "Training Loss: 0.006767432863125577\n",
      "Validation Loss: 0.003918198734776497\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005434497134410776\n",
      "Training Loss: 0.005262352182180621\n",
      "Training Loss: 0.005356485334923491\n",
      "Training Loss: 0.006051290548639372\n",
      "Training Loss: 0.007038001797627658\n",
      "Training Loss: 0.006837732661515474\n",
      "Training Loss: 0.006762924649519846\n",
      "Validation Loss: 0.003913951636996637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.005429602029034868\n",
      "Training Loss: 0.0052582034876104446\n",
      "Training Loss: 0.005352180246263743\n",
      "Training Loss: 0.006047085693571716\n",
      "Training Loss: 0.007033697683364153\n",
      "Training Loss: 0.006833623627899215\n",
      "Training Loss: 0.006758521879091859\n",
      "Validation Loss: 0.003909847253065012\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.005424800076871179\n",
      "Training Loss: 0.005254127000807784\n",
      "Training Loss: 0.0053479407960549\n",
      "Training Loss: 0.00604297784040682\n",
      "Training Loss: 0.007029513826128095\n",
      "Training Loss: 0.006829632777953521\n",
      "Training Loss: 0.0067542209080420435\n",
      "Validation Loss: 0.003905876472065651\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005420085152145475\n",
      "Training Loss: 0.005250118104740977\n",
      "Training Loss: 0.005343765951693058\n",
      "Training Loss: 0.006038962829043157\n",
      "Training Loss: 0.007025445239851252\n",
      "Training Loss: 0.006825751594733447\n",
      "Training Loss: 0.006750015919096768\n",
      "Validation Loss: 0.0039020293080056093\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.0054154536011628805\n",
      "Training Loss: 0.005246172621846199\n",
      "Training Loss: 0.005339650136884302\n",
      "Training Loss: 0.00603503807738889\n",
      "Training Loss: 0.0070214869373012335\n",
      "Training Loss: 0.006821978761581704\n",
      "Training Loss: 0.00674590594950132\n",
      "Validation Loss: 0.003898307308293042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005410904824966565\n",
      "Training Loss: 0.0052422905346611514\n",
      "Training Loss: 0.005335595888318494\n",
      "Training Loss: 0.006031200497527606\n",
      "Training Loss: 0.007017634744988755\n",
      "Training Loss: 0.006818309277296066\n",
      "Training Loss: 0.0067418852960690855\n",
      "Validation Loss: 0.003894696232325603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.00540643336775247\n",
      "Training Loss: 0.005238467758754268\n",
      "Training Loss: 0.005331598239718005\n",
      "Training Loss: 0.006027445702347905\n",
      "Training Loss: 0.007013884438201785\n",
      "Training Loss: 0.0068147380941081795\n",
      "Training Loss: 0.0067379501659888775\n",
      "Validation Loss: 0.003891199466611925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.005402039784821682\n",
      "Training Loss: 0.0052347039856249465\n",
      "Training Loss: 0.005327656902372837\n",
      "Training Loss: 0.006023771675536409\n",
      "Training Loss: 0.007010231062304228\n",
      "Training Loss: 0.006811260603135452\n",
      "Training Loss: 0.006734098175074905\n",
      "Validation Loss: 0.0038878074960525004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005397720267064869\n",
      "Training Loss: 0.005230997985345312\n",
      "Training Loss: 0.005323772493284196\n",
      "Training Loss: 0.006020176221500151\n",
      "Training Loss: 0.007006670769769698\n",
      "Training Loss: 0.006807874543592334\n",
      "Training Loss: 0.006730325606185943\n",
      "Validation Loss: 0.0038845170579662317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00539347569225356\n",
      "Training Loss: 0.0052273479837458585\n",
      "Training Loss: 0.005319941366324201\n",
      "Training Loss: 0.006016656856518239\n",
      "Training Loss: 0.0070032004779204725\n",
      "Training Loss: 0.006804576066788286\n",
      "Training Loss: 0.006726630197372288\n",
      "Validation Loss: 0.0038813250915457806\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.00538930099864956\n",
      "Training Loss: 0.0052237521426286545\n",
      "Training Loss: 0.005316163594834507\n",
      "Training Loss: 0.0060132099234033376\n",
      "Training Loss: 0.006999818136682734\n",
      "Training Loss: 0.006801361630205065\n",
      "Training Loss: 0.006723010012647137\n",
      "Validation Loss: 0.0038782235793943763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005385195455164649\n",
      "Training Loss: 0.005220209602848627\n",
      "Training Loss: 0.005312437756219879\n",
      "Training Loss: 0.0060098345385631545\n",
      "Training Loss: 0.0069965202535968276\n",
      "Training Loss: 0.006798228857805952\n",
      "Training Loss: 0.006719463204499334\n",
      "Validation Loss: 0.0038752090085597797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.0053811582317575815\n",
      "Training Loss: 0.005216719344025478\n",
      "Training Loss: 0.005308762862114236\n",
      "Training Loss: 0.006006527021527291\n",
      "Training Loss: 0.006993301005568355\n",
      "Training Loss: 0.0067951739823911336\n",
      "Training Loss: 0.0067159856751095506\n",
      "Validation Loss: 0.0038722815486508305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005377187078120187\n",
      "Training Loss: 0.005213280459865927\n",
      "Training Loss: 0.005305138451512903\n",
      "Training Loss: 0.006003286051563919\n",
      "Training Loss: 0.006990159509005025\n",
      "Training Loss: 0.006792193938745186\n",
      "Training Loss: 0.006712576097343117\n",
      "Validation Loss: 0.003869434032931031\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.005373281735228374\n",
      "Training Loss: 0.005209892090060748\n",
      "Training Loss: 0.00530156274791807\n",
      "Training Loss: 0.006000108927255496\n",
      "Training Loss: 0.006987092831404879\n",
      "Training Loss: 0.00678928526584059\n",
      "Training Loss: 0.006709232988068834\n",
      "Validation Loss: 0.003866661592885461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005369438377092592\n",
      "Training Loss: 0.005206551952287555\n",
      "Training Loss: 0.005298034733859822\n",
      "Training Loss: 0.0059969947033096105\n",
      "Training Loss: 0.006984099355759099\n",
      "Training Loss: 0.006786447078920901\n",
      "Training Loss: 0.006705952365882695\n",
      "Validation Loss: 0.0038639660523541635\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.00536566004913766\n",
      "Training Loss: 0.005203261899878271\n",
      "Training Loss: 0.005294557522283867\n",
      "Training Loss: 0.005993940002517775\n",
      "Training Loss: 0.0069811717176344245\n",
      "Training Loss: 0.006783673949539662\n",
      "Training Loss: 0.00670273311669007\n",
      "Validation Loss: 0.003861342816202019\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005361942576128058\n",
      "Training Loss: 0.005200018853647634\n",
      "Training Loss: 0.0052911263855639845\n",
      "Training Loss: 0.005990944075747393\n",
      "Training Loss: 0.006978312353603542\n",
      "Training Loss: 0.006780967073282227\n",
      "Training Loss: 0.006699573912192136\n",
      "Validation Loss: 0.003858783776341669\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005358284066314809\n",
      "Training Loss: 0.005196821540594101\n",
      "Training Loss: 0.005287741516949609\n",
      "Training Loss: 0.005988003537640907\n",
      "Training Loss: 0.006975516907405108\n",
      "Training Loss: 0.0067783202847931535\n",
      "Training Loss: 0.006696473363554105\n",
      "Validation Loss: 0.0038562921178439975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.0053546864085365084\n",
      "Training Loss: 0.005193671945598908\n",
      "Training Loss: 0.0052844031492713835\n",
      "Training Loss: 0.0059851173165952785\n",
      "Training Loss: 0.006972783015808091\n",
      "Training Loss: 0.006775732219684869\n",
      "Training Loss: 0.006693427617428824\n",
      "Validation Loss: 0.0038538627528798266\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005351145994500257\n",
      "Training Loss: 0.005190566444653087\n",
      "Training Loss: 0.005281109471106902\n",
      "Training Loss: 0.005982282831100747\n",
      "Training Loss: 0.00697010827017948\n",
      "Training Loss: 0.0067732015147339555\n",
      "Training Loss: 0.006690436179051176\n",
      "Validation Loss: 0.0038514915223264797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005347662639105692\n",
      "Training Loss: 0.005187504940549843\n",
      "Training Loss: 0.005277859914349392\n",
      "Training Loss: 0.005979499909444712\n",
      "Training Loss: 0.006967490720562637\n",
      "Training Loss: 0.00677072549238801\n",
      "Training Loss: 0.006687497425591573\n",
      "Validation Loss: 0.0038491780102280866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005344234679359943\n",
      "Training Loss: 0.005184487428050488\n",
      "Training Loss: 0.005274654128588736\n",
      "Training Loss: 0.005976765377563424\n",
      "Training Loss: 0.006964927659137175\n",
      "Training Loss: 0.00676830195938237\n",
      "Training Loss: 0.00668460952816531\n",
      "Validation Loss: 0.0038469168117744256\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.0053408612869679925\n",
      "Training Loss: 0.005181511731934734\n",
      "Training Loss: 0.005271490720333532\n",
      "Training Loss: 0.00597407830064185\n",
      "Training Loss: 0.006962417616741732\n",
      "Training Loss: 0.006765928987879306\n",
      "Training Loss: 0.006681771577568725\n",
      "Validation Loss: 0.0038447073736657083\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005337540767504834\n",
      "Training Loss: 0.005178578254417516\n",
      "Training Loss: 0.005268370558042079\n",
      "Training Loss: 0.005971435629180633\n",
      "Training Loss: 0.006959956447826698\n",
      "Training Loss: 0.006763602525461465\n",
      "Training Loss: 0.006678979301359505\n",
      "Validation Loss: 0.0038425520089479365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005334274654160254\n",
      "Training Loss: 0.0051756861893227325\n",
      "Training Loss: 0.005265291802352294\n",
      "Training Loss: 0.005968838471453637\n",
      "Training Loss: 0.006957544292090461\n",
      "Training Loss: 0.006761323070386425\n",
      "Training Loss: 0.006676233056932688\n",
      "Validation Loss: 0.0038404432639884255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005331059465534054\n",
      "Training Loss: 0.005172833811375312\n",
      "Training Loss: 0.005262255278648808\n",
      "Training Loss: 0.005966282665031031\n",
      "Training Loss: 0.006955179765354842\n",
      "Training Loss: 0.006759088654071092\n",
      "Training Loss: 0.006673531351843849\n",
      "Validation Loss: 0.0038383781446897415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.00532789311662782\n",
      "Training Loss: 0.005170020323712379\n",
      "Training Loss: 0.00525925696012564\n",
      "Training Loss: 0.005963766905479133\n",
      "Training Loss: 0.006952860156306997\n",
      "Training Loss: 0.006756896913284436\n",
      "Training Loss: 0.0066708731511607765\n",
      "Validation Loss: 0.0038363557637665657\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005324776388006285\n",
      "Training Loss: 0.0051672447071177885\n",
      "Training Loss: 0.005256298335734755\n",
      "Training Loss: 0.005961290966370143\n",
      "Training Loss: 0.006950582878198474\n",
      "Training Loss: 0.006754743314813822\n",
      "Training Loss: 0.006668254984542727\n",
      "Validation Loss: 0.0038343770416411622\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005321707873372361\n",
      "Training Loss: 0.005164507387671619\n",
      "Training Loss: 0.005253379608038813\n",
      "Training Loss: 0.005958852737094276\n",
      "Training Loss: 0.006948345672572032\n",
      "Training Loss: 0.006752630225382745\n",
      "Training Loss: 0.006665676954435184\n",
      "Validation Loss: 0.0038324375772039484\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0053186863334849475\n",
      "Training Loss: 0.005161806044634431\n",
      "Training Loss: 0.005250497084343806\n",
      "Training Loss: 0.005956449517398142\n",
      "Training Loss: 0.00694615003769286\n",
      "Training Loss: 0.006750554846366868\n",
      "Training Loss: 0.006663138518342748\n",
      "Validation Loss: 0.0038305344042622864\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005315708728157915\n",
      "Training Loss: 0.00515913967799861\n",
      "Training Loss: 0.005247651999816298\n",
      "Training Loss: 0.005954082130920142\n",
      "Training Loss: 0.006943990398431197\n",
      "Training Loss: 0.006748512942576781\n",
      "Training Loss: 0.006660635353764519\n",
      "Validation Loss: 0.0038286718807605266\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005312778071966022\n",
      "Training Loss: 0.005156508368090726\n",
      "Training Loss: 0.005244842134416104\n",
      "Training Loss: 0.005951747982180677\n",
      "Training Loss: 0.006941867881687358\n",
      "Training Loss: 0.006746505924966186\n",
      "Training Loss: 0.006658168950816616\n",
      "Validation Loss: 0.003826841997586135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005309889889904298\n",
      "Training Loss: 0.0051539106050040575\n",
      "Training Loss: 0.00524206839967519\n",
      "Training Loss: 0.005949446261511184\n",
      "Training Loss: 0.006939780323300511\n",
      "Training Loss: 0.0067445314954966305\n",
      "Training Loss: 0.006655737524852157\n",
      "Validation Loss: 0.003825046452571558\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005307043913053349\n",
      "Training Loss: 0.005151346086058765\n",
      "Training Loss: 0.005239328389288858\n",
      "Training Loss: 0.0059471749985823405\n",
      "Training Loss: 0.00693772652070038\n",
      "Training Loss: 0.006742588226916269\n",
      "Training Loss: 0.006653338359901681\n",
      "Validation Loss: 0.003823283227415204\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005304239490069449\n",
      "Training Loss: 0.00514881378330756\n",
      "Training Loss: 0.0052366233197972175\n",
      "Training Loss: 0.005944933122955263\n",
      "Training Loss: 0.00693570526316762\n",
      "Training Loss: 0.006740674499887973\n",
      "Training Loss: 0.006650971141643822\n",
      "Validation Loss: 0.003821549093880959\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005301474916632287\n",
      "Training Loss: 0.005146312861470505\n",
      "Training Loss: 0.005233950591646135\n",
      "Training Loss: 0.005942720141611062\n",
      "Training Loss: 0.006933712421450764\n",
      "Training Loss: 0.006738787520444021\n",
      "Training Loss: 0.00664863463724032\n",
      "Validation Loss: 0.0038198473530190154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005298750567017123\n",
      "Training Loss: 0.005143842448596843\n",
      "Training Loss: 0.005231310710660182\n",
      "Training Loss: 0.005940534144756384\n",
      "Training Loss: 0.006931750001385808\n",
      "Training Loss: 0.00673692760639824\n",
      "Training Loss: 0.006646327859489248\n",
      "Validation Loss: 0.0038181746170127045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.0052960643026744945\n",
      "Training Loss: 0.005141401642467827\n",
      "Training Loss: 0.005228703004540876\n",
      "Training Loss: 0.005938374976976774\n",
      "Training Loss: 0.00692981541971676\n",
      "Training Loss: 0.006735094942850992\n",
      "Training Loss: 0.006644049646565691\n",
      "Validation Loss: 0.0038165268693154167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005293414547340944\n",
      "Training Loss: 0.005138990203849971\n",
      "Training Loss: 0.005226125246263109\n",
      "Training Loss: 0.005936240670271218\n",
      "Training Loss: 0.0069279091677162796\n",
      "Training Loss: 0.0067332854564301665\n",
      "Training Loss: 0.0066417988413013515\n",
      "Validation Loss: 0.003814907192971125\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.0052908008493250235\n",
      "Training Loss: 0.005136605259031058\n",
      "Training Loss: 0.005223577155265957\n",
      "Training Loss: 0.005934131021494977\n",
      "Training Loss: 0.006926027564331889\n",
      "Training Loss: 0.006731499994639307\n",
      "Training Loss: 0.006639574237633497\n",
      "Validation Loss: 0.003813312222835807\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005288222878589295\n",
      "Training Loss: 0.005134249678230844\n",
      "Training Loss: 0.005221060334588401\n",
      "Training Loss: 0.005932043739594519\n",
      "Training Loss: 0.006924171377904713\n",
      "Training Loss: 0.006729737025452777\n",
      "Training Loss: 0.006637375864665955\n",
      "Validation Loss: 0.0038117427655324785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005285677693318575\n",
      "Training Loss: 0.005131919679697603\n",
      "Training Loss: 0.005218570899451151\n",
      "Training Loss: 0.005929978986969218\n",
      "Training Loss: 0.006922339535085484\n",
      "Training Loss: 0.006727995220571757\n",
      "Training Loss: 0.006635202540783211\n",
      "Validation Loss: 0.003810193916357189\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005283165912842378\n",
      "Training Loss: 0.005129614335019141\n",
      "Training Loss: 0.005216108350432478\n",
      "Training Loss: 0.005927936030784622\n",
      "Training Loss: 0.006920529792550951\n",
      "Training Loss: 0.006726273912936449\n",
      "Training Loss: 0.006633051617536694\n",
      "Validation Loss: 0.003808669759438474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005280685587204062\n",
      "Training Loss: 0.0051273347192909565\n",
      "Training Loss: 0.0052136731921928\n",
      "Training Loss: 0.0059259127243421975\n",
      "Training Loss: 0.006918741995468736\n",
      "Training Loss: 0.006724571394734084\n",
      "Training Loss: 0.006630923388293013\n",
      "Validation Loss: 0.003807171440181019\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005278237508027814\n",
      "Training Loss: 0.005125079657300375\n",
      "Training Loss: 0.005211266297847033\n",
      "Training Loss: 0.005923909725970589\n",
      "Training Loss: 0.006916974713094532\n",
      "Training Loss: 0.00672288719099015\n",
      "Training Loss: 0.006628816345473751\n",
      "Validation Loss: 0.003805689595653658\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005275819237576798\n",
      "Training Loss: 0.005122848818427883\n",
      "Training Loss: 0.005208884532912635\n",
      "Training Loss: 0.0059219254332128915\n",
      "Training Loss: 0.006915226966375485\n",
      "Training Loss: 0.006721220347099006\n",
      "Training Loss: 0.006626730898860842\n",
      "Validation Loss: 0.003804230099816001\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005273430771776475\n",
      "Training Loss: 0.0051206412952160465\n",
      "Training Loss: 0.005206529390416108\n",
      "Training Loss: 0.00591995916038286\n",
      "Training Loss: 0.006913499233778566\n",
      "Training Loss: 0.006719570660497993\n",
      "Training Loss: 0.006624666422139853\n",
      "Validation Loss: 0.003802790077165997\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005271069661248475\n",
      "Training Loss: 0.005118454474722966\n",
      "Training Loss: 0.005204197737039067\n",
      "Training Loss: 0.0059180097858188676\n",
      "Training Loss: 0.006911790744634345\n",
      "Training Loss: 0.006717936998466029\n",
      "Training Loss: 0.006622621354181319\n",
      "Validation Loss: 0.003801368260651492\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005268736057332717\n",
      "Training Loss: 0.00511628980981186\n",
      "Training Loss: 0.005201889397576451\n",
      "Training Loss: 0.005916078014415689\n",
      "Training Loss: 0.006910100332461298\n",
      "Training Loss: 0.00671631925040856\n",
      "Training Loss: 0.00662059435271658\n",
      "Validation Loss: 0.003799964522956853\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005266428951290436\n",
      "Training Loss: 0.005114146030391567\n",
      "Training Loss: 0.005199603602522984\n",
      "Training Loss: 0.005914161758846603\n",
      "Training Loss: 0.00690842785523273\n",
      "Training Loss: 0.006714715656125918\n",
      "Training Loss: 0.006618586294353008\n",
      "Validation Loss: 0.0037985815398904324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.00526414750085678\n",
      "Training Loss: 0.00511202302062884\n",
      "Training Loss: 0.005197342631872743\n",
      "Training Loss: 0.005912260243785567\n",
      "Training Loss: 0.006906770014902577\n",
      "Training Loss: 0.006713125674286858\n",
      "Training Loss: 0.0066165943501982835\n",
      "Validation Loss: 0.0037972144390131983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005261890754918568\n",
      "Training Loss: 0.005109919493552297\n",
      "Training Loss: 0.005195102673606016\n",
      "Training Loss: 0.005910373593796976\n",
      "Training Loss: 0.006905128723010421\n",
      "Training Loss: 0.006711549753090367\n",
      "Training Loss: 0.006614619767060503\n",
      "Validation Loss: 0.003795862726759068\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005259657849674113\n",
      "Training Loss: 0.005107834922382608\n",
      "Training Loss: 0.005192885822616518\n",
      "Training Loss: 0.00590850152017083\n",
      "Training Loss: 0.006903502088971436\n",
      "Training Loss: 0.006709983706241473\n",
      "Training Loss: 0.006612660282989964\n",
      "Validation Loss: 0.0037945310688971747\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0052574485039804135\n",
      "Training Loss: 0.005105770269874483\n",
      "Training Loss: 0.0051906884449999776\n",
      "Training Loss: 0.005906642795889638\n",
      "Training Loss: 0.006901891171000898\n",
      "Training Loss: 0.006708431827137247\n",
      "Training Loss: 0.0066107177489902825\n",
      "Validation Loss: 0.003793210796192149\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005255260737030767\n",
      "Training Loss: 0.005103720984770917\n",
      "Training Loss: 0.005188510283478536\n",
      "Training Loss: 0.005904796508257277\n",
      "Training Loss: 0.006900294938823208\n",
      "Training Loss: 0.006706891390495002\n",
      "Training Loss: 0.006608790116151795\n",
      "Validation Loss: 0.0037919092650458997\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005253094270592555\n",
      "Training Loss: 0.005101689953589812\n",
      "Training Loss: 0.005186353606404737\n",
      "Training Loss: 0.005902963544940576\n",
      "Training Loss: 0.006898711589165032\n",
      "Training Loss: 0.006705361927160993\n",
      "Training Loss: 0.006606876250589267\n",
      "Validation Loss: 0.0037906212219445223\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005250949789769948\n",
      "Training Loss: 0.005099677001126111\n",
      "Training Loss: 0.005184217159985564\n",
      "Training Loss: 0.005901141088688746\n",
      "Training Loss: 0.0068971405818592756\n",
      "Training Loss: 0.006703841423150152\n",
      "Training Loss: 0.00660497470991686\n",
      "Validation Loss: 0.003789350798977923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005248825965682044\n",
      "Training Loss: 0.005097681556362659\n",
      "Training Loss: 0.005182099798694253\n",
      "Training Loss: 0.005899331076070666\n",
      "Training Loss: 0.0068955822405405345\n",
      "Training Loss: 0.006702331255655736\n",
      "Training Loss: 0.006603087197290734\n",
      "Validation Loss: 0.003788095667403652\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005246721807634458\n",
      "Training Loss: 0.005095700747915544\n",
      "Training Loss: 0.005180000032996759\n",
      "Training Loss: 0.005897531474474817\n",
      "Training Loss: 0.006894036348676309\n",
      "Training Loss: 0.006700831021880731\n",
      "Training Loss: 0.006601213305257261\n",
      "Validation Loss: 0.003786851377714746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.0052446356555446985\n",
      "Training Loss: 0.0050937357230577615\n",
      "Training Loss: 0.005177918532281183\n",
      "Training Loss: 0.005895742626744322\n",
      "Training Loss: 0.006892502591945231\n",
      "Training Loss: 0.0066993381967768075\n",
      "Training Loss: 0.006599352062912658\n",
      "Validation Loss: 0.0037856220593850375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.0052425690949894485\n",
      "Training Loss: 0.005091786326374859\n",
      "Training Loss: 0.0051758555404376235\n",
      "Training Loss: 0.00589396393566858\n",
      "Training Loss: 0.0068909795745275915\n",
      "Training Loss: 0.006697854897938668\n",
      "Training Loss: 0.006597502888180316\n",
      "Validation Loss: 0.0037844041148758726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0052405184670351445\n",
      "Training Loss: 0.0050898502866039055\n",
      "Training Loss: 0.005173808775725775\n",
      "Training Loss: 0.005892195358755998\n",
      "Training Loss: 0.0068894691532477735\n",
      "Training Loss: 0.0066963803593534975\n",
      "Training Loss: 0.006595664614578709\n",
      "Validation Loss: 0.003783199221091101\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.00523848571756389\n",
      "Training Loss: 0.005087929168366827\n",
      "Training Loss: 0.005171778408112004\n",
      "Training Loss: 0.005890435203327798\n",
      "Training Loss: 0.006887967297807336\n",
      "Training Loss: 0.006694912271341309\n",
      "Training Loss: 0.006593836211832241\n",
      "Validation Loss: 0.0037820092489601075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005236469974042848\n",
      "Training Loss: 0.005086022319737822\n",
      "Training Loss: 0.005169765612808987\n",
      "Training Loss: 0.005888683869852684\n",
      "Training Loss: 0.006886476336512714\n",
      "Training Loss: 0.006693450500024483\n",
      "Training Loss: 0.006592019431991503\n",
      "Validation Loss: 0.0037808330264814506\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005234470259165391\n",
      "Training Loss: 0.005084129322203808\n",
      "Training Loss: 0.00516776890377514\n",
      "Training Loss: 0.005886941585922614\n",
      "Training Loss: 0.006884994184365496\n",
      "Training Loss: 0.0066919970372691755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [42:54<04:48, 288.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00659021306084469\n",
      "Validation Loss: 0.0037796654257128076\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.38476585164666177\n",
      "Training Loss: 0.3157054241001606\n",
      "Training Loss: 0.2532561443001032\n",
      "Training Loss: 0.18719807658344506\n",
      "Training Loss: 0.136425850559026\n",
      "Training Loss: 0.09383968364447355\n",
      "Training Loss: 0.07309822315350174\n",
      "Validation Loss: 0.06671758022591863\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06487504016607999\n",
      "Training Loss: 0.06169808741658926\n",
      "Training Loss: 0.06128638410940766\n",
      "Training Loss: 0.06141716549172997\n",
      "Training Loss: 0.059868228286504746\n",
      "Training Loss: 0.06022033628076315\n",
      "Training Loss: 0.05843422319740057\n",
      "Validation Loss: 0.05639902304174302\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.05689621965400875\n",
      "Training Loss: 0.05443812483921647\n",
      "Training Loss: 0.05290136311203241\n",
      "Training Loss: 0.05146911025978625\n",
      "Training Loss: 0.047623201636597515\n",
      "Training Loss: 0.04459522814489901\n",
      "Training Loss: 0.03992315351031721\n",
      "Validation Loss: 0.036448317545044066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.03517451294697821\n",
      "Training Loss: 0.03125352472066879\n",
      "Training Loss: 0.028239664384163917\n",
      "Training Loss: 0.02682197850663215\n",
      "Training Loss: 0.023891450627706944\n",
      "Training Loss: 0.022356027299538256\n",
      "Training Loss: 0.02066162245813757\n",
      "Validation Loss: 0.018572661094367504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.018898137349169702\n",
      "Training Loss: 0.017578190190251915\n",
      "Training Loss: 0.016922439832706006\n",
      "Training Loss: 0.017304041606839748\n",
      "Training Loss: 0.016640497155021874\n",
      "Training Loss: 0.01603473308030516\n",
      "Training Loss: 0.015510968556627631\n",
      "Validation Loss: 0.013831871354164143\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.014537975401617586\n",
      "Training Loss: 0.01372407661518082\n",
      "Training Loss: 0.013381360750645398\n",
      "Training Loss: 0.01396051150513813\n",
      "Training Loss: 0.014019606774672866\n",
      "Training Loss: 0.013667852047365158\n",
      "Training Loss: 0.013440577315632253\n",
      "Validation Loss: 0.011717588710511446\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.012644240173976869\n",
      "Training Loss: 0.012025405643507838\n",
      "Training Loss: 0.011824163692072034\n",
      "Training Loss: 0.012406214298680425\n",
      "Training Loss: 0.012732115066610277\n",
      "Training Loss: 0.012420917886774987\n",
      "Training Loss: 0.012244084239937366\n",
      "Validation Loss: 0.010457974788820028\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.011437058942392468\n",
      "Training Loss: 0.010868021710775793\n",
      "Training Loss: 0.010708940217737108\n",
      "Training Loss: 0.011241127685643733\n",
      "Training Loss: 0.011709576414432377\n",
      "Training Loss: 0.011380812223069369\n",
      "Training Loss: 0.01121548933093436\n",
      "Validation Loss: 0.009358097560187515\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.010382729258853942\n",
      "Training Loss: 0.009846427442971617\n",
      "Training Loss: 0.009707782897166908\n",
      "Training Loss: 0.010243339987937361\n",
      "Training Loss: 0.010822591809555889\n",
      "Training Loss: 0.010484984940849245\n",
      "Training Loss: 0.010333896295633168\n",
      "Validation Loss: 0.008390844053996916\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.009473189522977918\n",
      "Training Loss: 0.008972716710995882\n",
      "Training Loss: 0.008845351942582055\n",
      "Training Loss: 0.009424814725061879\n",
      "Training Loss: 0.010097953032236547\n",
      "Training Loss: 0.009763789649587124\n",
      "Training Loss: 0.00962861027684994\n",
      "Validation Loss: 0.007581022757276446\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.008730116176884621\n",
      "Training Loss: 0.008267520064255223\n",
      "Training Loss: 0.008148386583197862\n",
      "Training Loss: 0.008790240850066766\n",
      "Training Loss: 0.009545917476061731\n",
      "Training Loss: 0.009222128435503692\n",
      "Training Loss: 0.00910345105919987\n",
      "Validation Loss: 0.006935788977765635\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.008154772449051962\n",
      "Training Loss: 0.007729464577278122\n",
      "Training Loss: 0.00761846435489133\n",
      "Training Loss: 0.008324893215904012\n",
      "Training Loss: 0.009152219670359046\n",
      "Training Loss: 0.008840042024385184\n",
      "Training Loss: 0.008737218332244083\n",
      "Validation Loss: 0.0064432838703209205\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.007728770719841123\n",
      "Training Loss: 0.0073376106424257155\n",
      "Training Loss: 0.007234413342084736\n",
      "Training Loss: 0.007997834957204759\n",
      "Training Loss: 0.00888481602538377\n",
      "Training Loss: 0.008581967235077172\n",
      "Training Loss: 0.008492650706321\n",
      "Validation Loss: 0.006078415297404322\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.007421490011038259\n",
      "Training Loss: 0.007059735739603639\n",
      "Training Loss: 0.0069631141819991174\n",
      "Training Loss: 0.007771453354507685\n",
      "Training Loss: 0.00870572313782759\n",
      "Training Loss: 0.008408893259475008\n",
      "Training Loss: 0.008329598132986575\n",
      "Validation Loss: 0.00581119186244905\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.00719981383648701\n",
      "Training Loss: 0.0068623356719035656\n",
      "Training Loss: 0.0067707987758331\n",
      "Training Loss: 0.007611338084097951\n",
      "Training Loss: 0.008581560549791901\n",
      "Training Loss: 0.008288103275699541\n",
      "Training Loss: 0.00821529877721332\n",
      "Validation Loss: 0.005613839177045036\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.0070355644437950105\n",
      "Training Loss: 0.006717808916000649\n",
      "Training Loss: 0.006630215463228523\n",
      "Training Loss: 0.007491948838578537\n",
      "Training Loss: 0.008488869512220845\n",
      "Training Loss: 0.00819726137444377\n",
      "Training Loss: 0.008128072081599385\n",
      "Validation Loss: 0.0054646315164724535\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.006908489037305117\n",
      "Training Loss: 0.006606881794286892\n",
      "Training Loss: 0.0065225727250799535\n",
      "Training Loss: 0.007397092382889241\n",
      "Training Loss: 0.008413788796169684\n",
      "Training Loss: 0.008123339457670227\n",
      "Training Loss: 0.00805568582029082\n",
      "Validation Loss: 0.005348369074559837\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.0068056033318862315\n",
      "Training Loss: 0.0065174973860848695\n",
      "Training Loss: 0.00643615783425048\n",
      "Training Loss: 0.007317557208589278\n",
      "Training Loss: 0.00834910623030737\n",
      "Training Loss: 0.00805955229443498\n",
      "Training Loss: 0.007992005530977621\n",
      "Validation Loss: 0.005255033512345135\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.006719108704710379\n",
      "Training Loss: 0.006442555537214503\n",
      "Training Loss: 0.006364036316517741\n",
      "Training Loss: 0.007248435016954317\n",
      "Training Loss: 0.008291397960856556\n",
      "Training Loss: 0.00800263518700376\n",
      "Training Loss: 0.007934237336739898\n",
      "Validation Loss: 0.005178176724676336\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.006644464628770948\n",
      "Training Loss: 0.006377991354092956\n",
      "Training Loss: 0.006302178562618792\n",
      "Training Loss: 0.007187180572655052\n",
      "Training Loss: 0.008239130103029312\n",
      "Training Loss: 0.007951075499877334\n",
      "Training Loss: 0.007881193078355863\n",
      "Validation Loss: 0.005113603490533844\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.006579000618075952\n",
      "Training Loss: 0.006321444197092205\n",
      "Training Loss: 0.00624818405485712\n",
      "Training Loss: 0.007132420437410474\n",
      "Training Loss: 0.008191594001837076\n",
      "Training Loss: 0.007904129256494342\n",
      "Training Loss: 0.007832359923049807\n",
      "Validation Loss: 0.005058481380664342\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0065210523828864095\n",
      "Training Loss: 0.0062714561523171146\n",
      "Training Loss: 0.006200527547625825\n",
      "Training Loss: 0.00708329706278164\n",
      "Training Loss: 0.008148361247731373\n",
      "Training Loss: 0.007861324646510183\n",
      "Training Loss: 0.007787426068680361\n",
      "Validation Loss: 0.005010802246462763\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.0064694617956411096\n",
      "Training Loss: 0.006227008393034339\n",
      "Training Loss: 0.006158130958210677\n",
      "Training Loss: 0.00703913650882896\n",
      "Training Loss: 0.00810904712183401\n",
      "Training Loss: 0.007822245229035615\n",
      "Training Loss: 0.0077460879669524725\n",
      "Validation Loss: 0.004969060138453928\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.006423306507058441\n",
      "Training Loss: 0.006187285775667988\n",
      "Training Loss: 0.0061201405758038166\n",
      "Training Loss: 0.00699931378418114\n",
      "Training Loss: 0.008073230534791946\n",
      "Training Loss: 0.007786461751675233\n",
      "Training Loss: 0.007707993031945079\n",
      "Validation Loss: 0.0049320625765632824\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0063817835610825565\n",
      "Training Loss: 0.006151566887274385\n",
      "Training Loss: 0.006085831437958405\n",
      "Training Loss: 0.006963214413262904\n",
      "Training Loss: 0.008040456802118569\n",
      "Training Loss: 0.007753534364746883\n",
      "Training Loss: 0.007672746963799\n",
      "Validation Loss: 0.004898855376339863\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.006344171615783125\n",
      "Training Loss: 0.006119196437648498\n",
      "Training Loss: 0.006054568651597947\n",
      "Training Loss: 0.006930251396843232\n",
      "Training Loss: 0.008010262160096317\n",
      "Training Loss: 0.007723031693603844\n",
      "Training Loss: 0.007639941085362807\n",
      "Validation Loss: 0.004868650794105974\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.006309815234271809\n",
      "Training Loss: 0.006089576735394076\n",
      "Training Loss: 0.006025797417387366\n",
      "Training Loss: 0.006899873553775251\n",
      "Training Loss: 0.007982192748459056\n",
      "Training Loss: 0.007694546938873828\n",
      "Training Loss: 0.0076091749628540125\n",
      "Validation Loss: 0.004840813558862618\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.006278139400528744\n",
      "Training Loss: 0.006062179685686715\n",
      "Training Loss: 0.005999033760745078\n",
      "Training Loss: 0.006871584071777761\n",
      "Training Loss: 0.007955835509346799\n",
      "Training Loss: 0.007667715062852949\n",
      "Training Loss: 0.007580077648162842\n",
      "Validation Loss: 0.004814819523485636\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.0062486384809017186\n",
      "Training Loss: 0.006036543615628034\n",
      "Training Loss: 0.005973870492307469\n",
      "Training Loss: 0.006844949798542075\n",
      "Training Loss: 0.00793082446907647\n",
      "Training Loss: 0.007642216162057593\n",
      "Training Loss: 0.0075523198116570715\n",
      "Validation Loss: 0.004790248760032687\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006220886276569217\n",
      "Training Loss: 0.0060122820071410385\n",
      "Training Loss: 0.005949961454607546\n",
      "Training Loss: 0.006819600758608431\n",
      "Training Loss: 0.007906843803357332\n",
      "Training Loss: 0.007617773802485317\n",
      "Training Loss: 0.0075256114616058765\n",
      "Validation Loss: 0.004766757490134306\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006194514177041128\n",
      "Training Loss: 0.005989070336800069\n",
      "Training Loss: 0.005927021071547642\n",
      "Training Loss: 0.006795220726635307\n",
      "Training Loss: 0.00788362608407624\n",
      "Training Loss: 0.007594152664532885\n",
      "Training Loss: 0.007499703638022766\n",
      "Validation Loss: 0.004744070079805476\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006169221574673429\n",
      "Training Loss: 0.005966644487925805\n",
      "Training Loss: 0.005904814304085448\n",
      "Training Loss: 0.00677155026991386\n",
      "Training Loss: 0.007860950742615386\n",
      "Training Loss: 0.007571155875921249\n",
      "Training Loss: 0.007474390455754474\n",
      "Validation Loss: 0.004721970715968127\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.006144762528128922\n",
      "Training Loss: 0.005944796615513042\n",
      "Training Loss: 0.005883153702598065\n",
      "Training Loss: 0.006748374114977196\n",
      "Training Loss: 0.007838638705434277\n",
      "Training Loss: 0.00754862096044235\n",
      "Training Loss: 0.007449497840134427\n",
      "Validation Loss: 0.004700287630648984\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006120933084748686\n",
      "Training Loss: 0.005923362158355303\n",
      "Training Loss: 0.005861889736261219\n",
      "Training Loss: 0.00672552541422192\n",
      "Training Loss: 0.007816553433658556\n",
      "Training Loss: 0.007526420167414472\n",
      "Training Loss: 0.007424895502626896\n",
      "Validation Loss: 0.00467888528414852\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006097571855643764\n",
      "Training Loss: 0.005902214501984418\n",
      "Training Loss: 0.005840904783690348\n",
      "Training Loss: 0.006702867647982202\n",
      "Training Loss: 0.007794584827497602\n",
      "Training Loss: 0.007504445848753676\n",
      "Training Loss: 0.007400470417924226\n",
      "Validation Loss: 0.004657663871474871\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.006074554892838932\n",
      "Training Loss: 0.005881261475733482\n",
      "Training Loss: 0.005820110574131831\n",
      "Training Loss: 0.006680298154824413\n",
      "Training Loss: 0.0077726520271971826\n",
      "Training Loss: 0.007482617956120521\n",
      "Training Loss: 0.007376141445711255\n",
      "Validation Loss: 0.004636543670805234\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006051783624570817\n",
      "Training Loss: 0.005860436395741999\n",
      "Training Loss: 0.005799445640295744\n",
      "Training Loss: 0.0066577402729308234\n",
      "Training Loss: 0.0077506970067042855\n",
      "Training Loss: 0.007460874498356133\n",
      "Training Loss: 0.0073518502560909835\n",
      "Validation Loss: 0.004615483093964919\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006029191203415394\n",
      "Training Loss: 0.0058396998583339155\n",
      "Training Loss: 0.0057788705249549824\n",
      "Training Loss: 0.0066351434728130695\n",
      "Training Loss: 0.0077286804479081185\n",
      "Training Loss: 0.007439170791767538\n",
      "Training Loss: 0.007327556774253025\n",
      "Validation Loss: 0.004594447592200105\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006006731085944921\n",
      "Training Loss: 0.005819027977995574\n",
      "Training Loss: 0.005758362520136871\n",
      "Training Loss: 0.006612473185523413\n",
      "Training Loss: 0.007706577864009887\n",
      "Training Loss: 0.007417473171371967\n",
      "Training Loss: 0.007303239156026393\n",
      "Validation Loss: 0.004573425845755787\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.005984373565879651\n",
      "Training Loss: 0.005798414767486975\n",
      "Training Loss: 0.005737915211357176\n",
      "Training Loss: 0.006589714654255658\n",
      "Training Loss: 0.007684376981342211\n",
      "Training Loss: 0.007395762113155797\n",
      "Training Loss: 0.00727888428256847\n",
      "Validation Loss: 0.004552418666923147\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.005962106270017102\n",
      "Training Loss: 0.005777865805430338\n",
      "Training Loss: 0.005717534077703022\n",
      "Training Loss: 0.006566862056497484\n",
      "Training Loss: 0.007662069250363857\n",
      "Training Loss: 0.007374023500597104\n",
      "Training Loss: 0.007254491695202887\n",
      "Validation Loss: 0.004531436349083896\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.005939924315898679\n",
      "Training Loss: 0.0057573921722359955\n",
      "Training Loss: 0.005697231358499266\n",
      "Training Loss: 0.006543923456920311\n",
      "Training Loss: 0.007639658133266494\n",
      "Training Loss: 0.0073522522440180186\n",
      "Training Loss: 0.0072300723660737275\n",
      "Validation Loss: 0.004510496000058196\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.005917835261789151\n",
      "Training Loss: 0.005737015019985847\n",
      "Training Loss: 0.005677030079532415\n",
      "Training Loss: 0.0065209154109470546\n",
      "Training Loss: 0.0076171484310179945\n",
      "Training Loss: 0.007330447069834918\n",
      "Training Loss: 0.0072056354756932705\n",
      "Validation Loss: 0.004489619805467095\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0058958458888810125\n",
      "Training Loss: 0.005716752018779516\n",
      "Training Loss: 0.0056569520983612166\n",
      "Training Loss: 0.006497856173664332\n",
      "Training Loss: 0.007594541850266978\n",
      "Training Loss: 0.007308606893057004\n",
      "Training Loss: 0.007181192487478256\n",
      "Validation Loss: 0.00446882608132123\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.00587396772694774\n",
      "Training Loss: 0.005696620644885115\n",
      "Training Loss: 0.005637019317946397\n",
      "Training Loss: 0.006474769550259225\n",
      "Training Loss: 0.007571845456259325\n",
      "Training Loss: 0.007286733677610755\n",
      "Training Loss: 0.007156756665790453\n",
      "Validation Loss: 0.00444812724045879\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.005852207430871203\n",
      "Training Loss: 0.005676631744136102\n",
      "Training Loss: 0.0056172493880148975\n",
      "Training Loss: 0.006451677739387378\n",
      "Training Loss: 0.007549060379387811\n",
      "Training Loss: 0.007264828080078587\n",
      "Training Loss: 0.007132333751069381\n",
      "Validation Loss: 0.0044275251455249244\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.005830569732934237\n",
      "Training Loss: 0.0056567890488076955\n",
      "Training Loss: 0.005597654749872163\n",
      "Training Loss: 0.0064286044129403305\n",
      "Training Loss: 0.007526189108612016\n",
      "Training Loss: 0.007242886638268828\n",
      "Training Loss: 0.007107927524484694\n",
      "Validation Loss: 0.004407023648689553\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.005809052926488221\n",
      "Training Loss: 0.005637084759073332\n",
      "Training Loss: 0.005578239760361612\n",
      "Training Loss: 0.006405567983165383\n",
      "Training Loss: 0.00750322911888361\n",
      "Training Loss: 0.007220907984301448\n",
      "Training Loss: 0.007083540327148512\n",
      "Validation Loss: 0.004386596817240705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.005787651546415873\n",
      "Training Loss: 0.005617506717680953\n",
      "Training Loss: 0.0055590046622091905\n",
      "Training Loss: 0.006382588815758936\n",
      "Training Loss: 0.007480182969011366\n",
      "Training Loss: 0.007198889829451219\n",
      "Training Loss: 0.007059169842395931\n",
      "Validation Loss: 0.004366215013668694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.005766350227058865\n",
      "Training Loss: 0.005598029809189029\n",
      "Training Loss: 0.005539938431465999\n",
      "Training Loss: 0.006359682920156047\n",
      "Training Loss: 0.0074570541421417145\n",
      "Training Loss: 0.007176829726668075\n",
      "Training Loss: 0.007034818278625608\n",
      "Validation Loss: 0.004345842145735656\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.005745139219798147\n",
      "Training Loss: 0.005578630827949382\n",
      "Training Loss: 0.005521033955155872\n",
      "Training Loss: 0.006336874038097449\n",
      "Training Loss: 0.00743385806796141\n",
      "Training Loss: 0.0071547374746296554\n",
      "Training Loss: 0.00701049564871937\n",
      "Validation Loss: 0.004325433972131196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.0057240093511063605\n",
      "Training Loss: 0.005559283021721057\n",
      "Training Loss: 0.005502283034729771\n",
      "Training Loss: 0.00631419055047445\n",
      "Training Loss: 0.007410621818853542\n",
      "Training Loss: 0.007132628917461261\n",
      "Training Loss: 0.00698622633703053\n",
      "Validation Loss: 0.00430495746927301\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.005702966693788767\n",
      "Training Loss: 0.0055399759841384365\n",
      "Training Loss: 0.005483690956607461\n",
      "Training Loss: 0.006291673929081298\n",
      "Training Loss: 0.007387395417317748\n",
      "Training Loss: 0.0071105430228635665\n",
      "Training Loss: 0.006962059331126511\n",
      "Validation Loss: 0.0042843923220334205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.005682030333555303\n",
      "Training Loss: 0.00552071328216698\n",
      "Training Loss: 0.005465273860609159\n",
      "Training Loss: 0.006269383258768357\n",
      "Training Loss: 0.007364252952393144\n",
      "Training Loss: 0.0070885381940752265\n",
      "Training Loss: 0.006938065565191209\n",
      "Validation Loss: 0.004263734733616089\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0056612421089084815\n",
      "Training Loss: 0.0055015184648800645\n",
      "Training Loss: 0.005447067583445459\n",
      "Training Loss: 0.006247394200763665\n",
      "Training Loss: 0.007341295650694519\n",
      "Training Loss: 0.0070666970766615125\n",
      "Training Loss: 0.006914348669815809\n",
      "Validation Loss: 0.004243013689692101\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.005640676084440201\n",
      "Training Loss: 0.00548244402918499\n",
      "Training Loss: 0.00542913188226521\n",
      "Training Loss: 0.006225804269197397\n",
      "Training Loss: 0.007318651855457574\n",
      "Training Loss: 0.007045128818135709\n",
      "Training Loss: 0.006891035316511989\n",
      "Validation Loss: 0.004222292230248256\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.005620428084512241\n",
      "Training Loss: 0.005463566664257086\n",
      "Training Loss: 0.0054115420824382456\n",
      "Training Loss: 0.006204727530712262\n",
      "Training Loss: 0.007296470270957798\n",
      "Training Loss: 0.007023963396204635\n",
      "Training Loss: 0.006868276911554858\n",
      "Validation Loss: 0.004201650594218114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.005600616554729641\n",
      "Training Loss: 0.005444982374319807\n",
      "Training Loss: 0.005394385932595469\n",
      "Training Loss: 0.006184284879127517\n",
      "Training Loss: 0.007274910372216254\n",
      "Training Loss: 0.007003338652430103\n",
      "Training Loss: 0.00684623115696013\n",
      "Validation Loss: 0.00418119831141503\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.005581372278975323\n",
      "Training Loss: 0.005426800940767862\n",
      "Training Loss: 0.0053777557628927755\n",
      "Training Loss: 0.006164591879933141\n",
      "Training Loss: 0.007254128922941163\n",
      "Training Loss: 0.0069833970116451385\n",
      "Training Loss: 0.006825051994528621\n",
      "Validation Loss: 0.004161053706023298\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.005562825707020238\n",
      "Training Loss: 0.005409128312603571\n",
      "Training Loss: 0.005361738175270148\n",
      "Training Loss: 0.006145759266219102\n",
      "Training Loss: 0.007234266678569839\n",
      "Training Loss: 0.006964269680902362\n",
      "Training Loss: 0.006804875043453649\n",
      "Validation Loss: 0.004141336456045843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.0055450948845827954\n",
      "Training Loss: 0.0053920669865328815\n",
      "Training Loss: 0.005346405249438249\n",
      "Training Loss: 0.00612786750367377\n",
      "Training Loss: 0.007215440855361521\n",
      "Training Loss: 0.006946066648233682\n",
      "Training Loss: 0.006785812402376905\n",
      "Validation Loss: 0.00412215064942474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.005528271031798795\n",
      "Training Loss: 0.005375694752437994\n",
      "Training Loss: 0.00533180212485604\n",
      "Training Loss: 0.00611097150889691\n",
      "Training Loss: 0.007197728819446639\n",
      "Training Loss: 0.006928866833914071\n",
      "Training Loss: 0.006767931927461177\n",
      "Validation Loss: 0.004103587899786117\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.005512419331353158\n",
      "Training Loss: 0.005360069789458066\n",
      "Training Loss: 0.0053179570735665035\n",
      "Training Loss: 0.00609509228146635\n",
      "Training Loss: 0.007181173176504672\n",
      "Training Loss: 0.006912720949621871\n",
      "Training Loss: 0.006751266968203709\n",
      "Validation Loss: 0.004085719514784686\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.005497566661797464\n",
      "Training Loss: 0.005345219339360483\n",
      "Training Loss: 0.0053048656450118865\n",
      "Training Loss: 0.006080221626325511\n",
      "Training Loss: 0.007165778116323054\n",
      "Training Loss: 0.0068976450688205656\n",
      "Training Loss: 0.006735813928535208\n",
      "Validation Loss: 0.004068586321317413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.005483709094696678\n",
      "Training Loss: 0.0053311513742664825\n",
      "Training Loss: 0.005292510164435953\n",
      "Training Loss: 0.0060663254908286035\n",
      "Training Loss: 0.0071515185944736005\n",
      "Training Loss: 0.006883629360236228\n",
      "Training Loss: 0.0067215383995790036\n",
      "Validation Loss: 0.004052214824144592\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.005470819817855954\n",
      "Training Loss: 0.00531785266357474\n",
      "Training Loss: 0.005280855123419315\n",
      "Training Loss: 0.00605334899679292\n",
      "Training Loss: 0.007138343513943255\n",
      "Training Loss: 0.006870639120461419\n",
      "Training Loss: 0.0067083807801827785\n",
      "Validation Loss: 0.0040366170436932185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.005458849470014684\n",
      "Training Loss: 0.005305296324659139\n",
      "Training Loss: 0.005269854558282532\n",
      "Training Loss: 0.006041229452821426\n",
      "Training Loss: 0.007126186394598335\n",
      "Training Loss: 0.006858625861350447\n",
      "Training Loss: 0.006696270038373768\n",
      "Validation Loss: 0.004021786818671483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.00544773890695069\n",
      "Training Loss: 0.005293443924165331\n",
      "Training Loss: 0.005259458122891374\n",
      "Training Loss: 0.006029893375816755\n",
      "Training Loss: 0.007114974664291367\n",
      "Training Loss: 0.006847529364749789\n",
      "Training Loss: 0.006685124348150566\n",
      "Validation Loss: 0.004007709144583244\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.005437419789959676\n",
      "Training Loss: 0.0052822538913460445\n",
      "Training Loss: 0.005249615689390339\n",
      "Training Loss: 0.006019268217496574\n",
      "Training Loss: 0.007104627002263442\n",
      "Training Loss: 0.006837281222688034\n",
      "Training Loss: 0.006674857389880344\n",
      "Validation Loss: 0.003994369180001012\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.005427825081860647\n",
      "Training Loss: 0.005271680153091438\n",
      "Training Loss: 0.005240276972181164\n",
      "Training Loss: 0.00600928258325439\n",
      "Training Loss: 0.007095066040055826\n",
      "Training Loss: 0.006827812882838771\n",
      "Training Loss: 0.006665387246757745\n",
      "Validation Loss: 0.003981738470316854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0054188851098297165\n",
      "Training Loss: 0.005261680245748721\n",
      "Training Loss: 0.005231392988935113\n",
      "Training Loss: 0.005999871136154979\n",
      "Training Loss: 0.007086216895841062\n",
      "Training Loss: 0.006819055834785104\n",
      "Training Loss: 0.006656631786609068\n",
      "Validation Loss: 0.003969791726373164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.005410531996749342\n",
      "Training Loss: 0.005252206836012192\n",
      "Training Loss: 0.005222918406361714\n",
      "Training Loss: 0.005990969048580155\n",
      "Training Loss: 0.007078007019590587\n",
      "Training Loss: 0.006810939341085032\n",
      "Training Loss: 0.006648514652624726\n",
      "Validation Loss: 0.003958498497061041\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.005402709834743291\n",
      "Training Loss: 0.005243221225100569\n",
      "Training Loss: 0.005214814062346704\n",
      "Training Loss: 0.0059825260145589705\n",
      "Training Loss: 0.00707037415006198\n",
      "Training Loss: 0.006803406231338158\n",
      "Training Loss: 0.006640969699947164\n",
      "Validation Loss: 0.003947829581755265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.005395362690323964\n",
      "Training Loss: 0.005234686417970806\n",
      "Training Loss: 0.005207045750576071\n",
      "Training Loss: 0.00597449153137859\n",
      "Training Loss: 0.007063258669804781\n",
      "Training Loss: 0.006796396941645071\n",
      "Training Loss: 0.006633933001430705\n",
      "Validation Loss: 0.003937759821665951\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.005388442092807964\n",
      "Training Loss: 0.005226565308985301\n",
      "Training Loss: 0.005199577945750207\n",
      "Training Loss: 0.005966824338538572\n",
      "Training Loss: 0.007056607409613207\n",
      "Training Loss: 0.006789855152601376\n",
      "Training Loss: 0.006627348170150071\n",
      "Validation Loss: 0.003928253372210298\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.005381902062799782\n",
      "Training Loss: 0.00521882739441935\n",
      "Training Loss: 0.00519238269829657\n",
      "Training Loss: 0.005959488040534779\n",
      "Training Loss: 0.007050373487873003\n",
      "Training Loss: 0.006783734861528501\n",
      "Training Loss: 0.006621167263947427\n",
      "Validation Loss: 0.003919280713601979\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.0053757020441116765\n",
      "Training Loss: 0.00521144257101696\n",
      "Training Loss: 0.00518543467100244\n",
      "Training Loss: 0.005952450672048144\n",
      "Training Loss: 0.007044515730813146\n",
      "Training Loss: 0.006777992082061246\n",
      "Training Loss: 0.006615345447789878\n",
      "Validation Loss: 0.003910813756611491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.005369808219838888\n",
      "Training Loss: 0.005204384763492271\n",
      "Training Loss: 0.005178711297921836\n",
      "Training Loss: 0.005945684787002392\n",
      "Training Loss: 0.007038995816837996\n",
      "Training Loss: 0.006772588164312765\n",
      "Training Loss: 0.0066098435211461035\n",
      "Validation Loss: 0.0039028233546791564\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00536418966948986\n",
      "Training Loss: 0.005197627948364243\n",
      "Training Loss: 0.005172190401353873\n",
      "Training Loss: 0.0059391675447113814\n",
      "Training Loss: 0.007033781054196879\n",
      "Training Loss: 0.006767488402547314\n",
      "Training Loss: 0.006604628886561841\n",
      "Validation Loss: 0.003895279701122463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.005358815133804456\n",
      "Training Loss: 0.00519114971277304\n",
      "Training Loss: 0.0051658559782663364\n",
      "Training Loss: 0.005932878466555849\n",
      "Training Loss: 0.007028842689469457\n",
      "Training Loss: 0.00676266090827994\n",
      "Training Loss: 0.006599671595031395\n",
      "Validation Loss: 0.0038881570555109267\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.005353662732522935\n",
      "Training Loss: 0.005184932375559583\n",
      "Training Loss: 0.005159691478474997\n",
      "Training Loss: 0.005926800685119815\n",
      "Training Loss: 0.007024154702667147\n",
      "Training Loss: 0.006758080495055765\n",
      "Training Loss: 0.006594943786039948\n",
      "Validation Loss: 0.0038814283951014725\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.005348708790261298\n",
      "Training Loss: 0.005178953498834744\n",
      "Training Loss: 0.005153681975789368\n",
      "Training Loss: 0.005920918594347313\n",
      "Training Loss: 0.007019693470792845\n",
      "Training Loss: 0.006753721470013261\n",
      "Training Loss: 0.0065904225839767605\n",
      "Validation Loss: 0.003875067110054809\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.005343934682896361\n",
      "Training Loss: 0.005173196906107478\n",
      "Training Loss: 0.005147815069067292\n",
      "Training Loss: 0.005915217799483798\n",
      "Training Loss: 0.0070154383277986196\n",
      "Training Loss: 0.006749560460448265\n",
      "Training Loss: 0.006586088467156514\n",
      "Validation Loss: 0.0038690544335234366\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.0053393214440438895\n",
      "Training Loss: 0.005167646842310205\n",
      "Training Loss: 0.005142079918878153\n",
      "Training Loss: 0.005909686769009568\n",
      "Training Loss: 0.007011372080305591\n",
      "Training Loss: 0.006745581382419914\n",
      "Training Loss: 0.006581923305056989\n",
      "Validation Loss: 0.003863362123237483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.005334854301181622\n",
      "Training Loss: 0.00516228889755439\n",
      "Training Loss: 0.005136465884279459\n",
      "Training Loss: 0.00590431492368225\n",
      "Training Loss: 0.007007477056467906\n",
      "Training Loss: 0.006741767098428681\n",
      "Training Loss: 0.0065779105469118805\n",
      "Validation Loss: 0.003857968495426218\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.005330518062110059\n",
      "Training Loss: 0.0051571066287579015\n",
      "Training Loss: 0.005130962041439488\n",
      "Training Loss: 0.005899091858300381\n",
      "Training Loss: 0.007003740859217942\n",
      "Training Loss: 0.006738099690992385\n",
      "Training Loss: 0.006574035123921931\n",
      "Validation Loss: 0.0038528608124221812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.005326300835586153\n",
      "Training Loss: 0.005152089292532764\n",
      "Training Loss: 0.005125561943277717\n",
      "Training Loss: 0.005894010043703019\n",
      "Training Loss: 0.00700014847679995\n",
      "Training Loss: 0.006734569808468222\n",
      "Training Loss: 0.0065702852664981035\n",
      "Validation Loss: 0.003848016204500801\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.005322191106970422\n",
      "Training Loss: 0.005147224924876355\n",
      "Training Loss: 0.005120255567017011\n",
      "Training Loss: 0.0058890588284702975\n",
      "Training Loss: 0.006996688219951466\n",
      "Training Loss: 0.006731162914074957\n",
      "Training Loss: 0.006566647326108069\n",
      "Validation Loss: 0.003843414897189828\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.005318177186418325\n",
      "Training Loss: 0.0051425012142863125\n",
      "Training Loss: 0.005115038498770446\n",
      "Training Loss: 0.005884231405798346\n",
      "Training Loss: 0.006993349972181023\n",
      "Training Loss: 0.006727869374444708\n",
      "Training Loss: 0.006563112206058577\n",
      "Validation Loss: 0.0038390374866275863\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.005314250773517415\n",
      "Training Loss: 0.005137909079203382\n",
      "Training Loss: 0.005109901552204974\n",
      "Training Loss: 0.005879520466551185\n",
      "Training Loss: 0.006990121408598497\n",
      "Training Loss: 0.006724679679609835\n",
      "Training Loss: 0.006559669396374375\n",
      "Validation Loss: 0.00383487453096466\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.005310403463663533\n",
      "Training Loss: 0.005133436551550403\n",
      "Training Loss: 0.005104838940314949\n",
      "Training Loss: 0.005874918765039183\n",
      "Training Loss: 0.006986997441854328\n",
      "Training Loss: 0.0067215849063359204\n",
      "Training Loss: 0.006556310994783416\n",
      "Validation Loss: 0.003830913972276985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0053066272067371755\n",
      "Training Loss: 0.0051290789304766805\n",
      "Training Loss: 0.00509984734817408\n",
      "Training Loss: 0.005870421251747757\n",
      "Training Loss: 0.0069839682208839805\n",
      "Training Loss: 0.006718578665750101\n",
      "Training Loss: 0.006553028080379591\n",
      "Validation Loss: 0.0038271350764289276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.005302913061459549\n",
      "Training Loss: 0.005124822648940608\n",
      "Training Loss: 0.005094917754177004\n",
      "Training Loss: 0.005866020605317317\n",
      "Training Loss: 0.006981024906272068\n",
      "Training Loss: 0.006715653701685369\n",
      "Training Loss: 0.006549814169993624\n",
      "Validation Loss: 0.0038235270442643415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.005299256768194027\n",
      "Training Loss: 0.00512066375464201\n",
      "Training Loss: 0.005090048085548915\n",
      "Training Loss: 0.005861709238961339\n",
      "Training Loss: 0.006978161482838914\n",
      "Training Loss: 0.006712802269030363\n",
      "Training Loss: 0.00654666330316104\n",
      "Validation Loss: 0.0038200804178013504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.005295650527114048\n",
      "Training Loss: 0.005116591915721074\n",
      "Training Loss: 0.005085232058772818\n",
      "Training Loss: 0.005857483466970734\n",
      "Training Loss: 0.006975370574509725\n",
      "Training Loss: 0.006710020418977365\n",
      "Training Loss: 0.006543566663749516\n",
      "Validation Loss: 0.0038167794317021018\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.005292089394642972\n",
      "Training Loss: 0.005112601618166081\n",
      "Training Loss: 0.005080465992796235\n",
      "Training Loss: 0.005853337156586349\n",
      "Training Loss: 0.006972648103255779\n",
      "Training Loss: 0.00670730194891803\n",
      "Training Loss: 0.006540521590504795\n",
      "Validation Loss: 0.0038136232719313675\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.005288566923118196\n",
      "Training Loss: 0.005108684411970899\n",
      "Training Loss: 0.005075743434135802\n",
      "Training Loss: 0.005849264304852113\n",
      "Training Loss: 0.006969985319301486\n",
      "Training Loss: 0.006704643222037703\n",
      "Training Loss: 0.006537519681733102\n",
      "Validation Loss: 0.003810591006839878\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.005285079121240415\n",
      "Training Loss: 0.005104834950761869\n",
      "Training Loss: 0.005071062060887925\n",
      "Training Loss: 0.005845259348279797\n",
      "Training Loss: 0.006967378518311307\n",
      "Training Loss: 0.0067020395596046\n",
      "Training Loss: 0.006534557859413326\n",
      "Validation Loss: 0.0038076796789552445\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.005281619573943317\n",
      "Training Loss: 0.005101046658819541\n",
      "Training Loss: 0.005066416079062037\n",
      "Training Loss: 0.005841318352031522\n",
      "Training Loss: 0.006964820832945406\n",
      "Training Loss: 0.006699485613498837\n",
      "Training Loss: 0.006531629109522328\n",
      "Validation Loss: 0.003804877427431026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.005278185522765853\n",
      "Training Loss: 0.005097314664744772\n",
      "Training Loss: 0.005061804041615687\n",
      "Training Loss: 0.005837434331187979\n",
      "Training Loss: 0.006962307479698211\n",
      "Training Loss: 0.00669697820325382\n",
      "Training Loss: 0.006528730527497828\n",
      "Validation Loss: 0.003802181129110105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0052747722342610355\n",
      "Training Loss: 0.0050936323386849836\n",
      "Training Loss: 0.005057219148729928\n",
      "Training Loss: 0.005833604074432515\n",
      "Training Loss: 0.006959834942827001\n",
      "Training Loss: 0.006694514100672677\n",
      "Training Loss: 0.006525857743108645\n",
      "Validation Loss: 0.003799579978732162\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.005271373203140683\n",
      "Training Loss: 0.005089993039146066\n",
      "Training Loss: 0.005052657945198007\n",
      "Training Loss: 0.005829822900122963\n",
      "Training Loss: 0.006957396642537788\n",
      "Training Loss: 0.006692088949494064\n",
      "Training Loss: 0.00652300675981678\n",
      "Validation Loss: 0.003797063222922357\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.005267985020764172\n",
      "Training Loss: 0.005086391934892163\n",
      "Training Loss: 0.005048115949030034\n",
      "Training Loss: 0.00582608521450311\n",
      "Training Loss: 0.006954989067744464\n",
      "Training Loss: 0.006689700292190537\n",
      "Training Loss: 0.006520170188741759\n",
      "Validation Loss: 0.003794628994675416\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.005264604300027714\n",
      "Training Loss: 0.005082823803531937\n",
      "Training Loss: 0.005043589063570835\n",
      "Training Loss: 0.0058223871613154185\n",
      "Training Loss: 0.006952605644473806\n",
      "Training Loss: 0.006687345007667318\n",
      "Training Loss: 0.006517346751643345\n",
      "Validation Loss: 0.0037922659022942857\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0052612257294822485\n",
      "Training Loss: 0.005079283659579232\n",
      "Training Loss: 0.00503907380043529\n",
      "Training Loss: 0.0058187218068633225\n",
      "Training Loss: 0.006950240884907544\n",
      "Training Loss: 0.006685016195988282\n",
      "Training Loss: 0.006514530086424202\n",
      "Validation Loss: 0.003789971438707428\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.005257845598389394\n",
      "Training Loss: 0.005075765766086988\n",
      "Training Loss: 0.005034564728848636\n",
      "Training Loss: 0.005815085832728073\n",
      "Training Loss: 0.0069478930719196795\n",
      "Training Loss: 0.006682716232025996\n",
      "Training Loss: 0.006511717167450115\n",
      "Validation Loss: 0.0037877355267953\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.005254459206480533\n",
      "Training Loss: 0.005072264673071913\n",
      "Training Loss: 0.0050300579296890646\n",
      "Training Loss: 0.005811473554931581\n",
      "Training Loss: 0.0069455553800798955\n",
      "Training Loss: 0.006680436878232285\n",
      "Training Loss: 0.0065089039981830864\n",
      "Validation Loss: 0.003785553529565589\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.005251060022856109\n",
      "Training Loss: 0.0050687743781600144\n",
      "Training Loss: 0.0050255474261939525\n",
      "Training Loss: 0.005807881085784175\n",
      "Training Loss: 0.006943223071284592\n",
      "Training Loss: 0.006678178570000455\n",
      "Training Loss: 0.006506085414439439\n",
      "Validation Loss: 0.003783417051162724\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.005247646787320264\n",
      "Training Loss: 0.0050652890832861884\n",
      "Training Loss: 0.005021029599593021\n",
      "Training Loss: 0.005804302673786879\n",
      "Training Loss: 0.006940889299148694\n",
      "Training Loss: 0.006675935804378241\n",
      "Training Loss: 0.006503258014563471\n",
      "Validation Loss: 0.0037813228799190635\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.005244214229751378\n",
      "Training Loss: 0.005061805819859728\n",
      "Training Loss: 0.005016500253695995\n",
      "Training Loss: 0.005800735115190037\n",
      "Training Loss: 0.006938551547937095\n",
      "Training Loss: 0.006673706590663642\n",
      "Training Loss: 0.006500416374765336\n",
      "Validation Loss: 0.0037792630830242504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.005240756383864209\n",
      "Training Loss: 0.005058316787471995\n",
      "Training Loss: 0.0050119530409574505\n",
      "Training Loss: 0.005797170536825433\n",
      "Training Loss: 0.0069362019328400495\n",
      "Training Loss: 0.0066714854317251595\n",
      "Training Loss: 0.00649755651014857\n",
      "Validation Loss: 0.003777231102851167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.005237267473712563\n",
      "Training Loss: 0.005054815958719701\n",
      "Training Loss: 0.005007381940959022\n",
      "Training Loss: 0.005793606000952423\n",
      "Training Loss: 0.006933835570234806\n",
      "Training Loss: 0.006669270762940869\n",
      "Training Loss: 0.006494673222769052\n",
      "Validation Loss: 0.003775226711365307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.005233745409641415\n",
      "Training Loss: 0.005051299791666679\n",
      "Training Loss: 0.005002782912924886\n",
      "Training Loss: 0.005790036444086582\n",
      "Training Loss: 0.006931447035167366\n",
      "Training Loss: 0.00666705819661729\n",
      "Training Loss: 0.0064917616872116925\n",
      "Validation Loss: 0.003773234298449247\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.005230181901715696\n",
      "Training Loss: 0.005047759145963937\n",
      "Training Loss: 0.004998149405000731\n",
      "Training Loss: 0.00578645393892657\n",
      "Training Loss: 0.00692902859300375\n",
      "Training Loss: 0.006664844017941505\n",
      "Training Loss: 0.0064888168964535\n",
      "Validation Loss: 0.00377125565960222\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.005226573832333088\n",
      "Training Loss: 0.005044191579800099\n",
      "Training Loss: 0.004993475758237764\n",
      "Training Loss: 0.0057828571938443925\n",
      "Training Loss: 0.006926575476536527\n",
      "Training Loss: 0.006662623594747856\n",
      "Training Loss: 0.006485833125188947\n",
      "Validation Loss: 0.0037692831070528605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.005222915365593508\n",
      "Training Loss: 0.005040588726988062\n",
      "Training Loss: 0.004988755629165098\n",
      "Training Loss: 0.005779236822854727\n",
      "Training Loss: 0.00692407984053716\n",
      "Training Loss: 0.0066603920923080295\n",
      "Training Loss: 0.00648280696477741\n",
      "Validation Loss: 0.00376730711436534\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.00521919759514276\n",
      "Training Loss: 0.005036944887251593\n",
      "Training Loss: 0.0049839837686158715\n",
      "Training Loss: 0.005775588385295123\n",
      "Training Loss: 0.00692153338342905\n",
      "Training Loss: 0.006658148056594655\n",
      "Training Loss: 0.006479731278959662\n",
      "Validation Loss: 0.00376532864469114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.0052154175279429185\n",
      "Training Loss: 0.005033253188012168\n",
      "Training Loss: 0.004979151847655885\n",
      "Training Loss: 0.005771908001624979\n",
      "Training Loss: 0.006918931166874245\n",
      "Training Loss: 0.006655884055653587\n",
      "Training Loss: 0.006476600005989894\n",
      "Validation Loss: 0.0037633313449476373\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.005211567408987321\n",
      "Training Loss: 0.005029506593709812\n",
      "Training Loss: 0.004974253579275683\n",
      "Training Loss: 0.005768187279463746\n",
      "Training Loss: 0.006916262784507125\n",
      "Training Loss: 0.00665359688224271\n",
      "Training Loss: 0.00647340665338561\n",
      "Validation Loss: 0.003761314716772612\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.005207639474538155\n",
      "Training Loss: 0.005025699801626616\n",
      "Training Loss: 0.004969282622914762\n",
      "Training Loss: 0.0057644198270281775\n",
      "Training Loss: 0.006913520334055647\n",
      "Training Loss: 0.006651280273217708\n",
      "Training Loss: 0.006470146492356435\n",
      "Validation Loss: 0.0037592694787692265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.005203628357849084\n",
      "Training Loss: 0.005021824360010214\n",
      "Training Loss: 0.004964231388294138\n",
      "Training Loss: 0.005760600781068206\n",
      "Training Loss: 0.006910695414990187\n",
      "Training Loss: 0.00664892946369946\n",
      "Training Loss: 0.006466813519364223\n",
      "Validation Loss: 0.003757191364119729\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.005199526284122839\n",
      "Training Loss: 0.005017874303157441\n",
      "Training Loss: 0.004959093113429844\n",
      "Training Loss: 0.005756723139784299\n",
      "Training Loss: 0.006907778442837298\n",
      "Training Loss: 0.006646538991481066\n",
      "Training Loss: 0.006463400145294145\n",
      "Validation Loss: 0.0037550713874797306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.005195323947118595\n",
      "Training Loss: 0.005013841139734723\n",
      "Training Loss: 0.004953859715606086\n",
      "Training Loss: 0.005752781609771773\n",
      "Training Loss: 0.006904759845929221\n",
      "Training Loss: 0.006644103216240182\n",
      "Training Loss: 0.0064598996611312035\n",
      "Validation Loss: 0.0037529026145073636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.005191013683797791\n",
      "Training Loss: 0.005009717621724121\n",
      "Training Loss: 0.00494852309115231\n",
      "Training Loss: 0.005748769356869161\n",
      "Training Loss: 0.006901629670755938\n",
      "Training Loss: 0.006641617898130789\n",
      "Training Loss: 0.006456305227475241\n",
      "Validation Loss: 0.003750671882859051\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.005186589909135364\n",
      "Training Loss: 0.005005500154802576\n",
      "Training Loss: 0.004943079479271546\n",
      "Training Loss: 0.005744679658673704\n",
      "Training Loss: 0.006898376131430268\n",
      "Training Loss: 0.006639073578407988\n",
      "Training Loss: 0.006452612992143258\n",
      "Validation Loss: 0.0037483799019270344\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00518204205611255\n",
      "Training Loss: 0.005001178910606541\n",
      "Training Loss: 0.004937519294326193\n",
      "Training Loss: 0.005740507353330031\n",
      "Training Loss: 0.0068949896853882815\n",
      "Training Loss: 0.006636466770432889\n",
      "Training Loss: 0.0064488125988282265\n",
      "Validation Loss: 0.0037460139759010478\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.005177364305127412\n",
      "Training Loss: 0.0049967479798942805\n",
      "Training Loss: 0.004931836524629034\n",
      "Training Loss: 0.005736246614833363\n",
      "Training Loss: 0.0068914581125136464\n",
      "Training Loss: 0.006633790626656264\n",
      "Training Loss: 0.006444898995105177\n",
      "Validation Loss: 0.0037435663638162088\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.005172547155525536\n",
      "Training Loss: 0.004992202695575543\n",
      "Training Loss: 0.004926025990280323\n",
      "Training Loss: 0.005731891395989805\n",
      "Training Loss: 0.0068877712753601375\n",
      "Training Loss: 0.0066310381272342055\n",
      "Training Loss: 0.0064408657210879025\n",
      "Validation Loss: 0.003741031521774326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.005167582619469613\n",
      "Training Loss: 0.004987536363187246\n",
      "Training Loss: 0.00492008266155608\n",
      "Training Loss: 0.005727436965680681\n",
      "Training Loss: 0.00688391849398613\n",
      "Training Loss: 0.00662820418481715\n",
      "Training Loss: 0.006436709415866062\n",
      "Validation Loss: 0.0037383960463169418\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.005162464017048478\n",
      "Training Loss: 0.004982743025757372\n",
      "Training Loss: 0.00491400022059679\n",
      "Training Loss: 0.005722879463573918\n",
      "Training Loss: 0.0068798864597920326\n",
      "Training Loss: 0.006625283135799691\n",
      "Training Loss: 0.006432421003701165\n",
      "Validation Loss: 0.0037356563621420372\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.005157183108385652\n",
      "Training Loss: 0.004977821141947061\n",
      "Training Loss: 0.004907776108011603\n",
      "Training Loss: 0.005718213978689164\n",
      "Training Loss: 0.0068756657559424636\n",
      "Training Loss: 0.006622269020881504\n",
      "Training Loss: 0.006427996921120211\n",
      "Validation Loss: 0.0037328020154153195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.005151733958045952\n",
      "Training Loss: 0.004972766331047751\n",
      "Training Loss: 0.0049014076858293265\n",
      "Training Loss: 0.005713438304956071\n",
      "Training Loss: 0.0068712471879553046\n",
      "Training Loss: 0.0066191566281486305\n",
      "Training Loss: 0.006423434821190312\n",
      "Validation Loss: 0.003729825187561692\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.0051461110496893525\n",
      "Training Loss: 0.004967577334609814\n",
      "Training Loss: 0.004894895182806067\n",
      "Training Loss: 0.005708551544230432\n",
      "Training Loss: 0.006866618494968862\n",
      "Training Loss: 0.006615939571056515\n",
      "Training Loss: 0.00641872946289368\n",
      "Validation Loss: 0.00372671830896916\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.005140308807604015\n",
      "Training Loss: 0.004962252370896749\n",
      "Training Loss: 0.004888235442922451\n",
      "Training Loss: 0.005703552052727901\n",
      "Training Loss: 0.006861772793345153\n",
      "Training Loss: 0.0066126159578561786\n",
      "Training Loss: 0.006413878285093233\n",
      "Validation Loss: 0.0037234744867170404\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005134323130478151\n",
      "Training Loss: 0.004956792103475891\n",
      "Training Loss: 0.004881433151313104\n",
      "Training Loss: 0.00569843914359808\n",
      "Training Loss: 0.0068567030073609205\n",
      "Training Loss: 0.006609180950326845\n",
      "Training Loss: 0.006408882886171341\n",
      "Validation Loss: 0.003720092122819735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.0051281512377318\n",
      "Training Loss: 0.004951199928182177\n",
      "Training Loss: 0.004874492206727155\n",
      "Training Loss: 0.005693215745850466\n",
      "Training Loss: 0.006851404488552362\n",
      "Training Loss: 0.006605635235318914\n",
      "Training Loss: 0.006403740308014676\n",
      "Validation Loss: 0.0037165623204433794\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.005121793810976669\n",
      "Training Loss: 0.004945478891022504\n",
      "Training Loss: 0.004867416739580221\n",
      "Training Loss: 0.0056878852262161675\n",
      "Training Loss: 0.006845872229896486\n",
      "Training Loss: 0.006601974230725318\n",
      "Training Loss: 0.006398455172311515\n",
      "Validation Loss: 0.0037128794667998168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.0051152484933845695\n",
      "Training Loss: 0.004939633172471076\n",
      "Training Loss: 0.004860214008367621\n",
      "Training Loss: 0.005682449871383142\n",
      "Training Loss: 0.00684010490658693\n",
      "Training Loss: 0.006598199714208022\n",
      "Training Loss: 0.006393028432503342\n",
      "Validation Loss: 0.003709041297676439\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.0051085162936942655\n",
      "Training Loss: 0.004933667772565969\n",
      "Training Loss: 0.004852891865884885\n",
      "Training Loss: 0.005676917202072218\n",
      "Training Loss: 0.006834101085551083\n",
      "Training Loss: 0.006594312182860449\n",
      "Training Loss: 0.006387464964063838\n",
      "Validation Loss: 0.0037050483271319156\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005101601856295019\n",
      "Training Loss: 0.004927593658212572\n",
      "Training Loss: 0.004845461962395348\n",
      "Training Loss: 0.0056712924552266486\n",
      "Training Loss: 0.006827863972866908\n",
      "Training Loss: 0.006590312630869448\n",
      "Training Loss: 0.0063817713980097324\n",
      "Validation Loss: 0.0037008954150155745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.005094508567708544\n",
      "Training Loss: 0.0049214178032707424\n",
      "Training Loss: 0.004837935189134441\n",
      "Training Loss: 0.005665584759844933\n",
      "Training Loss: 0.006821398987667635\n",
      "Training Loss: 0.006586204025661573\n",
      "Training Loss: 0.00637595332460478\n",
      "Validation Loss: 0.003696585875457983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.005087242607260123\n",
      "Training Loss: 0.004915148742147721\n",
      "Training Loss: 0.004830324101494626\n",
      "Training Loss: 0.00565979998500552\n",
      "Training Loss: 0.006814713190542534\n",
      "Training Loss: 0.006581991439452395\n",
      "Training Loss: 0.006370017701992765\n",
      "Validation Loss: 0.0036921212231663617\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00507981040282175\n",
      "Training Loss: 0.004908798874239438\n",
      "Training Loss: 0.004822644061641767\n",
      "Training Loss: 0.005653948089457117\n",
      "Training Loss: 0.006807813422055915\n",
      "Training Loss: 0.006577677718596533\n",
      "Training Loss: 0.006363978099543601\n",
      "Validation Loss: 0.003687502892621643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005072220737347379\n",
      "Training Loss: 0.004902378057013265\n",
      "Training Loss: 0.004814909286797047\n",
      "Training Loss: 0.005648037769424264\n",
      "Training Loss: 0.0068007135612424465\n",
      "Training Loss: 0.0065732701076194645\n",
      "Training Loss: 0.006357841788558289\n",
      "Validation Loss: 0.0036827422214553373\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.0050644851761171596\n",
      "Training Loss: 0.004895899004186504\n",
      "Training Loss: 0.004807135501177982\n",
      "Training Loss: 0.005642081036930904\n",
      "Training Loss: 0.006793425712967292\n",
      "Training Loss: 0.006568772234022618\n",
      "Training Loss: 0.006351621005451307\n",
      "Validation Loss: 0.0036778370971871674\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005056612057378515\n",
      "Training Loss: 0.004889370991731994\n",
      "Training Loss: 0.0047993380221305415\n",
      "Training Loss: 0.005636087735183537\n",
      "Training Loss: 0.0067859647108707575\n",
      "Training Loss: 0.0065641937707550824\n",
      "Training Loss: 0.006345328242750838\n",
      "Validation Loss: 0.0036728032589942385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.00504861673747655\n",
      "Training Loss: 0.004882809483679012\n",
      "Training Loss: 0.004791535533731803\n",
      "Training Loss: 0.0056300650650518945\n",
      "Training Loss: 0.006778346562059596\n",
      "Training Loss: 0.006559536398854106\n",
      "Training Loss: 0.006338976848637685\n",
      "Validation Loss: 0.003667647842220153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005040512629202567\n",
      "Training Loss: 0.0048762238823110236\n",
      "Training Loss: 0.00478374331141822\n",
      "Training Loss: 0.005624024554563221\n",
      "Training Loss: 0.006770589102525264\n",
      "Training Loss: 0.006554812452523037\n",
      "Training Loss: 0.006332580347079783\n",
      "Validation Loss: 0.003662383431928016\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.005032314258860424\n",
      "Training Loss: 0.004869626669096761\n",
      "Training Loss: 0.004775978605612181\n",
      "Training Loss: 0.005617977731744759\n",
      "Training Loss: 0.006762714702636004\n",
      "Training Loss: 0.006550028846831992\n",
      "Training Loss: 0.0063261516543570906\n",
      "Validation Loss: 0.0036570196544005074\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005024037699913606\n",
      "Training Loss: 0.00486302969744429\n",
      "Training Loss: 0.00476825701422058\n",
      "Training Loss: 0.0056119324336759745\n",
      "Training Loss: 0.006754739374155178\n",
      "Training Loss: 0.006545190562028438\n",
      "Training Loss: 0.006319708320079371\n",
      "Validation Loss: 0.003651575226343694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005015699699288234\n",
      "Training Loss: 0.004856444940087385\n",
      "Training Loss: 0.004760597379645333\n",
      "Training Loss: 0.00560590015957132\n",
      "Training Loss: 0.00674668851075694\n",
      "Training Loss: 0.006540308189578355\n",
      "Training Loss: 0.006313264156924561\n",
      "Validation Loss: 0.003646061069433218\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0050073200307087975\n",
      "Training Loss: 0.004849884122377262\n",
      "Training Loss: 0.004753014618181624\n",
      "Training Loss: 0.005599888853612356\n",
      "Training Loss: 0.006738580634118989\n",
      "Training Loss: 0.006535388637566939\n",
      "Training Loss: 0.006306834532879293\n",
      "Validation Loss: 0.0036404929311304223\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.00499891416868195\n",
      "Training Loss: 0.0048433570523047816\n",
      "Training Loss: 0.004745523909223266\n",
      "Training Loss: 0.005593906017020344\n",
      "Training Loss: 0.006730437835212797\n",
      "Training Loss: 0.006530439638299868\n",
      "Training Loss: 0.006300430078990758\n",
      "Validation Loss: 0.0036348883168988125\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.004990503700100817\n",
      "Training Loss: 0.0048368754464900125\n",
      "Training Loss: 0.004738141692359932\n",
      "Training Loss: 0.005587962016288656\n",
      "Training Loss: 0.00672228149138391\n",
      "Training Loss: 0.0065254700998775665\n",
      "Training Loss: 0.00629406844615005\n",
      "Validation Loss: 0.003629261155591838\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.004982109943521209\n",
      "Training Loss: 0.004830450223525986\n",
      "Training Loss: 0.0047308834060095254\n",
      "Training Loss: 0.005582065129419789\n",
      "Training Loss: 0.00671413310454227\n",
      "Training Loss: 0.006520488513633609\n",
      "Training Loss: 0.006287764041917399\n",
      "Validation Loss: 0.003623624891217114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.004973750929930247\n",
      "Training Loss: 0.004824090516776778\n",
      "Training Loss: 0.004723760813940317\n",
      "Training Loss: 0.005576223037787713\n",
      "Training Loss: 0.00670601419871673\n",
      "Training Loss: 0.0065155008854344484\n",
      "Training Loss: 0.006281529054977\n",
      "Validation Loss: 0.003617994723168345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.004965444744448178\n",
      "Training Loss: 0.004817805141792633\n",
      "Training Loss: 0.0047167859779438\n",
      "Training Loss: 0.005570443023461848\n",
      "Training Loss: 0.00669794331653975\n",
      "Training Loss: 0.006510516834678129\n",
      "Training Loss: 0.006275374341057614\n",
      "Validation Loss: 0.0036123821194447604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.004957210613647476\n",
      "Training Loss: 0.004811601699329913\n",
      "Training Loss: 0.004709968653623946\n",
      "Training Loss: 0.005564730201731436\n",
      "Training Loss: 0.006689941644435748\n",
      "Training Loss: 0.006505542653612793\n",
      "Training Loss: 0.0062693127640523014\n",
      "Validation Loss: 0.0036068007206955877\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.004949066899134777\n",
      "Training Loss: 0.004805488407146186\n",
      "Training Loss: 0.004703318966203369\n",
      "Training Loss: 0.005559091197792441\n",
      "Training Loss: 0.006682022695895284\n",
      "Training Loss: 0.006500581244472415\n",
      "Training Loss: 0.006263349800137803\n",
      "Validation Loss: 0.0036012536042966366\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00494102735130582\n",
      "Training Loss: 0.004799470063298941\n",
      "Training Loss: 0.0046968415717128665\n",
      "Training Loss: 0.005553532163612544\n",
      "Training Loss: 0.006674203514121473\n",
      "Training Loss: 0.006495644327951595\n",
      "Training Loss: 0.006257495727622881\n",
      "Validation Loss: 0.0035957539352075604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.00493310802790802\n",
      "Training Loss: 0.0047935510735260325\n",
      "Training Loss: 0.004690542125608772\n",
      "Training Loss: 0.005548054855607915\n",
      "Training Loss: 0.006666498292470351\n",
      "Training Loss: 0.006490732695674524\n",
      "Training Loss: 0.006251755388220772\n",
      "Validation Loss: 0.0035903010175525026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.0049253207381116226\n",
      "Training Loss: 0.004787737226579338\n",
      "Training Loss: 0.004684423903236166\n",
      "Training Loss: 0.005542666291003116\n",
      "Training Loss: 0.006658918329048902\n",
      "Training Loss: 0.006485854329075664\n",
      "Training Loss: 0.006246131828520447\n",
      "Validation Loss: 0.003584904520779374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.004917676359182224\n",
      "Training Loss: 0.004782030433998444\n",
      "Training Loss: 0.00467848727828823\n",
      "Training Loss: 0.005537367188662756\n",
      "Training Loss: 0.006651472767116502\n",
      "Training Loss: 0.006481009217677638\n",
      "Training Loss: 0.006240628266241401\n",
      "Validation Loss: 0.003579561425502018\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.004910183844040148\n",
      "Training Loss: 0.004776430235360749\n",
      "Training Loss: 0.004672729779267684\n",
      "Training Loss: 0.005532159782305825\n",
      "Training Loss: 0.006644170738290996\n",
      "Training Loss: 0.006476203021593392\n",
      "Training Loss: 0.006235245710704476\n",
      "Validation Loss: 0.0035742723251333676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.004902848525671288\n",
      "Training Loss: 0.004770939800655469\n",
      "Training Loss: 0.004667150973109529\n",
      "Training Loss: 0.005527045896160416\n",
      "Training Loss: 0.00663701472687535\n",
      "Training Loss: 0.006471435205312446\n",
      "Training Loss: 0.006229981106007472\n",
      "Validation Loss: 0.003569037114594425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.004895677606691606\n",
      "Training Loss: 0.004765556129277684\n",
      "Training Loss: 0.004661743852193467\n",
      "Training Loss: 0.005522025179525372\n",
      "Training Loss: 0.006630009151995182\n",
      "Training Loss: 0.006466708777006716\n",
      "Training Loss: 0.006224833220476285\n",
      "Validation Loss: 0.0035638545543018184\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.004888672076631338\n",
      "Training Loss: 0.004760280635673552\n",
      "Training Loss: 0.004656505011953413\n",
      "Training Loss: 0.0055170997744426135\n",
      "Training Loss: 0.006623155974084511\n",
      "Training Loss: 0.0064620223827660086\n",
      "Training Loss: 0.006219799158861861\n",
      "Validation Loss: 0.003558724460694311\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.004881834554835223\n",
      "Training Loss: 0.004755109414691106\n",
      "Training Loss: 0.004651426561758853\n",
      "Training Loss: 0.00551226669253083\n",
      "Training Loss: 0.006616453881142661\n",
      "Training Loss: 0.006457377491751686\n",
      "Training Loss: 0.0062148750224150714\n",
      "Validation Loss: 0.0035536415301165667\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.004875164865516126\n",
      "Training Loss: 0.004750040278304368\n",
      "Training Loss: 0.0046465007856022564\n",
      "Training Loss: 0.005507526876463089\n",
      "Training Loss: 0.006609901419142261\n",
      "Training Loss: 0.00645277181873098\n",
      "Training Loss: 0.00621005542925559\n",
      "Validation Loss: 0.003548604126213893\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.004868662980734371\n",
      "Training Loss: 0.004745071983197704\n",
      "Training Loss: 0.00464172100124415\n",
      "Training Loss: 0.005502876351238228\n",
      "Training Loss: 0.006603496024617925\n",
      "Training Loss: 0.006448205119231716\n",
      "Training Loss: 0.006205335903214291\n",
      "Validation Loss: 0.003543613813398929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.00486232788185589\n",
      "Training Loss: 0.0047402005299227315\n",
      "Training Loss: 0.004637078443774954\n",
      "Training Loss: 0.005498316077864729\n",
      "Training Loss: 0.0065972353634424505\n",
      "Training Loss: 0.006443675170885399\n",
      "Training Loss: 0.00620071187033318\n",
      "Validation Loss: 0.003538665696559997\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.004856155754532665\n",
      "Training Loss: 0.004735421344521456\n",
      "Training Loss: 0.004632564011844806\n",
      "Training Loss: 0.0054938410199247304\n",
      "Training Loss: 0.006591112490277737\n",
      "Training Loss: 0.006439181016758084\n",
      "Training Loss: 0.006196175712393597\n",
      "Validation Loss: 0.003533758437138902\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.004850141305942088\n",
      "Training Loss: 0.00473073193221353\n",
      "Training Loss: 0.004628169781062752\n",
      "Training Loss: 0.005489450942841359\n",
      "Training Loss: 0.006585121987154707\n",
      "Training Loss: 0.006434719095705077\n",
      "Training Loss: 0.006191722335061058\n",
      "Validation Loss: 0.0035288894552500285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.0048442839528433975\n",
      "Training Loss: 0.004726129170157947\n",
      "Training Loss: 0.004623886561603285\n",
      "Training Loss: 0.0054851420034538025\n",
      "Training Loss: 0.006579257426783443\n",
      "Training Loss: 0.006430289470590651\n",
      "Training Loss: 0.006187346581136808\n",
      "Validation Loss: 0.003524061917651738\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.004838577377959155\n",
      "Training Loss: 0.004721608508843928\n",
      "Training Loss: 0.004619706388912164\n",
      "Training Loss: 0.005480911171762273\n",
      "Training Loss: 0.006573513655457646\n",
      "Training Loss: 0.00642588920542039\n",
      "Training Loss: 0.006183043058263138\n",
      "Validation Loss: 0.003519269942972209\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.0048330175451701506\n",
      "Training Loss: 0.004717167372000404\n",
      "Training Loss: 0.0046156217792304235\n",
      "Training Loss: 0.005476756517018657\n",
      "Training Loss: 0.006567883164389059\n",
      "Training Loss: 0.006421515487600118\n",
      "Training Loss: 0.00617880655801855\n",
      "Validation Loss: 0.003514516068803851\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.004827598744304851\n",
      "Training Loss: 0.004712801718269475\n",
      "Training Loss: 0.004611623659729958\n",
      "Training Loss: 0.005472674439661204\n",
      "Training Loss: 0.00656236236798577\n",
      "Training Loss: 0.006417165182065219\n",
      "Training Loss: 0.0061746310291346165\n",
      "Validation Loss: 0.0035098009639897487\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.004822315080091357\n",
      "Training Loss: 0.004708507728646509\n",
      "Training Loss: 0.004607705321977846\n",
      "Training Loss: 0.005468661677732598\n",
      "Training Loss: 0.0065569398493971675\n",
      "Training Loss: 0.00641283830977045\n",
      "Training Loss: 0.006170513344695792\n",
      "Validation Loss: 0.0035051217713403734\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.0048171632969751955\n",
      "Training Loss: 0.004704281980521046\n",
      "Training Loss: 0.004603860605275258\n",
      "Training Loss: 0.0054647157719591635\n",
      "Training Loss: 0.006551611679606139\n",
      "Training Loss: 0.006408533054636792\n",
      "Training Loss: 0.0061664473998825995\n",
      "Validation Loss: 0.003500476238851467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.004812135335523635\n",
      "Training Loss: 0.004700121547211893\n",
      "Training Loss: 0.00460008010151796\n",
      "Training Loss: 0.0054608341283164915\n",
      "Training Loss: 0.006546372271841392\n",
      "Training Loss: 0.006404245505109429\n",
      "Training Loss: 0.006162430874537677\n",
      "Validation Loss: 0.0034958698794643364\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.00480722779291682\n",
      "Training Loss: 0.0046960235602455215\n",
      "Training Loss: 0.004596359741990455\n",
      "Training Loss: 0.00545701221213676\n",
      "Training Loss: 0.006541215503821149\n",
      "Training Loss: 0.006399973642546683\n",
      "Training Loss: 0.006158459242433309\n",
      "Validation Loss: 0.0034913000723081973\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.004802433238364756\n",
      "Training Loss: 0.004691983566735871\n",
      "Training Loss: 0.00459269204584416\n",
      "Training Loss: 0.005453247309778817\n",
      "Training Loss: 0.00653613613336347\n",
      "Training Loss: 0.006395718532148749\n",
      "Training Loss: 0.006154527171747759\n",
      "Validation Loss: 0.003486762764647408\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.004797746253316291\n",
      "Training Loss: 0.004687999153393321\n",
      "Training Loss: 0.004589071363443509\n",
      "Training Loss: 0.005449537389795296\n",
      "Training Loss: 0.0065311270777601745\n",
      "Training Loss: 0.006391476757125929\n",
      "Training Loss: 0.006150633445940912\n",
      "Validation Loss: 0.003482260309205101\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.004793161252746358\n",
      "Training Loss: 0.004684065863257274\n",
      "Training Loss: 0.004585491946199909\n",
      "Training Loss: 0.005445879329636227\n",
      "Training Loss: 0.006526185038965195\n",
      "Training Loss: 0.0063872486341279\n",
      "Training Loss: 0.006146772374631837\n",
      "Validation Loss: 0.003477788991155277\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.004788671417045407\n",
      "Training Loss: 0.004680182226002217\n",
      "Training Loss: 0.004581948641571216\n",
      "Training Loss: 0.005442269468912855\n",
      "Training Loss: 0.006521303158951924\n",
      "Training Loss: 0.006383030143333599\n",
      "Training Loss: 0.006142942337319255\n",
      "Validation Loss: 0.003473356383162017\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.004784272926044651\n",
      "Training Loss: 0.004676344831241294\n",
      "Training Loss: 0.004578436188166961\n",
      "Training Loss: 0.005438706668792292\n",
      "Training Loss: 0.006516479364363476\n",
      "Training Loss: 0.006378822718979791\n",
      "Training Loss: 0.006139141097664833\n",
      "Validation Loss: 0.0034689500828514273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.0047799594781827185\n",
      "Training Loss: 0.004672551834955812\n",
      "Training Loss: 0.004574950892128981\n",
      "Training Loss: 0.0054351866673096084\n",
      "Training Loss: 0.006511708446778357\n",
      "Training Loss: 0.006374622592702508\n",
      "Training Loss: 0.006135364635847509\n",
      "Validation Loss: 0.003464575677286931\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.00477572456584312\n",
      "Training Loss: 0.004668799342471175\n",
      "Training Loss: 0.00457148693036288\n",
      "Training Loss: 0.005431708774412982\n",
      "Training Loss: 0.006506987257162109\n",
      "Training Loss: 0.0063704318949021396\n",
      "Training Loss: 0.006131611821474508\n",
      "Validation Loss: 0.0034602312209810154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.004771563830436207\n",
      "Training Loss: 0.00466508579265792\n",
      "Training Loss: 0.004568040336016566\n",
      "Training Loss: 0.005428268659161403\n",
      "Training Loss: 0.006502312130760401\n",
      "Training Loss: 0.006366247604601086\n",
      "Training Loss: 0.006127879141131416\n",
      "Validation Loss: 0.0034559120595008693\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.0047674701327923685\n",
      "Training Loss: 0.004661407042294741\n",
      "Training Loss: 0.004564607135835104\n",
      "Training Loss: 0.00542486462101806\n",
      "Training Loss: 0.006497679429594427\n",
      "Training Loss: 0.006362070421455428\n",
      "Training Loss: 0.006124163470230997\n",
      "Validation Loss: 0.003451621285918751\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.00476344074762892\n",
      "Training Loss: 0.004657762576825917\n",
      "Training Loss: 0.004561183239566162\n",
      "Training Loss: 0.005421494967595209\n",
      "Training Loss: 0.006493087612325326\n",
      "Training Loss: 0.006357900507282466\n",
      "Training Loss: 0.006120465274434537\n",
      "Validation Loss: 0.003447350369215235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.0047594682616181674\n",
      "Training Loss: 0.00465414839272853\n",
      "Training Loss: 0.004557765214703977\n",
      "Training Loss: 0.005418157500098459\n",
      "Training Loss: 0.006488533413503319\n",
      "Training Loss: 0.00635373544297181\n",
      "Training Loss: 0.006116780675947666\n",
      "Validation Loss: 0.0034431082275887944\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.004755549432011321\n",
      "Training Loss: 0.004650564514449797\n",
      "Training Loss: 0.004554348652018234\n",
      "Training Loss: 0.00541484834451694\n",
      "Training Loss: 0.006484014766756445\n",
      "Training Loss: 0.006349575335625559\n",
      "Training Loss: 0.006113108231220394\n",
      "Validation Loss: 0.0034388824072160693\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.004751678333268501\n",
      "Training Loss: 0.004647006699233316\n",
      "Training Loss: 0.004550930213881656\n",
      "Training Loss: 0.005411568667041138\n",
      "Training Loss: 0.006479529694188386\n",
      "Training Loss: 0.006345421449514106\n",
      "Training Loss: 0.0061094459646847095\n",
      "Validation Loss: 0.003434675089903315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.004747849474078976\n",
      "Training Loss: 0.004643472928437404\n",
      "Training Loss: 0.004547506609815173\n",
      "Training Loss: 0.005408314196683932\n",
      "Training Loss: 0.006475077155046165\n",
      "Training Loss: 0.006341272603021935\n",
      "Training Loss: 0.006105792419984937\n",
      "Validation Loss: 0.003430483231411668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.004744060574448667\n",
      "Training Loss: 0.0046399628306971865\n",
      "Training Loss: 0.004544075743178837\n",
      "Training Loss: 0.005405083578953054\n",
      "Training Loss: 0.006470653732540086\n",
      "Training Loss: 0.0063371291663497685\n",
      "Training Loss: 0.006102145230979659\n",
      "Validation Loss: 0.0034263058515851584\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.004740305287996307\n",
      "Training Loss: 0.0046364719542907554\n",
      "Training Loss: 0.004540631456184201\n",
      "Training Loss: 0.005401875683455728\n",
      "Training Loss: 0.006466260587330908\n",
      "Training Loss: 0.006332991048693657\n",
      "Training Loss: 0.006098502061795443\n",
      "Validation Loss: 0.003422141670727272\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.004736577842850238\n",
      "Training Loss: 0.004633000639732927\n",
      "Training Loss: 0.004537173989228904\n",
      "Training Loss: 0.0053986885590711605\n",
      "Training Loss: 0.006461896960390731\n",
      "Training Loss: 0.0063288571080192925\n",
      "Training Loss: 0.006094862874015235\n",
      "Validation Loss: 0.0034179844508080114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.004732875712797977\n",
      "Training Loss: 0.004629547156509943\n",
      "Training Loss: 0.004533699822495692\n",
      "Training Loss: 0.005395519790472463\n",
      "Training Loss: 0.006457560728304088\n",
      "Training Loss: 0.006324728645849973\n",
      "Training Loss: 0.006091225158888847\n",
      "Validation Loss: 0.003413836817494297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.004729194428655319\n",
      "Training Loss: 0.004626107620424591\n",
      "Training Loss: 0.004530203931499273\n",
      "Training Loss: 0.00539236886164872\n",
      "Training Loss: 0.0064532516675535586\n",
      "Training Loss: 0.006320606428198516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [47:45<00:00, 286.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.006087588657974266\n",
      "Validation Loss: 0.0034096938128886597\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Load old data and plot it ###\n",
    "if syn_data_type == 'all':\n",
    "    # load all results\n",
    "    pass\n",
    "\n",
    "### Get new data and plot it ###\n",
    "else:\n",
    "    # evaluate predictive performance\n",
    "    predictive_results = predictive_evaluation(data_real_numpy, data_syn_numpy, predictive_model_hyperparameters, include_baseline=True, verbose=True)\n",
    "\n",
    "    # save results\n",
    "    predictive_results.to_csv(DATA_FOLDER / f\"results/results_{syn_data_type}_{predictive_model_hyperparameters['num_epochs']}_{predictive_model_hyperparameters['num_evaluation_runs']}.csv\", index=False)\n",
    "\n",
    "    # split in mse and mae results\n",
    "    mse_results = predictive_results.loc[predictive_results['Metric'] == 'MSE']\n",
    "    mae_results = predictive_results.loc[predictive_results['Metric'] == 'MAE']\n",
    "\n",
    "    # plot results\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='Model', y='Error', hue='Model', data=mse_results)\n",
    "    sns.stripplot(x='Model', y='Error', hue='Metric', data=mse_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.xlabel('Metric')\n",
    "    plt.title(f'MSE | {syn_data_type} | {predictive_model_hyperparameters[\"num_evaluation_runs\"]} Training Runs {\" | jitter factor = \" + str(jitter_factor) if syn_data_type == \"jitter\" else \"\"}')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='Model', y='Error', hue='Model', data=mae_results)\n",
    "    sns.stripplot(x='Model', y='Error', hue='Metric', data=mae_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.xlabel('Metric')\n",
    "    plt.title(f'MAE | {syn_data_type} | {predictive_model_hyperparameters[\"num_evaluation_runs\"]} Training Runs {\" | jitter factor = \" + str(jitter_factor) if syn_data_type == \"jitter\" else \"\"}')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Discriminative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminative_model_hyperparameters = {\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 32,\n",
    "    \"hidden_size\": 4,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"output_logits\": True,\n",
    "    \"num_epochs\": 1000,\n",
    "    \"type\": 'lstm',\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load discriminative model\n",
    "discriminative_model = LSTMClassification(\n",
    "    device=discriminative_model_hyperparameters['device'],\n",
    "    batch_size=discriminative_model_hyperparameters['batch_size'],\n",
    "    input_size=data_real_numpy.shape[-1],\n",
    "    hidden_size=discriminative_model_hyperparameters['hidden_size'],\n",
    "    num_stacked_layers=discriminative_model_hyperparameters['num_layers'],\n",
    "    bidirectional=discriminative_model_hyperparameters['bidirectional'],\n",
    "    output_logits=discriminative_model_hyperparameters['output_logits']\n",
    ")\n",
    "\n",
    "# Lade die Modellparameter\n",
    "discriminative_model.load_state_dict(torch.load(f'discriminative_model_{discriminative_model_hyperparameters['type']}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "autoencoder_test = load_sequential_time_series(SYNTHETIC_DATA_FOLDER / 'discriminative_test' / 'discriminative_test_autoencoder_3000_13_5.csv', shape=(3000, 13, 5))\n",
    "jitt_01_test = load_sequential_time_series(SYNTHETIC_DATA_FOLDER / 'discriminative_test' / 'discriminative_test_jitt_01_3000_13_5.csv', shape=(3000, 13, 5))\n",
    "jitt_02_test = load_sequential_time_series(SYNTHETIC_DATA_FOLDER / 'discriminative_test' / 'discriminative_test_jitt_02_3000_13_5.csv', shape=(3000, 13, 5))\n",
    "jitt_005_test = load_sequential_time_series(SYNTHETIC_DATA_FOLDER / 'discriminative_test' / 'discriminative_test_jitt_005_3000_13_5.csv', shape=(3000, 13, 5))\n",
    "timegan_gru_test = load_sequential_time_series(SYNTHETIC_DATA_FOLDER / 'discriminative_test' / 'discriminative_test_timegan_gru_3000_13_5.csv', shape=(3000, 13, 5))\n",
    "timegan_lstm_test = load_sequential_time_series(SYNTHETIC_DATA_FOLDER / 'discriminative_test' / 'discriminative_test_timegan_lstm_3000_13_5.csv', shape=(3000, 13, 5))\n",
    "timewarp_test = load_sequential_time_series(SYNTHETIC_DATA_FOLDER / 'discriminative_test' / 'discriminative_test_timewarp_3000_13_5.csv', shape=(3000, 13, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminative_results = pd.DataFrame(columns=['Method', 'Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results\n",
    "discriminative_results = get_discriminative_test_performance(discriminative_model, discriminative_model_hyperparameters['device'], autoencoder_test, 'Autoencoder', discriminative_results)\n",
    "discriminative_results = get_discriminative_test_performance(discriminative_model, discriminative_model_hyperparameters['device'], jitt_01_test, 'Jitter 0.1', discriminative_results)\n",
    "discriminative_results = get_discriminative_test_performance(discriminative_model, discriminative_model_hyperparameters['device'], jitt_02_test, 'Jitter 0.2', discriminative_results)\n",
    "discriminative_results = get_discriminative_test_performance(discriminative_model, discriminative_model_hyperparameters['device'], jitt_005_test, 'Jitter 0.05', discriminative_results)\n",
    "discriminative_results = get_discriminative_test_performance(discriminative_model, discriminative_model_hyperparameters['device'], timegan_gru_test, 'TimeGAN GRU', discriminative_results)\n",
    "discriminative_results = get_discriminative_test_performance(discriminative_model, discriminative_model_hyperparameters['device'], timegan_lstm_test, 'TimeGAN LSTM', discriminative_results)\n",
    "discriminative_results = get_discriminative_test_performance(discriminative_model, discriminative_model_hyperparameters['device'], timewarp_test, 'Timewarp', discriminative_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(discriminative_results['Method'], discriminative_results['Accuracy'], color='skyblue')\n",
    "\n",
    "# Diagramm anpassen\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy per Method')\n",
    "plt.xticks(rotation=45)  # Optional: Dreht die Beschriftungen der x-Achse um 45 Grad\n",
    "plt.tight_layout()\n",
    "\n",
    "# Diagramm anzeigen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Visual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (28500, 12, 5)\n",
      "Shape of the data after splitting into sequences: (28500, 12, 5)\n"
     ]
    }
   ],
   "source": [
    "if syn_data_type == 'all':\n",
    "    # load all results\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    \n",
    "    # split data before feeding into visual evaluation\n",
    "    data_real_seq = split_data_into_sequences(data_real_numpy, seq_len=predictive_model_hyperparameters['seq_len'], shuffle_data=True)\n",
    "\n",
    "    if data_syn_numpy.ndim == 3:\n",
    "        data_syn_seq = data_syn_numpy\n",
    "    else:\n",
    "        data_syn_seq = split_data_into_sequences(data_syn_numpy, seq_len=predictive_model_hyperparameters['seq_len'], shuffle_data=True)\n",
    "\n",
    "    # evaluate visual performance\n",
    "    visualize(data_real_seq, data_syn_seq, metric='pca')\n",
    "    visualize(data_real_seq, data_syn_seq, metric='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispielaussage:\n",
    "PCA-Analyse von Realen und Synthetischen Daten\n",
    "Um die Ähnlichkeit zwischen den realen und synthetischen Daten zu bewerten, wurde eine Principal Component Analysis (PCA) durchgeführt. Die PCA reduziert die Dimensionalität der Daten und projiziert sie auf eine zweidimensionale Ebene, wobei die Hauptkomponenten beibehalten werden, die den größten Teil der Varianz erklären.\n",
    "\n",
    "Abbildung X zeigt den PCA-Plot der realen (rote Punkte) und synthetischen Daten (blaue Punkte). Die folgenden Beobachtungen können gemacht werden:\n",
    "\n",
    "Verteilung und Clusterbildung:\n",
    "\n",
    "Die roten Punkte, die die realen Daten repräsentieren, sind in einem spezifischen Bereich konzentriert.\n",
    "Die blauen Punkte, die die synthetischen Daten darstellen, zeigen eine größere Verteilung und decken einen breiteren Bereich ab.\n",
    "Ähnlichkeit und Unterschiede:\n",
    "\n",
    "Die Tatsache, dass die synthetischen Daten eine größere Variabilität aufweisen, könnte darauf hinweisen, dass sie eine breitere Vielfalt an Mustern generieren.\n",
    "Die Cluster der realen und synthetischen Daten überlappen sich teilweise, was darauf hindeutet, dass die synthetischen Daten einige der Eigenschaften der realen Daten gut nachahmen. Allerdings gibt es auch Bereiche, in denen die synthetischen Daten stark von den realen Daten abweichen, was auf Unterschiede in den zugrunde liegenden Verteilungen hinweist.\n",
    "Schlussfolgerung:\n",
    "\n",
    "Die PCA-Analyse zeigt, dass die synthetischen Daten in gewissem Maße die Struktur der realen Daten einfangen, jedoch eine größere Variabilität aufweisen.\n",
    "Weitere Untersuchungen und Anpassungen am Generierungsprozess der synthetischen Daten könnten notwendig sein, um deren Genauigkeit und Übereinstimmung mit den realen Daten zu verbessern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_data_augmentation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
