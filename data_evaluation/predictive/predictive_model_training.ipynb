{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Füge das übergeordnete Verzeichnis zu sys.path hinzu\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "from utilities import split_data_into_sequences, load_sequential_time_series, reconstruct_sequential_data, Scaler, extract_features_and_targets_reg, get_discriminative_test_performance\n",
    "from data_evaluation.visual.visual_evaluation import visual_evaluation\n",
    "from predictive_evaluation import predictive_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../../data\")\n",
    "REAL_DATA_FOLDER = DATA_FOLDER / \"real\"\n",
    "SYNTHETIC_DATA_FOLDER = DATA_FOLDER / \"synthetic\" / \"usable\" / \"1y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of loading data\n",
    "- Laden der Originaldaten: als pd dataframe \n",
    "- Laden der synthetischen, sequentiellen Daten: als np array (GAN, (V)AE)\n",
    "- Laden der synthetischen, sequentiellen Daten: als pd dataframe (brownian, algorithmit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible types: 'timegan_lstm', 'timegan_gru', 'jitter', 'timewarp', 'autoencoder', 'vae'\n",
    "syn_data_type = 'timegan_lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " syn data:\n",
      "\n",
      "       traffic_volume           temp        rain_1h       snow_1h  \\\n",
      "count   104964.000000  104964.000000  104964.000000  1.049640e+05   \n",
      "mean      3248.579861     281.000357       0.062043  1.739048e-04   \n",
      "std       1890.470603      12.596705       0.103785  2.575326e-04   \n",
      "min        182.408776     252.197870       0.000008  0.000000e+00   \n",
      "25%       1378.146040     269.195816       0.001772  1.564622e-07   \n",
      "50%       3149.244204     282.956513       0.022171  1.003593e-05   \n",
      "75%       5277.247455     291.973236       0.074397  2.385899e-04   \n",
      "max       7025.055456     304.007423       1.790775  1.602069e-03   \n",
      "\n",
      "          clouds_all  \n",
      "count  104964.000000  \n",
      "mean       40.279533  \n",
      "std        37.971107  \n",
      "min         0.013751  \n",
      "25%         0.741871  \n",
      "50%        23.024623  \n",
      "75%        82.414405  \n",
      "max        98.330384  \n",
      "\n",
      "\n",
      "real train data:\n",
      "\n",
      "       traffic_volume         temp      rain_1h      snow_1h   clouds_all\n",
      "count     8759.000000  8759.000000  8759.000000  8759.000000  8759.000000\n",
      "mean      3244.668912   282.208136     0.086792     0.000233    44.397306\n",
      "std       1946.247953    12.114907     0.901360     0.006145    39.195308\n",
      "min          0.000000   243.390000     0.000000     0.000000     0.000000\n",
      "25%       1252.500000   273.605500     0.000000     0.000000     1.000000\n",
      "50%       3402.000000   283.650000     0.000000     0.000000    40.000000\n",
      "75%       4849.500000   292.060000     0.000000     0.000000    90.000000\n",
      "max       7260.000000   307.330000    42.000000     0.250000   100.000000\n",
      "\n",
      "\n",
      "real test data:\n",
      "\n",
      "       traffic_volume         temp  rain_1h  snow_1h   clouds_all\n",
      "count     2135.000000  2135.000000   2135.0   2135.0  2135.000000\n",
      "mean      3325.263700   270.553730      0.0      0.0    45.065105\n",
      "std       1996.851023     7.864566      0.0      0.0    40.781402\n",
      "min        216.000000   248.660000      0.0      0.0     0.000000\n",
      "25%       1222.500000   265.735000      0.0      0.0     1.000000\n",
      "50%       3563.000000   271.550000      0.0      0.0    40.000000\n",
      "75%       4946.000000   275.680000      0.0      0.0    90.000000\n",
      "max       7280.000000   290.150000      0.0      0.0    92.000000\n"
     ]
    }
   ],
   "source": [
    "# Load real time series\n",
    "data_train_real_df = pd.read_csv(REAL_DATA_FOLDER/'mitv_prep_1y.csv')\n",
    "data_train_real_numpy = dc(data_train_real_df).to_numpy()\n",
    "\n",
    "data_test_real_df = pd.read_csv(REAL_DATA_FOLDER/'mitv_prep_3mo.csv')\n",
    "data_test_real_numpy = dc(data_test_real_df).to_numpy()\n",
    "\n",
    "if syn_data_type == 'timegan_lstm':\n",
    "    # load sequential data (which should already be scaled)\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'8747_12_5_timegan_lstm_16_8k.csv', shape=(8747, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'timegan_gru':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_gru_unscaled.csv', shape=(1, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'autoencoder':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'8726_12_5_lstm_autoencoder.csv', shape=(8726, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'vae':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'8759_12_5_fc_vae.csv', shape=(8759, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'jitter':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'jittered_01.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "elif syn_data_type == 'timewarp':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'time_warped.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "# Loot at real and syn data\n",
    "df = pd.DataFrame(data_syn_numpy.reshape(-1, data_syn_numpy.shape[-1]), columns=data_train_real_df.columns)\n",
    "\n",
    "print('\\n\\n syn data:\\n')\n",
    "print(df.describe())\n",
    "\n",
    "print('\\n\\nreal train data:\\n')\n",
    "print(data_train_real_df.describe())\n",
    "\n",
    "print('\\n\\nreal test data:\\n')\n",
    "print(data_test_real_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Predictive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"seq_len\": 12,\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 32,\n",
    "    \"hidden_size\": 12,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"num_evaluation_runs\": 10,\n",
    "    \"num_epochs\": 500,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETERS:\n",
      "seq_len :  12\n",
      "lr :  0.0001\n",
      "batch_size :  32\n",
      "hidden_size :  12\n",
      "num_layers :  1\n",
      "bidirectional :  True\n",
      "num_evaluation_runs :  10\n",
      "num_epochs :  500\n",
      "device :  cpu\n",
      "Synthetic Data is sequential: True\n",
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1057, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1056, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1040173303063986 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.09041005635962766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02278291817795295 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.025249425869654205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011846832688863858 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013015929606113145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010115577215600732 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010770021886427832 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009138242368345033 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009397164401461315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008389323743750225 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008386218769694953 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007760011600789336 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007553826866890578 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0072972002875970774 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00723172036711784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007071070786885047 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007172883447150097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006892635106758289 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007037822150296587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006722809439677963 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00681798263480339 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006562312894558575 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00655668430879493 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006414471311413132 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0062948295312440575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006273776093960135 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006049591842490961 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006129803519226937 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005812756443286643 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005987912303016922 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005603178760365528 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005872074587705688 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005481970311580773 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0057781092052681055 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005421298480702236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005697291634167905 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005379035897717318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005624321416397681 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005340224782498006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005555923345599011 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053013374837224975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00549044093902284 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005262778859640307 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005427274914385316 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005226375952856068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005366255010879279 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005193839780986309 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0053072306127831285 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005165869657717207 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005249811070427353 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005142328966244617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00519340225099416 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005122300004586577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005137385113035621 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005104320500429501 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005081391284908474 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005086610395023052 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005025643089711394 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005067790228435222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004971140094040248 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005047098843052107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004919389691067873 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005023874027435394 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00487173582409359 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004996878160026801 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004828771030960305 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004964366497006267 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.00479022793307812 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004925075565766105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0047553138995210026 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004878807487651049 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004723136525483883 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004826482049846912 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004692967710030829 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00476980978034108 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004664287593319373 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004710812741370105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004636742220299291 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0046513686949551545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.00461011348112935 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004592986256979844 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004584274134924975 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004536719225785311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004559101805921107 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004483189040055389 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0045345366843653175 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0044324579443234735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004510547691190541 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004384283109924153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004487121233633332 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004338335180792081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004464261515179044 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004294324269079987 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004441985708574703 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004252062518122222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004420323685821941 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004211603937780156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004399326694517458 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004173168659155422 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive\\predictive_evaluation.py:277: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([{'Model': evaluation_method, 'Metric': 'MAE', 'Error': mae}])], ignore_index=True)\n",
      " 10%|█         | 1/10 [03:22<30:21, 202.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.16486267432787993 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.13070110997924692 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02196354568633421 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.023726748042356444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01377998227865374 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.015014191151267904 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01086515902641848 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011730549052385065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00939902666077548 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009929349865106976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008457508158231031 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008908676774487556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00775706224580275 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008057262827231385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0070463905296200054 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0072312964201795265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006597598565548631 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006805780343711376 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006313831118414736 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006601832546841572 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006082799328137597 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006438178469098228 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005896115936015998 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006325135555337457 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005745274960964129 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006276285159401596 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 126\n",
      "INFO: Validation loss did not improve in epoch 127\n",
      "INFO: Validation loss did not improve in epoch 128\n",
      "INFO: Validation loss did not improve in epoch 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [04:15<15:14, 114.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 130\n",
      "Early stopping after 130 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.10601016342721499 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.09217777881113921 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02729078061389227 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.02932259812951088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01208681535285075 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013708233545698664 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010774378665641331 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01187156533877201 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00996360300324626 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010757884284590973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009104757326277801 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009639123995678829 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00841920702279729 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008725329644649345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00809023358237107 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008310350205968408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00785463560885838 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008048826749847434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007639152280829955 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007832434254369754 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007432986890394105 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007655479931546485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007277541163370666 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007548819871290642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007171815602418144 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007474473520072506 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007083804382735278 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0074006773408173635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006991539394353809 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007313807399066932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0068786201087662775 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007216574925491039 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006727663048944116 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007154850727495025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 164\n",
      "INFO: Validation loss did not improve in epoch 165\n",
      "INFO: Validation loss did not improve in epoch 166\n",
      "INFO: Validation loss did not improve in epoch 167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [05:23<10:53, 93.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 168\n",
      "Early stopping after 168 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.1628832123208329 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.1298867324623964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02706979980597096 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.029087118722279284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012096018497713155 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01329667932997622 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009223215975781672 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009682268393910764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007691269187701961 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008028176904190332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007158322204762635 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007459409254164819 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0069078139592825445 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00724182724062463 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00675183414552959 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007115017732276636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006637193911783669 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007022881106583073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006543624026609082 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006955115264966427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006461814665796412 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006903386911076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006386555778457491 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006860985163845779 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006314664723922628 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006824216918389806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0062442481017290835 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006791904897374266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006174332265959658 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0067642652018762685 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006104622261826439 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0067418192321544185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0060352591337146655 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0067244427388205245 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005966536408412631 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0067105275674668305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005898667126809022 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006697077728698359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005831586406056355 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006680115290424403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005765013144790011 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006655743585296851 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005698655409497094 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006620620986830224 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005632612024889375 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006572515422971372 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005567738901905335 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006510709241196951 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005505544483155203 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006436323466272477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005447537326814783 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006352488074780387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0053945572196631725 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006264637244920082 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005346654081567578 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0061783373650327765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005303355895134069 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00609728812431807 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005263976687539954 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006022892895784667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005227821082451184 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005955008053741253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00519427565545911 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005892810079625205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005162836052052486 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005835262414834955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005133101546900673 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005781515371328329 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005104748872712078 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00573097268009887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005077511583387777 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005683054010767271 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005051191265687999 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005637351961066837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005025620786936267 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00559345172266202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0050006714438336364 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0055509742198731095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004976240338194252 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005509605305954157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004952245678694878 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005469070987173301 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004928622893495383 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005429168084554155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004905316969876884 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005389766675173579 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004882284774263152 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00535073275130023 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0048594885951003235 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053120204247534275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0048369002604116106 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005273603556869442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004814502692090545 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00523539785268333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004792290875355944 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005197542551767957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0047702718852002885 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005160065614344443 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0047484711004823895 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005123091717798482 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [08:46<13:40, 136.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3033712907102856 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.21458736196269884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.030874484181268154 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.033897503224365855 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014068621643699951 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01597649560597561 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011345372299058703 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012592474388999535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009772639539361298 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01037532840649981 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008772500292279072 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00889839553137255 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008157898005298645 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00816541914965081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007585748818101375 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00772044833694749 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006953598050669135 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007279009058359353 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006487839079859673 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006841492352952414 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006223236943126051 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006505675716599559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0060438164650041095 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006247458938399658 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005912149612440041 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006062158232773928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005810634855921958 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005935661775498267 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005727889131411339 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0058458128135980055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00565707904943537 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005777125116711592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00559405753976176 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005720863495405544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005536136440898761 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005671565797563423 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005481498493952337 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005625197013347026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005428886852978328 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005579286645276143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005377390673027177 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005532518652377322 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005326194353027772 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00548394668422749 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005274430510033955 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005433011824018596 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005221211064664688 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053789730676833325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005165960751881782 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053209588513709605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005109220074428531 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005259077516658341 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0050527734198033076 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005194838857278228 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00499850022585308 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005129276576232822 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004947484957383959 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005061925419688444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00490000189016879 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004991540414801634 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004855860123473703 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00491753123317133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004814709599612512 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004840442270506173 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004776228464210243 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004761687151211149 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004740182785351292 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004682974201654468 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004706427839565104 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0046059738173533015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004674873113514087 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004532057429006433 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0046454312319758545 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004462306332938811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0046179727692604 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004397337824818404 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0045923194757715394 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004337302022202707 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004568273148369418 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004282004730638993 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004545633101166247 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004231147255803294 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004524202638154469 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0041842615704380855 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004503811639584907 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004140851005692692 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004484319118536451 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00410046342500102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004465607537915183 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004062742498182856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.00444758508671381 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004027383080135812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004430177891019385 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.003994153658448554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004413332030993723 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.003962873203871662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004397004422558349 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.003933374639874434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0043811583443269485 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0039055410683538545 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [12:11<13:26, 161.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.10833219249127772 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.09030748739400331 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.017573778141531278 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.019013474702232462 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011237988765538418 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012409745051306398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008904725062936191 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009974489625379005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007904871371804132 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00863512814976275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007396241324107387 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007871808367781341 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006942985483350056 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00731810846927521 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006688098444001762 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0070606627611114695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0065151504011498424 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006907087784972699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006344866884505227 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006753204143880045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006174716421651117 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0066136163316995785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006010134355165064 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006496059639817651 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005857047822877058 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006388301238664152 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00571946984617529 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006272546864826889 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005600379103961244 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006145417779300581 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0055003287559579105 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006015243063516477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005415828750693124 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005887557454544175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005341700904061134 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005762395151096451 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005273724786544654 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00563844371422687 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005209104137377895 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0055148265981937155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005146049366699681 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005391195878687808 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005083598389399041 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005267986616886714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005021662375062065 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005146700512234341 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004961076931625156 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005029881013529923 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004903362156401803 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004920324594850707 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004850100062130679 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004820212256163359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00480223153348176 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0047305688255137816 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004759713124171967 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004651267002747559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004721826706107121 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004581329460694071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004687665011134699 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004519576823119731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004656437790700248 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004464919309077018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004627552274979963 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004416404178996077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004600586333413214 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0043732248595915735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004575243012182564 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004334686296608518 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004551304519252111 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004300221448134193 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004528613590693142 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004269333768581205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004507047664008722 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004241620450664093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0044865062210477724 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004216750525622903 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0044669108773347154 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004194417958120432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004448184455884281 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004174363593954374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0044302597062417515 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004156344573643497 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004413071874296293 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004140160193748991 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004396559046853306 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004125600547173663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004380662871794911 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004112502207349548 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004365329279302599 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004100703425458907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004350510215978172 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004090058271471849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004336158672018813 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004080433744307169 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004322236250656025 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0040717362734379575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004308708039402153 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0040638512685237565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004295544546198637 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0040567096131032005 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [15:34<11:41, 175.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.16357429335097762 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.13447531455997652 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.031018903145879288 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03379543867948301 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012762668128737187 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0141529581605402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009434149755644237 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010395198334556292 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008041404302819992 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008685646643725169 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0074050532025964864 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00767389075447093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007060011047071158 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00718959333265529 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0068812205823287915 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006993357174317627 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0067625654520157615 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00688257093733067 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00666425876632965 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006787527742905214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006575325816652201 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006700206000138731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006491708869189296 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006620370543233174 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006410110825766218 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006546525948900072 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00632652513475504 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006475606056697229 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006236879827614439 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006403983817161883 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006140480515628673 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006334134829504525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006042458046904092 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006277684060692349 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005949058510060592 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006238859393359984 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0058625342981230445 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006210792056449196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0057820974195709135 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00619380089783055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 198\n",
      "INFO: Validation loss did not improve in epoch 199\n",
      "INFO: Validation loss did not improve in epoch 200\n",
      "INFO: Validation loss did not improve in epoch 201\n",
      "Epoch: 201\n",
      "Train Loss: 0.0057074911488784325 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006192346747197649 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [16:56<07:14, 144.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 202\n",
      "Early stopping after 202 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.10125060049803371 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.08144550902001998 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02133606700566563 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.023118735921076116 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011445977115820087 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012628594522967058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00937462255764875 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01005247067865532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008105439475965244 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008831325541798244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007319806831511567 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007819048834362012 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006813882072592141 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007340226411435972 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006613704142667163 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007186798618503791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006449216231108936 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007063278031316312 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006276945948488603 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006933503236402483 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006104063672220919 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006842593333738691 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005953961474749593 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006821854437208351 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005828859506728957 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006799522602437612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005721557470731927 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006740146159084842 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005624922319573697 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006651709089055657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005534537488349924 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006544544875128742 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005448641422473193 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.006426134016638731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00536723517817887 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.006304167443886399 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005291255849522341 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0061869566082297005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005221922743817397 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.006082072539035888 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005160211590985448 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005994787679382545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005106478905514304 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005926702356459025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005060207250189838 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005875197478898746 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005020198205963498 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00583488419515026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004985019480716097 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00580020052353468 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004953365336405113 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005766806411830818 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0049242450431648665 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005731802515904693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0048969161471973995 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005693734575556044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004870828804067152 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005651886903626078 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0048455807695209465 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005605982370017206 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004820875100363182 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005556040703702499 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00479649711059522 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005502271007143837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004772295546335556 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005445024183513049 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004748171462420796 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0053847714067053265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004724068691857146 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005322078277137788 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00469996931730688 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005257573690922821 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0046758901892505235 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0051918861743829705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004651880139241974 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005125637686647037 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004628017989376451 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00505946678629912 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004604398793401536 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004993899650049999 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004581150672114692 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004929467131767203 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004558367109356149 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0048666306470027745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004536183248762633 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004805787897887914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004514697604463436 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004747245851082399 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004494000726229049 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004691241740468232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004474157241664242 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0046379172571879975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004455206575837449 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004587328712017659 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004437158099332328 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004539446771720096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004420007230101296 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004494138805003946 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004403738429779221 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004451231859788737 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [20:19<05:26, 163.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.17014681910892038 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.12160085968892365 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02582041925689491 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.02873471030034125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012022858574378039 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013185520372216535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009943728367067249 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010784659021095756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00902802168381448 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00965344165588784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008538610352471769 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008976899505154613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008225769644524957 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008538118652615915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007975492889425262 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008223677588188472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007718884610880275 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007967614945407738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007412598618343608 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007756878854706883 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007112048051394794 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007543216281406143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006878563714890068 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007298220072270316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006680250568927204 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007109310039702584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006497944686493843 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006952153861193973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006326447279671765 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006793239640126771 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0061595686294994975 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006638264680719551 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005984895830773443 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006492212154519032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005795994951104215 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006353517090353896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005618382172223969 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006240619038341238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005478630199294238 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006149256334859221 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0053661911587747505 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006059842663543189 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005267334939448 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005965649732388556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00517646434999846 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0058653640821028285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005092952403420052 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005761898389798315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005017942147130609 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005660388262613732 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00495186341666433 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005563425177516525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004893834714142157 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0054702152950031795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004842394870284428 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005379369410494452 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004796199495249842 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005290578181088409 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0047542627120771225 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005204793175353724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004715931822119253 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005123501068309826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004680767815337469 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005048018633661901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004648445724521553 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004979174734805436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004618682850357892 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00491715526646551 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004591212777232234 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004861680599039092 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0045657769354556545 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004812100299579256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004542129467340037 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004767632724799435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004520041553684286 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004727375433396767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004499313768031701 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004690566377522533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004479781172389545 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004656482433133265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0044613158176092255 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0046245753799346 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.00444381794128329 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00459436939769041 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0044272099557862935 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004565501171301174 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004411421573455751 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004537675372215316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004396383225939909 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004510688030517057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0043820240354270805 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004484341257279191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004368274105775653 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004458599624580101 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.00435507198923988 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0044334575513323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004342353821818778 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004408940234605004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004330056632807764 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004385015756056151 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [23:41<02:55, 175.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.13869514342152725 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.10771236092071323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01942270722692275 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.02109284805106547 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011523882955751198 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012789607030914767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009441271621434794 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010694416164530112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008678446139759608 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009752622374114306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008167444031522439 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009010992108789436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0077450186996482805 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00836941531366285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007340191853280268 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007764055653858711 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006862145390133815 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0072320428991909415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006479812635577889 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006923981284832253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0062281727394850476 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006820181884583743 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006047357651269077 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006725059589371085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005906093331991973 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006614120279931847 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005784452435251217 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006491782746332533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005669340666342717 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006358896532808156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005556128024341144 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006229676566470195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005449974930456017 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006126450888319489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005359433170316238 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006048189702115077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005284048285059985 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0059740012717049784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00521920633376542 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005894247723250266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005162191660743356 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005811158355380244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005111881405162714 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005730314358301899 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005067561662596154 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005656101836768144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005028404686540488 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005590272533214267 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0049935005172699635 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005532404743408894 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004962049172698336 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005481090691104969 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0049334316655490415 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005434802822385202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004907193651038773 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053922905353829265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.00488299838595269 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00535261191372924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004860589069562427 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005315105697852285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004839755751508676 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005279344890047522 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004820320207793877 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005245031186324709 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004802116598418988 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005211974994060309 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00478499792333536 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005180039347204215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004768821098127939 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005149079123785829 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004753457127184859 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005119007204056662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004738783855356588 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0050896940040676035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004724694904531386 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0050610618131673515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004711094285177924 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005033028811928542 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004697911085707074 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005005487583248931 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0046851032837990835 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004978390225647565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0046726504691713315 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0049516655716990285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004660553883728877 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004925326338065241 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004648819093526184 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00489940176553586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004637447347478723 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0048739706398919225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0046264221222585165 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004849020319799071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004615721406022522 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004824646399356425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004605324816954642 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004800810043782215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0045952003074888085 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004777493725722546 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004585320456080941 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004754717848674559 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [27:04<00:00, 162.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (6996, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1741, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.13282210631593722 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0875066872347485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03038302224265522 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.024901104921644383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015265785519152744 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012795483270152048 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011708613014652406 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009667652159590612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009804888833080286 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007870118688283996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008685435289113046 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006811776736073873 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007966215254475264 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006258759308945049 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007537899028560887 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006002246719700369 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007288772219974871 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005849432775920088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00701847224662157 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00567975280044431 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0067442963303802495 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005462694676084952 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0065340465997860316 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005275249112905426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006378244477872775 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051285981877960945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006255727246420705 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005006413959728723 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006155452625103057 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004899713794954798 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006070198173332772 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048041797581721436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005995289900459213 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004718056141229516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00592772303293693 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046405649278312925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005865600874400846 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004571160297332839 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005807625216465937 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004509146528487855 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005752945209313269 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004453651668419214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0057011318188926125 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004403922886757011 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005652207812107249 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043594593330371105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005606578677875674 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043198509692129764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005564697801250301 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004284537930718877 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005526711202128769 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004252794359556653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005492358893734407 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042239268691363656 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005461244982110175 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00419741651805287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005432770052206061 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004172925659540025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00540644936497455 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004150208839300004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0053818630273602735 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004129067558625883 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005358660167835857 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004109340360049497 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005336564077281278 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004090868074192919 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005315309167163421 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040735076033425605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005294689072984947 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004057125850919295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005274536990194866 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004041610478254205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005254699192892284 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004026857378299941 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005235045342695512 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004012768785469234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005215457602314753 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003999272983690554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005195839069018217 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003986306566829708 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005176097477816011 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003973811733621088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0051561623041467 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003961746974594214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005135968361011617 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003950086464597421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005115463852363567 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003938809790733186 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005094604757529381 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003927902665666559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005073372118885216 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003917370478368618 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005051726287445607 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0039072053274139765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005029672416050378 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038974259662526577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.00500721502708926 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038880324139344423 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.00498437917404319 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003879050125198608 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:44<24:41, 164.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2496239779236382 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.16709443588148465 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.046444741906980946 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03999055695127357 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01864810072344868 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.015474561720409176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012159914995615953 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010076417892493986 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010829289436880414 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008972015045583249 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.01006467168147093 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00833358006467196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009511357011192703 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007837322261184453 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.009008367787770179 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0073778096163137396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008478849513889991 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006833168436688456 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007923832769743826 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006111612010069869 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007528547875340773 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005837009068239819 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0072324735826927505 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005754905350675637 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0070029053715861414 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005639950033615936 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006843388648320959 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0055216227065433155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006725137680131075 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005422182965346358 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006629625624513517 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005344858719035983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006546864890467937 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005281045423312621 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006470775363018489 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00522120074707676 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006397520936706601 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005159462713213129 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00632492485977114 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050932838327505375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006252254181877459 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005022477477111599 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006180082191075262 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004948855271901597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0061099596388561475 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004875846977599642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006043866633960645 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004807691771367734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005983492563644501 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004747938076880845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005929621894414362 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046983690614896744 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005882057070532244 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004658815004354851 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005839972851955285 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004627766392447732 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005802320179818666 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004603195175613192 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005768088166725281 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004583186721852557 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005736397007867252 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004566163394007493 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0057065545437223865 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004551004542207176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00567803052349705 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004536973581310701 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005650434835195184 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004523610332134095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005623489822958464 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004510654049756175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005597002935047223 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004497922931543805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005570834562114385 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004485283914784139 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005544879334738487 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004472598452544348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005519040437897914 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004459733369929547 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005493223986356651 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004446549804627218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005467333917274272 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004432933460074393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005441275160264258 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004418770657767627 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0054149433149696755 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004403976708735255 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005388235043921427 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004388470920226113 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005361051819623197 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043721999147568235 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005333298673485427 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004355122650634836 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005304891234168094 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004337226569821889 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.00527576401645794 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004318520246835595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005245875424203637 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004299037118273025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005215212733316826 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042788406389511445 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [05:29<21:57, 164.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1171284087138361 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.07699348496442492 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.028529716800200885 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.023996250094337895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01371208226656941 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011941038685935464 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011497020270126834 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009527526177804578 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010408112542176424 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008513749022544785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009574131016326074 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007875648386437785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008875831464607456 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007332305466248231 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008464420648327853 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0069041190160946415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008243127581961112 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006597160302441229 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00808361853413352 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006375100501728329 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007943416324222978 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006210183301432567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007806369736599289 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0060825381851331755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007664822537417962 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005981692248447375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0075186746344863345 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005903062592683868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0073734272901844825 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005835720591924407 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.007231681623079359 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005760384439914064 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.007096073801786592 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005672277899628336 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006972010675328677 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0055830859706144445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0068623108361937 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0055029729550535025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006765422847687551 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005433118296787142 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006677211572194991 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005369476499882612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0065928885567567595 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005306680263443427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006507990641527932 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005240012231198224 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006418546392756714 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005166651223870841 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006320965524176008 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005086677000773224 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006212146437118791 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005001108830964023 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.006091072588210678 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004908304881643165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005962370121936616 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004806741005317731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005836572035611288 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004702674453570084 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005723929228835216 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004607909651134502 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005628950577590734 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004529764619655907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005550944630681754 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004468438738364388 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0054869243474337 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004420627846213227 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00543359590595435 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004382562582296404 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005388143562209967 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004351108629171821 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0053484202280202585 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004323954461142421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005312859230902318 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042994693175635555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0052803354927861736 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00427659043661234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005250037171225144 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004254646764390848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0052213750189046915 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004233250879174606 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0051939142657949945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004212183937091719 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005167329422629876 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004191326330923899 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005141374144488392 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004170596356165002 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005115856905839912 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004149935946969146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005090635897140933 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004129304806701839 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005065594945468655 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004108662029135634 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005040651682730271 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004087981700219892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005015746035430257 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004067234167913822 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004990842319348801 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004046403172171928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004965925544019805 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004025471187196672 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [08:14<19:14, 164.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.08282710423004137 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.06117728813128038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.027714856321919182 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02333696247501807 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013250116258859634 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01126813548617065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010835817529177762 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009114707130092112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009456882581272115 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007871460884978825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00878869697476044 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007180590512738986 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008473861135954896 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006801340005106547 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0082521173012226 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006518324473026124 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008054993171712647 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006280023816295646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007832610524646495 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0060550782448527485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007534603778587561 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00580939828756858 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007184682639324229 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005564668893136762 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0069300716691841815 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005420784317803654 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0067599044148448735 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00532650292258371 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006625720191338699 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005248729452829469 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006508703123497432 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051804177336056125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006398985081046105 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00511719186112962 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006291738942533425 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005055364170535044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0061876384156530715 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004994385503232479 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006091371610267385 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00493657285855575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006007566254428635 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00488481613045389 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005937295752156475 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004840299288149584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005878444865758459 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048022082054310225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005828010362730314 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004768851454454389 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005783487030530357 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004738674471578138 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005743134158161643 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004710611540146849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005705825854241678 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004684073894961991 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005670831969845615 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004658787640404295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005637648534540036 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004634612709791823 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005605902170101138 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004611455513672395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005575289689567893 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045892012707719745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005545555563700985 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004567758798260581 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005516474747596538 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004547005269507115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005487847361196955 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004526832505044612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005459494876556234 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004507131040604277 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005431256118521826 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004487799558873204 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0054029948450199684 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004468728303485973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005374590927664803 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004449817739342424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00534594266172541 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004430962874638763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005316971310044397 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004412081494758075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005287624045315552 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00439311012160033 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005257873560438043 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00437399559781294 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005227724668042538 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004354729226113043 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005197218647992162 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004335326771251857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005166430543251251 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00431586204054342 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005135477605843534 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042964458888904615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005104505926897945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004277241526340896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005073696659020995 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00425845405340872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005043241255714656 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004240301130762832 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005013334389444287 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004223009035922587 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [10:59<16:28, 164.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.30334264713607423 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.18742863711985674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03570213306644191 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0308645118705251 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.016490095773431128 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.014887311072512106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01245695385937345 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010727721317247911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010835136004647061 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00910592470318079 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.010100843887202034 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008361851157281887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00956746884547259 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007813006487082351 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00907443303159825 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007286960034715858 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008587579007396585 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006762635898353024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008123779907018866 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006307749826969071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007752843714397843 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005947092205116695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0074602556845479425 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005673724946311929 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007238277852892467 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005505537051199512 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0070623065649051235 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005410225943408229 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006913879348313836 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0053440358659083195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0067855007478703645 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005289199135520241 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006671686767872644 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052387665406885475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006568400973356307 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005188879142092033 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006474093778577705 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005139065885239027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006388783765867455 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005091105850244111 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006312391887627304 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00504623110067438 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0062438820480245705 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005004058604721319 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006181639161483984 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004963336245749485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006124075604751536 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004922634193843061 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006069799543881457 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048805494962090796 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006017572195992112 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004835905278609558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005966343326902349 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004788047236136414 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005915320298706764 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004736816032196988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005863945252904201 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00468231875195422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005811805070854179 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004624682427807288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005758550199871398 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004564127711240541 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005703870383554806 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00450082019564103 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0056475814672178465 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00443510438146239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005589827288635347 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004367836518213153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005531341641754354 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004300699302587997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0054735613250084325 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042359863153912805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005418245828940137 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004175679364495657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005366746675367701 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041208297522230585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005319641142356375 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040717601924288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0052769282082155275 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004028477496467531 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005238327525181857 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003990786954421888 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005203461927280987 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003958300300027159 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005171928614309934 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0039305284682830625 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005143343219023249 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0039069385161962025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005117325920751956 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003886999503116716 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0050935265416764235 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003870238617739894 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00507166200598115 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0038561954014849934 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005051476919181543 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003844457001171329 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005032745502531937 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0038346402855081994 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0050152595825791015 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003826406358910555 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [13:44<13:44, 164.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2512228045969793 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.17725315429270266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02444946820120493 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02100897113030607 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.016317671072375896 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.013764700801535086 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01225923472955891 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010171430888162417 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009922834616643483 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008201531193811786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008827381103393116 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007211167293346741 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008391559398286555 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0066626336044547235 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008115496937202536 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006268208757550879 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00781671420381868 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005930444856428288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007391183023111359 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005589368461038579 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007028338461093707 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005399028986523097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0068059188210656315 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00526225345463238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006632298131720014 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005144519849934361 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006473322175698329 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005035593220964074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006313493439170732 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00492329950529066 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006151113120656138 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004801392961632122 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005999135105449655 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004674192310564897 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005871784848322641 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004553852549923415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005770677648544244 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004449130046520044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005688389747288742 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043599093735048715 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00561725889910948 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004282391293567013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005552296319946681 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004213422265919772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005490605742776911 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0041508553655479445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005430539513908361 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004092869899151 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005371428939479841 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004037793116136031 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005313711702562103 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003984786208126355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005258724049050531 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003934760740958154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005207877643895442 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003889784940772436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.00516192916675458 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003851512899960984 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005120870357987441 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0038203739307143473 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005084198165991127 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0037958111783320254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005051218304670412 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0037767702036283234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005021240238204479 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0037620709468187256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004993648934879736 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0037506663219325923 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004967944233542198 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003741790153170851 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004943753134982329 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0037349964851852166 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.00492084407503823 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0037301114210012286 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00489911830693815 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003727026053026996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004878526249715472 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003725411812774837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0048589871576498155 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003724779383364049 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 398\n",
      "INFO: Validation loss did not improve in epoch 399\n",
      "INFO: Validation loss did not improve in epoch 400\n",
      "INFO: Validation loss did not improve in epoch 401\n",
      "Epoch: 401\n",
      "Train Loss: 0.004840403066982077 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0037247156808999453 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [15:56<10:15, 153.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 402\n",
      "Early stopping after 402 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.09582725531298276 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0710301956331188 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.031874456868154 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.028569329072805967 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014260040908694675 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012748717863790013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010657219419821451 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009050397193905982 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00920010233546722 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007652146119455045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008553015378082689 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006960849366574125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008169564501420683 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006473145274106752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007872331469664222 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006144954615526579 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007633390091146984 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006016800700771538 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007456605224848882 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005977721973745661 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007311907448576228 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0059350625950504434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007182891892065129 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005876469679854133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007066746326333515 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00580641427077353 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006962365646246974 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005732052341442216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006867701664554236 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005658419798551635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006779743152241048 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005587764325636354 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0066949999811485 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005520738195627928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006609845832606871 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005456760496070439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006520445593079917 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0053939414718611675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006422591358465759 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0053291102490303196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0063121752248923866 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052576803348281164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0061871381938993315 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005173303445123813 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0060506581852944 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005070040442726829 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005912859154945945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004949977854266763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005787506392310676 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004829116188921035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005683008487842399 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004728280037472194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005596839597440202 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004654836449348791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005522346986802167 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004602226155640727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005455363828892985 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004562122956849634 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005394296869997054 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004529717029072344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005338588530425254 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004502816870808601 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005287773754887844 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004480150987563485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005241252831183374 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004460637988945977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0051984154591634526 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004443158986131576 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005158732651060949 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004426583544012498 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0051218077652560355 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004409992428157817 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005087339052171382 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004392739105969668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005055086483879092 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004374490136449988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0050248451991810475 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004355182356878437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004996420316967994 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043349577579647304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004969620668131552 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004314069437201727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004944260111748406 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004292799513363703 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004920157559267054 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00427142421037636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004897144193852482 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004250163593414155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004875066277377529 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004229177991774949 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.00485378489367128 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042085716183382004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004833178837107469 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004188401950523257 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004813143768460494 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004168700860728594 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004793589985472273 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004149468833665279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004774443218622731 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004130692176774821 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [18:41<07:52, 157.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2590185897127134 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.17008930986577814 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03997866727717935 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.035160160606557675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014976701523776944 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012572960919615897 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010451319048045093 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008749958004971798 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009201165705442972 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007528180679814382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008576137912629777 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00686785264194689 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007995870227491713 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006318576257167892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007389874552104401 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005778912924738093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007078461931087077 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005444210700013421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0068840131016881904 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052705289457332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0067477600120611865 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00518594212745401 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006639631926486073 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005141504244370894 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0065476204774695445 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005115479984405366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006464572594135353 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005095912973311814 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006384851970735376 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050748288335109295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006304681269753911 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005046776758337563 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006222136898102349 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005008143707263199 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006136932406047163 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004957419489933686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006051346970101197 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004896604769270528 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005969919538078617 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004832493924451145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005896494226300553 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00477361110970378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0058324258071774425 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004724578053521162 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005777072880127931 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004685232212597674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005728895332107159 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004653001111000777 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005686374931366579 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004625216520137408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0056483133166569145 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004600062741982666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0056137786667845024 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004576523585075682 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005582038346639745 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004554129743271253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005552538037491478 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004532748530618846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005524877261595809 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004512375796383077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005498776647430254 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004493084905499762 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005474043480061063 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004474917291240259 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005450518838487157 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044578206767751415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005428098098823554 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004441787713122639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005406688427558959 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044268209614198315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005386213437371279 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004412940409119157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005366594231630076 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004400142225098203 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005347755466498537 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004388446375642988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005329624341203316 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004377842643721537 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005312130316564617 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004368332411501218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005295207311205787 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004359890903684903 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005278799241115275 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004352486135692082 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.00526285081645729 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004346057762053202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0052473166939955565 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004340534377843141 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005232160475868391 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004335819055664946 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.00521734604488667 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004331802879460156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0052028473226893605 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00432837500914254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0051886355041921755 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043253983561457555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0051746787809018365 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004322756767611612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005160961581073004 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004320330875502391 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [21:26<05:19, 159.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.23282424256749892 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.13743743071840567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03083399952002312 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.026649269326166673 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01488081228039036 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012725945583290675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01160234039315537 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009528655309060758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009639019075861907 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007772577507421374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00878720270782352 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006956744439561259 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008293344778780609 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006392163601280613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007643805258613947 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005838024954904209 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007295800920748088 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005530105031688105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0071088562910359985 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005360544100403786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006965481200665831 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005267830061810938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006839768914824786 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005193431912498041 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00672333014567625 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051203004761853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006613558609055602 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005046997845850208 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0065091721278713105 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0049760335125029085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006408681582178937 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004909045482054353 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006310858323077191 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004845737720924345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006215256916839613 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004784592770209367 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006121908335257577 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004723692592233419 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00603074547154276 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046617645897309885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005941535739576844 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004598880804736506 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0058541670781836665 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004536123730411584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005768926283153258 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044749124102633105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005686564528868112 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004416527332399379 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0056081361277666815 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043619413860142234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00553470341378151 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004311817143620415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005466994663889703 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004266464483754878 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005405233069121004 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004225892270915211 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005349180448511148 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004189874665726992 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005298286442513063 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004158013510856439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005251865486080703 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004129795685664497 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005209232508534455 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004104696430096572 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005169796104092134 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004082239685918797 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005133105959152148 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004062059648673642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005098837509391974 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004043910000473261 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005066763142125945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004027604129673405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005036730393150728 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004012997569092972 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00500862748860743 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003999954813413999 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004982350266019742 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00398832013703544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004957799410094511 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003977931494062597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004934858204903226 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003968611702492291 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004913399470294932 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00396019375290383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0048932992900003035 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003952531264671548 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004874430042925423 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003945504671851681 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004856675337935207 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003939023032911461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004839923901251649 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003933019703254104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004824079410624839 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003927466560112821 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004809050329723799 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00392232755812901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.00479475989420558 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00391757972326807 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004781135485229002 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003913207305595278 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [24:11<02:41, 161.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.34585926643483444 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.21898916682059114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03522623249555015 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.029893527569418602 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013633024382307173 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011193433903496374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011971469580506286 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009738823318515312 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.011096277092923686 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008877662577751008 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.010415702186988403 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008285498758777977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009817316469557788 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00781932991760021 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.009243581643897922 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0073953032578257 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008690085255149667 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006964184856042266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008276776484528462 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006574981651184234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.008051113046909848 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006317876685749401 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007892618030224625 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0061671934259886095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007728903518764293 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006068769342858683 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007516803789233202 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005969778630374507 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.007293216839483748 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005785503996197473 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.007147548998786842 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005587917066771876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.007060713520630786 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005468864582309669 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.007000032477463557 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005402423708106984 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0069512619762405 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0053632563361051405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006908652515120839 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005338345349512317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006869647147444387 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005320916350253604 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006832917532394534 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005307286960834806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006797632180711416 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005295343248342926 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006763180595563223 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005283747401765801 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006729065728709782 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005271619283170863 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006694849423179616 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005258301996879957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.006660136199645596 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052432690112089566 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.006624559977104607 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005226073621518232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0065877837518051475 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005206309297037396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.006549501041541531 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005183570726181973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.006509451139782084 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051574744453484365 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.006467421352544365 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005127666721289808 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.006423264034900423 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050938200205564495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0063768838559911 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005055610319091515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0063282539392927805 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005012754545631734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.006277424951517725 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004965044435283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0062245558086170315 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004912454419007356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.006169975058549854 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004855386866256595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00611422115885844 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004794950457289815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.006058019543912891 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004733108157630671 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.006002161557760651 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004672415969385342 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005947330757682148 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004615274410356175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0058941386733629385 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00456336334305392 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005843280523283977 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004517318115738983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005795651346181422 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004477006347257305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0057522479724351604 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004442186691713604 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005713883626857373 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004413055385124277 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005680796781716417 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00438998800448396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005652578481681407 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004372952886941758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005628458080179649 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00436136971693486 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [26:57<00:00, 161.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1856582865255064 // Train Acc: 0.0\n",
      "Val Loss: 0.12424629479646683 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01396045473842248 // Train Acc: 0.0\n",
      "Val Loss: 0.014180103172971444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.006077037587842679 // Train Acc: 0.0\n",
      "Val Loss: 0.006008332466113974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0036045698504565396 // Train Acc: 0.0\n",
      "Val Loss: 0.003491595571606674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0028781990998277877 // Train Acc: 0.0\n",
      "Val Loss: 0.0027288295259826224 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0024983644401484497 // Train Acc: 0.0\n",
      "Val Loss: 0.0023503830772824584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0023246553645295954 // Train Acc: 0.0\n",
      "Val Loss: 0.0021758271095512266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.002204432501789867 // Train Acc: 0.0\n",
      "Val Loss: 0.0020591081083032557 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0021030835502464152 // Train Acc: 0.0\n",
      "Val Loss: 0.0019658482845195317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.002008760829570971 // Train Acc: 0.0\n",
      "Val Loss: 0.0018797277163354342 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0019168433525703727 // Train Acc: 0.0\n",
      "Val Loss: 0.0017946123822846197 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0018298454527141416 // Train Acc: 0.0\n",
      "Val Loss: 0.0017137040614298629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0017513967464416454 // Train Acc: 0.0\n",
      "Val Loss: 0.001640817329329862 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0016807668735927023 // Train Acc: 0.0\n",
      "Val Loss: 0.0015747774429407649 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0016154589695411045 // Train Acc: 0.0\n",
      "Val Loss: 0.001513224716340615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0015545493996418805 // Train Acc: 0.0\n",
      "Val Loss: 0.0014556878528144972 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0014989512107924019 // Train Acc: 0.0\n",
      "Val Loss: 0.0014033048348871737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0014499208116324483 // Train Acc: 0.0\n",
      "Val Loss: 0.0013574519099413672 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0014077947754359732 // Train Acc: 0.0\n",
      "Val Loss: 0.0013187626981314017 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0013718768087075917 // Train Acc: 0.0\n",
      "Val Loss: 0.0012869473042717967 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0013409505593130625 // Train Acc: 0.0\n",
      "Val Loss: 0.001261071158726488 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0013136706587015417 // Train Acc: 0.0\n",
      "Val Loss: 0.0012398620065968398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0012887303321609022 // Train Acc: 0.0\n",
      "Val Loss: 0.0012219052719460292 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0012651333271352506 // Train Acc: 0.0\n",
      "Val Loss: 0.0012058273595440287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0012430078860653508 // Train Acc: 0.0\n",
      "Val Loss: 0.0011908268149868077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0012226571367743115 // Train Acc: 0.0\n",
      "Val Loss: 0.001176324065520682 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0012031778760709412 // Train Acc: 0.0\n",
      "Val Loss: 0.001161628410706974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.001183835328949964 // Train Acc: 0.0\n",
      "Val Loss: 0.0011463128133486448 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0011641451929213233 // Train Acc: 0.0\n",
      "Val Loss: 0.0011299779403171587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0011437681395816723 // Train Acc: 0.0\n",
      "Val Loss: 0.001112313502050132 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0011225313152352424 // Train Acc: 0.0\n",
      "Val Loss: 0.0010931977617226287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0011004421783033886 // Train Acc: 0.0\n",
      "Val Loss: 0.0010727337162031538 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0010776393889363421 // Train Acc: 0.0\n",
      "Val Loss: 0.0010511570078299635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0010543284540319258 // Train Acc: 0.0\n",
      "Val Loss: 0.0010287282858784735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0010307651475209053 // Train Acc: 0.0\n",
      "Val Loss: 0.0010057349508067338 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.001007255699957182 // Train Acc: 0.0\n",
      "Val Loss: 0.0009825335107796656 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.000984136489011054 // Train Acc: 0.0\n",
      "Val Loss: 0.0009595445039766756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.000961738238153 // Train Acc: 0.0\n",
      "Val Loss: 0.0009372080137836746 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0009403508444752312 // Train Acc: 0.0\n",
      "Val Loss: 0.0009159288959133185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0009202145152463744 // Train Acc: 0.0\n",
      "Val Loss: 0.0008960486717776141 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0009015400729330308 // Train Acc: 0.0\n",
      "Val Loss: 0.0008778475767361339 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0008844909387160641 // Train Acc: 0.0\n",
      "Val Loss: 0.0008615171908538535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0008690956544163467 // Train Acc: 0.0\n",
      "Val Loss: 0.0008470609924353829 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.000855192352668648 // Train Acc: 0.0\n",
      "Val Loss: 0.0008342494316325015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0008424969972310626 // Train Acc: 0.0\n",
      "Val Loss: 0.0008227231783993458 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0008307231117912236 // Train Acc: 0.0\n",
      "Val Loss: 0.0008121538027560084 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0008196462899188407 // Train Acc: 0.0\n",
      "Val Loss: 0.0008023173479490321 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.000809116862125835 // Train Acc: 0.0\n",
      "Val Loss: 0.0007930961676968516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0007990464789672759 // Train Acc: 0.0\n",
      "Val Loss: 0.000784450452721847 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0007893986629280177 // Train Acc: 0.0\n",
      "Val Loss: 0.0007763830553316935 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:46<25:02, 166.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.4310308470301432 // Train Acc: 0.0\n",
      "Val Loss: 0.33372789106585765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.015288646458371725 // Train Acc: 0.0\n",
      "Val Loss: 0.015511314308440143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.006954082189506285 // Train Acc: 0.0\n",
      "Val Loss: 0.0068143951875919645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0036161872183534073 // Train Acc: 0.0\n",
      "Val Loss: 0.0035192903395826845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0027612704463668815 // Train Acc: 0.0\n",
      "Val Loss: 0.002644231644014574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00242237722728142 // Train Acc: 0.0\n",
      "Val Loss: 0.0022847373154945673 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0022493304477310336 // Train Acc: 0.0\n",
      "Val Loss: 0.0021086263562954794 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.002131328687800592 // Train Acc: 0.0\n",
      "Val Loss: 0.0019951623520517553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.002031293629546922 // Train Acc: 0.0\n",
      "Val Loss: 0.0019003291032277047 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0019429591372072245 // Train Acc: 0.0\n",
      "Val Loss: 0.0018158748314652422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0018655085266667704 // Train Acc: 0.0\n",
      "Val Loss: 0.001740742599147118 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0017970043796536694 // Train Acc: 0.0\n",
      "Val Loss: 0.0016735417481289585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0017344379524081588 // Train Acc: 0.0\n",
      "Val Loss: 0.0016118688585596498 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.001675021870353591 // Train Acc: 0.0\n",
      "Val Loss: 0.0015534688739783384 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0016171678079897644 // Train Acc: 0.0\n",
      "Val Loss: 0.0014972721821819009 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0015605081763630413 // Train Acc: 0.0\n",
      "Val Loss: 0.0014434718688293784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0015059431574145784 // Train Acc: 0.0\n",
      "Val Loss: 0.0013934626880324106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0014560689994045767 // Train Acc: 0.0\n",
      "Val Loss: 0.0013486277144031853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0014119533976164145 // Train Acc: 0.0\n",
      "Val Loss: 0.0013079595332700675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0013725155254905154 // Train Acc: 0.0\n",
      "Val Loss: 0.0012712007676865058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.001336769594892766 // Train Acc: 0.0\n",
      "Val Loss: 0.0012381103139804592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0013041109152719778 // Train Acc: 0.0\n",
      "Val Loss: 0.0012082297169730405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0012740892627301464 // Train Acc: 0.0\n",
      "Val Loss: 0.0011810261893763462 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00124619663711616 // Train Acc: 0.0\n",
      "Val Loss: 0.0011558147164495577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0012199855439131548 // Train Acc: 0.0\n",
      "Val Loss: 0.0011319578881904652 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0011953888334079804 // Train Acc: 0.0\n",
      "Val Loss: 0.0011092337205561556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0011726162111246852 // Train Acc: 0.0\n",
      "Val Loss: 0.0010877253645131448 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0011518120177762081 // Train Acc: 0.0\n",
      "Val Loss: 0.0010675400418800895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0011330107221984278 // Train Acc: 0.0\n",
      "Val Loss: 0.0010488217091981576 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0011159158960307978 // Train Acc: 0.0\n",
      "Val Loss: 0.0010314416891725902 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.001100144822324693 // Train Acc: 0.0\n",
      "Val Loss: 0.0010152056138560345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0010853808901101812 // Train Acc: 0.0\n",
      "Val Loss: 0.0009999811568625525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0010713747450438664 // Train Acc: 0.0\n",
      "Val Loss: 0.0009856989881055515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0010579084648361873 // Train Acc: 0.0\n",
      "Val Loss: 0.0009723558090627193 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0010447640596374925 // Train Acc: 0.0\n",
      "Val Loss: 0.0009599893240109932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0010317211913077275 // Train Acc: 0.0\n",
      "Val Loss: 0.000948640509953045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0010186078948146594 // Train Acc: 0.0\n",
      "Val Loss: 0.0009382859102598476 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0010054225403271705 // Train Acc: 0.0\n",
      "Val Loss: 0.0009287752062928947 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0009924858729877311 // Train Acc: 0.0\n",
      "Val Loss: 0.0009198449475330894 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0009802809920456722 // Train Acc: 0.0\n",
      "Val Loss: 0.0009112404558849945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0009689294804139042 // Train Acc: 0.0\n",
      "Val Loss: 0.0009028110470601611 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0009581004414608018 // Train Acc: 0.0\n",
      "Val Loss: 0.0008944817191646011 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0009473734179452661 // Train Acc: 0.0\n",
      "Val Loss: 0.0008861728574794886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0009364669534008839 // Train Acc: 0.0\n",
      "Val Loss: 0.0008777748994444582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0009253315356337852 // Train Acc: 0.0\n",
      "Val Loss: 0.0008691970774882727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0009140599414758856 // Train Acc: 0.0\n",
      "Val Loss: 0.0008607301543551413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0009026997117020007 // Train Acc: 0.0\n",
      "Val Loss: 0.000852600008180492 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0008913705771449808 // Train Acc: 0.0\n",
      "Val Loss: 0.0008448195543182506 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0008802012227168392 // Train Acc: 0.0\n",
      "Val Loss: 0.0008374079694145952 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0008692636969100607 // Train Acc: 0.0\n",
      "Val Loss: 0.0008303673230164515 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [05:31<22:05, 165.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.09924013373446247 // Train Acc: 0.0\n",
      "Val Loss: 0.06845231381329624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.011586276641047505 // Train Acc: 0.0\n",
      "Val Loss: 0.011677493806928396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.004730957686224927 // Train Acc: 0.0\n",
      "Val Loss: 0.004577582268129018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.003005196507430112 // Train Acc: 0.0\n",
      "Val Loss: 0.002883791712917049 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.002509448931262229 // Train Acc: 0.0\n",
      "Val Loss: 0.002394292944915254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.002300751399975504 // Train Acc: 0.0\n",
      "Val Loss: 0.0021829647342780267 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00215664153151027 // Train Acc: 0.0\n",
      "Val Loss: 0.0020457203542305663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0020404449140054457 // Train Acc: 0.0\n",
      "Val Loss: 0.0019390963491539217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0019394029757793627 // Train Acc: 0.0\n",
      "Val Loss: 0.0018454528781479563 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0018489437050730432 // Train Acc: 0.0\n",
      "Val Loss: 0.0017588542980692264 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0017685546619726183 // Train Acc: 0.0\n",
      "Val Loss: 0.0016794846803267402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.001697383863138052 // Train Acc: 0.0\n",
      "Val Loss: 0.0016073385750959542 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0016340874350450605 // Train Acc: 0.0\n",
      "Val Loss: 0.0015417803184721959 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0015778204652222435 // Train Acc: 0.0\n",
      "Val Loss: 0.0014826707055114886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0015277344187015676 // Train Acc: 0.0\n",
      "Val Loss: 0.0014298365332334386 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0014827537847912475 // Train Acc: 0.0\n",
      "Val Loss: 0.0013827950598418036 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.001442030825822021 // Train Acc: 0.0\n",
      "Val Loss: 0.001341131626660089 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.001405257391466942 // Train Acc: 0.0\n",
      "Val Loss: 0.0013047369940481573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0013723492006543752 // Train Acc: 0.0\n",
      "Val Loss: 0.0012734946219345809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0013429514567934394 // Train Acc: 0.0\n",
      "Val Loss: 0.0012468985649651254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0013167378457768642 // Train Acc: 0.0\n",
      "Val Loss: 0.0012241736133794554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0012940071461659528 // Train Acc: 0.0\n",
      "Val Loss: 0.001204367968338457 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0012740994057140725 // Train Acc: 0.0\n",
      "Val Loss: 0.0011866574248415418 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.001256015510221233 // Train Acc: 0.0\n",
      "Val Loss: 0.0011707382737023925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0012391323240853412 // Train Acc: 0.0\n",
      "Val Loss: 0.0011561549346714112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0012230322038840936 // Train Acc: 0.0\n",
      "Val Loss: 0.0011424970057718879 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.001207438358202732 // Train Acc: 0.0\n",
      "Val Loss: 0.0011294897095384922 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0011921795937921064 // Train Acc: 0.0\n",
      "Val Loss: 0.0011169560831728053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0011771602328395211 // Train Acc: 0.0\n",
      "Val Loss: 0.001104778468354859 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0011623283348417815 // Train Acc: 0.0\n",
      "Val Loss: 0.0010928642473035407 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0011476458331514745 // Train Acc: 0.0\n",
      "Val Loss: 0.0010811281054471196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0011330699655703125 // Train Acc: 0.0\n",
      "Val Loss: 0.001069483529532921 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0011185424045335871 // Train Acc: 0.0\n",
      "Val Loss: 0.0010578463721851055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0011039941128650063 // Train Acc: 0.0\n",
      "Val Loss: 0.0010461455080076122 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0010893538608664933 // Train Acc: 0.0\n",
      "Val Loss: 0.001034320344280621 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0010745576988176345 // Train Acc: 0.0\n",
      "Val Loss: 0.0010223318744630723 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0010595616835890312 // Train Acc: 0.0\n",
      "Val Loss: 0.0010101578989468345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0010443466763555914 // Train Acc: 0.0\n",
      "Val Loss: 0.0009977977376283062 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0010289249676051146 // Train Acc: 0.0\n",
      "Val Loss: 0.0009852684392667884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0010133430093102111 // Train Acc: 0.0\n",
      "Val Loss: 0.0009726105362377976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0009976788317307842 // Train Acc: 0.0\n",
      "Val Loss: 0.0009598797660807825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0009820307530547794 // Train Acc: 0.0\n",
      "Val Loss: 0.0009471398376097734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0009665052834674971 // Train Acc: 0.0\n",
      "Val Loss: 0.0009344476359811696 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0009511966113034066 // Train Acc: 0.0\n",
      "Val Loss: 0.0009218386624442328 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0009361705928163807 // Train Acc: 0.0\n",
      "Val Loss: 0.0009093129430601203 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0009214559192502112 // Train Acc: 0.0\n",
      "Val Loss: 0.0008968372608333911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0009070456457864142 // Train Acc: 0.0\n",
      "Val Loss: 0.0008843488485912199 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.000892905278438883 // Train Acc: 0.0\n",
      "Val Loss: 0.0008717770643257112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0008789895656018956 // Train Acc: 0.0\n",
      "Val Loss: 0.000859056097264825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0008652563526694606 // Train Acc: 0.0\n",
      "Val Loss: 0.0008461458255293441 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [08:16<19:17, 165.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.22774367415469532 // Train Acc: 0.0\n",
      "Val Loss: 0.15216834775426172 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.014573806794743016 // Train Acc: 0.0\n",
      "Val Loss: 0.014208829555321824 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0059189535555170345 // Train Acc: 0.0\n",
      "Val Loss: 0.005743695958517492 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.003309599072598006 // Train Acc: 0.0\n",
      "Val Loss: 0.003168594801205803 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0025804001427591526 // Train Acc: 0.0\n",
      "Val Loss: 0.0024310238971586595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.002297546338985279 // Train Acc: 0.0\n",
      "Val Loss: 0.0021361215738579633 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0021560793938157764 // Train Acc: 0.0\n",
      "Val Loss: 0.001994669929263182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.002045143304098202 // Train Acc: 0.0\n",
      "Val Loss: 0.0018917414066064256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0019428764933589462 // Train Acc: 0.0\n",
      "Val Loss: 0.0017994937005410478 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0018441972898725488 // Train Acc: 0.0\n",
      "Val Loss: 0.00171059794745154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0017533582353421709 // Train Acc: 0.0\n",
      "Val Loss: 0.001628152740092694 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.001671374578320753 // Train Acc: 0.0\n",
      "Val Loss: 0.0015530375165822491 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0015973289594275889 // Train Acc: 0.0\n",
      "Val Loss: 0.0014848574535624886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0015314787070237134 // Train Acc: 0.0\n",
      "Val Loss: 0.0014241029365538536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0014744107008802165 // Train Acc: 0.0\n",
      "Val Loss: 0.0013713834737162952 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.001426152900608467 // Train Acc: 0.0\n",
      "Val Loss: 0.0013267485752955757 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0013857350736066006 // Train Acc: 0.0\n",
      "Val Loss: 0.0012892606031858701 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0013515020499192498 // Train Acc: 0.0\n",
      "Val Loss: 0.0012573743147764947 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0013219012581019293 // Train Acc: 0.0\n",
      "Val Loss: 0.0012297841835375452 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0012957552929590116 // Train Acc: 0.0\n",
      "Val Loss: 0.0012055298851919361 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0012721879097521965 // Train Acc: 0.0\n",
      "Val Loss: 0.001183811158857266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0012505513076820904 // Train Acc: 0.0\n",
      "Val Loss: 0.0011639508554997684 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0012303764773912826 // Train Acc: 0.0\n",
      "Val Loss: 0.0011454027552231722 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0012113317710636651 // Train Acc: 0.0\n",
      "Val Loss: 0.0011277555899472315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.001193214751125928 // Train Acc: 0.0\n",
      "Val Loss: 0.00111073969905688 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.001175958387237138 // Train Acc: 0.0\n",
      "Val Loss: 0.001094252147463108 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0011595693227785032 // Train Acc: 0.0\n",
      "Val Loss: 0.001078319586866366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0011440328673922593 // Train Acc: 0.0\n",
      "Val Loss: 0.0010630058840351096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.001129273823823899 // Train Acc: 0.0\n",
      "Val Loss: 0.0010483423700365661 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0011151731811949162 // Train Acc: 0.0\n",
      "Val Loss: 0.0010343115811172704 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0011016081559929473 // Train Acc: 0.0\n",
      "Val Loss: 0.0010208545007944023 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.001088491635004773 // Train Acc: 0.0\n",
      "Val Loss: 0.0010079039316306907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0010758052366696932 // Train Acc: 0.0\n",
      "Val Loss: 0.0009954094549439931 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.001063599319883769 // Train Acc: 0.0\n",
      "Val Loss: 0.0009833564809841018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0010519621164971733 // Train Acc: 0.0\n",
      "Val Loss: 0.000971764287731441 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0010409691374944033 // Train Acc: 0.0\n",
      "Val Loss: 0.0009606694292001934 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0010306481436771005 // Train Acc: 0.0\n",
      "Val Loss: 0.0009501050742321902 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.001020980538627862 // Train Acc: 0.0\n",
      "Val Loss: 0.0009400960503791628 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0010119159394964474 // Train Acc: 0.0\n",
      "Val Loss: 0.0009306508795072494 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.001003392008834914 // Train Acc: 0.0\n",
      "Val Loss: 0.0009217644157946449 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0009953415849364095 // Train Acc: 0.0\n",
      "Val Loss: 0.0009134162726695649 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0009877008225685879 // Train Acc: 0.0\n",
      "Val Loss: 0.0009055751148314978 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0009804110038564668 // Train Acc: 0.0\n",
      "Val Loss: 0.0008982032373420556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0009734199136045345 // Train Acc: 0.0\n",
      "Val Loss: 0.0008912583761478096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0009666802413104055 // Train Acc: 0.0\n",
      "Val Loss: 0.0008846982001242313 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0009601522011075862 // Train Acc: 0.0\n",
      "Val Loss: 0.0008784805641467259 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0009537993807485965 // Train Acc: 0.0\n",
      "Val Loss: 0.0008725663583556359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0009475912896519714 // Train Acc: 0.0\n",
      "Val Loss: 0.0008669163185087117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0009415001144564124 // Train Acc: 0.0\n",
      "Val Loss: 0.0008614978489772925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0009355040151292573 // Train Acc: 0.0\n",
      "Val Loss: 0.000856279409675732 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [11:01<16:31, 165.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3145459448774111 // Train Acc: 0.0\n",
      "Val Loss: 0.21567172462289982 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.013553785306899107 // Train Acc: 0.0\n",
      "Val Loss: 0.013626842573285104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.006339938430323959 // Train Acc: 0.0\n",
      "Val Loss: 0.006182958143339916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.003436809354653097 // Train Acc: 0.0\n",
      "Val Loss: 0.0033205645984377375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0025982973091389357 // Train Acc: 0.0\n",
      "Val Loss: 0.002510584217221053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0023126904028744253 // Train Acc: 0.0\n",
      "Val Loss: 0.0022311461541738194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0021792726014335704 // Train Acc: 0.0\n",
      "Val Loss: 0.0020988355340457268 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.002096734819407511 // Train Acc: 0.0\n",
      "Val Loss: 0.002014504813070578 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0020306911559368284 // Train Acc: 0.0\n",
      "Val Loss: 0.0019450346468312834 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00196994733155469 // Train Acc: 0.0\n",
      "Val Loss: 0.0018801485921192745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0019112761141987491 // Train Acc: 0.0\n",
      "Val Loss: 0.0018167995980051769 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.001854737358022647 // Train Acc: 0.0\n",
      "Val Loss: 0.001755207635075997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.001801430477298792 // Train Acc: 0.0\n",
      "Val Loss: 0.001696721412802369 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0017519276197781768 // Train Acc: 0.0\n",
      "Val Loss: 0.001642165208679878 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0017057996660924928 // Train Acc: 0.0\n",
      "Val Loss: 0.0015913902312273752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0016618299145292976 // Train Acc: 0.0\n",
      "Val Loss: 0.0015435405086133291 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0016186052547809222 // Train Acc: 0.0\n",
      "Val Loss: 0.001497685494409366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0015753783037703692 // Train Acc: 0.0\n",
      "Val Loss: 0.0014534831492875873 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0015323440591218885 // Train Acc: 0.0\n",
      "Val Loss: 0.0014110199655988254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0014900116139417408 // Train Acc: 0.0\n",
      "Val Loss: 0.001370284984163432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0014486451809758057 // Train Acc: 0.0\n",
      "Val Loss: 0.0013312573528806255 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0014082559096158086 // Train Acc: 0.0\n",
      "Val Loss: 0.0012939363048644736 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0013688811024671662 // Train Acc: 0.0\n",
      "Val Loss: 0.0012582559389887717 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0013308981042469588 // Train Acc: 0.0\n",
      "Val Loss: 0.0012242431339638477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0012949854509278253 // Train Acc: 0.0\n",
      "Val Loss: 0.001192070993023332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0012617515671554927 // Train Acc: 0.0\n",
      "Val Loss: 0.0011619242846129716 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0012314870924522493 // Train Acc: 0.0\n",
      "Val Loss: 0.0011339208382361737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0012041929912977728 // Train Acc: 0.0\n",
      "Val Loss: 0.001108137814761986 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0011796597573764253 // Train Acc: 0.0\n",
      "Val Loss: 0.0010845730361655694 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0011575331344451547 // Train Acc: 0.0\n",
      "Val Loss: 0.00106307507799515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0011374104210745955 // Train Acc: 0.0\n",
      "Val Loss: 0.0010433691818351773 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0011189215679742826 // Train Acc: 0.0\n",
      "Val Loss: 0.00102515556209255 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0011017628007575862 // Train Acc: 0.0\n",
      "Val Loss: 0.0010081771630211732 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.001085698300422133 // Train Acc: 0.0\n",
      "Val Loss: 0.0009922396209069782 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.001070553730936991 // Train Acc: 0.0\n",
      "Val Loss: 0.0009772071268261326 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0010562046036907242 // Train Acc: 0.0\n",
      "Val Loss: 0.0009629863176748833 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0010425656597097043 // Train Acc: 0.0\n",
      "Val Loss: 0.000949515258914537 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0010295783021191207 // Train Acc: 0.0\n",
      "Val Loss: 0.0009367465385532176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0010172021254664819 // Train Acc: 0.0\n",
      "Val Loss: 0.0009246466881119307 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0010054138908415211 // Train Acc: 0.0\n",
      "Val Loss: 0.0009131965870355171 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0009942019241862 // Train Acc: 0.0\n",
      "Val Loss: 0.0009023934310376221 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0009835580900828694 // Train Acc: 0.0\n",
      "Val Loss: 0.0008922417779103853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0009734700051174588 // Train Acc: 0.0\n",
      "Val Loss: 0.0008827406805887056 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0009639134469293759 // Train Acc: 0.0\n",
      "Val Loss: 0.0008738752537862059 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0009548511692935699 // Train Acc: 0.0\n",
      "Val Loss: 0.0008656129632568495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.000946234615758055 // Train Acc: 0.0\n",
      "Val Loss: 0.0008579016714727252 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0009380088445369567 // Train Acc: 0.0\n",
      "Val Loss: 0.0008506794075153515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0009301171923661179 // Train Acc: 0.0\n",
      "Val Loss: 0.0008438765293579887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0009225039104596163 // Train Acc: 0.0\n",
      "Val Loss: 0.0008374241440006617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.000915116008815513 // Train Acc: 0.0\n",
      "Val Loss: 0.000831259782187937 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [13:46<13:45, 165.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.16126837836552974 // Train Acc: 0.0\n",
      "Val Loss: 0.10682838586243716 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01546431122109566 // Train Acc: 0.0\n",
      "Val Loss: 0.01583764852786606 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.007479088046570993 // Train Acc: 0.0\n",
      "Val Loss: 0.0073447773233056065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.003881632244335615 // Train Acc: 0.0\n",
      "Val Loss: 0.0037219800996932794 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.002828559727268719 // Train Acc: 0.0\n",
      "Val Loss: 0.002693113467169248 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00239849517957284 // Train Acc: 0.0\n",
      "Val Loss: 0.0022690653382927517 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0021949706839787726 // Train Acc: 0.0\n",
      "Val Loss: 0.002066466835623776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.002052686708438695 // Train Acc: 0.0\n",
      "Val Loss: 0.001930644322419539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0019348343373403873 // Train Acc: 0.0\n",
      "Val Loss: 0.001819162512071092 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0018339114846793517 // Train Acc: 0.0\n",
      "Val Loss: 0.0017226216064459136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0017452101451350104 // Train Acc: 0.0\n",
      "Val Loss: 0.001637145103251731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0016662935361483552 // Train Acc: 0.0\n",
      "Val Loss: 0.001561403426669792 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.001596132194171245 // Train Acc: 0.0\n",
      "Val Loss: 0.001494963567132469 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0015342737842812607 // Train Acc: 0.0\n",
      "Val Loss: 0.0014374811592543582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.001480332040540251 // Train Acc: 0.0\n",
      "Val Loss: 0.001388483873663724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0014337320129900583 // Train Acc: 0.0\n",
      "Val Loss: 0.0013473388051021505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0013937329021657255 // Train Acc: 0.0\n",
      "Val Loss: 0.0013132764625796963 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0013594310187158435 // Train Acc: 0.0\n",
      "Val Loss: 0.001285272605838889 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.001329575674032918 // Train Acc: 0.0\n",
      "Val Loss: 0.0012619366047808647 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0013028804738936758 // Train Acc: 0.0\n",
      "Val Loss: 0.0012419439021455632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0012784550617237417 // Train Acc: 0.0\n",
      "Val Loss: 0.0012243874627694657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0012557060017155325 // Train Acc: 0.0\n",
      "Val Loss: 0.0012087404627395286 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0012340209233286455 // Train Acc: 0.0\n",
      "Val Loss: 0.0011944676794386892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0012128549254792806 // Train Acc: 0.0\n",
      "Val Loss: 0.0011810023738441735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0011918735492734778 // Train Acc: 0.0\n",
      "Val Loss: 0.0011679204663695682 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0011709334585123556 // Train Acc: 0.0\n",
      "Val Loss: 0.0011549220089694823 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0011500099155048256 // Train Acc: 0.0\n",
      "Val Loss: 0.0011417746144368058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.001129112267098868 // Train Acc: 0.0\n",
      "Val Loss: 0.001128278716747776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.001108230828230756 // Train Acc: 0.0\n",
      "Val Loss: 0.0011142588359441354 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0010873293185837303 // Train Acc: 0.0\n",
      "Val Loss: 0.0010995612074938517 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0010663612593737745 // Train Acc: 0.0\n",
      "Val Loss: 0.0010840686095021242 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.001045305971764549 // Train Acc: 0.0\n",
      "Val Loss: 0.0010677220977165483 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0010242087396218433 // Train Acc: 0.0\n",
      "Val Loss: 0.0010505571034049022 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0010032083484815693 // Train Acc: 0.0\n",
      "Val Loss: 0.0010327183809501796 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0009825320835116311 // Train Acc: 0.0\n",
      "Val Loss: 0.0010144492574247785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0009624395485540541 // Train Acc: 0.0\n",
      "Val Loss: 0.0009960522089386358 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0009431460171342045 // Train Acc: 0.0\n",
      "Val Loss: 0.0009778108208344995 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0009247509867764082 // Train Acc: 0.0\n",
      "Val Loss: 0.0009599108598195016 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0009072397082175456 // Train Acc: 0.0\n",
      "Val Loss: 0.0009423926439591345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0008905820156031365 // Train Acc: 0.0\n",
      "Val Loss: 0.0009252420528686013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0008748418353306614 // Train Acc: 0.0\n",
      "Val Loss: 0.0009086325607230802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0008600287584079744 // Train Acc: 0.0\n",
      "Val Loss: 0.0008928656957736662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.000846084571326817 // Train Acc: 0.0\n",
      "Val Loss: 0.0008780951566719026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0008329762822707061 // Train Acc: 0.0\n",
      "Val Loss: 0.0008643521525135094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.000820681025496297 // Train Acc: 0.0\n",
      "Val Loss: 0.0008516103558411653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0008091673770729138 // Train Acc: 0.0\n",
      "Val Loss: 0.0008398146212460812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0007983899546269387 // Train Acc: 0.0\n",
      "Val Loss: 0.0008288794071466492 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0007882902197570106 // Train Acc: 0.0\n",
      "Val Loss: 0.0008187021673868664 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0007788022771051374 // Train Acc: 0.0\n",
      "Val Loss: 0.0008091812860749831 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.000769856872213005 // Train Acc: 0.0\n",
      "Val Loss: 0.000800219878543761 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [16:32<11:00, 165.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1644855877852331 // Train Acc: 0.0\n",
      "Val Loss: 0.11650535694577477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.012871782803593298 // Train Acc: 0.0\n",
      "Val Loss: 0.012756684642623771 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0056292195166524955 // Train Acc: 0.0\n",
      "Val Loss: 0.005408780130727047 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0035228010667828187 // Train Acc: 0.0\n",
      "Val Loss: 0.003375306398480792 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0027493467347400866 // Train Acc: 0.0\n",
      "Val Loss: 0.002609384136634286 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0024131971983383646 // Train Acc: 0.0\n",
      "Val Loss: 0.0022621038933390413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0022359272865387163 // Train Acc: 0.0\n",
      "Val Loss: 0.002082588744815439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.002105371991412174 // Train Acc: 0.0\n",
      "Val Loss: 0.0019555736603003673 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.001993400420364711 // Train Acc: 0.0\n",
      "Val Loss: 0.0018467456573324108 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0018930013036247998 // Train Acc: 0.0\n",
      "Val Loss: 0.0017481024990874258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.001801785808556275 // Train Acc: 0.0\n",
      "Val Loss: 0.0016588129238119687 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0017179395279809343 // Train Acc: 0.0\n",
      "Val Loss: 0.0015782378910278731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0016407109736201787 // Train Acc: 0.0\n",
      "Val Loss: 0.0015058416574240917 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0015718260846822628 // Train Acc: 0.0\n",
      "Val Loss: 0.0014431398436003788 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00151321915197335 // Train Acc: 0.0\n",
      "Val Loss: 0.0013918163479750298 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.001463750348858042 // Train Acc: 0.0\n",
      "Val Loss: 0.0013506604488198223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.001421438789962085 // Train Acc: 0.0\n",
      "Val Loss: 0.0013175917699382725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0013850884126407864 // Train Acc: 0.0\n",
      "Val Loss: 0.0012909385914099403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.001353928575338211 // Train Acc: 0.0\n",
      "Val Loss: 0.0012693541598061777 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0013273163037398536 // Train Acc: 0.0\n",
      "Val Loss: 0.0012516816284253516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0013045742661603429 // Train Acc: 0.0\n",
      "Val Loss: 0.0012369154546335763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0012849438373375786 // Train Acc: 0.0\n",
      "Val Loss: 0.0012242336093384602 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0012676643120822896 // Train Acc: 0.0\n",
      "Val Loss: 0.001213040539402176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.001252091549667551 // Train Acc: 0.0\n",
      "Val Loss: 0.001202964743689253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0012377641573346318 // Train Acc: 0.0\n",
      "Val Loss: 0.0011938166590301658 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0012243973441686711 // Train Acc: 0.0\n",
      "Val Loss: 0.001185533596700142 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0012118269280864344 // Train Acc: 0.0\n",
      "Val Loss: 0.0011781108295756647 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0011999399077517222 // Train Acc: 0.0\n",
      "Val Loss: 0.0011715454954272983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0011886219053101816 // Train Acc: 0.0\n",
      "Val Loss: 0.001165789476098408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0011777411598141293 // Train Acc: 0.0\n",
      "Val Loss: 0.001160742004603063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0011671562077101218 // Train Acc: 0.0\n",
      "Val Loss: 0.0011562580904203722 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0011567369221175965 // Train Acc: 0.0\n",
      "Val Loss: 0.0011521691609893672 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0011463868647367803 // Train Acc: 0.0\n",
      "Val Loss: 0.0011482966325605628 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0011360612512844163 // Train Acc: 0.0\n",
      "Val Loss: 0.0011444587538293985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0011257632819525834 // Train Acc: 0.0\n",
      "Val Loss: 0.001140476814312437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.001115528240052507 // Train Acc: 0.0\n",
      "Val Loss: 0.0011361945458867755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.001105381964635982 // Train Acc: 0.0\n",
      "Val Loss: 0.0011315103693050332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0010953112565510966 // Train Acc: 0.0\n",
      "Val Loss: 0.0011263750056969002 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.001085257401178776 // Train Acc: 0.0\n",
      "Val Loss: 0.001120760454134804 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.001075136694689967 // Train Acc: 0.0\n",
      "Val Loss: 0.0011146299560840073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0010648580344871472 // Train Acc: 0.0\n",
      "Val Loss: 0.001107927271699406 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0010543295257738378 // Train Acc: 0.0\n",
      "Val Loss: 0.001100582584463568 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0010434602205583094 // Train Acc: 0.0\n",
      "Val Loss: 0.0010925051941409367 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0010321566007912295 // Train Acc: 0.0\n",
      "Val Loss: 0.0010835879895074124 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0010203213467354117 // Train Acc: 0.0\n",
      "Val Loss: 0.0010737048331066034 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0010078611066253946 // Train Acc: 0.0\n",
      "Val Loss: 0.0010627183702044104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0009946951404220285 // Train Acc: 0.0\n",
      "Val Loss: 0.0010504973893561823 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0009807746490749602 // Train Acc: 0.0\n",
      "Val Loss: 0.0010369495642159811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0009661026865926456 // Train Acc: 0.0\n",
      "Val Loss: 0.0010220658499747516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0009507617870527512 // Train Acc: 0.0\n",
      "Val Loss: 0.001005960652996278 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [19:16<08:15, 165.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.19387998445529372 // Train Acc: 0.0\n",
      "Val Loss: 0.1479530161077326 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.013311331275187126 // Train Acc: 0.0\n",
      "Val Loss: 0.013236611865630203 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.005360307990037517 // Train Acc: 0.0\n",
      "Val Loss: 0.005277406849729067 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0034213511809625887 // Train Acc: 0.0\n",
      "Val Loss: 0.003293920860795135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0029060000906694153 // Train Acc: 0.0\n",
      "Val Loss: 0.002743143154922026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.002593659254609071 // Train Acc: 0.0\n",
      "Val Loss: 0.0024173526199195872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0023370186694484144 // Train Acc: 0.0\n",
      "Val Loss: 0.0021600759389746767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0021459204031151324 // Train Acc: 0.0\n",
      "Val Loss: 0.0019818356652236122 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0019995591463149017 // Train Acc: 0.0\n",
      "Val Loss: 0.0018550320427907123 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.001877782622232106 // Train Acc: 0.0\n",
      "Val Loss: 0.0017502926872111856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0017760530851470928 // Train Acc: 0.0\n",
      "Val Loss: 0.001660412291742184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0016920556746262178 // Train Acc: 0.0\n",
      "Val Loss: 0.001583935008040333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0016195545771776393 // Train Acc: 0.0\n",
      "Val Loss: 0.0015159537674414672 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.001553095657496445 // Train Acc: 0.0\n",
      "Val Loss: 0.0014528275265051475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0014904153272940152 // Train Acc: 0.0\n",
      "Val Loss: 0.0013944954807150432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.001433709568189102 // Train Acc: 0.0\n",
      "Val Loss: 0.0013444368083252232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0013844301338311812 // Train Acc: 0.0\n",
      "Val Loss: 0.0013017492962800051 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.001339410719997037 // Train Acc: 0.0\n",
      "Val Loss: 0.0012632522235931406 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0012966326850394135 // Train Acc: 0.0\n",
      "Val Loss: 0.0012274314695704643 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0012550598561466612 // Train Acc: 0.0\n",
      "Val Loss: 0.001192843458821616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0012145971319014652 // Train Acc: 0.0\n",
      "Val Loss: 0.0011586089836782775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0011761108585835337 // Train Acc: 0.0\n",
      "Val Loss: 0.0011247628982263532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0011407345902883304 // Train Acc: 0.0\n",
      "Val Loss: 0.0010920265364058484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.001108654947122442 // Train Acc: 0.0\n",
      "Val Loss: 0.0010611114854823841 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0010793259222890764 // Train Acc: 0.0\n",
      "Val Loss: 0.0010324808657142884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.001052295109601298 // Train Acc: 0.0\n",
      "Val Loss: 0.0010062991866355085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0010274208636731697 // Train Acc: 0.0\n",
      "Val Loss: 0.0009825213231537914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0010047274733456085 // Train Acc: 0.0\n",
      "Val Loss: 0.0009610587709837339 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0009841988030696164 // Train Acc: 0.0\n",
      "Val Loss: 0.0009418393171720461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0009656806543060646 // Train Acc: 0.0\n",
      "Val Loss: 0.000924760048722171 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0009489125994682103 // Train Acc: 0.0\n",
      "Val Loss: 0.0009096421825233846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0009336199031142863 // Train Acc: 0.0\n",
      "Val Loss: 0.0008962396835241551 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0009195674563620928 // Train Acc: 0.0\n",
      "Val Loss: 0.0008842766362993808 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0009065745259860008 // Train Acc: 0.0\n",
      "Val Loss: 0.0008734800588254901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0008944976014963811 // Train Acc: 0.0\n",
      "Val Loss: 0.0008636022755209442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0008832118829439041 // Train Acc: 0.0\n",
      "Val Loss: 0.0008544322387685745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0008726013113625483 // Train Acc: 0.0\n",
      "Val Loss: 0.0008457973473758267 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.000862558685782946 // Train Acc: 0.0\n",
      "Val Loss: 0.0008375659209295092 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0008529874487988945 // Train Acc: 0.0\n",
      "Val Loss: 0.0008296399811198088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0008438053784679864 // Train Acc: 0.0\n",
      "Val Loss: 0.0008219533430581743 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0008349417599575109 // Train Acc: 0.0\n",
      "Val Loss: 0.0008144585823174566 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0008263386857148885 // Train Acc: 0.0\n",
      "Val Loss: 0.0008071231745733795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0008179465846741172 // Train Acc: 0.0\n",
      "Val Loss: 0.0007999224004875445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.000809725173512478 // Train Acc: 0.0\n",
      "Val Loss: 0.0007928376471020535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0008016386120920346 // Train Acc: 0.0\n",
      "Val Loss: 0.0007858515203803439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0007936579243601066 // Train Acc: 0.0\n",
      "Val Loss: 0.0007789512859413993 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0007857590655692041 // Train Acc: 0.0\n",
      "Val Loss: 0.0007721246078621003 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0007779230907868773 // Train Acc: 0.0\n",
      "Val Loss: 0.0007653631334753961 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0007701367750465147 // Train Acc: 0.0\n",
      "Val Loss: 0.0007586599743014879 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0007623934744685553 // Train Acc: 0.0\n",
      "Val Loss: 0.0007520151866959746 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [22:02<05:30, 165.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.410647676551723 // Train Acc: 0.0\n",
      "Val Loss: 0.3130906102332202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.014537788125937251 // Train Acc: 0.0\n",
      "Val Loss: 0.014296631718223745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.006602227487431236 // Train Acc: 0.0\n",
      "Val Loss: 0.00635326872156425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.003664184223486272 // Train Acc: 0.0\n",
      "Val Loss: 0.003540257112631066 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.002788673028228677 // Train Acc: 0.0\n",
      "Val Loss: 0.002625351668525995 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00248493203060323 // Train Acc: 0.0\n",
      "Val Loss: 0.0022939489360644734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0022739205475137505 // Train Acc: 0.0\n",
      "Val Loss: 0.002081962924619967 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0020975398177362887 // Train Acc: 0.0\n",
      "Val Loss: 0.0019095834918764674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0019545384271938957 // Train Acc: 0.0\n",
      "Val Loss: 0.0017713676291433247 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0018466685776472236 // Train Acc: 0.0\n",
      "Val Loss: 0.0016689834581815044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0017662933835181404 // Train Acc: 0.0\n",
      "Val Loss: 0.001593892488480461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.001703310290752319 // Train Acc: 0.0\n",
      "Val Loss: 0.0015356748941650782 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0016511396680226056 // Train Acc: 0.0\n",
      "Val Loss: 0.001488025944457728 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.001606549961746513 // Train Acc: 0.0\n",
      "Val Loss: 0.0014480131858197803 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.001568212638643398 // Train Acc: 0.0\n",
      "Val Loss: 0.001414294314004523 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0015354266956134394 // Train Acc: 0.0\n",
      "Val Loss: 0.0013856978052486243 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0015071506706889311 // Train Acc: 0.0\n",
      "Val Loss: 0.0013609128366541964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0014822790178589387 // Train Acc: 0.0\n",
      "Val Loss: 0.0013389899028962563 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.001459907338117009 // Train Acc: 0.0\n",
      "Val Loss: 0.001319160864328627 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0014392752646557353 // Train Acc: 0.0\n",
      "Val Loss: 0.0013006893895428882 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0014197120783727224 // Train Acc: 0.0\n",
      "Val Loss: 0.001282845455650452 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0014005909070131245 // Train Acc: 0.0\n",
      "Val Loss: 0.0012648604578730143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0013812945612262908 // Train Acc: 0.0\n",
      "Val Loss: 0.0012458963821038859 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0013612482232505723 // Train Acc: 0.0\n",
      "Val Loss: 0.001225165733723605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0013401143226735867 // Train Acc: 0.0\n",
      "Val Loss: 0.0012025017481805248 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0013181272199173824 // Train Acc: 0.0\n",
      "Val Loss: 0.0011793361631465484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.001295983456657268 // Train Acc: 0.0\n",
      "Val Loss: 0.0011580215055305002 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.001274157164923498 // Train Acc: 0.0\n",
      "Val Loss: 0.0011392669768114998 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0012527802350030379 // Train Acc: 0.0\n",
      "Val Loss: 0.0011223049932265316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0012320075714354697 // Train Acc: 0.0\n",
      "Val Loss: 0.001106467001028994 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0012120958518321396 // Train Acc: 0.0\n",
      "Val Loss: 0.0010915072620264255 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0011932343848822552 // Train Acc: 0.0\n",
      "Val Loss: 0.0010773342385866933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0011754363558170616 // Train Acc: 0.0\n",
      "Val Loss: 0.0010638216563331132 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0011585824867403752 // Train Acc: 0.0\n",
      "Val Loss: 0.001050785831731363 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0011425058671845267 // Train Acc: 0.0\n",
      "Val Loss: 0.0010380396181558767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.001127046071354153 // Train Acc: 0.0\n",
      "Val Loss: 0.001025453431330706 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0011120783052137163 // Train Acc: 0.0\n",
      "Val Loss: 0.0010129946743042885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0010975120020947913 // Train Acc: 0.0\n",
      "Val Loss: 0.0010007277090070684 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0010832741803550225 // Train Acc: 0.0\n",
      "Val Loss: 0.0009887723305093293 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0010692734969912742 // Train Acc: 0.0\n",
      "Val Loss: 0.0009772530435425738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0010553713405922643 // Train Acc: 0.0\n",
      "Val Loss: 0.0009662455181321341 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0010413814793018194 // Train Acc: 0.0\n",
      "Val Loss: 0.000955755722415316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0010271531329851718 // Train Acc: 0.0\n",
      "Val Loss: 0.0009457645040873269 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0010127418890614572 // Train Acc: 0.0\n",
      "Val Loss: 0.0009362932902480349 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0009983996764999139 // Train Acc: 0.0\n",
      "Val Loss: 0.0009273529739320193 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0009843323584212752 // Train Acc: 0.0\n",
      "Val Loss: 0.0009188246766412208 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0009706038116867075 // Train Acc: 0.0\n",
      "Val Loss: 0.0009105104881613939 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0009571589819157228 // Train Acc: 0.0\n",
      "Val Loss: 0.0009022269268329679 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0009438775493215863 // Train Acc: 0.0\n",
      "Val Loss: 0.000893837173961484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0009306430671862157 // Train Acc: 0.0\n",
      "Val Loss: 0.0008852663092759692 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [24:47<02:45, 165.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.12459009334649125 // Train Acc: 0.0\n",
      "Val Loss: 0.09580416896126487 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.014141361971202915 // Train Acc: 0.0\n",
      "Val Loss: 0.014241412756117908 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.006293323323700458 // Train Acc: 0.0\n",
      "Val Loss: 0.006077021851458333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0035966755189724506 // Train Acc: 0.0\n",
      "Val Loss: 0.0034435246711258184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00280504693919325 // Train Acc: 0.0\n",
      "Val Loss: 0.0026913152266801756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.002460533253753932 // Train Acc: 0.0\n",
      "Val Loss: 0.00237238195206208 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0023096068650611664 // Train Acc: 0.0\n",
      "Val Loss: 0.002227203177013011 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0022130669345127706 // Train Acc: 0.0\n",
      "Val Loss: 0.0021280008653940803 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0021282628885726552 // Train Acc: 0.0\n",
      "Val Loss: 0.0020386396401921624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0020423561369694054 // Train Acc: 0.0\n",
      "Val Loss: 0.0019464135640935804 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0019545939954430437 // Train Acc: 0.0\n",
      "Val Loss: 0.0018526797227836637 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0018674704092331487 // Train Acc: 0.0\n",
      "Val Loss: 0.00176130741747329 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0017841264196739588 // Train Acc: 0.0\n",
      "Val Loss: 0.0016755464260561645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.001706287049668199 // Train Acc: 0.0\n",
      "Val Loss: 0.0015961252999576656 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0016342936786418977 // Train Acc: 0.0\n",
      "Val Loss: 0.0015228293388976123 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0015677713485249662 // Train Acc: 0.0\n",
      "Val Loss: 0.001455229763814714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0015063695900607854 // Train Acc: 0.0\n",
      "Val Loss: 0.0013930960062266836 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0014505206129394608 // Train Acc: 0.0\n",
      "Val Loss: 0.0013370344417952848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.001401342385922682 // Train Acc: 0.0\n",
      "Val Loss: 0.0012879351138094948 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0013582657466945027 // Train Acc: 0.0\n",
      "Val Loss: 0.0012445982064078138 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0013191242870815166 // Train Acc: 0.0\n",
      "Val Loss: 0.0012052690099649639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.001282823032673744 // Train Acc: 0.0\n",
      "Val Loss: 0.0011693154447129928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.001249100055276325 // Train Acc: 0.0\n",
      "Val Loss: 0.0011364600408176723 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0012179966174429135 // Train Acc: 0.0\n",
      "Val Loss: 0.001106553451311563 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0011895883793981968 // Train Acc: 0.0\n",
      "Val Loss: 0.0010794281856347384 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0011638856184821402 // Train Acc: 0.0\n",
      "Val Loss: 0.0010548449171685868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0011407925550186407 // Train Acc: 0.0\n",
      "Val Loss: 0.0010325084593866697 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0011200835960813518 // Train Acc: 0.0\n",
      "Val Loss: 0.0010120893344388936 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0011014120162094701 // Train Acc: 0.0\n",
      "Val Loss: 0.0009932461168236015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0010843727769626233 // Train Acc: 0.0\n",
      "Val Loss: 0.00097566854581766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0010685983150025306 // Train Acc: 0.0\n",
      "Val Loss: 0.0009591384942971424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0010538136571729194 // Train Acc: 0.0\n",
      "Val Loss: 0.0009435573588813317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0010398317699665607 // Train Acc: 0.0\n",
      "Val Loss: 0.0009289092677962882 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0010265144047068994 // Train Acc: 0.0\n",
      "Val Loss: 0.0009151876598480158 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.001013742739424539 // Train Acc: 0.0\n",
      "Val Loss: 0.0009023403674787418 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0010014168038904735 // Train Acc: 0.0\n",
      "Val Loss: 0.0008902675021090545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0009894709258096305 // Train Acc: 0.0\n",
      "Val Loss: 0.0008788652048679069 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0009778850710486725 // Train Acc: 0.0\n",
      "Val Loss: 0.0008680686437979934 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0009666781635759887 // Train Acc: 0.0\n",
      "Val Loss: 0.0008578614068524489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0009558865095894852 // Train Acc: 0.0\n",
      "Val Loss: 0.0008482539088370025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0009455415940515009 // Train Acc: 0.0\n",
      "Val Loss: 0.0008392526301163757 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0009356577768783642 // Train Acc: 0.0\n",
      "Val Loss: 0.0008308423042763024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0009262324179938386 // Train Acc: 0.0\n",
      "Val Loss: 0.0008229796634457836 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0009172485415256565 // Train Acc: 0.0\n",
      "Val Loss: 0.0008156025561302985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0009086823671525684 // Train Acc: 0.0\n",
      "Val Loss: 0.0008086445279662836 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0009005068604394232 // Train Acc: 0.0\n",
      "Val Loss: 0.0008020381154809994 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0008926946774072473 // Train Acc: 0.0\n",
      "Val Loss: 0.0007957261074228551 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0008852162209223025 // Train Acc: 0.0\n",
      "Val Loss: 0.0007896589041179554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0008780402590195886 // Train Acc: 0.0\n",
      "Val Loss: 0.0007837937627548605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0008711318606239655 // Train Acc: 0.0\n",
      "Val Loss: 0.0007780920376800085 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [27:32<00:00, 165.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1057, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1056, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.23808700971686753 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.08297102717573152 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.00993245646274109 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.016974817002739978 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.006323891523252863 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01056453085723607 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.005829102873099437 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008959748936296605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.005559281756285445 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008595447668887894 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.005269941239470065 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008399725467076196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.005104332969594624 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008280777639490278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.004989632922346011 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008156412061961257 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.004883130188155374 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008015335496405469 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.004788753839335881 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007848097716786843 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.004706841687639065 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007668234853019171 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.004634404023991127 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00749570968122605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.004567778170701726 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007331743172150762 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.004505865295306725 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007169797468711348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00444920281041073 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007011410889818388 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0043977401245911514 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006862226267591776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004350680089571643 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006726024552549729 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00430722299136304 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006604926133364001 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004266600305652093 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006498578925589647 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004227957193427072 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00640411877675968 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0041904188486661465 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006317651063642081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004153169491187834 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00623597193386077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004115496269885665 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006157648722257684 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004077062247772411 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0060825491981471285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004038143516899132 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006009230581933961 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.003999037908814112 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005936270203057896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0039596733184049495 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005864926257773358 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0039198091342758775 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005797160392188851 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.003879194884046817 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005734632675097708 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.003837616496115635 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00567835578284062 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.003794945882289829 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005628944823162302 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0037512502610316106 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005587150185259388 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0037070318018562983 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00555233856978114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0036632510510426937 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005519984016085372 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0036208724445672756 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005486758898341042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.003580521685015955 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0054521430318024665 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.003542604416922089 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005415801273878007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0035074452012547478 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0053772456389304035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.003475286734058016 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005336154118755504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.003446173692824212 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005292520803563735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0034199396779722837 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0052464952459558845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.003396246409588715 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005198425346273272 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0033747225265213163 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005148970847264589 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.003355037119308524 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005098880607607391 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.00333692360191795 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005048911159118528 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0033201748905665115 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00499973094145603 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.003304629970892712 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004951835681876058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0032901505198392684 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00490563310107545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0032766212949900033 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004861350267139428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0032639393479294283 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004819171712049009 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [06:31<58:47, 391.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.24267525461517697 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0873667955508127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.008968846608971466 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01547695042373722 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.006601348588972266 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010877085349741666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.005897839414785826 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009097331004929455 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.005578418241605636 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008556840235970038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0054139182849666295 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00845583518008318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.005253950038733696 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0084223430535263 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0051199740579545875 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008328269646667382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.005011346625490625 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008153818609357318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00491360714371781 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007985555539455484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00482086497708011 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007831198916606167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.004728768228186538 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0076769885182490245 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.004640960625899144 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00753590338589514 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.004556038136684661 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007420001215958858 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.004464239315137265 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007328996982644586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.004359980363953625 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0072603243581183695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004252598548697059 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007205457419759649 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004160612212579997 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.0071343672530287325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00408931728138527 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.007036979864899288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004032531244849694 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.00692957009681884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.003984207578469068 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.0068259532426429145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.003940749961446112 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006731840309358257 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.003899949451996153 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006648541250101784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.003860262361988493 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006575606325093438 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0038205568267232985 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.00651171470663565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0037801050805515257 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006455151590189952 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.003738788482213778 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006403709405704457 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0036972163744316347 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006355368600719992 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.003656507800975934 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006309044259764692 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0036178489599129615 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006265418220530538 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0035822515353991207 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.00622643531738397 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0035502273437255905 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006193688971346573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.003521691801339742 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006167478136964799 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00349611964526244 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.0061471370110452615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.003472829992710306 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006131827581317767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0034511872792086125 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006121246715980198 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.003430677542018779 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006115689729888211 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 367\n",
      "INFO: Validation loss did not improve in epoch 368\n",
      "INFO: Validation loss did not improve in epoch 369\n",
      "INFO: Validation loss did not improve in epoch 370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [11:22<44:18, 332.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 371\n",
      "Early stopping after 371 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.2725865573671662 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.10575468427337267 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.010566218381141478 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.017413304447579908 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.006453488055874987 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010582163128727937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.005754911418241437 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008943812565549332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.005494528814960421 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008837608339758041 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.005305322337845831 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00877038822920226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 61\n",
      "Epoch: 61\n",
      "Train Loss: 0.005175214306760189 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008650538710165112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 62\n",
      "INFO: Validation loss did not improve in epoch 63\n",
      "INFO: Validation loss did not improve in epoch 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [12:13<23:46, 203.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 65\n",
      "Early stopping after 65 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.20029538991082957 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.07738306137787945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.009101688815770293 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01607672609936665 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0061763873184078515 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010130721596343553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00550271213248115 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008559686302974382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.005225540036305792 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008298560100443223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.005111356799262388 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008265716283965637 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.004985409036690116 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00823236645801979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0048454785554042385 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008192647495033109 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.004727187990114115 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00807933381292969 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.004628109685638008 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007893665700548273 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.004536853277898509 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007690804736579166 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.004449441463682695 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007511551889097866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00436134282662895 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007390790543628528 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00424816203463254 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007364865289727116 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 133\n",
      "INFO: Validation loss did not improve in epoch 134\n",
      "INFO: Validation loss did not improve in epoch 135\n",
      "INFO: Validation loss did not improve in epoch 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [14:00<16:34, 165.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 137\n",
      "Early stopping after 137 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.19196582450046182 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.06809509096338469 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.009093869972435189 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01582882017828524 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0063928512559898905 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.011023640297079348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.005483808083306714 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008796426008784157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00522303742500681 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008457863155533285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.005124452948526409 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008362772313001402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.005043878305098553 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008230895687387708 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0049699031693554 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008085989345358136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.004902211114422842 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007951884434613235 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.004841438059422011 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007844706565853865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0047864120042471255 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007776893243905814 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 111\n",
      "Epoch: 111\n",
      "Train Loss: 0.00473445466086626 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007755683536422165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 112\n",
      "INFO: Validation loss did not improve in epoch 113\n",
      "INFO: Validation loss did not improve in epoch 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [15:30<11:32, 138.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 115\n",
      "Early stopping after 115 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.10674633370913307 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07790028851698427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.00883784800171137 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.015372582067571142 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.006168545533699056 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009801822618636139 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0057221206075446325 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008614938178866664 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.005332170145911117 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008337848737616749 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 46\n",
      "INFO: Validation loss did not improve in epoch 47\n",
      "INFO: Validation loss did not improve in epoch 48\n",
      "INFO: Validation loss did not improve in epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [16:09<06:58, 104.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 50\n",
      "Early stopping after 50 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.11816601688808454 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07118541533675264 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.008969233154448624 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.015761486473767197 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.00639559507196117 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010850262766539612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.005712422487444936 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009194080587750411 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.005255725445443768 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008421024143257561 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.005045842757947416 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008267367320299587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.004906886557101781 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00823170242502409 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.004795488260699805 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008153118487611851 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.004693455871302503 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00799857106689802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.004599177209383526 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007826955783564378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.004510095316656632 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007655196542413358 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.004426893366408582 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007488586570081466 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.004350663943782745 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007333179182537338 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.004282658901149654 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0071997499737121605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0042224385120071236 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007097268429146532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0041678403624439 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007024265332695316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0041168109452951246 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00697047366788063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004068216781768909 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0069244722835719585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004021628215368216 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006885156487388646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.003976756339790376 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006863816161913907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 194\n",
      "INFO: Validation loss did not improve in epoch 195\n",
      "INFO: Validation loss did not improve in epoch 196\n",
      "INFO: Validation loss did not improve in epoch 197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [18:46<06:05, 121.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 198\n",
      "Early stopping after 198 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.20822842258421986 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.07672256023129996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.009848984665368467 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.016781099893919686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.006395449643578786 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010461567634004442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.005603035997632989 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009040965876706383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.005256352819637711 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008596416309421115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.005100736053259549 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008156663320465562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.004976656826542243 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007756309419432107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0048294924937286095 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007333088488153675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.004699741980906702 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007086626260870081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.004637774752192897 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006934453863376642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.004586926551161792 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006838926929049194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0045384119199444175 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006786992026986007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 118\n",
      "INFO: Validation loss did not improve in epoch 119\n",
      "INFO: Validation loss did not improve in epoch 120\n",
      "INFO: Validation loss did not improve in epoch 121\n",
      "Epoch: 121\n",
      "Train Loss: 0.0044879064047317506 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006783488412004183 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [20:23<03:47, 113.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 122\n",
      "Early stopping after 122 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.08667873502308861 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07344523499555447 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.008815987193761173 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01508836492704337 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.006553489402220865 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010633515132426778 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.005983106564197066 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009016963947728714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0057341031205774004 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008529299551703255 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.005564489973631519 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008368806215003133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.005445191620400438 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008308890761862345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.005341892930629222 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008233424351441072 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.005218184015543798 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00806173586341388 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005063649116118458 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007797919953351512 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.004931397587041774 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007584336638341055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.004826024054337661 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007378002827274887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.004735096194605782 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007114474552080911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.004657893504693524 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006855106036014417 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.004587896768292293 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006664713385843617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.004518904363339804 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006538126145160812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004451100878646066 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00645200875313843 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004387008217784833 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00638730637044372 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0043315381735139335 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006347786107867518 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 191\n",
      "Epoch: 191\n",
      "Train Loss: 0.004284991072622763 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006337627056328689 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 192\n",
      "INFO: Validation loss did not improve in epoch 193\n",
      "INFO: Validation loss did not improve in epoch 194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [22:57<02:06, 126.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 195\n",
      "Early stopping after 195 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.08974867203771741 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07058947511455592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.009755376830460398 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01638749410973533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0062026763106083375 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010039168307283783 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.005553599572259511 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00844319023684982 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00526597778781822 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008166364757070207 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0050877826952356745 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008057727930409944 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.004947656946002643 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007947151884710527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.004803382798495585 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0077928628786193095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.004672701191992441 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007575945750645855 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.004561032289212401 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007319095359622117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.004472178063513728 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007086105336544707 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.004406027256725466 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006920453756773735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00435255620873025 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006808597258949543 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.004304859813190383 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006722810349481947 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0042607484734563964 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006645244416123366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.004220059415369365 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006571007625418989 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00418262868811367 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006502421610468232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004147730930990342 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006441830845741446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004114366171208391 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006390105467289686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004081666080582924 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006347471279749537 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004049111001654928 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006313557733151624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004016540786287606 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.0062870527001316935 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.003983915129932277 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006265561575727428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00395129874205622 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006245968188843964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.003918644787094146 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006225066447822268 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0038858887226730704 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006200042108128614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0038530130152781266 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006168878438663395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.003820111986167322 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006130745001000297 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0037873822423300433 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006086150814286049 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0037549930526019405 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.006036791275940178 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0037228925316637107 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.005984803356969839 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.003691123538603538 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.005933388834819198 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0036590475932696247 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.005884565745124265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0036260420400130015 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.0058418942841372504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0035916524127311326 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.005810863019980709 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.003556045446550419 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.005798776130712426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 352\n",
      "INFO: Validation loss did not improve in epoch 353\n",
      "INFO: Validation loss did not improve in epoch 354\n",
      "INFO: Validation loss did not improve in epoch 355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [27:39<00:00, 165.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 356\n",
      "Early stopping after 356 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictive performance\n",
    "predictive_results = predictive_evaluation(\n",
    "    data_train_real=data_train_real_numpy, \n",
    "    data_test_real=data_test_real_numpy,\n",
    "    data_syn=data_syn_numpy, \n",
    "    hyperparameters=hyperparameters, \n",
    "    include_baseline=True, \n",
    "    verbose=True)\n",
    "\n",
    "# save results\n",
    "bidirectionality = \"bi\" if hyperparameters[\"bidirectional\"] else 'no_bi'\n",
    "predictive_results.to_csv(DATA_FOLDER / f\"results_{syn_data_type}_{hyperparameters['num_epochs']}_{hyperparameters['num_evaluation_runs']}_{bidirectionality}.csv\", index=False)\n",
    "\n",
    "# split in mse and mae results\n",
    "mse_results = predictive_results.loc[predictive_results['Metric'] == 'MSE']\n",
    "mae_results = predictive_results.loc[predictive_results['Metric'] == 'MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x17d159e6ff0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mse_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mse_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MSE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mae_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mae_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MAE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2*1e06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_data_augmentation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
