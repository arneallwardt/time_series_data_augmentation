{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Füge das übergeordnete Verzeichnis zu sys.path hinzu\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "from utilities import split_data_into_sequences, load_sequential_time_series, reconstruct_sequential_data, Scaler, extract_features_and_targets_reg, get_discriminative_test_performance\n",
    "from data_evaluation.visual.visual_evaluation import visual_evaluation\n",
    "from predictive_evaluation import predictive_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../../data\")\n",
    "REAL_DATA_FOLDER = DATA_FOLDER / \"real\"\n",
    "SYNTHETIC_DATA_FOLDER = DATA_FOLDER / \"synthetic\" / \"usable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of loading data\n",
    "- Laden der Originaldaten: als pd dataframe \n",
    "- Laden der synthetischen, sequentiellen Daten: als np array (GAN, (V)AE)\n",
    "- Laden der synthetischen, sequentiellen Daten: als pd dataframe (brownian, algorithmit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible types: 'timegan_lstm', 'timegan_gru', 'jitter', 'timewarp', 'autoencoder', 'vae'\n",
    "syn_data_type = 'timegan_lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " syn data:\n",
      "\n",
      "       traffic_volume           temp        rain_1h        snow_1h  \\\n",
      "count   341988.000000  341988.000000  341988.000000  341988.000000   \n",
      "mean      3223.797936     282.704303       0.086439       0.000249   \n",
      "std       1943.974204      12.922822       0.321004       0.000466   \n",
      "min         41.627638     250.083873       0.000008       0.000000   \n",
      "25%       1152.987320     270.511312       0.000130       0.000002   \n",
      "50%       3608.409516     285.328962       0.000575       0.000006   \n",
      "75%       5010.047921     293.711888       0.037928       0.000324   \n",
      "max       7076.619110     305.881726      12.279954       0.004205   \n",
      "\n",
      "          clouds_all  \n",
      "count  341988.000000  \n",
      "mean       39.871618  \n",
      "std        39.339560  \n",
      "min         0.016394  \n",
      "25%         4.172619  \n",
      "50%        15.465574  \n",
      "75%        87.893841  \n",
      "max        97.951007  \n",
      "\n",
      "\n",
      "real data:\n",
      "\n",
      "       traffic_volume          temp       rain_1h       snow_1h    clouds_all\n",
      "count     28511.00000  28511.000000  28511.000000  28511.000000  28511.000000\n",
      "mean       3313.74238    282.688768      0.061611      0.000250     42.122795\n",
      "std        1971.53206     12.367361      0.678185      0.008298     39.316195\n",
      "min           0.00000    243.390000      0.000000      0.000000      0.000000\n",
      "25%        1289.00000    273.480000      0.000000      0.000000      1.000000\n",
      "50%        3507.00000    284.550000      0.000000      0.000000     40.000000\n",
      "75%        4948.00000    292.790000      0.000000      0.000000     90.000000\n",
      "max        7280.00000    310.070000     42.000000      0.510000    100.000000\n"
     ]
    }
   ],
   "source": [
    "# Load real time series\n",
    "data_real_df = pd.read_csv(REAL_DATA_FOLDER/'metro_interstate_traffic_volume_label_encoded_no_categorical.csv')\n",
    "data_real_numpy = dc(data_real_df).to_numpy()\n",
    "\n",
    "if syn_data_type == 'timegan_lstm':\n",
    "    # load sequential data (which should already be scaled)\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_lstm_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'timegan_gru':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_gru_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'autoencoder':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28478_12_5_lstm_autoencoder_unscaled_15.csv', shape=(28478, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'vae':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28511_12_5_lstm_vae_unscaled.csv', shape=(28511, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'jitter':\n",
    "    jitter_factor = 0.1\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'mitv_jittered_{str(jitter_factor).replace(\".\", \"\")}.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "elif syn_data_type == 'timewarp':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'mitv_time_warped.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "# Loot at real and syn data\n",
    "df = pd.DataFrame(data_syn_numpy.reshape(-1, data_syn_numpy.shape[-1]), columns=data_real_df.columns)\n",
    "\n",
    "print('\\n\\n syn data:\\n')\n",
    "print(df.describe())\n",
    "\n",
    "print('\\n\\nreal data:\\n')\n",
    "print(data_real_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>traffic_volume</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>341988.000000</td>\n",
       "      <td>341988.000000</td>\n",
       "      <td>341988.000000</td>\n",
       "      <td>341988.000000</td>\n",
       "      <td>341988.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3223.797936</td>\n",
       "      <td>282.704303</td>\n",
       "      <td>0.086439</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>39.871618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1943.974204</td>\n",
       "      <td>12.922822</td>\n",
       "      <td>0.321004</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>39.339560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>41.627638</td>\n",
       "      <td>250.083873</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1152.987320</td>\n",
       "      <td>270.511312</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.172619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3608.409516</td>\n",
       "      <td>285.328962</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>15.465574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5010.047921</td>\n",
       "      <td>293.711888</td>\n",
       "      <td>0.037928</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>87.893841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7076.619110</td>\n",
       "      <td>305.881726</td>\n",
       "      <td>12.279954</td>\n",
       "      <td>0.004205</td>\n",
       "      <td>97.951007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       traffic_volume           temp        rain_1h        snow_1h  \\\n",
       "count   341988.000000  341988.000000  341988.000000  341988.000000   \n",
       "mean      3223.797936     282.704303       0.086439       0.000249   \n",
       "std       1943.974204      12.922822       0.321004       0.000466   \n",
       "min         41.627638     250.083873       0.000008       0.000000   \n",
       "25%       1152.987320     270.511312       0.000130       0.000002   \n",
       "50%       3608.409516     285.328962       0.000575       0.000006   \n",
       "75%       5010.047921     293.711888       0.037928       0.000324   \n",
       "max       7076.619110     305.881726      12.279954       0.004205   \n",
       "\n",
       "          clouds_all  \n",
       "count  341988.000000  \n",
       "mean       39.871618  \n",
       "std        39.339560  \n",
       "min         0.016394  \n",
       "25%         4.172619  \n",
       "50%        15.465574  \n",
       "75%        87.893841  \n",
       "max        97.951007  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>traffic_volume</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28511.00000</td>\n",
       "      <td>28511.000000</td>\n",
       "      <td>28511.000000</td>\n",
       "      <td>28511.000000</td>\n",
       "      <td>28511.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3313.74238</td>\n",
       "      <td>282.688768</td>\n",
       "      <td>0.061611</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>42.122795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1971.53206</td>\n",
       "      <td>12.367361</td>\n",
       "      <td>0.678185</td>\n",
       "      <td>0.008298</td>\n",
       "      <td>39.316195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>243.390000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1289.00000</td>\n",
       "      <td>273.480000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3507.00000</td>\n",
       "      <td>284.550000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4948.00000</td>\n",
       "      <td>292.790000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7280.00000</td>\n",
       "      <td>310.070000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       traffic_volume          temp       rain_1h       snow_1h    clouds_all\n",
       "count     28511.00000  28511.000000  28511.000000  28511.000000  28511.000000\n",
       "mean       3313.74238    282.688768      0.061611      0.000250     42.122795\n",
       "std        1971.53206     12.367361      0.678185      0.008298     39.316195\n",
       "min           0.00000    243.390000      0.000000      0.000000      0.000000\n",
       "25%        1289.00000    273.480000      0.000000      0.000000      1.000000\n",
       "50%        3507.00000    284.550000      0.000000      0.000000     40.000000\n",
       "75%        4948.00000    292.790000      0.000000      0.000000     90.000000\n",
       "max        7280.00000    310.070000     42.000000      0.510000    100.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_real_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Predictive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"seq_len\": 12,\n",
    "    \"lr\": 0.0004,\n",
    "    \"batch_size\": 32,\n",
    "    \"hidden_size\": 4,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"num_evaluation_runs\": 5,\n",
    "    \"num_epochs\": 1000,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETERS:\n",
      "seq_len :  12\n",
      "lr :  0.0004\n",
      "batch_size :  32\n",
      "hidden_size :  4\n",
      "num_layers :  1\n",
      "bidirectional :  True\n",
      "num_evaluation_runs :  5\n",
      "num_epochs :  10000\n",
      "device :  cpu\n",
      "Synthetic Data is sequential: True\n",
      "Shape of the data after splitting into sequences: (22797, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2841, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2840, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1307481420167616 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.057235098768318636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.007702380190845083 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004758966900931483 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.007258741607788955 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004056591142931681 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.006985218860770656 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.003740652693236728 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.006692405236097311 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0034072780772373915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006235066437532114 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0029142766908397165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.005791620403458985 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0026800188097286593 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.005371480508639542 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0024481325644445133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0050158391286701295 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022003102368737958 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.004809586920345732 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002091470820007825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0047229121771914075 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020501835263867893 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.004674848414981035 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020203397006288253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.004640599018972508 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019958866917099177 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.004612969934710447 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019751214180298653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.004589347981016425 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001957109733615489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.004568506922079107 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001941349438464876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004549719448603669 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019275349724430884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004532468223880303 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019154279836809283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004516345974783509 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019048053334372935 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004501020278668013 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001895444285846577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004486224453257551 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018871350140635217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0044717574067581 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018796895210777692 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004457476657673555 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018729322133428847 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004443292093850531 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001866710266972149 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004429157793771366 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018608973164038125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004415048501515685 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018553849259977427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004400949007821718 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018500951923210215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004386839606089917 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001844972651975041 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004372696815039649 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018399602097846316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00435849222712817 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018350201674601 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0043441963929527535 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018300996802935606 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004329782885092417 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001825148283551039 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004315227617249728 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018200987898953928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004300514978995393 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018148792422313788 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0042856393486776015 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018094081720703523 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004270606482976873 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001803606782589903 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004255436322792874 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017973862266628428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004240161403105823 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017906543432494228 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004224829771931494 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017833264166583422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004209500707037418 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017753447467471692 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0041942413627571035 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017667301110347753 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004179114736678656 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017575831172119282 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004164167822059113 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017480476526543498 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004149431889800963 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017382474753513848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004134920504901605 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017282772047989322 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004120635918928062 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017182039171509613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004106578158494216 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017080672550946474 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004092753602931117 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016979136304103174 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004079182211588546 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001687800779211429 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.00406590021036756 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001677800272068197 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.004052956770944865 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016680064302440104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.0040404108269852 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016585168771424822 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.004028316734892252 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001649410101543233 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.0040167156426656634 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016407451776819031 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.004005632313215899 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001632555625322955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.00399507335845974 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016248454483686371 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.003985028980243664 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016175978202268146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.003975478582117487 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016107846769567035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.0039663962842857396 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016043809297484127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.003957749137293735 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001598353459096901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.003949504101722074 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015926732062989993 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.003941629871074241 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015873089079546292 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.003934096178346213 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015822365511145987 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.003926874041429304 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015774301110683113 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.003919937600202723 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015728666723098899 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.003913262585908838 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015685222055396755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.003906826297314981 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015643729397953811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.003900607915345036 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015603988810724925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.003894588234164944 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015565838757640776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.003888750497315753 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015529074942891936 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.003883080349792302 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015493571258600005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.0038775601869202145 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015459117849796926 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.0038721769643871723 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015425684736267234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.0038669212515724544 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015393130290269684 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.0038617805633001647 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015361312583214446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.0038567436000999707 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015330174238044308 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.0038518025629616865 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015299616844571206 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.003846946219929336 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015269614124325303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.003842167976638237 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015240149635277437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.0038374598798646396 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015211029899086844 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.003832814288159067 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015182341108872985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.0038282255871354655 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015154002316746064 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.0038236864170759686 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015125930004522958 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.00381919294174468 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015098204361431802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.0038147405584095903 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015070697219435418 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.0038103257521606856 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015043527665521866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 861\n",
      "Train Loss: 0.003805947514364514 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001501664098775998 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 871\n",
      "Train Loss: 0.0038016033987482017 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014990125544070989 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 881\n",
      "Train Loss: 0.0037972956839824805 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001496404926737331 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 891\n",
      "Train Loss: 0.003793025311594633 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014938599262595846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 901\n",
      "Train Loss: 0.00378879642502312 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014914061538639656 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 911\n",
      "Train Loss: 0.0037846136826768435 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014890819906356503 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 921\n",
      "Train Loss: 0.003780476694735914 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014868911075813884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 931\n",
      "Train Loss: 0.003776386348846751 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001484818042391974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 941\n",
      "Train Loss: 0.0037723451738995548 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001482849115940083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 951\n",
      "Train Loss: 0.003768352936165666 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00148097086001941 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 961\n",
      "Train Loss: 0.0037644123397297313 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014791928618895203 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 971\n",
      "Train Loss: 0.003760522611401348 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014775045575139772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 981\n",
      "Train Loss: 0.0037566843010918833 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014759017660784838 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 991\n",
      "Train Loss: 0.0037528951827052663 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014743841324509068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1001\n",
      "Train Loss: 0.003749156255241703 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014729377338819707 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1011\n",
      "Train Loss: 0.0037454662750328813 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014715644722579444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1021\n",
      "Train Loss: 0.003741823706637446 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014702510955256901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1031\n",
      "Train Loss: 0.0037382267777015146 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014689949941507467 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1041\n",
      "Train Loss: 0.0037346749337420107 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014677856322194796 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1051\n",
      "Train Loss: 0.0037311682659482157 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014666198423195086 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1061\n",
      "Train Loss: 0.003727705547795729 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014654865362184394 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1071\n",
      "Train Loss: 0.003724284932150559 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014643893780558255 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1081\n",
      "Train Loss: 0.003720906754706818 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014633185120974405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1091\n",
      "Train Loss: 0.0037175707618978923 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001462276666582133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1101\n",
      "Train Loss: 0.0037142761576748327 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014612633523039436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1111\n",
      "Train Loss: 0.0037110236424072363 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014602857575819858 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1121\n",
      "Train Loss: 0.0037078121044349086 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014593519085332792 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1131\n",
      "Train Loss: 0.0037046422069006682 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014584663061142553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1141\n",
      "Train Loss: 0.0037015145638830916 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014576359538837555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1151\n",
      "Train Loss: 0.0036984288214828667 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014568556641657534 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1161\n",
      "Train Loss: 0.0036953844544432043 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014561237378197648 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1171\n",
      "Train Loss: 0.003692381684736938 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014554265542531366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1181\n",
      "Train Loss: 0.003689419253021322 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014547624499694014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1191\n",
      "Train Loss: 0.0036864980889045403 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001454117425699101 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1201\n",
      "Train Loss: 0.0036836173770711514 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014534852260807425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1211\n",
      "Train Loss: 0.003680776502626202 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014528623952797164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1221\n",
      "Train Loss: 0.0036779763949329625 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014522461824935306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1231\n",
      "Train Loss: 0.003675215873673955 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014516394516699057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1241\n",
      "Train Loss: 0.003672497189681465 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00145104181268671 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1251\n",
      "Train Loss: 0.0036698183066670117 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014504585346452933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1261\n",
      "Train Loss: 0.003667179843199245 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014498873677214587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1271\n",
      "Train Loss: 0.0036645819471239896 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014493322922727826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1281\n",
      "Train Loss: 0.0036620243408731585 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014487958808816718 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1291\n",
      "Train Loss: 0.003659505715223387 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014482700475883993 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1301\n",
      "Train Loss: 0.003657025632025285 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001447762629825601 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1311\n",
      "Train Loss: 0.003654582704254785 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014472680373687633 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1321\n",
      "Train Loss: 0.0036521759230335247 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014467817084377191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1331\n",
      "Train Loss: 0.0036498038196997643 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014463041557831 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1341\n",
      "Train Loss: 0.0036474648971002166 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014458270602875254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1351\n",
      "Train Loss: 0.0036451578152341163 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014453566913738888 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1361\n",
      "Train Loss: 0.0036428805095663486 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001444888876076118 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1371\n",
      "Train Loss: 0.003640631141616515 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014444170159999322 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1381\n",
      "Train Loss: 0.003638408354484851 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001443943123887764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1391\n",
      "Train Loss: 0.003636210042572493 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014434658367647214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1401\n",
      "Train Loss: 0.003634035012499935 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014429857681003096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1411\n",
      "Train Loss: 0.0036318805356912293 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001442501732812713 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1421\n",
      "Train Loss: 0.003629746404541774 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014420124382340464 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1431\n",
      "Train Loss: 0.0036276296026211854 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014415180661814977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1441\n",
      "Train Loss: 0.0036255283207569902 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014410181882728436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1451\n",
      "Train Loss: 0.00362344175057295 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014405116717477612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1461\n",
      "Train Loss: 0.0036213673754245156 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014400007363455894 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1471\n",
      "Train Loss: 0.00361930510436732 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00143948315709484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1481\n",
      "Train Loss: 0.0036172524946389084 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014389605625019816 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1491\n",
      "Train Loss: 0.0036152091805841434 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014384307410029086 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1501\n",
      "Train Loss: 0.0036131731886939154 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014378968132476668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1511\n",
      "Train Loss: 0.003611144037469229 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001437355734452375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1521\n",
      "Train Loss: 0.0036091203953996353 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014368074271158221 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1531\n",
      "Train Loss: 0.003607101884339433 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014362546783385358 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1541\n",
      "Train Loss: 0.0036050882398641995 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014356958370504043 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1551\n",
      "Train Loss: 0.003603077008516944 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014351315814687797 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1561\n",
      "Train Loss: 0.003601069003735443 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014345685789869114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1571\n",
      "Train Loss: 0.0035990633726509244 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014340052259510403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1581\n",
      "Train Loss: 0.0035970602415568687 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014334481858303038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1591\n",
      "Train Loss: 0.0035950589443715015 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014328967009440818 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1601\n",
      "Train Loss: 0.0035930608893446833 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014323455594176574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1611\n",
      "Train Loss: 0.0035910654081904225 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001431794040849474 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1621\n",
      "Train Loss: 0.0035890743781670114 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001431232647907533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1631\n",
      "Train Loss: 0.003587087623426902 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014306616304269625 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1641\n",
      "Train Loss: 0.003585107649022094 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014300703082830131 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1651\n",
      "Train Loss: 0.0035831337766251614 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014294613492178072 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1661\n",
      "Train Loss: 0.0035811675646992827 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014288349608822695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1671\n",
      "Train Loss: 0.003579210087677874 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014281929696235136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1681\n",
      "Train Loss: 0.0035772621052811214 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001427534169431127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1691\n",
      "Train Loss: 0.0035753236166110097 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014268650553644416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1701\n",
      "Train Loss: 0.003573395669242161 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014261863573082837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1711\n",
      "Train Loss: 0.003571478335423915 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014255016030393164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1721\n",
      "Train Loss: 0.003569571836149453 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014248071349581724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1731\n",
      "Train Loss: 0.003567676473804717 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001424113443851199 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1741\n",
      "Train Loss: 0.0035657925368972427 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00142342269288512 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1751\n",
      "Train Loss: 0.0035639209126652614 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014227330118935723 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1761\n",
      "Train Loss: 0.003562061896392237 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014220494597109055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1771\n",
      "Train Loss: 0.003560214623290298 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014213727917286745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1781\n",
      "Train Loss: 0.0035583804894452928 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014207109143784835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1791\n",
      "Train Loss: 0.0035565590216852672 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014200576161180905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1801\n",
      "Train Loss: 0.0035547504612495457 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014194174266807483 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1811\n",
      "Train Loss: 0.0035529555508776777 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014187908869398905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1821\n",
      "Train Loss: 0.0035511747890492958 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014181698985094352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1831\n",
      "Train Loss: 0.0035494079373823936 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001417559823034373 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1841\n",
      "Train Loss: 0.003547657160908062 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014169495683581976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1851\n",
      "Train Loss: 0.0035459220867054106 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014163451030552178 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1861\n",
      "Train Loss: 0.003544202990873049 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014157403872630905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1871\n",
      "Train Loss: 0.0035425010858120614 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00141513385951226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1881\n",
      "Train Loss: 0.0035408155939236105 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014145260250321432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1891\n",
      "Train Loss: 0.003539146900455876 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014139191807362806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1901\n",
      "Train Loss: 0.0035374947520871586 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014133104299760182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1911\n",
      "Train Loss: 0.00353586025811016 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014127054202025023 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1921\n",
      "Train Loss: 0.0035342421493856145 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00141210223187096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1931\n",
      "Train Loss: 0.0035326412955180154 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014115037819046234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1941\n",
      "Train Loss: 0.003531057307014025 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014109071883702587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1951\n",
      "Train Loss: 0.003529490363639799 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014103141271906874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1961\n",
      "Train Loss: 0.003527941251566129 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014097286850015182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1971\n",
      "Train Loss: 0.003526408797414741 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014091467228456494 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1981\n",
      "Train Loss: 0.003524893370432829 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014085713061085495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 1991\n",
      "Train Loss: 0.003523395253331655 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014080022147129384 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2001\n",
      "Train Loss: 0.0035219145266876034 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014074373280033052 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2011\n",
      "Train Loss: 0.003520450978079448 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014068844928955555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2021\n",
      "Train Loss: 0.003519004020695594 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014063431502037336 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2031\n",
      "Train Loss: 0.0035175741215851706 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014058100680946215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2041\n",
      "Train Loss: 0.0035161613256488975 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014052921579104294 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2051\n",
      "Train Loss: 0.003514765161184299 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014047914670565784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2061\n",
      "Train Loss: 0.0035133867122580363 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014043072349093646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2071\n",
      "Train Loss: 0.00351202513473657 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001403844137702221 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2081\n",
      "Train Loss: 0.0035106798049080777 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001403397326540788 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2091\n",
      "Train Loss: 0.00350935152510761 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014029698246263303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2101\n",
      "Train Loss: 0.003508039308009149 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014025596116952013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2111\n",
      "Train Loss: 0.0035067439148839964 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014021695379607296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2121\n",
      "Train Loss: 0.0035054646293615705 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014017924592500616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2131\n",
      "Train Loss: 0.003504201203018432 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014014315403595022 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2141\n",
      "Train Loss: 0.0035029526612640398 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014010857404183512 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2151\n",
      "Train Loss: 0.003501720624034741 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001400753620258543 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2161\n",
      "Train Loss: 0.003500503447229437 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014004346694185116 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2171\n",
      "Train Loss: 0.0034993012543659274 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014001283064756667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2181\n",
      "Train Loss: 0.0034981145524141622 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013998314408648215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2191\n",
      "Train Loss: 0.003496942848445871 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001399545577809926 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2201\n",
      "Train Loss: 0.003495784818461272 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013992716852586004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2211\n",
      "Train Loss: 0.003494641630858742 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013990054957390753 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2221\n",
      "Train Loss: 0.003493512552287064 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013987494938682472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2231\n",
      "Train Loss: 0.003492397342894754 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013985030073149318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2241\n",
      "Train Loss: 0.0034912955961262006 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013982615665155934 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2251\n",
      "Train Loss: 0.003490207043638372 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013980262362126136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2261\n",
      "Train Loss: 0.0034891307823553285 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013977952992799954 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2271\n",
      "Train Loss: 0.003488065505176286 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013975733959801454 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2281\n",
      "Train Loss: 0.0034870115159566645 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001397356643077258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2291\n",
      "Train Loss: 0.003485966178697637 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00139715261378043 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2301\n",
      "Train Loss: 0.003484930792417858 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013969588918177139 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2311\n",
      "Train Loss: 0.003483905017504607 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001396784161575913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2321\n",
      "Train Loss: 0.003482890143588206 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013966227510781811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2331\n",
      "Train Loss: 0.0034818879704527966 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013964738683079191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2341\n",
      "Train Loss: 0.0034808984881385817 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013963394893825138 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2351\n",
      "Train Loss: 0.0034799237939173834 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013962141771178191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2361\n",
      "Train Loss: 0.003478962807641149 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013960989302003316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2371\n",
      "Train Loss: 0.0034780157116884405 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013959906273259875 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2381\n",
      "Train Loss: 0.0034770815597153512 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001395892291369042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2391\n",
      "Train Loss: 0.0034761611027068354 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001395796813800659 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2401\n",
      "Train Loss: 0.0034752530643395126 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013957035804973142 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2411\n",
      "Train Loss: 0.0034743569573181124 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013956136123821391 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2421\n",
      "Train Loss: 0.0034734725789362917 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00139552744411539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2431\n",
      "Train Loss: 0.00347259990992756 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013954428585738028 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2441\n",
      "Train Loss: 0.003471738542838291 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013953635418065562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2451\n",
      "Train Loss: 0.003470887797626668 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013952804163616318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2461\n",
      "Train Loss: 0.003470048010313353 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013952013154205436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2471\n",
      "Train Loss: 0.0034692179576394527 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013951185879459752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2481\n",
      "Train Loss: 0.0034683976547895744 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013950411070091126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2491\n",
      "Train Loss: 0.0034675873233283626 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00139495828372081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2501\n",
      "Train Loss: 0.0034667865531897856 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013948788014868422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2511\n",
      "Train Loss: 0.003465994312228742 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013947999876599466 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2521\n",
      "Train Loss: 0.003465211427679774 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001394721876249094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2531\n",
      "Train Loss: 0.0034644367587380372 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001394642952220054 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2541\n",
      "Train Loss: 0.0034636699686481304 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013945634222427236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2551\n",
      "Train Loss: 0.003462912303445858 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013944858369898847 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2561\n",
      "Train Loss: 0.0034621619168136056 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013944082595852695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2571\n",
      "Train Loss: 0.003461419348866218 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013943283195382381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2581\n",
      "Train Loss: 0.003460684826801339 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001394249142403976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2591\n",
      "Train Loss: 0.0034599569048974167 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013941628976170434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2601\n",
      "Train Loss: 0.003459236434390468 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013940822034864948 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2611\n",
      "Train Loss: 0.003458522844878071 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013939978324630287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2621\n",
      "Train Loss: 0.003457816431317139 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001393912401602321 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2631\n",
      "Train Loss: 0.0034571167553950045 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013938301512343662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2641\n",
      "Train Loss: 0.003456423560279548 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013937438050745411 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2651\n",
      "Train Loss: 0.0034557369413235725 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001393659575319106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2661\n",
      "Train Loss: 0.0034550567209651385 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013935715751460848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2671\n",
      "Train Loss: 0.003454382487260547 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013934851093008445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2681\n",
      "Train Loss: 0.0034537148429407394 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013933966300591534 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2691\n",
      "Train Loss: 0.003453052794638509 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013933109303967755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2701\n",
      "Train Loss: 0.003452396394112141 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013932227762023595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2711\n",
      "Train Loss: 0.0034517454451046684 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013931358391366731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2721\n",
      "Train Loss: 0.0034511002300824413 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013930473850747005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2731\n",
      "Train Loss: 0.0034504598886660218 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013929634960629883 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2741\n",
      "Train Loss: 0.0034498232091890665 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013928777051650072 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 2751\n",
      "Epoch: 2751\n",
      "Train Loss: 0.0034491826606578707 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013928376282094403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 2752\n",
      "INFO: Validation loss did not improve in epoch 2753\n",
      "INFO: Validation loss did not improve in epoch 2754\n",
      "Epoch: 2761\n",
      "Train Loss: 0.0034485219579077683 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013925521206136877 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2771\n",
      "Train Loss: 0.0034479008603685627 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013922667882949366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2781\n",
      "Train Loss: 0.0034472925568405713 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013920409428976016 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2791\n",
      "Train Loss: 0.003446693498044318 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013918555953168425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2801\n",
      "Train Loss: 0.003446101883787474 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013916892937405475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2811\n",
      "Train Loss: 0.0034455167647055385 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001391535935847591 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2821\n",
      "Train Loss: 0.003444937573579959 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001391398190034114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2831\n",
      "Train Loss: 0.0034443640492518197 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001391269479815461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2841\n",
      "Train Loss: 0.003443794238294878 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013911501714420828 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2851\n",
      "Train Loss: 0.0034432299026950336 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013910360595739833 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2861\n",
      "Train Loss: 0.0034426705980984307 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013909258803200982 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2871\n",
      "Train Loss: 0.003442115652708658 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013908260355420996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2881\n",
      "Train Loss: 0.0034415652809062446 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013907337561249733 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2891\n",
      "Train Loss: 0.003441019454199643 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013906466918526574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2901\n",
      "Train Loss: 0.0034404782876163347 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001390569649762317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2911\n",
      "Train Loss: 0.003439941994149008 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013904966148443184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2921\n",
      "Train Loss: 0.0034394109938234918 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013904293791097955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2931\n",
      "Train Loss: 0.003438885906841188 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013903717947286668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2941\n",
      "Train Loss: 0.0034383650369345254 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013903215844079516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2951\n",
      "Train Loss: 0.0034378501917248614 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013902716697036719 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2961\n",
      "Train Loss: 0.0034373408399958577 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001390231509033716 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2971\n",
      "Train Loss: 0.003436837497289419 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013901946413477234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2981\n",
      "Train Loss: 0.003436339928733961 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013901611438198957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 2991\n",
      "Train Loss: 0.003435848400631572 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013901312028455517 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3001\n",
      "Train Loss: 0.0034353629093085577 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013901059426827712 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3011\n",
      "Train Loss: 0.003434883763845148 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013900832914004332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3021\n",
      "Train Loss: 0.003434410591891916 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013900639782293458 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3027\n",
      "Epoch: 3031\n",
      "Train Loss: 0.0034339435466827738 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013900486604582643 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3041\n",
      "Train Loss: 0.003433483014527191 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013900300915604144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3043\n",
      "Epoch: 3051\n",
      "Train Loss: 0.0034330289132976683 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001390019351265948 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3055\n",
      "INFO: Validation loss did not improve in epoch 3060\n",
      "Epoch: 3061\n",
      "Train Loss: 0.0034325804641708044 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013900049458508998 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3063\n",
      "INFO: Validation loss did not improve in epoch 3069\n",
      "INFO: Validation loss did not improve in epoch 3071\n",
      "Epoch: 3071\n",
      "Train Loss: 0.003432138580100697 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013899953559752595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3081\n",
      "Train Loss: 0.003431702788813408 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001389984807308261 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3086\n",
      "INFO: Validation loss did not improve in epoch 3088\n",
      "INFO: Validation loss did not improve in epoch 3089\n",
      "Epoch: 3091\n",
      "Train Loss: 0.0034312735155595762 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013899745411773243 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3094\n",
      "INFO: Validation loss did not improve in epoch 3099\n",
      "Epoch: 3101\n",
      "Train Loss: 0.0034308498401193505 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001389965410422785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3104\n",
      "Epoch: 3111\n",
      "Train Loss: 0.003430431866172576 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013899549768630708 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3112\n",
      "Epoch: 3121\n",
      "Train Loss: 0.0034300202202883928 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001389944757167459 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3123\n",
      "INFO: Validation loss did not improve in epoch 3124\n",
      "Epoch: 3131\n",
      "Train Loss: 0.003429613978491618 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013899346329585723 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3141\n",
      "Train Loss: 0.0034292139718537632 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013899221765191368 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3151\n",
      "Train Loss: 0.0034288196483408828 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013899106187013428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3154\n",
      "INFO: Validation loss did not improve in epoch 3160\n",
      "Epoch: 3161\n",
      "Train Loss: 0.003428430542128414 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013898956272855737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3171\n",
      "Train Loss: 0.0034280473017460323 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013898770734301527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3180\n",
      "Epoch: 3181\n",
      "Train Loss: 0.0034276696470132433 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013898639727823352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3182\n",
      "Epoch: 3191\n",
      "Train Loss: 0.003427297071286648 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013898476072733573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3201\n",
      "Train Loss: 0.003426929947323862 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013898271482615742 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3211\n",
      "Train Loss: 0.0034265679911074202 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013898116571755472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3221\n",
      "Train Loss: 0.003426210971602708 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013897877730680316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3231\n",
      "Train Loss: 0.003425859503786316 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013897659072621066 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3241\n",
      "Train Loss: 0.003425512129282853 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013897435620605024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3251\n",
      "Train Loss: 0.00342517051350085 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013897164608351886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3261\n",
      "Train Loss: 0.0034248328283278933 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013896924498480526 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3271\n",
      "Train Loss: 0.003424499649881042 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013896678195052434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3281\n",
      "Train Loss: 0.003424170675447804 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013896435253280268 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3291\n",
      "Train Loss: 0.0034238469783186673 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013896198256537736 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3301\n",
      "Train Loss: 0.0034235270493396075 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001389596210347928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3311\n",
      "Train Loss: 0.0034232108654071633 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013895744092898507 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3321\n",
      "Train Loss: 0.003422898514281833 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013895524394949585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3331\n",
      "Train Loss: 0.0034225902326312308 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001389531998795698 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3332\n",
      "Epoch: 3341\n",
      "Train Loss: 0.003422285404907302 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013895135894784014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3351\n",
      "Train Loss: 0.003421984894918265 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013894986359957145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 3361\n",
      "Train Loss: 0.003421686846631622 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013894819114304911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3363\n",
      "INFO: Validation loss did not improve in epoch 3367\n",
      "Epoch: 3371\n",
      "Train Loss: 0.0034213925216759738 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013894705066530642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3379\n",
      "Epoch: 3381\n",
      "Train Loss: 0.003421100779328114 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013894594923247782 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3386\n",
      "INFO: Validation loss did not improve in epoch 3389\n",
      "Epoch: 3391\n",
      "Train Loss: 0.003420813015837165 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013894503838068732 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3392\n",
      "INFO: Validation loss did not improve in epoch 3393\n",
      "INFO: Validation loss did not improve in epoch 3395\n",
      "INFO: Validation loss did not improve in epoch 3400\n",
      "INFO: Validation loss did not improve in epoch 3401\n",
      "Epoch: 3401\n",
      "Train Loss: 0.003420527583754884 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001389446168656597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 3403\n",
      "INFO: Validation loss did not improve in epoch 3404\n",
      "INFO: Validation loss did not improve in epoch 3405\n",
      "INFO: Validation loss did not improve in epoch 3406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive\\predictive_evaluation.py:272: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([{'Model': evaluation_method, 'Metric': 'MAE', 'Error': mae}])], ignore_index=True)\n",
      " 20%|██        | 1/5 [58:23<3:53:33, 3503.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 3407\n",
      "Early stopping after 3407 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.20681631878843898 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.052279942784081684 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.008027567916293726 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.005492700690278009 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.00726045894473765 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004481011373775728 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.006883121861359118 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.003991645980584487 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 34\n",
      "Epoch: 41\n",
      "Train Loss: 0.006513323982312837 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0036769580721248236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006279433178284361 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0034420786535346443 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006082534472631789 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.003235014819990048 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.005901630345613109 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0030560563946289295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.005717186170707223 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0028785148261407955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00552853644348101 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002691741041928069 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005366144455455996 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002536369210993348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005224751740913488 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0024175823344378157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005089642082603562 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002319688423473932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0049660576103832494 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022451992832082375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0048621476315703835 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021953437741573773 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.004778138509961982 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021462935072715196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004714212718982704 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002097567203297709 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004666797705562877 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.002052025152030244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004631375502742733 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020146954083930324 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004604094810107903 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001989772615253172 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004581660014720239 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019712585627540864 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004562235124253107 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001955496291813095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004544810609526927 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019411227405542152 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004528793033397251 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019276182787241727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004513806771868409 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019147598244814892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00449960446687282 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019023976173628582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004486012777547401 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001890423328333189 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004472910493581594 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001878735202552981 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004460208270216144 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018672716792076409 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004447834735470438 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001855945799321857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004435732819712728 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018447157691410753 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004423849564911027 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018336542357502276 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004412123491774675 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001822748524267645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004400496248470627 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018119831429532823 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0043889086763407165 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001801360317432646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00437729878806304 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017908587306214685 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004365601082956797 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017804607390583063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004353739326946479 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017701308681169169 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004341632798905151 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017598287112163275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004329184537686846 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017495012634140806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004316292232918706 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017391021286262973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0043028546333134035 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017286115038219127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004288784282962526 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017180337140263382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004274017496398934 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001707403914050691 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004258516364434387 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016967981558010567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0042422639336182275 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001686268926891132 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0042252637463723675 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016758877298731901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004207554348413983 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001665703885602566 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.00418924280967391 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001655786404820431 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004170522558944349 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016462301064983764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.004151655756101028 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016371342863657334 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.004132925116814292 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016285833631893307 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.004114582608578019 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016206413026103813 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.004096858069362043 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016133606020528614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.004079995623075888 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016067929786964833 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.004064244220211585 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016009645089193174 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.004049796547229973 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001595834446050687 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.004036715079156413 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001591291429024985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.004024917826296169 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015872009353597094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.004014237823606786 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015834472331825351 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.0040044817259746795 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015799481306601776 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [1:08:44<4:34:59, 4124.97s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# evaluate predictive performance\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictive_results \u001b[38;5;241m=\u001b[39m \u001b[43mpredictive_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_real_numpy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_syn_numpy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_baseline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# save results\u001b[39;00m\n\u001b[0;32m      5\u001b[0m bidirectionality \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbi\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbidirectional\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_bi\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive\\predictive_evaluation.py:35\u001b[0m, in \u001b[0;36mpredictive_evaluation\u001b[1;34m(data_real, data_syn, hyperparameters, include_baseline, verbose)\u001b[0m\n\u001b[0;32m     29\u001b[0m     baseline_train_data, baseline_test_data \u001b[38;5;241m=\u001b[39m train_test_split(data_real_dc, split_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m) \u001b[38;5;66;03m# split real data into train and test\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     baseline_data, baseline_scaler \u001b[38;5;241m=\u001b[39m get_distinct_data(train_data\u001b[38;5;241m=\u001b[39mbaseline_train_data, test_data\u001b[38;5;241m=\u001b[39mbaseline_test_data,\n\u001b[0;32m     31\u001b[0m                                                     evaluation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m                                                     syn_data_is_sequential\u001b[38;5;241m=\u001b[39mdata_syn_is_sequential,\n\u001b[0;32m     33\u001b[0m                                                     hyperparameters\u001b[38;5;241m=\u001b[39mhyperparameters)\n\u001b[1;32m---> 35\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mevaluation_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbaseline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m### TRTS ###\u001b[39;00m\n\u001b[0;32m     43\u001b[0m TRTS_data, TRTS_scaler \u001b[38;5;241m=\u001b[39m get_distinct_data(train_data\u001b[38;5;241m=\u001b[39mdata_real_dc, test_data\u001b[38;5;241m=\u001b[39mdata_syn_dc,\n\u001b[0;32m     44\u001b[0m                                         evaluation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRTS\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     45\u001b[0m                                         syn_data_is_sequential\u001b[38;5;241m=\u001b[39mdata_syn_is_sequential,\n\u001b[0;32m     46\u001b[0m                                         hyperparameters\u001b[38;5;241m=\u001b[39mhyperparameters)\n",
      "File \u001b[1;32mc:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive\\predictive_evaluation.py:236\u001b[0m, in \u001b[0;36mrun_model\u001b[1;34m(data, scaler, evaluation_method, hyperparameters, results, verbose)\u001b[0m\n\u001b[0;32m    227\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMRegression(\n\u001b[0;32m    228\u001b[0m     device\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    229\u001b[0m     input_size\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m     bidirectional\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbidirectional\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    233\u001b[0m )\u001b[38;5;241m.\u001b[39mto(hyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    234\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 236\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_MSE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m### Evaluation ###\u001b[39;00m\n\u001b[0;32m    250\u001b[0m test_model \u001b[38;5;241m=\u001b[39m LSTMRegression(\n\u001b[0;32m    251\u001b[0m     device\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    252\u001b[0m     input_size\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m     bidirectional\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbidirectional\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    256\u001b[0m )\u001b[38;5;241m.\u001b[39mto(hyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive\\LSTM.py:132\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, device, save_path, verbose, patience, num_epochs)\u001b[0m\n\u001b[0;32m    130\u001b[0m num_epoch_without_improvement \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m--> 132\u001b[0m     current_train_loss, current_train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     current_validation_loss, current_validation_acc \u001b[38;5;241m=\u001b[39m validate_one_epoch(model, val_loader, criterion, device)\n\u001b[0;32m    135\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(current_train_loss)\n",
      "File \u001b[1;32mc:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive\\LSTM.py:59\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     56\u001b[0m running_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     57\u001b[0m running_train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 59\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fanny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\fanny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\fanny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fanny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fanny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\fanny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\fanny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# evaluate predictive performance\n",
    "predictive_results = predictive_evaluation(data_real_numpy, data_syn_numpy, hyperparameters, include_baseline=True, verbose=True)\n",
    "\n",
    "# save results\n",
    "bidirectionality = \"bi\" if hyperparameters[\"bidirectional\"] else 'no_bi'\n",
    "predictive_results.to_csv(DATA_FOLDER / f\"results_{syn_data_type}_{hyperparameters['num_epochs']}_{hyperparameters['num_evaluation_runs']}_{bidirectionality}.csv\", index=False)\n",
    "\n",
    "# split in mse and mae results\n",
    "mse_results = predictive_results.loc[predictive_results['Metric'] == 'MSE']\n",
    "mae_results = predictive_results.loc[predictive_results['Metric'] == 'MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22fa053ea80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mse_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mse_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MSE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs {\" | jitter factor = \" + str(jitter_factor) if syn_data_type == \"jitter\" else \"\"}')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mae_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mae_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MAE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs {\" | jitter factor = \" + str(jitter_factor) if syn_data_type == \"jitter\" else \"\"}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2*1e06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_data_augmentation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
