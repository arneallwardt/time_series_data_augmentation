{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Füge das übergeordnete Verzeichnis zu sys.path hinzu\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "from utilities import split_data_into_sequences, load_sequential_time_series, reconstruct_sequential_data, Scaler, extract_features_and_targets_reg, get_discriminative_test_performance\n",
    "from data_evaluation.visual.visual_evaluation import visual_evaluation\n",
    "from predictive_evaluation import predictive_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../../data\")\n",
    "REAL_DATA_FOLDER = DATA_FOLDER / \"real\"\n",
    "SYNTHETIC_DATA_FOLDER = DATA_FOLDER / \"synthetic\" / \"usable\" / \"1y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of loading data\n",
    "- Laden der Originaldaten: als pd dataframe \n",
    "- Laden der synthetischen, sequentiellen Daten: als np array (GAN, (V)AE)\n",
    "- Laden der synthetischen, sequentiellen Daten: als pd dataframe (brownian, algorithmit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible types: 'timegan_lstm', 'timegan_gru', 'jitter', 'timewarp', 'autoencoder', 'vae'\n",
    "syn_data_type = 'timegan_lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " syn data:\n",
      "\n",
      "       traffic_volume           temp        rain_1h        snow_1h  \\\n",
      "count   104964.000000  104964.000000  104964.000000  104964.000000   \n",
      "mean      3110.944216     282.134022       0.036676       0.000156   \n",
      "std       1928.981595      11.968117       0.143883       0.000477   \n",
      "min         50.628945     249.730619       0.000003       0.000000   \n",
      "25%       1229.601124     271.711338       0.000453       0.000000   \n",
      "50%       3169.789237     281.234170       0.001995       0.000001   \n",
      "75%       4905.010753     293.498541       0.009782       0.000111   \n",
      "max       7066.810412     303.700879       4.466873       0.011250   \n",
      "\n",
      "          clouds_all  \n",
      "count  104964.000000  \n",
      "mean       47.137230  \n",
      "std        39.115225  \n",
      "min         0.011721  \n",
      "25%         2.917495  \n",
      "50%        55.389917  \n",
      "75%        87.674758  \n",
      "max        96.796656  \n",
      "\n",
      "\n",
      "real train data:\n",
      "\n",
      "       traffic_volume         temp      rain_1h      snow_1h   clouds_all\n",
      "count     8759.000000  8759.000000  8759.000000  8759.000000  8759.000000\n",
      "mean      3244.668912   282.208136     0.086792     0.000233    44.397306\n",
      "std       1946.247953    12.114907     0.901360     0.006145    39.195308\n",
      "min          0.000000   243.390000     0.000000     0.000000     0.000000\n",
      "25%       1252.500000   273.605500     0.000000     0.000000     1.000000\n",
      "50%       3402.000000   283.650000     0.000000     0.000000    40.000000\n",
      "75%       4849.500000   292.060000     0.000000     0.000000    90.000000\n",
      "max       7260.000000   307.330000    42.000000     0.250000   100.000000\n",
      "\n",
      "\n",
      "real test data:\n",
      "\n",
      "       traffic_volume         temp  rain_1h  snow_1h   clouds_all\n",
      "count     2135.000000  2135.000000   2135.0   2135.0  2135.000000\n",
      "mean      3325.263700   270.553730      0.0      0.0    45.065105\n",
      "std       1996.851023     7.864566      0.0      0.0    40.781402\n",
      "min        216.000000   248.660000      0.0      0.0     0.000000\n",
      "25%       1222.500000   265.735000      0.0      0.0     1.000000\n",
      "50%       3563.000000   271.550000      0.0      0.0    40.000000\n",
      "75%       4946.000000   275.680000      0.0      0.0    90.000000\n",
      "max       7280.000000   290.150000      0.0      0.0    92.000000\n"
     ]
    }
   ],
   "source": [
    "# Load real time series\n",
    "data_train_real_df = pd.read_csv(REAL_DATA_FOLDER/'mitv_prep_1y.csv')\n",
    "data_train_real_numpy = dc(data_train_real_df).to_numpy()\n",
    "\n",
    "data_test_real_df = pd.read_csv(REAL_DATA_FOLDER/'mitv_prep_3mo.csv')\n",
    "data_test_real_numpy = dc(data_test_real_df).to_numpy()\n",
    "\n",
    "if syn_data_type == 'timegan_lstm':\n",
    "    # load sequential data (which should already be scaled)\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'8747_12_5_timegan_lstm_16_2_20k_64.csv', shape=(8747, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'timegan_gru':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'8747_12_5_timegan_gru_24_2_10k_64.csv', shape=(8747, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'autoencoder':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'8726_12_5_lstm_autoencoder.csv', shape=(8726, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'vae':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'8759_12_5_fc_vae.csv', shape=(8759, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'jitter':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'jittered_01.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "elif syn_data_type == 'timewarp':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'time_warped.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "# Loot at real and syn data\n",
    "df = pd.DataFrame(data_syn_numpy.reshape(-1, data_syn_numpy.shape[-1]), columns=data_train_real_df.columns)\n",
    "\n",
    "print('\\n\\n syn data:\\n')\n",
    "print(df.describe())\n",
    "\n",
    "print('\\n\\nreal train data:\\n')\n",
    "print(data_train_real_df.describe())\n",
    "\n",
    "print('\\n\\nreal test data:\\n')\n",
    "print(data_test_real_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Predictive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"seq_len\": 12,\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 32,\n",
    "    \"hidden_size\": 12,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"num_evaluation_runs\": 10,\n",
    "    \"num_epochs\": 500,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETERS:\n",
      "seq_len :  12\n",
      "lr :  0.0001\n",
      "batch_size :  32\n",
      "hidden_size :  12\n",
      "num_layers :  1\n",
      "bidirectional :  True\n",
      "num_evaluation_runs :  10\n",
      "num_epochs :  500\n",
      "device :  cpu\n",
      "Synthetic Data is sequential: True\n",
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1057, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1056, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.21555846293259712 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.14593363145147176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02474954461230196 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.026075016816749293 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012587684081751772 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013803024180507398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01040902143271205 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011090150339227608 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00905232136962038 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009486339695286006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008148245108534792 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00869554135373191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007549882190043692 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008018880486762262 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007092846741043303 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007579927277915618 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006852365570928031 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007368267089238062 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006699058306267498 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0072759596411796175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006570257602963107 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007184343083816416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006440247932489771 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007055515916470219 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00630511809905216 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00687881261932061 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006173319714381801 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0066888528766439245 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006048208345611491 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006521822987398242 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005926070364368876 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006373457199729541 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005805760570614851 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006230138632578447 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005690044749368417 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006089992451426738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005583093229283572 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005960324313491583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005488380278209156 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005845775629174621 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0054062323251324465 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005740276592619279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0053335275483168115 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005633482134298366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005266608021997024 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005520720390931648 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005203453023887382 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005404119516777641 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005143637447295044 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005289051048320663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005087579992883601 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005180549609255703 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005035897027947566 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005081168636131813 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004988901935310449 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004990964593804058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004946436590462393 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004908743156941936 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004908048769375853 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004833203045350006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004873192007058593 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004763458564411849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004841344302093583 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.00469898700933246 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004812047014227069 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004639465122154969 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004784893648984441 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004584563615293626 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004759544450373314 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004533943679074154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004735716951910719 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0044871685506009005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0047131691847751124 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004443775951478849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004691712048208939 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004403318764696664 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004671191961511317 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0043653941316091835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004651481320318446 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.00432962942731512 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004632477903905168 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004295717065414304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004614098619560908 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004263397919096272 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004596277372061826 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004232480416677016 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004578962634669372 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.00420281444059904 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004562110925623536 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004174293936504161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004545685961157641 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004146840875310933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00452966246576334 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004120415894944659 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004514015298404724 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0040949787533677675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004498725105459414 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004070507619283436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004483775891079805 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004046980000566691 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive\\predictive_evaluation.py:277: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([{'Model': evaluation_method, 'Metric': 'MAE', 'Error': mae}])], ignore_index=True)\n",
      " 10%|█         | 1/10 [03:24<30:38, 204.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2106305353792153 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.15997338493573754 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02936526698257475 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03120576297206914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012851943400430147 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014114130617064588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010763298320740352 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011614971506573698 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009311052610295533 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009627736204385976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008097956201357318 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008327655439429423 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00735150501650357 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007647170582512284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006882194617005188 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0070863251766080364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006664129691266448 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006846599506816882 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0065306138449491265 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006721753579126123 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006423175673979423 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006628804846995455 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006322673677439617 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006543435800053617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006216794344593166 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006453282932531746 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006094146194871189 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006346352727097624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005949037798330269 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006210056243135649 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005797105530331278 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0060439376256373875 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005662053043403438 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005871376276071019 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005549139703494789 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005716100561103839 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0054531757053613225 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005585018381038133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005368493448511228 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005475245977697127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005291443533964292 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053805782363804825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005220013637270738 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005294604473473395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005153053483278253 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0052122939241063945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005089841860373688 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005130454747225432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005029856707020425 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005047435101861244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004972666832550435 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004962517208803226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004917955492966448 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004875471356653553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004865588280831185 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004786575124051203 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004815669151521831 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00469679809654789 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00476849571624125 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004607775919687222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004724465893606152 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004521239908439491 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0046838858109825784 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004438874448704369 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004646885548046974 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004362011619377881 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0046134042783247745 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004291417974266498 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004583224055657238 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004227320088402313 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004556037010059923 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004169535378048963 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004531505245955008 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004117637115996331 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00450930525790934 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00407108770313618 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0044891458739956544 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004029319946900667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.00447077069838997 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.003991800045375438 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004453961904110544 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.003958044104579398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004438524233346404 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.003927620507858913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0044242919117414535 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00390016012619633 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0044111147160509285 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.003875317654627211 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004398860337818551 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.003852815880878445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004387411921659831 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0038323738845065236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004376665837331981 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0038137918317635707 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0043665326215721065 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0037968640886794996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004356932782578235 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.003781417552518713 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004347799836473546 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0037672988152788844 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [06:47<27:11, 203.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.12331428089906482 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.09391133722794406 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01812124420909116 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.020149814252577284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012227586655789158 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0135941421989735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010083496407010884 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01091933306238121 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00865705265751938 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009238464867367464 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007977782192905127 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008541468106319798 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007663818052460025 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00818401264191112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007455313880757912 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007959468119010767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007243066144525924 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00775264882181278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006991470534266075 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0074972352766267516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006774458269432296 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007267769741113572 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0066025451804599625 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007093074375434834 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00644486264561908 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006955063552595675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006285114520582894 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00683577186154092 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006110384909480293 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006703548256636542 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0059074027536871986 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006517846055109711 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005696174365733444 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0063035361852277726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005537463977072986 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006139454466071637 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0054256698702135715 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006002826342249618 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005333110567116022 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0058448255089495115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005242598398136533 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0056479675626820505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00515857402445157 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005471835762043209 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005091251317882421 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005331058774198241 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005037263832199489 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0052121120148940995 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004991923155089718 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005108928910511381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004952123447280568 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0050176993861575335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004915898732066046 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004935422686257344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00488205396729168 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00485987525762004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004849910113856835 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004789530600914184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004819118676547396 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004723408690872876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004789526971357868 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004660900853409925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004761078319109528 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004601637647925492 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004733740167699101 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004545422948842102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004707467810668894 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00449217323843828 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004682179739448942 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004441853872883846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004657773296969066 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0043944533749976575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004634139399948346 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004349884857861873 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004611159476407389 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004308058551567442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004588719762659489 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004268822568359182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004566716985355546 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004231985966565416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004545059962194739 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00419736431350055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004523663454218326 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00416473818340284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.00450246061035262 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004133910728235017 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004481398876939975 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004104688558179666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004460452041201239 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004076901597299558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004439615010603392 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004050415392745943 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004418899727064138 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004025115580161047 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004398350493836278 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004000927739338402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004378039531544 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0039777962648419336 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004358057959606858 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0039556908270563275 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [10:11<23:46, 203.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.11838397510812013 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0962708400671973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.028472720400641 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.030907829894739038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012860618336262168 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01438419777653454 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009581494484569224 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010465464232872953 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008357177478308877 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009308027363765766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007951723306310655 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008665995884631924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007642513721200712 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008133672123007914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007275866357954627 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0076108094654046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006969797755523591 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0072824452414779976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0067552665036501374 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007138712588451146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006575425044568867 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007068176397725064 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006418049982137108 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007016676467815962 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006284322980094538 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006962852559381109 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006174521802424922 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006905682426055565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006084122045397296 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006848113878410967 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006007305026074776 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006788300555747221 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0059395739676967865 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006723550915279809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0058778823186799774 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0066533196676412925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005820250724965747 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006579011932070202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005765476260660556 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006502976750626284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00571299417506917 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006427504026385791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005662746987064254 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006354397628456354 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0056149949197297115 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0062847902162877075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005570076154052096 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006219158763997257 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005528223232185998 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0061574828920557215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005489444265815083 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006099505829350913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0054535156493602036 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006044832592391793 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005420059922710401 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005993040566168287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0053886468874895605 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005943785565357436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005358853026243933 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005896744790815693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005330297847178146 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005851679200799588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005302655415007858 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005808302872430752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0052756536730741875 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005766426178845851 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0052490653502024085 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005725669506800306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005222704428304191 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005685688270365491 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005196418608309375 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005646006471258314 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005170086260849311 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005606197700698805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0051436113262885545 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0055657351416919165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005116932068392295 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005524132287759772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005090013560652053 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005480904282782884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005062851494213865 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005435562928837231 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005035472934790309 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005387715242036125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005007933965037801 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005337031599243774 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.00498031286066432 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005283382094656939 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004952713862030909 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0052268669584437325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004925318835575106 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005168050733011435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0048983566950323455 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00510748043683741 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.00487201555396675 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005045706461019376 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004846469775126078 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004983509736656047 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004821867199383513 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004921832005493343 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [13:37<20:28, 204.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.16296303107056523 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.11971092032378211 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02127275432725131 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.022253871703629986 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012126354995628913 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013251798398628393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010526764974334569 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011460011797573636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009780062386344601 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01057633537771728 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009241872546976827 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009908608677933979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00875953086405435 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009302135522696464 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008289910303547733 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008722324677578667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007807336331270363 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008164701562183088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007298748298245652 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007577452178606216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006794502543978883 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006880197146743098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006384729455738417 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006355125662487219 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006125929412333444 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006166383836363607 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005979815223951056 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006100997657460325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00587760005146265 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006056605968350435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0057993411557259456 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00598668135866961 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005737439928538961 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005895152058014099 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00568592853387002 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005821392782415976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005641588127583836 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005771381650393938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005602173020733507 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005737077930996961 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005566144944168627 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005713083931957097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005532574749816835 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005695432335521807 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005500720870803471 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005684162140823901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005470109565173323 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005674898014951716 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005440683528971281 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005662981790545232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005412364152963715 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005646945593659492 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005385154833588885 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005626232404371395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005359084640262499 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00560095817918944 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005334127887685777 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00557195399340023 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005310185458834018 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005540380603633821 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005287091479718305 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005507560734472731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005264662825638415 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005474693004918449 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005242741356151736 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00544256393742912 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00522107556445293 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005416276084040017 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005199617094846347 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005396024481502964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0051784361874086466 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005377757561612217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005157503885740455 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005361099111135392 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005136801639442625 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005345753750161213 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005116313895173235 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005331426571287653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0050960245060421745 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005317921394153553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005075916711217214 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005305029403911356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0050559703241115994 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0052926561774631195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0050361623522743966 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005280720679472913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005016465195502243 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005269176169189022 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004996847353253622 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005258000750259003 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004977268675413581 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005247113342835184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004957691056783019 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0052363865885554865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0049380709954204354 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005225489388548714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004918379274266602 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005213883578749921 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004898610884488567 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005200732283650295 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [17:02<17:03, 204.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.12149234238440973 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.09856909595649033 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.022807595736421916 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0248127982896917 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01257374660064378 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0140437494881232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00976534683366109 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010525160705369404 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008391995624516731 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009109479436815223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007881658889326083 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008600559065063648 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007635934385437056 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008235629421987516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0074636240160079114 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007961325614493997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007310916416017325 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0077475233701989055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0071608155904623275 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00758861649014494 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007016273220086701 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0074756697970716395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006869791656814135 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00736216753584278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0067052353610264235 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007191634510972482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006512059756626042 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00694970998858266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006260059902233744 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006646448328598019 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005918881131495845 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0062497099519104645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005642653878414528 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005871612852548852 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005482078238006301 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005614856312818387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005367978139488149 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005430342869230491 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005271475421167897 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005274447372785825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005184328735017483 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005126766524935032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005105651545433504 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004981730229166501 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00503919160088701 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0048551067833186076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004985108822465187 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004759457428008318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00493997215534676 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004686659571769483 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0049012297236302824 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0046268590684870586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004867466797977414 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004574698094716843 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0048377057358989645 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004527629346258062 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004811112755139691 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004484501092092079 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004786942234682599 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004444717518601786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00476456378258588 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0044078653787865355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004743475490547105 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004373596364851384 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004723300555097211 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004341608324252507 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004703783585875206 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004311652128201197 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.00468475412089636 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0042834957335692115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0046660924943446105 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004256942405310624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004647726023333474 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004231842734631808 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004629614236331239 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0042080507435671545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004611743257821191 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004185463137486402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004594125336267217 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0041639670483110585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004576784589822425 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00414349403067985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004559758916984985 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00412394424133441 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004543093441650747 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0041052340735297866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0045268309651449136 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004087291000520482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004511011442353092 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004070045227897079 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004495652432869576 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004053456092472462 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004480760352283268 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00403748604688136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004466331674569511 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0040220904424238734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004452345652096177 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004007242229657576 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004438794702147032 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.003992909490240409 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [20:27<13:38, 204.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.15406323422467078 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.12632203345899196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.024618909281277416 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.026249097413657344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011562823211301771 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012869467876632424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009087261070844031 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010244679452358362 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008300792631934513 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009134983548017986 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007939735238855012 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008424602387308636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007720367216087035 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007991378722876748 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007561457833278598 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0077527041259386085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007431001208270526 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007620075786047999 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0072997356713987395 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007513912600081633 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007135841754149564 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007384013720130657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006894260516600942 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007139761309029863 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006562570429893115 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006744306486593012 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006237437938387594 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006303128683665658 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005967832011949053 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00593948077328284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005761831486002835 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005664648666210911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005619831108335188 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005479104549843161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0055209275999575525 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005374741103664479 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005447157142415344 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005333807237227173 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 184\n",
      "INFO: Validation loss did not improve in epoch 185\n",
      "INFO: Validation loss did not improve in epoch 186\n",
      "INFO: Validation loss did not improve in epoch 187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [21:44<08:08, 162.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 188\n",
      "Early stopping after 188 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.30348900529168493 // Train Acc: 0.0\n",
      "Val Loss: 0.23708742922719786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.021135747772607492 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.024185406954904252 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013268702593801992 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014788818336776732 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009450276427728933 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010720096278579576 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008210104030011439 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009213049759102218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0074766887652256055 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0081285263192566 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007029011550726518 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007564435952671748 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006781574045936747 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00729821329621379 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006572717150680068 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007070539861588794 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006369495562984723 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00682714712627999 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006180809451739582 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006612304652876714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006011807587912075 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006426693152581506 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005868656593068999 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006266052088261966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005752754484364477 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006124588743071346 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005658118210199296 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00598864328554448 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0055777701446412635 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005850521820213865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0055072151186508905 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005713083863477497 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005443361562330031 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005579736734302167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005383934808238987 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005449981031501118 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005327540402637156 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005321995430516408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005273585046945738 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005195334416759365 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0052220093680759826 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005072329419336337 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005172827605023831 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004956924000426251 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005125806997674619 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004852208041805117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005080599828969932 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004759080639547285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00503705064685383 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004676612422746771 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004995294895667216 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004603147554649588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004955556575378002 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0045373541891903565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.00491795769402329 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004478472593130872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004882474103602615 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00442601802676697 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0048489761349734895 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004379406146814718 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004817292131407078 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004338014651747311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004787227968017428 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004301229149431866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004758602999984355 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004268421185449423 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004731258607608008 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004239044980803395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004705062492151088 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004212605636840796 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004679907461869221 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004188627504524501 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004655718882007348 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004166711347780246 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004632437867337641 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004146505219122285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004609986368768991 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004127621277481975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0045883038278688665 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004109773305518662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004567335268829262 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004092704515000258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004546998746204105 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004076115663765985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0045272578160494336 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004059900214676471 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004508060579862442 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0040439593756352275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004489355752923115 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004028209348154419 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004471101777404581 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0040126245790709026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004453273289477925 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.003997253151336575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004435836182746622 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0039821257849004775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004418763646475264 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.003967256583136451 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [25:09<05:52, 176.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2317183439375112 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.15795652164481855 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.028883478862580157 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.031803494456278926 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014356408382929101 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.016470336190917912 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010817289807881316 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011822541531942347 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010204759378404269 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010896596205009915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009712079146887128 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010252514182973434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0089785172850451 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00939456038930289 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008029383644597592 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008375331266399692 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0076184523229004585 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007960544851170304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007490636266761181 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007816489909172934 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007393029600001165 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007705737227190505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0073010509758915775 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007599934031639029 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007207884315578063 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007492135203553035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007108780399073649 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007378084009841961 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006998838481228173 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007254468044266105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006872777620595574 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007118699229870211 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006726281567439301 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006970631554448868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006559063746750246 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006818651961272254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006378759121450929 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006682246122235323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006199963794032071 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006570277934181778 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006041700636074083 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006473994074279771 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005915232767507325 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0063895404968849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005814492722028309 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006315075299319099 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005729502298101021 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006245642907314879 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0056545232744703924 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006175849696292597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005587084867641411 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006101688000318759 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005525980371196693 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006022122811854762 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005470186007127546 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005938494441044682 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005418615183711432 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005852778469595839 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005370313932194653 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005766710019944345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005324652102388387 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005681693074567353 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0052813762632354985 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005599092082668315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005240502872208582 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005520241382970091 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0052021463186204785 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00544616487561999 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005166368280073465 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00537737990346025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005133111242300076 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005313844618606655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0051022042999601495 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005255104535642792 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005073417399382817 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005200483808841775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0050465158285848445 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005149316203588729 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005021275271811135 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005101113217225408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004997490409397976 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005055463105878409 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004974978522611714 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005012026330565705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004953573518653611 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0049705275211154535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004933130358265931 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004930729498429333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004913522008114773 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00489244848380194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004894644608738132 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004855497671729501 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0048764119528907 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004819740705630358 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004858759456623454 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004785075650194331 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004841632927223245 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004751416481346549 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004824985501893463 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004718673175803441 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [28:34<03:05, 185.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.19769808091903038 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.13092892316062182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.027086167976967175 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.029794649796231705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012018089250229761 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013198044959127027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009687577804251418 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010337430199690382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008238998165245365 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008740089635145576 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007676079407008025 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008153153282097158 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007269806017840866 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007665977505145266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006797130234933791 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007256734410903472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0064867546171779285 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007210545118569452 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006265763457640189 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007144140374079785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005995318294846092 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006847597042317776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005852901475779358 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006744060028508744 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005751535368668609 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006680627805454766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00566981767897365 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006628393387312398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005600972552307929 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006581819417667301 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0055409749739558645 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00653841787510935 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005487035869885862 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00649621244519949 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0054371866752753835 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00645313792012851 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0053900086166282744 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006406900157932849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0053444668959935 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006354815764900516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005299742156631198 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006293336871107493 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005255130062476276 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006217342458994073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005209889765964808 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006119369720930562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005163054779234825 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005989321113755817 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005113416967656885 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005817466617479701 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005060381392428284 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0056084580205873966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005005966035188617 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00539846536155571 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004953825229823072 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005223670347785468 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004905575015378205 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005082721346030559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004861046845198738 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004961807386922266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004819777634005015 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004853765377500916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004781448103854582 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004756715689676211 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004745872956607151 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004670539179214221 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004712904319524031 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004595262286088923 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004682372382163906 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004530457540086526 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004654057695272032 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0044752337202868045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.00462770907314117 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004428367750198745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004603063635316501 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004388503319419482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004579882493839994 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004354293430771898 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004557954863582255 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0043245633166995555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004537104522336927 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004298295435386107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004517188123648265 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00427467652651317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004498090948926516 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004253067545291474 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004479720429508832 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004233000841101303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004461997051818473 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004214149608742446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004444854643198417 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004196245295187349 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004428238056669773 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0041791288873783364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0044120976772120245 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004162682463442359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004396385447390677 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004146824940107763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0043810544889012395 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004131463952088619 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [31:58<00:00, 191.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (6996, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1741, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.27272338707629407 // Train Acc: 0.0\n",
      "Val Loss: 0.17556095008145678 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03367755992006358 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02775507511740381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.016640718882801393 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01457369527356191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012084171561881492 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010461340340870347 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009602066365466134 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008324722619727254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008559666714071802 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007202315436337482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007743401267378672 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006293202475221319 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007269435940144507 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005848153951493177 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007010442360525432 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005626308905299414 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006806549885667452 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005468691322444515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0066414448521929246 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005339836032891816 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0065040776046783955 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005235373241488229 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006388174681435233 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005151338578963821 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006291170163318243 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005084162552587011 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006210866543149669 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050312380645085465 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006143116909419463 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004987947909500111 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006083559736579691 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00494926663284952 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006029201176051544 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0049121318892999125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00597833224134222 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004875254872339693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005930050976306506 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004838333270427856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005883895612394089 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004801556332544847 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005839608073315358 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004765318236737088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005796988907729416 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00473000579090281 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005755863648937105 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004695839434862137 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005716105771557211 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00466280621053143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005677651609698115 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046306423292579975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005640468217292163 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004598959427411583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00560451818462121 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00456741422584111 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005569745058227824 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004535857660018585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005536080382181731 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004504352280954746 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0055034570241789225 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004473156255500561 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005471807379343602 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044426245953549034 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005441059032561402 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004413107903250916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0054111280930377375 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004384881962852722 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005381915745427449 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004358151362446899 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005353325706285735 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00433306960939345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005325264716839852 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004309710523706268 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005297653892154052 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004288067956539718 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005270429829998818 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004268043325282633 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005243550697405351 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00424948315855793 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.00521699857119828 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004232190926135941 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005190776974934507 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004215981785885312 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.00516492657849276 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004200753755867481 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.00513950369100737 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004186481631106951 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005114619106375378 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004173054855147546 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005090343727906698 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004160272776657208 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005066708880093918 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004148068947886879 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0050437419258534365 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004136417903514071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005021434780167731 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004125306784937327 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004999787327950421 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004114708456803452 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:47<25:08, 167.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1567900315964739 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.09343612365085971 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03509601045689082 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02968331194398078 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014858061202151114 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012004976435987786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011222131684800976 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008955459694632075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010211914381199436 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008098805162378334 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009476068554037851 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0074988471217114815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008854308057965532 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007008321439339356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008317210098630727 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006554904389618472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008011223387917222 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006260996083305641 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007781283144273511 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0060436744920232075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007554281958523441 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005848881339823658 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007350799063599102 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005699595063924789 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007161167198813363 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005578072352165526 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006956895154190662 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005446499196643179 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006748199368269953 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005314879077063365 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006553524844646113 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005202794468707659 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006376355451097941 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005104679677804763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006226051702976363 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005020516065203331 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0061062830080359015 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004952739682895216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006008241540373869 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004896274619651112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005925195535100554 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048456334339624105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005853426301709406 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004801492475565862 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005789698208427082 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004765522065149112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0057317708772033045 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004736656553789296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005678417288685498 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004712357363578948 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005628982266479227 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004690226269039241 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005583066789892643 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004668787591667338 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005540364786034203 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004647522093728184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005500592185571348 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004626459327780387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005463467936667286 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046058493539352305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005428729608620955 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004585916965945878 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005396166271221777 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004566818143410439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005365619946220149 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004548635875636881 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005336961482245407 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004531421613964168 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.00531007925481174 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004515209688212384 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005284867010219661 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00449999642812393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005261214119626728 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004485736670903862 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00523900535419056 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004472351633012295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005218120628848808 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004459765085696497 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005198419432434334 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004447883060625331 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0051797724408140094 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004436594582247463 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0051620486070142555 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00442580293271352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0051451257002629236 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044153939720920545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005128898184755009 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004405288938009603 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005113271427060133 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004395538494414227 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0050981591123934475 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004386073968965899 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0050834913668435355 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004376843681728298 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.00506920766560616 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00436782593711872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005055256632845594 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004359000838700344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0050415958896636555 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004350345073775811 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [05:34<22:16, 167.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3456444311036503 // Train Acc: 0.0\n",
      "Val Loss: 0.2366648155857216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04394809877899684 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03781111372465437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015812011814110627 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.014682470659979366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012710352454836305 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011507452482526952 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.01163150805854515 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01036921653642573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.010756569498295144 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009515573960644277 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009794722864570173 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008687205434861508 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008851538278315573 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007906290533190424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008027686484978794 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0071682860143482685 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007461698153842938 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0065673613878475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007129686048411717 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006153820277276364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006856554174544843 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005814498544416644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0066279601029303244 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005516005734997717 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006452500205657117 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005274615504524924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006322939948765689 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005098250893537294 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006227496473467377 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004977574453435161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006155790570808668 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00489684118127281 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0060994701425982935 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048410184342752806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006052781318497236 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004799547626382925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00601222949074879 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00476630158637735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005975787662879721 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047380218155343426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005942286370156256 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004713034037161957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005911049967287236 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004690512265502052 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005881681892123567 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004670040785673667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005853940024716805 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004651424375532026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005827662146960695 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004634538968093693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005802725692126543 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004619273280894214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005779032589076725 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004605522275563668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005756493424465157 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00459316564004191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005735023910409375 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004582076602276754 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005714543830905297 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004572125837545503 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005694976665470834 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004563174710016359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005676250770841151 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045551024250347506 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005658295635478443 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004547809985127639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005641049163237418 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004541176301427185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005624448608917494 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045351112600077285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005608438242005703 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004529532830400223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005592968492964325 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004524376289919019 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0055779910632558 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004519577383656393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005563464657335487 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004515085066668689 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005549353083859295 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004510861158963632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005535628414844678 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00450687815900892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005522254066112277 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004503117317587815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005509201456371644 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004499543801119382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0054964416643889565 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004496123756028035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005483951044032886 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004492843426256017 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005471704345139075 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044896794610064135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0054596822209694515 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004486619723452763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005447864748822466 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044836344514888795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0054362330845659215 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004480719739909877 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [08:20<19:27, 166.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2917548299312047 // Train Acc: 0.0\n",
      "Val Loss: 0.18908558562397956 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04740195875332508 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.041125504537062214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.018196062450658648 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.014706010185182094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012052478049201338 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009628003120253031 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010733404337666657 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008571408955718984 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009822480145046598 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007858091779053212 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009176978326591846 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00735781151716682 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00876842463361397 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006999286653643305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008462422258660758 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0066833228761838245 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00814905467939661 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0063596765172075145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 100\n",
      "INFO: Validation loss did not improve in epoch 101\n",
      "Epoch: 101\n",
      "Train Loss: 0.007739933909578291 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00618169899945232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 102\n",
      "INFO: Validation loss did not improve in epoch 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [08:55<11:27, 114.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 104\n",
      "Early stopping after 104 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.1848711857240494 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.1215701395815069 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04150673888249484 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03576792292296886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0155573142410711 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.013311403807760639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011199215319662538 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009355853823944927 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009406843134435957 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007825450501828032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008691771344768657 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007097009891136126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008247488302435964 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006625189111483368 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007889749110170288 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00626218823546713 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007602337341953375 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005968529210341247 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0073835885994238395 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0057451562447981405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007217281668119507 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005593871913680976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007087946082167668 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005501730282875625 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006982911626391589 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005437628895213658 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006892185188302574 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0053801731333475225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00680977808307329 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005324985125017437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006731836118344133 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005271847309036688 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006655668362456302 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005219557542692531 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006579359615387472 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005166149846362797 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006501620958588983 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005109807311303236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0064218403600351925 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005049581453204155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006340179759286106 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004985812120139599 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0062575959345351345 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004920394058254632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0061756261894436865 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004856327226893468 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006095724631117904 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004796197112988342 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006018597441810485 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004740734991024841 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005944254730517666 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004688913574103604 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005872698860629951 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004639516753906553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0058044779899032554 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004592366880652579 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005740557699974575 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004548077962615273 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005681712246823154 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004507034623318098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005628094300904128 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004468995961360633 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005579283357317215 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00443343126680702 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005534559473943935 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004399867984466255 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005493177135568537 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004367982371794907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005454489464773775 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004337530921805988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005417980091565514 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004308286791836673 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.00538324146716511 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004280000294304706 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005349967671775144 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004252467505549165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005317932513614768 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004225501189516349 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005286977118195872 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004198988571509041 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005257001133916313 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004172873124480248 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005227952451364363 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004147153737192804 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.00519981317766453 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004121865357526324 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005172595397949848 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004097080061381513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005146327946807117 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004072869528846984 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005121046291572812 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004049325093034316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00509678119805229 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040265313222665675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005073558218877116 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004004565042189576 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005051386347529585 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0039834770416332915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005030259438981749 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003963296464644372 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [11:42<11:07, 133.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.32289856583832605 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.21120131733742628 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03721921951361171 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.031948881562460556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01612974276774686 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.013075359622863205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011640802995922664 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009125464028594168 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010378423468507486 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0081026059279049 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009595805182138704 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0075245469512248584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009133621112052267 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007170688873156905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008834083086160356 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006902397563681007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008615353143110564 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0066790078766644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008450804323438537 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006498515165664933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00830118342506865 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006338512994856997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.008117484150998737 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00615901355208321 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007844618448804741 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005912222281437028 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007479492787029339 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005675404832105745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.007236811123494926 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0056269683705812154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.007058155253002224 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005604736422273246 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006876802835804857 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005536970233714039 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006694192821627747 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005425329972058535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006520068064038578 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005286775732582265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006372920901952609 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005139484053308313 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006262651751494061 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005013940919359977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0061792938759524935 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004918879774314436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006110730725574439 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004843656752597202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006050233755230938 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004777611783620986 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005994743015298186 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004716313236647032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0059426731863760785 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004659379455684261 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00589321409277703 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046071213458410715 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0058462051472724435 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004559339824217287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005801638032279943 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004515855700116266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0057593274597912076 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004476461800831285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005718977218727713 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004440492472018708 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005680306246886923 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004407081439752471 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005643099532079921 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004375453386455774 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005607211139992934 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004345033523118632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005572543704359193 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004315453996373848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005539041393134556 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004286538949236274 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005506677337237485 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004258254629729146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005475447584660171 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042305993424220515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005445355730722382 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00420366486068815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005416402430592539 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004177551773715426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005388576545078546 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004152370853857561 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005361846441639052 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004128199227323586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0053361675690872315 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004105078344318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005311478661334984 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004083026852458716 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005287708827298662 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004062023938802833 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005264782570578906 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004042021964084018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0052426289378975826 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.004022962121631612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.00522118075548769 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.00400485122280026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005200372671030172 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.003987635824490677 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005180134737256013 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.0039712290999225595 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [14:29<09:39, 144.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.14939253690512214 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.09770362265408039 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03438310340029872 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02949458441951058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014088235821545158 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012518096368082545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010970023894984462 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009263178536837752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009399834213104337 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007772202032025565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00826444589085324 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006629306217655539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007399720836167126 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005833985131572593 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006986930395463762 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005481826196509329 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0066913091200169975 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052857473492622375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006432647554769465 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00512503896501254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006231393342408177 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004993921658024192 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0060834704109437795 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004891342314129526 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005968457863976558 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004805011374198578 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005872924006267634 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00472958299585364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005792225188507643 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004663313468071547 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005724394890122524 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004604353874244473 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005666714085618961 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045502766628157006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005616294533188655 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004499527799304236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00557105226727143 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004451619732108983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005529692758525377 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004406479373574257 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005491422381906953 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004364107316359877 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005455731395121715 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004324463356963613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0054222781845840486 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00428749451583082 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00539082321327724 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00425312911498953 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0053611949291377916 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004221344002607194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005333267860369731 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004192117222754115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005306947467365986 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041654673997651446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005282152356945625 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041414070599289105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005258806504056147 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004119932338256728 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005236826158770824 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004100997066548602 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005216110482828939 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004084507864899933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005196575190208546 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004070314424197105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005178034859475548 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040582861920649355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005160426801244928 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004048156744631177 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005143602851238186 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040397190213711425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005127436056686353 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004032714068042961 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005111816212679494 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004026910101740875 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005096640435183531 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004022081137042154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005081820779067536 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004018045032650909 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005067281010369323 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004014641797932034 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.00505296763173011 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004011746381663463 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005038838431698411 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004009241265752776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005024863455389291 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004007030678489669 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005011026621806822 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004005047086287628 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004997332806839314 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00400320096576417 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004983771370711086 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004001465611244467 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004970351017195068 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003999808087775653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004957084065318091 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003998202756470577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0049439798615294505 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003996646332300522 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004931052238786738 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0039951198628510945 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [17:15<07:35, 151.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.14091808655082363 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.09417679252272303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03336956408980502 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.028979781608689914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014677639518009678 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012906982885165648 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011942387120500548 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009897871239280159 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010415762278025962 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008532567411153155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009392924188980705 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007713258719410409 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00879322539549321 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0071593435354192145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008420984838561891 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006714817149225961 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008093863182390729 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00630139715292237 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007689738994332447 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00589447106573392 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007317872183741874 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005571477788246491 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007118778761484783 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005402725473554297 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006989660025382899 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005330660007894039 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006886670503076421 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052934310568327255 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006794119193154859 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005258719381791624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006700327877193418 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005208855617622083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006593402353919124 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005134023751386187 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006456410505949226 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005024891960519281 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006277801212216791 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004874413845721971 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006113902691721951 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004720139473846013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0060053331619043535 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004606257844716311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0059329679881150255 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004527442699128931 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005879205349645658 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00446987953460352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00583459119276318 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004424381239170378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005794848608848167 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00438587734983726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005758197307820564 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043518255422399805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005723932139218262 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004320991873233156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005691765924515925 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004292913831093094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005661553745243069 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00426729406145486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0056331807202085445 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004243924540721557 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005606552698653719 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004222658170725812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005581527081377736 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004203386270356449 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005557963128103853 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004186000251634555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005535708865274135 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004170390811156143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005514617082790459 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004156453162431717 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00549454532630665 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004144077828492631 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0054753722806916184 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004133140786804936 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005456991243860833 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004123521392995661 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005439314336598889 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041151062458414924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005422266682590779 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004107776870527728 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005405788148804377 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004101438413966786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0053898300608773144 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004095984341322698 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005374353015678403 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004091323067603464 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005359324075113641 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040873721771111544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005344717895461153 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004084056886759671 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0053305116412442465 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004081295053897933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005316687447761002 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00407902349463918 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.00530322529104449 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004077184661714868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.00529010318560934 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004075699439272285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005277298606419723 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004074508965608071 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [20:02<05:13, 156.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.20762240987049935 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.14240479325367647 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03604315476891793 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.030454201911660757 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015744474971641418 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012907876781272616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011567055471517894 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009182979517870329 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00959210871712445 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007676245373758403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008577480469405072 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006938340324400501 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008017323390244756 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0064538657792251215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007610894359018841 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006085195024074478 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007349006935303444 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005859951484440402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007163521251109637 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005714496216652068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006999997474193641 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005576856514777649 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006846183225231832 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005438291450793093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006699100150046317 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005298482796007937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0065568199835198855 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005154028433290395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0064237378266550705 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050075782420621676 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00630640896190315 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004868089424615556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006207708185368545 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004746055183932185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006126305601372464 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004648110960525545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0060586992573613925 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004574681979350069 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006000827685355732 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004521450709382241 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0059492486294095304 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004482206947762858 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005901559142259771 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004451339721510356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005856251276042908 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004424846726893024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005812390450150046 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004400266175666316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005769385926991054 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004376185067336668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005726864092056236 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004351873467253013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005684589672106484 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004327001587741754 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0056424279352777626 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00430147449104962 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005600323416845508 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004275362443348223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005558295425805837 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004248820541595871 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0055164179459562925 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042220414975996726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005474811155650952 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004195247510109436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0054336239633758525 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004168682236393744 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005393009338863874 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004142605233937502 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005353102276046425 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041172286664897745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005314010034897896 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004092725044624372 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005275809389462516 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004069222581826828 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005238546121793216 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004046806978823787 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0052022460723704055 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004025532535954632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005166926081547488 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004005445779131895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005132591425755899 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003986538470384072 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0050992327965540835 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003968773528256199 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005066828108520234 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003952087823894213 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005035342917603241 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003936418109912087 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005004738713189382 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003921708236025138 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0049749873796494926 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00390793620608747 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004946061542372487 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038950698005712843 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004917943287485589 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038830988701771606 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004890632355579316 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038720138138160108 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.00486413771648079 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003861815721558576 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [22:48<02:39, 159.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.33517186743415656 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.21719968115741556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03878067284567307 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03341870456933975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015872099405988098 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.014613095958801833 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012253026110672957 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010936901455914432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010233410481537893 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009159349811009385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009305947684028003 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008301508634097197 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00883305303860864 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007760562722317197 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00852090023568572 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007353234066712585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008292216026185923 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007015640678053553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008112407864079935 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0067292706702243195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007952511759228223 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006479336782781915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007763047298080539 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006218497929247943 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007404320132833603 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005913258047605102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007128126141572686 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0057311319957741285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00697057771913231 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00559715913947333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006845370955131607 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005502688778902996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006736888827173439 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005420346401462501 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00663729602062743 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005338257652792063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006542428435447316 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00525251737033779 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006453727054639324 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005167029611766338 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006374602485129176 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050874786493791775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006306470301083064 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005016631599177014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00624846194054268 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004955092432316054 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006198690496129941 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0049024692482569 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00615509212990908 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004857612625611099 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006115796443683066 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048188381108709355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.006079252156509792 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004784343867901374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.006044230983618086 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00475252117453651 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.006009767373139251 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004721986413510008 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005975125045473071 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004691561620513146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005939797884210536 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004660363838245923 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0059035312061168285 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004627894178371538 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005866308190506887 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004594074280678548 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005828310589340077 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004559132596477866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0057898593022483805 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004523466427979821 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005751310477190604 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004487480760805986 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005712970712521968 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00445156269071793 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0056750503383028285 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004416093634526161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005637654425474267 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043813814260912215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005600818645232463 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004347689394754442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005564535702217251 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043152266563001 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005528779374010283 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004284123483706604 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005493520977789936 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004254408383911306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005458734159306814 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00422601297032088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.00542439847225621 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004198805814270269 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005390507557384721 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00417262923438102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0053570710208169165 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004147323618896983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005324109633916725 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004122748321176253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005291654965436282 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040988184363496575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0052597435761655605 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004075528276999566 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [25:35<00:00, 153.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.32670760590191844 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.2363598815419457 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04325915116935832 // Train Acc: 0.0\n",
      "Val Loss: 0.04214474497870965 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.02828059490072673 // Train Acc: 0.0\n",
      "Val Loss: 0.027621111730960282 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.023842226947789594 // Train Acc: 0.0\n",
      "Val Loss: 0.0231267933818427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.02040040548295463 // Train Acc: 0.0\n",
      "Val Loss: 0.019937304940752007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.018659351780568354 // Train Acc: 0.0\n",
      "Val Loss: 0.01862321247939359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.01772323167206694 // Train Acc: 0.0\n",
      "Val Loss: 0.01784754696048119 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.016626585306829377 // Train Acc: 0.0\n",
      "Val Loss: 0.016951328075744888 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.015721284293199647 // Train Acc: 0.0\n",
      "Val Loss: 0.01627093782987107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.015218332067523236 // Train Acc: 0.0\n",
      "Val Loss: 0.015856857665560463 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.01488872364027315 // Train Acc: 0.0\n",
      "Val Loss: 0.015562380299988118 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.014632334318182104 // Train Acc: 0.0\n",
      "Val Loss: 0.01532430982386524 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.01441213213009377 // Train Acc: 0.0\n",
      "Val Loss: 0.015116268438710407 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.014210937410746126 // Train Acc: 0.0\n",
      "Val Loss: 0.014925648593767122 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.014019862450999484 // Train Acc: 0.0\n",
      "Val Loss: 0.014746107753704895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.013834297004236615 // Train Acc: 0.0\n",
      "Val Loss: 0.014574198687279766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.013653055811513505 // Train Acc: 0.0\n",
      "Val Loss: 0.014408232601867481 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.013478218956284871 // Train Acc: 0.0\n",
      "Val Loss: 0.014247972890734673 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.013313267581998349 // Train Acc: 0.0\n",
      "Val Loss: 0.014093933059749278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.013159959991653897 // Train Acc: 0.0\n",
      "Val Loss: 0.013946297053586352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.013017351955991902 // Train Acc: 0.0\n",
      "Val Loss: 0.013804413310506128 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.012883085893577636 // Train Acc: 0.0\n",
      "Val Loss: 0.01366686782884327 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.012754677322833505 // Train Acc: 0.0\n",
      "Val Loss: 0.013531989087773994 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.012630158754652494 // Train Acc: 0.0\n",
      "Val Loss: 0.013398479035293514 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.01250832871912413 // Train Acc: 0.0\n",
      "Val Loss: 0.013265891999683596 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.012388852820412754 // Train Acc: 0.0\n",
      "Val Loss: 0.01313482356173071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.012272222886142665 // Train Acc: 0.0\n",
      "Val Loss: 0.013006802800704133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.012159472609950936 // Train Acc: 0.0\n",
      "Val Loss: 0.01288386480036107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.012051627473676042 // Train Acc: 0.0\n",
      "Val Loss: 0.012767809299244122 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.011949145420109981 // Train Acc: 0.0\n",
      "Val Loss: 0.01265957717021758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.011851723316512559 // Train Acc: 0.0\n",
      "Val Loss: 0.012558987381106073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.011758553682822206 // Train Acc: 0.0\n",
      "Val Loss: 0.012465101446617733 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.011668659610563218 // Train Acc: 0.0\n",
      "Val Loss: 0.01237671815536239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.011581117695529167 // Train Acc: 0.0\n",
      "Val Loss: 0.012292717752808874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.01149514411920554 // Train Acc: 0.0\n",
      "Val Loss: 0.012212188804352824 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.011410113188042624 // Train Acc: 0.0\n",
      "Val Loss: 0.01213441960174929 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.011325546806108325 // Train Acc: 0.0\n",
      "Val Loss: 0.012058880925178529 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.011241109746180849 // Train Acc: 0.0\n",
      "Val Loss: 0.011985200245610693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.011156612029086509 // Train Acc: 0.0\n",
      "Val Loss: 0.011913158151913773 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.011072012484201218 // Train Acc: 0.0\n",
      "Val Loss: 0.011842680917206135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.010987417835138364 // Train Acc: 0.0\n",
      "Val Loss: 0.01177383056757125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0109030884290837 // Train Acc: 0.0\n",
      "Val Loss: 0.011706806837835095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.01081941212680548 // Train Acc: 0.0\n",
      "Val Loss: 0.01164190900427374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.010736899252674759 // Train Acc: 0.0\n",
      "Val Loss: 0.011579512364485046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.010656127559454881 // Train Acc: 0.0\n",
      "Val Loss: 0.011519993028857492 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.010577691994292756 // Train Acc: 0.0\n",
      "Val Loss: 0.011463620124215429 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.010502118733179964 // Train Acc: 0.0\n",
      "Val Loss: 0.011410456497899511 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.010429777860956172 // Train Acc: 0.0\n",
      "Val Loss: 0.011360302829945629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.010360831506422733 // Train Acc: 0.0\n",
      "Val Loss: 0.011312723401087252 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.010295225529544498 // Train Acc: 0.0\n",
      "Val Loss: 0.011267128201540222 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:48<25:19, 168.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.17288598894528603 // Train Acc: 0.0\n",
      "Val Loss: 0.11798332252285697 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.037185868920256555 // Train Acc: 0.0\n",
      "Val Loss: 0.035885822942311116 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.027017165753633193 // Train Acc: 0.0\n",
      "Val Loss: 0.02637242559682239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.024115233250455632 // Train Acc: 0.0\n",
      "Val Loss: 0.02343853529204022 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.02103155695080553 // Train Acc: 0.0\n",
      "Val Loss: 0.02056284774602814 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.01898471859483618 // Train Acc: 0.0\n",
      "Val Loss: 0.018937837180088866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.018296845574102036 // Train Acc: 0.0\n",
      "Val Loss: 0.018457526400346647 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.017909579236298528 // Train Acc: 0.0\n",
      "Val Loss: 0.01812220969322053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.017470318069213737 // Train Acc: 0.0\n",
      "Val Loss: 0.017699247564781796 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.01684427063393334 // Train Acc: 0.0\n",
      "Val Loss: 0.017117042339999566 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.016079132815134036 // Train Acc: 0.0\n",
      "Val Loss: 0.016453266109932554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.015451212872891393 // Train Acc: 0.0\n",
      "Val Loss: 0.015907812922854314 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.014991628563193153 // Train Acc: 0.0\n",
      "Val Loss: 0.01546652087603103 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.014615910763524847 // Train Acc: 0.0\n",
      "Val Loss: 0.01509119499982758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.014285718635166889 // Train Acc: 0.0\n",
      "Val Loss: 0.014758609497750347 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.013983180512542322 // Train Acc: 0.0\n",
      "Val Loss: 0.014450945222580974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.013703856362055426 // Train Acc: 0.0\n",
      "Val Loss: 0.014160420940342274 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.013449557920807301 // Train Acc: 0.0\n",
      "Val Loss: 0.013888269061730667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.01322129158277626 // Train Acc: 0.0\n",
      "Val Loss: 0.0136396374553442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.01301719755419109 // Train Acc: 0.0\n",
      "Val Loss: 0.013418474387038838 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.01283376696881026 // Train Acc: 0.0\n",
      "Val Loss: 0.01322462677278302 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.012667610686374446 // Train Acc: 0.0\n",
      "Val Loss: 0.013054622616618872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.012516353787539621 // Train Acc: 0.0\n",
      "Val Loss: 0.012904351204633712 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.012378244833025622 // Train Acc: 0.0\n",
      "Val Loss: 0.01277027962390672 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.012251473861197903 // Train Acc: 0.0\n",
      "Val Loss: 0.012649270358749411 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.01213401179446001 // Train Acc: 0.0\n",
      "Val Loss: 0.012538440220735289 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.012023883374178247 // Train Acc: 0.0\n",
      "Val Loss: 0.012435332948172634 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.011919442596143647 // Train Acc: 0.0\n",
      "Val Loss: 0.012338106071745807 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.011819460059273732 // Train Acc: 0.0\n",
      "Val Loss: 0.012245533098890022 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.011723097845418676 // Train Acc: 0.0\n",
      "Val Loss: 0.012156900771978226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.011629797173791553 // Train Acc: 0.0\n",
      "Val Loss: 0.012071868184615265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.011539202236642713 // Train Acc: 0.0\n",
      "Val Loss: 0.011990331443534657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0114510795866994 // Train Acc: 0.0\n",
      "Val Loss: 0.011912313459271734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.011365273982269443 // Train Acc: 0.0\n",
      "Val Loss: 0.01183789318258112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.011281680940867287 // Train Acc: 0.0\n",
      "Val Loss: 0.0117671386169439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.011200225775950848 // Train Acc: 0.0\n",
      "Val Loss: 0.011700076432051983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.011120852333872014 // Train Acc: 0.0\n",
      "Val Loss: 0.011636679560284724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.011043530990918204 // Train Acc: 0.0\n",
      "Val Loss: 0.011576829215681012 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.010968246724728715 // Train Acc: 0.0\n",
      "Val Loss: 0.011520346533507109 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.010895001553169125 // Train Acc: 0.0\n",
      "Val Loss: 0.011466966958885843 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.010823806194954266 // Train Acc: 0.0\n",
      "Val Loss: 0.011416367094286464 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.010754664485942419 // Train Acc: 0.0\n",
      "Val Loss: 0.011368187694725666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.01068756675675137 // Train Acc: 0.0\n",
      "Val Loss: 0.011322040220891888 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.010622473931169673 // Train Acc: 0.0\n",
      "Val Loss: 0.011277554446662013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.010559316469381934 // Train Acc: 0.0\n",
      "Val Loss: 0.01123438643460924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.010498005192999017 // Train Acc: 0.0\n",
      "Val Loss: 0.011192243334583261 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.010438434199162135 // Train Acc: 0.0\n",
      "Val Loss: 0.011150899876586415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.010380492801884333 // Train Acc: 0.0\n",
      "Val Loss: 0.011110177356749773 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.010324075613936333 // Train Acc: 0.0\n",
      "Val Loss: 0.011069957454773512 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.010269077496866794 // Train Acc: 0.0\n",
      "Val Loss: 0.011030166291377761 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [05:34<22:16, 167.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.13210575786083256 // Train Acc: 0.0\n",
      "Val Loss: 0.10499191433191299 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.036780479623506605 // Train Acc: 0.0\n",
      "Val Loss: 0.035446928238326854 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.026461269655456282 // Train Acc: 0.0\n",
      "Val Loss: 0.02583132264288989 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.02368920901152369 // Train Acc: 0.0\n",
      "Val Loss: 0.02295845567502759 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0206566292656475 // Train Acc: 0.0\n",
      "Val Loss: 0.020098753519017586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.018912611339989727 // Train Acc: 0.0\n",
      "Val Loss: 0.018937982584942472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.018419245517383154 // Train Acc: 0.0\n",
      "Val Loss: 0.01859298689629544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.017989377616628256 // Train Acc: 0.0\n",
      "Val Loss: 0.018202369973402133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.01752604107206611 // Train Acc: 0.0\n",
      "Val Loss: 0.01775010862303051 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.017041707022294197 // Train Acc: 0.0\n",
      "Val Loss: 0.017268395745618777 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.01653746210600007 // Train Acc: 0.0\n",
      "Val Loss: 0.016774099379439245 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.01602570372569847 // Train Acc: 0.0\n",
      "Val Loss: 0.01628803534602577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.015548257622867823 // Train Acc: 0.0\n",
      "Val Loss: 0.015844602341001685 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.015109213775015312 // Train Acc: 0.0\n",
      "Val Loss: 0.01543468235229904 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.014706905933360668 // Train Acc: 0.0\n",
      "Val Loss: 0.015057876316661185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.014342447238884969 // Train Acc: 0.0\n",
      "Val Loss: 0.014717433652417226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.014014447190837092 // Train Acc: 0.0\n",
      "Val Loss: 0.014414210414344614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.013720470344537198 // Train Acc: 0.0\n",
      "Val Loss: 0.01414870403029702 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.013456851099383885 // Train Acc: 0.0\n",
      "Val Loss: 0.013917837622152134 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.013219924877529547 // Train Acc: 0.0\n",
      "Val Loss: 0.01371525406668132 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.013007106774031572 // Train Acc: 0.0\n",
      "Val Loss: 0.013533751378682526 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.012816202314357915 // Train Acc: 0.0\n",
      "Val Loss: 0.013366843954744663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.012644323133374458 // Train Acc: 0.0\n",
      "Val Loss: 0.013209488335996866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.012487960218124466 // Train Acc: 0.0\n",
      "Val Loss: 0.01305863715877587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.01234394575567006 // Train Acc: 0.0\n",
      "Val Loss: 0.012913459581746296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.012210061848078553 // Train Acc: 0.0\n",
      "Val Loss: 0.01277485845441168 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.012084838128372279 // Train Acc: 0.0\n",
      "Val Loss: 0.012644343289800665 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.011967061595676561 // Train Acc: 0.0\n",
      "Val Loss: 0.012522951327264309 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.011855506180832375 // Train Acc: 0.0\n",
      "Val Loss: 0.012410800091244957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.011749000805238746 // Train Acc: 0.0\n",
      "Val Loss: 0.01230723581022837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.011646605465671719 // Train Acc: 0.0\n",
      "Val Loss: 0.012211261291734197 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.011547730776340994 // Train Acc: 0.0\n",
      "Val Loss: 0.012121833319013769 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.011452173166716956 // Train Acc: 0.0\n",
      "Val Loss: 0.012038011493330652 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.011360049051058278 // Train Acc: 0.0\n",
      "Val Loss: 0.011958962077782912 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.011271663023941388 // Train Acc: 0.0\n",
      "Val Loss: 0.011883942859077996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.01118734156389691 // Train Acc: 0.0\n",
      "Val Loss: 0.011812309869988397 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.011107290808970457 // Train Acc: 0.0\n",
      "Val Loss: 0.011743543940511617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.011031528513827552 // Train Acc: 0.0\n",
      "Val Loss: 0.01167726202613928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.01095987975780244 // Train Acc: 0.0\n",
      "Val Loss: 0.011613223159855062 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.010892036929088708 // Train Acc: 0.0\n",
      "Val Loss: 0.011551306193525141 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.010827619713182524 // Train Acc: 0.0\n",
      "Val Loss: 0.01149143497882919 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.010766224477431613 // Train Acc: 0.0\n",
      "Val Loss: 0.011433576242151586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.010707465550101035 // Train Acc: 0.0\n",
      "Val Loss: 0.011377688819034533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.01065099060633241 // Train Acc: 0.0\n",
      "Val Loss: 0.011323713993822987 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.01059649414999504 // Train Acc: 0.0\n",
      "Val Loss: 0.011271580113944682 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.01054371825257767 // Train Acc: 0.0\n",
      "Val Loss: 0.011221187231554226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.010492451858949172 // Train Acc: 0.0\n",
      "Val Loss: 0.01117244383150881 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.010442527826475702 // Train Acc: 0.0\n",
      "Val Loss: 0.011125232533297755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.010393814754685106 // Train Acc: 0.0\n",
      "Val Loss: 0.011079447496343743 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.01034621469098139 // Train Acc: 0.0\n",
      "Val Loss: 0.011034964880144054 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [08:20<19:25, 166.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.11903889055393602 // Train Acc: 0.0\n",
      "Val Loss: 0.0910156327215108 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.033252080974751685 // Train Acc: 0.0\n",
      "Val Loss: 0.032326810773123396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.025959153865893547 // Train Acc: 0.0\n",
      "Val Loss: 0.025292859802191908 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0219745702539745 // Train Acc: 0.0\n",
      "Val Loss: 0.021457441862333903 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.019449948996611652 // Train Acc: 0.0\n",
      "Val Loss: 0.019440184093334457 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.018516694950407635 // Train Acc: 0.0\n",
      "Val Loss: 0.018752033669840205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.01797993207716098 // Train Acc: 0.0\n",
      "Val Loss: 0.018287601474333892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.017523710798795365 // Train Acc: 0.0\n",
      "Val Loss: 0.01785814878107472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.017077258832701673 // Train Acc: 0.0\n",
      "Val Loss: 0.017434411478990858 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.016648539646048115 // Train Acc: 0.0\n",
      "Val Loss: 0.01702551201663234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.016245460046293663 // Train Acc: 0.0\n",
      "Val Loss: 0.016628739440982993 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0158691794960228 // Train Acc: 0.0\n",
      "Val Loss: 0.016242529680444436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.015515332401530248 // Train Acc: 0.0\n",
      "Val Loss: 0.015864927770400588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.015168476262933587 // Train Acc: 0.0\n",
      "Val Loss: 0.015484511928463524 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.014809333053778975 // Train Acc: 0.0\n",
      "Val Loss: 0.015088893202218142 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.014429566361112138 // Train Acc: 0.0\n",
      "Val Loss: 0.014679749674079094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.014043028889538626 // Train Acc: 0.0\n",
      "Val Loss: 0.0142776671289043 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.013679815668901897 // Train Acc: 0.0\n",
      "Val Loss: 0.013911826925521547 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.013364201851750482 // Train Acc: 0.0\n",
      "Val Loss: 0.01360164632682096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.013101904803980567 // Train Acc: 0.0\n",
      "Val Loss: 0.01334823558784344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.012885007101481091 // Train Acc: 0.0\n",
      "Val Loss: 0.013141007602892139 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.012701493617759584 // Train Acc: 0.0\n",
      "Val Loss: 0.012967094592750073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.012540870888337288 // Train Acc: 0.0\n",
      "Val Loss: 0.012816161975603212 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.012395756795458053 // Train Acc: 0.0\n",
      "Val Loss: 0.012681290490383452 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.012261472750863391 // Train Acc: 0.0\n",
      "Val Loss: 0.012558150198310613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.012135181493087583 // Train Acc: 0.0\n",
      "Val Loss: 0.012443956546485424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.012015155654774817 // Train Acc: 0.0\n",
      "Val Loss: 0.012336694533852013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.011900289051158175 // Train Acc: 0.0\n",
      "Val Loss: 0.012234738155860793 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.011789809303256134 // Train Acc: 0.0\n",
      "Val Loss: 0.012136710113422437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.011683142767394105 // Train Acc: 0.0\n",
      "Val Loss: 0.012041515078056942 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.01157986314235825 // Train Acc: 0.0\n",
      "Val Loss: 0.011948378942906856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.011479678544636867 // Train Acc: 0.0\n",
      "Val Loss: 0.01185689823363315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.011382434027031336 // Train Acc: 0.0\n",
      "Val Loss: 0.01176701124588197 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.011288085678107527 // Train Acc: 0.0\n",
      "Val Loss: 0.011678910636427727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.011196669686551643 // Train Acc: 0.0\n",
      "Val Loss: 0.011592947928742929 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.011108257043383777 // Train Acc: 0.0\n",
      "Val Loss: 0.011509519392116504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.011022913845927884 // Train Acc: 0.0\n",
      "Val Loss: 0.011428966508670287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.010940683973723487 // Train Acc: 0.0\n",
      "Val Loss: 0.011351546289568597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.010861569661519502 // Train Acc: 0.0\n",
      "Val Loss: 0.011277385631745513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.010785535456332034 // Train Acc: 0.0\n",
      "Val Loss: 0.011206500181420283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.010712505512781544 // Train Acc: 0.0\n",
      "Val Loss: 0.011138787103647536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.010642371624155336 // Train Acc: 0.0\n",
      "Val Loss: 0.011074060658839617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.010574998659375187 // Train Acc: 0.0\n",
      "Val Loss: 0.011012063979763877 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.010510234698040844 // Train Acc: 0.0\n",
      "Val Loss: 0.010952530699697408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.01044792328327554 // Train Acc: 0.0\n",
      "Val Loss: 0.010895190557295625 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.010387907936939434 // Train Acc: 0.0\n",
      "Val Loss: 0.010839814345606349 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.010330047246842692 // Train Acc: 0.0\n",
      "Val Loss: 0.010786233520643278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.010274218870582033 // Train Acc: 0.0\n",
      "Val Loss: 0.010734331895681945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.010220322010562367 // Train Acc: 0.0\n",
      "Val Loss: 0.010684063768183644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.010168273540928143 // Train Acc: 0.0\n",
      "Val Loss: 0.010635419934988021 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [11:06<16:38, 166.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.13152733836408076 // Train Acc: 0.0\n",
      "Val Loss: 0.10004269934513352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03556404794783217 // Train Acc: 0.0\n",
      "Val Loss: 0.03416333875872872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.025269245779881738 // Train Acc: 0.0\n",
      "Val Loss: 0.02473779588260434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.022467132736439712 // Train Acc: 0.0\n",
      "Val Loss: 0.02193467415530573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.020314095754465557 // Train Acc: 0.0\n",
      "Val Loss: 0.019969506019895725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.01902607784576748 // Train Acc: 0.0\n",
      "Val Loss: 0.01893368223682046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0182939998010284 // Train Acc: 0.0\n",
      "Val Loss: 0.018405064847320317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.017796635240758665 // Train Acc: 0.0\n",
      "Val Loss: 0.018038220381872222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.01738200579519005 // Train Acc: 0.0\n",
      "Val Loss: 0.017699043977666983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.01701041285935194 // Train Acc: 0.0\n",
      "Val Loss: 0.017366150478747757 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.016669379147492586 // Train Acc: 0.0\n",
      "Val Loss: 0.017036513992669908 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.01634865659650534 // Train Acc: 0.0\n",
      "Val Loss: 0.016706024762243032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.016043827503273204 // Train Acc: 0.0\n",
      "Val Loss: 0.016375800577754323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.015755045750778015 // Train Acc: 0.0\n",
      "Val Loss: 0.016052006735381756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.015483094213453874 // Train Acc: 0.0\n",
      "Val Loss: 0.015744470153003932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.015230520286057229 // Train Acc: 0.0\n",
      "Val Loss: 0.015467025703665885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.015001437728841827 // Train Acc: 0.0\n",
      "Val Loss: 0.015228850462219931 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.014794745786566305 // Train Acc: 0.0\n",
      "Val Loss: 0.01502593362026594 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.014605100183709436 // Train Acc: 0.0\n",
      "Val Loss: 0.014848671794276345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.014427379964421328 // Train Acc: 0.0\n",
      "Val Loss: 0.01468870300291614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.01425754720106993 // Train Acc: 0.0\n",
      "Val Loss: 0.014539781102741307 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.014092310459340273 // Train Acc: 0.0\n",
      "Val Loss: 0.014396974605254152 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.013929194998268259 // Train Acc: 0.0\n",
      "Val Loss: 0.014256112015044148 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.013767273220569577 // Train Acc: 0.0\n",
      "Val Loss: 0.01411397098140283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0136078401813156 // Train Acc: 0.0\n",
      "Val Loss: 0.013969419109211727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.013453859040624202 // Train Acc: 0.0\n",
      "Val Loss: 0.01382464502345432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.013308065175823017 // Train Acc: 0.0\n",
      "Val Loss: 0.01368410706689412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.01317149635242749 // Train Acc: 0.0\n",
      "Val Loss: 0.01355162240056829 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.01304365378978862 // Train Acc: 0.0\n",
      "Val Loss: 0.013428674985400655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.012923407164236454 // Train Acc: 0.0\n",
      "Val Loss: 0.013314817591824315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.012809591412493219 // Train Acc: 0.0\n",
      "Val Loss: 0.013208755029534752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.012701184648305995 // Train Acc: 0.0\n",
      "Val Loss: 0.013109022810716521 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.01259731794983897 // Train Acc: 0.0\n",
      "Val Loss: 0.013014309125190431 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.012497245614761359 // Train Acc: 0.0\n",
      "Val Loss: 0.012923534883355552 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.01240035115856014 // Train Acc: 0.0\n",
      "Val Loss: 0.012835842303254387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.012306138503122819 // Train Acc: 0.0\n",
      "Val Loss: 0.012750602098689837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.01221423780494561 // Train Acc: 0.0\n",
      "Val Loss: 0.012667334960265593 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.012124387168891082 // Train Acc: 0.0\n",
      "Val Loss: 0.012585720580748536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.012036427749176275 // Train Acc: 0.0\n",
      "Val Loss: 0.012505528169938109 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.011950285903275966 // Train Acc: 0.0\n",
      "Val Loss: 0.012426638179882005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.01186597657487749 // Train Acc: 0.0\n",
      "Val Loss: 0.012349083638665352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.011783599583474604 // Train Acc: 0.0\n",
      "Val Loss: 0.012273064907640218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.011703340561005826 // Train Acc: 0.0\n",
      "Val Loss: 0.012198972278697925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.011625438358753784 // Train Acc: 0.0\n",
      "Val Loss: 0.012127346536991272 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.011550138664078903 // Train Acc: 0.0\n",
      "Val Loss: 0.012058733285150745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.011477639491469961 // Train Acc: 0.0\n",
      "Val Loss: 0.01199352875013243 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.01140805741678498 // Train Acc: 0.0\n",
      "Val Loss: 0.011931904244490645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.01134141338202405 // Train Acc: 0.0\n",
      "Val Loss: 0.011873815759000454 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.011277654489978604 // Train Acc: 0.0\n",
      "Val Loss: 0.011819080627438697 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.011216659782471381 // Train Acc: 0.0\n",
      "Val Loss: 0.011767442448234017 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [13:52<13:50, 166.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.18390936605190034 // Train Acc: 0.0\n",
      "Val Loss: 0.13825847384604542 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.039317933451299254 // Train Acc: 0.0\n",
      "Val Loss: 0.037840592454780234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.026513347874255213 // Train Acc: 0.0\n",
      "Val Loss: 0.026002757539126006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.024335235127643365 // Train Acc: 0.0\n",
      "Val Loss: 0.02376869684931907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.022744584014239514 // Train Acc: 0.0\n",
      "Val Loss: 0.02220104416324334 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.021338369012661448 // Train Acc: 0.0\n",
      "Val Loss: 0.02086347205225717 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.02020412656908098 // Train Acc: 0.0\n",
      "Val Loss: 0.01978149108419364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.019136048315441637 // Train Acc: 0.0\n",
      "Val Loss: 0.018786952407522636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.017983025280103836 // Train Acc: 0.0\n",
      "Val Loss: 0.017791789651594378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0169098991475421 // Train Acc: 0.0\n",
      "Val Loss: 0.016935717551545663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.016081571075044675 // Train Acc: 0.0\n",
      "Val Loss: 0.01628075483339754 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.015479977123575395 // Train Acc: 0.0\n",
      "Val Loss: 0.015799533448774706 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.015024883772531465 // Train Acc: 0.0\n",
      "Val Loss: 0.015426926297897643 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.014648642871511854 // Train Acc: 0.0\n",
      "Val Loss: 0.01511020568961447 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.014321340165062719 // Train Acc: 0.0\n",
      "Val Loss: 0.014834599383175374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.014031944924021421 // Train Acc: 0.0\n",
      "Val Loss: 0.014595680243589662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.013773910422728622 // Train Acc: 0.0\n",
      "Val Loss: 0.014388142636215144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.013541654148883267 // Train Acc: 0.0\n",
      "Val Loss: 0.014205580179325559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.013330475263484673 // Train Acc: 0.0\n",
      "Val Loss: 0.01404170220379125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.01313675222514393 // Train Acc: 0.0\n",
      "Val Loss: 0.013891263771802187 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.012957933267416883 // Train Acc: 0.0\n",
      "Val Loss: 0.0137504053640772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.012792370611691176 // Train Acc: 0.0\n",
      "Val Loss: 0.013616553677076643 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.012638995017788318 // Train Acc: 0.0\n",
      "Val Loss: 0.013488153706897389 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.012496878354720874 // Train Acc: 0.0\n",
      "Val Loss: 0.013364320079034025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.012364860811189973 // Train Acc: 0.0\n",
      "Val Loss: 0.013244553579186852 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0122414686308842 // Train Acc: 0.0\n",
      "Val Loss: 0.013128601556474512 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.01212511840807997 // Train Acc: 0.0\n",
      "Val Loss: 0.013016403652727604 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.012014394866294104 // Train Acc: 0.0\n",
      "Val Loss: 0.01290807541121136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.011908181413548859 // Train Acc: 0.0\n",
      "Val Loss: 0.012803809920495207 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.011805642653494802 // Train Acc: 0.0\n",
      "Val Loss: 0.012703788348219612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.011706088673850716 // Train Acc: 0.0\n",
      "Val Loss: 0.012608092803169381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.01160887089136937 // Train Acc: 0.0\n",
      "Val Loss: 0.01251667645675215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.011513323010664144 // Train Acc: 0.0\n",
      "Val Loss: 0.012429401346228339 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.011418759027606548 // Train Acc: 0.0\n",
      "Val Loss: 0.012346103846688163 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.011324543749369238 // Train Acc: 0.0\n",
      "Val Loss: 0.012266667424277827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.011230217225926176 // Train Acc: 0.0\n",
      "Val Loss: 0.012191093247383832 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.011135658414047806 // Train Acc: 0.0\n",
      "Val Loss: 0.012119499543173746 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.011041198872012771 // Train Acc: 0.0\n",
      "Val Loss: 0.012052100397307764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.010947577579831492 // Train Acc: 0.0\n",
      "Val Loss: 0.011989128606563265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.01085569797462218 // Train Acc: 0.0\n",
      "Val Loss: 0.01193074013360522 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.010766332376892913 // Train Acc: 0.0\n",
      "Val Loss: 0.01187687705016949 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.010679892093138812 // Train Acc: 0.0\n",
      "Val Loss: 0.011827192938124592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.010596387389171274 // Train Acc: 0.0\n",
      "Val Loss: 0.011781064268540252 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.010515505425657516 // Train Acc: 0.0\n",
      "Val Loss: 0.011737700103020126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.010436775066069067 // Train Acc: 0.0\n",
      "Val Loss: 0.011696283688599414 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.010359695355607766 // Train Acc: 0.0\n",
      "Val Loss: 0.01165609003298662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.010283839431714658 // Train Acc: 0.0\n",
      "Val Loss: 0.01161650482734496 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.01020888726707531 // Train Acc: 0.0\n",
      "Val Loss: 0.011577060784805904 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.010134634605426018 // Train Acc: 0.0\n",
      "Val Loss: 0.011537388686768034 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.010060993801445177 // Train Acc: 0.0\n",
      "Val Loss: 0.011497206054627895 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [16:38<11:04, 166.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.4314349252883702 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.34532473927194424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03914589206922 // Train Acc: 0.0\n",
      "Val Loss: 0.03806818318976597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.026596376629860977 // Train Acc: 0.0\n",
      "Val Loss: 0.026118547994304787 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.024006240142160625 // Train Acc: 0.0\n",
      "Val Loss: 0.023501371626149525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.021840331102104763 // Train Acc: 0.0\n",
      "Val Loss: 0.021396936916492203 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.01981177267546183 // Train Acc: 0.0\n",
      "Val Loss: 0.019637333406982097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.01846429622268568 // Train Acc: 0.0\n",
      "Val Loss: 0.018547360065647146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.01773087162158737 // Train Acc: 0.0\n",
      "Val Loss: 0.01797073557126251 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.017316928603282395 // Train Acc: 0.0\n",
      "Val Loss: 0.017616105926307767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.01700156917327751 // Train Acc: 0.0\n",
      "Val Loss: 0.01731770117343827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.01670582085575687 // Train Acc: 0.0\n",
      "Val Loss: 0.017026924138719386 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.016406234502622255 // Train Acc: 0.0\n",
      "Val Loss: 0.01673082851550796 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.016101650306827402 // Train Acc: 0.0\n",
      "Val Loss: 0.016432474883781238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.01580078372110923 // Train Acc: 0.0\n",
      "Val Loss: 0.016142989229410886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.015511443036859302 // Train Acc: 0.0\n",
      "Val Loss: 0.01587065284733068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.015233435385252379 // Train Acc: 0.0\n",
      "Val Loss: 0.015614002824506977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.014959481417214216 // Train Acc: 0.0\n",
      "Val Loss: 0.01536407507109371 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.014683550748906043 // Train Acc: 0.0\n",
      "Val Loss: 0.015112281929362904 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.014410912824718103 // Train Acc: 0.0\n",
      "Val Loss: 0.01485941136594523 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.014156122494182766 // Train Acc: 0.0\n",
      "Val Loss: 0.014615468223663894 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.01392909119407472 // Train Acc: 0.0\n",
      "Val Loss: 0.014388815880837765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.013729298181117398 // Train Acc: 0.0\n",
      "Val Loss: 0.014181362109428103 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.013551205407283757 // Train Acc: 0.0\n",
      "Val Loss: 0.013991868614473126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.013389236744510256 // Train Acc: 0.0\n",
      "Val Loss: 0.013818318269808184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.01323925924474654 // Train Acc: 0.0\n",
      "Val Loss: 0.013658514779738404 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.013098450575330078 // Train Acc: 0.0\n",
      "Val Loss: 0.013510351661931385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.012964898174985836 // Train Acc: 0.0\n",
      "Val Loss: 0.013371988191184672 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.01283727526839091 // Train Acc: 0.0\n",
      "Val Loss: 0.013241953673687848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.012714655502150729 // Train Acc: 0.0\n",
      "Val Loss: 0.013119152222167361 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.012596390620469366 // Train Acc: 0.0\n",
      "Val Loss: 0.013002797656438567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.012481994755761741 // Train Acc: 0.0\n",
      "Val Loss: 0.012892292516136711 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.012371070784002066 // Train Acc: 0.0\n",
      "Val Loss: 0.012787149287760258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.012263257228705573 // Train Acc: 0.0\n",
      "Val Loss: 0.012686910370195453 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.012158222031911418 // Train Acc: 0.0\n",
      "Val Loss: 0.012591182279654525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.012055681104087121 // Train Acc: 0.0\n",
      "Val Loss: 0.01249961910600012 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.01195540621485476 // Train Acc: 0.0\n",
      "Val Loss: 0.012411970577456735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.011857234780383233 // Train Acc: 0.0\n",
      "Val Loss: 0.012328067565844817 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.011761066923768484 // Train Acc: 0.0\n",
      "Val Loss: 0.01224779827858914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.011666854109921276 // Train Acc: 0.0\n",
      "Val Loss: 0.012171111619946632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.011574593248254019 // Train Acc: 0.0\n",
      "Val Loss: 0.01209794067862359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.01148431837881945 // Train Acc: 0.0\n",
      "Val Loss: 0.012028231522576375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.01139612283055845 // Train Acc: 0.0\n",
      "Val Loss: 0.011961920576339418 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.011310159109668916 // Train Acc: 0.0\n",
      "Val Loss: 0.011898926230655475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.011226656215188846 // Train Acc: 0.0\n",
      "Val Loss: 0.011839201191270894 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.011145924919680373 // Train Acc: 0.0\n",
      "Val Loss: 0.011782669335265051 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.011068331954796633 // Train Acc: 0.0\n",
      "Val Loss: 0.011729296330701222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.010994253553594697 // Train Acc: 0.0\n",
      "Val Loss: 0.011679094936698674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.010923985307866103 // Train Acc: 0.0\n",
      "Val Loss: 0.011632133055139672 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.010857655727368307 // Train Acc: 0.0\n",
      "Val Loss: 0.011588511294261975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.01079518642578461 // Train Acc: 0.0\n",
      "Val Loss: 0.01154826357960701 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [19:24<08:18, 166.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.16395026214046565 // Train Acc: 0.0\n",
      "Val Loss: 0.1289422264153307 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.034452808038522936 // Train Acc: 0.0\n",
      "Val Loss: 0.032959895699538964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.025747164154688924 // Train Acc: 0.0\n",
      "Val Loss: 0.025220652771267026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.024037996868360532 // Train Acc: 0.0\n",
      "Val Loss: 0.023513402671299196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.022603239594069926 // Train Acc: 0.0\n",
      "Val Loss: 0.022129656865515494 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.021190461577192697 // Train Acc: 0.0\n",
      "Val Loss: 0.020806994145228103 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.02015912455196182 // Train Acc: 0.0\n",
      "Val Loss: 0.019838304445147514 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.019402206069513407 // Train Acc: 0.0\n",
      "Val Loss: 0.01908949761736122 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.018758494255970738 // Train Acc: 0.0\n",
      "Val Loss: 0.018437777485021137 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.01822048001050881 // Train Acc: 0.0\n",
      "Val Loss: 0.01788662899793549 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.017788223845705595 // Train Acc: 0.0\n",
      "Val Loss: 0.017449767227199944 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.01742152456134626 // Train Acc: 0.0\n",
      "Val Loss: 0.01709741571579467 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.017083248508388185 // Train Acc: 0.0\n",
      "Val Loss: 0.016789164893667805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.01675651759785264 // Train Acc: 0.0\n",
      "Val Loss: 0.0165015401403335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.01643649747352792 // Train Acc: 0.0\n",
      "Val Loss: 0.016225117766721683 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.01612269849575988 // Train Acc: 0.0\n",
      "Val Loss: 0.015956876452334904 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.015815087326177178 // Train Acc: 0.0\n",
      "Val Loss: 0.015694984378801152 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.015513180994288358 // Train Acc: 0.0\n",
      "Val Loss: 0.015437033264474435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.015217168048262324 // Train Acc: 0.0\n",
      "Val Loss: 0.0151814314621416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.014929889962842611 // Train Acc: 0.0\n",
      "Val Loss: 0.014930534709922292 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.01465726364085017 // Train Acc: 0.0\n",
      "Val Loss: 0.014691816228018566 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.014405511200564092 // Train Acc: 0.0\n",
      "Val Loss: 0.014473851363767278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.014177225811883264 // Train Acc: 0.0\n",
      "Val Loss: 0.014280150991610506 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.013970514048171574 // Train Acc: 0.0\n",
      "Val Loss: 0.014107989545234225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.013781249221227213 // Train Acc: 0.0\n",
      "Val Loss: 0.013952074826441029 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.01360528834789277 // Train Acc: 0.0\n",
      "Val Loss: 0.01380769382654266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.013439335689144252 // Train Acc: 0.0\n",
      "Val Loss: 0.01367159876972437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.013281003590120369 // Train Acc: 0.0\n",
      "Val Loss: 0.013541755639016629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.013128609441169729 // Train Acc: 0.0\n",
      "Val Loss: 0.01341691372746771 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.012981011974518975 // Train Acc: 0.0\n",
      "Val Loss: 0.013296288387341933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.012837494683098983 // Train Acc: 0.0\n",
      "Val Loss: 0.013179409199140289 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.012697668510093673 // Train Acc: 0.0\n",
      "Val Loss: 0.013065979291092265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.01256141751115827 // Train Acc: 0.0\n",
      "Val Loss: 0.012955857364630157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.012428845177919149 // Train Acc: 0.0\n",
      "Val Loss: 0.012849010823463852 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.012300203734005219 // Train Acc: 0.0\n",
      "Val Loss: 0.012745485895059326 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.012175823667099418 // Train Acc: 0.0\n",
      "Val Loss: 0.012645388022065163 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.012056024118909213 // Train Acc: 0.0\n",
      "Val Loss: 0.012548806526782837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.011941044260726468 // Train Acc: 0.0\n",
      "Val Loss: 0.012455779932100664 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.011830983174216325 // Train Acc: 0.0\n",
      "Val Loss: 0.012366262467747385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.011725786071883081 // Train Acc: 0.0\n",
      "Val Loss: 0.012280115425925364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.01162525251950031 // Train Acc: 0.0\n",
      "Val Loss: 0.012197139842266386 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.011529082734324889 // Train Acc: 0.0\n",
      "Val Loss: 0.012117119099606168 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.011436937954149221 // Train Acc: 0.0\n",
      "Val Loss: 0.01203986658630046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.011348487771026892 // Train Acc: 0.0\n",
      "Val Loss: 0.011965261662209575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.011263454173474687 // Train Acc: 0.0\n",
      "Val Loss: 0.011893250166692517 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.011181624465915438 // Train Acc: 0.0\n",
      "Val Loss: 0.011823846171186729 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.011102849600589984 // Train Acc: 0.0\n",
      "Val Loss: 0.011757101347161965 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.011027037831593187 // Train Acc: 0.0\n",
      "Val Loss: 0.011693059636110609 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.010954126345682633 // Train Acc: 0.0\n",
      "Val Loss: 0.011631737903437831 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.010884072043673565 // Train Acc: 0.0\n",
      "Val Loss: 0.0115731140259992 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [22:10<05:32, 166.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3159210988103527 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.22948573340069164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03802323948642979 // Train Acc: 0.0\n",
      "Val Loss: 0.03659805425188758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.025909153651020842 // Train Acc: 0.0\n",
      "Val Loss: 0.025406873124566944 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.02361602160110049 // Train Acc: 0.0\n",
      "Val Loss: 0.023051565817811273 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0211444620476806 // Train Acc: 0.0\n",
      "Val Loss: 0.020668234455991873 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.018803945761684277 // Train Acc: 0.0\n",
      "Val Loss: 0.01866423167626966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.017844092628182887 // Train Acc: 0.0\n",
      "Val Loss: 0.017963385344906286 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.017403902422177465 // Train Acc: 0.0\n",
      "Val Loss: 0.01763114339079369 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.01711131800306306 // Train Acc: 0.0\n",
      "Val Loss: 0.017375822737812997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.016832290132228096 // Train Acc: 0.0\n",
      "Val Loss: 0.017118087292394855 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.016516391751377688 // Train Acc: 0.0\n",
      "Val Loss: 0.01682791986756704 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.016146876370814973 // Train Acc: 0.0\n",
      "Val Loss: 0.016503479390997777 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.015752121348988655 // Train Acc: 0.0\n",
      "Val Loss: 0.016173744133927605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.015361643553664695 // Train Acc: 0.0\n",
      "Val Loss: 0.015836737465790725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.014969173657227325 // Train Acc: 0.0\n",
      "Val Loss: 0.015465777155689218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.014551248234488308 // Train Acc: 0.0\n",
      "Val Loss: 0.015042834466492587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.014109489652562087 // Train Acc: 0.0\n",
      "Val Loss: 0.014586492339995774 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.013729480576816283 // Train Acc: 0.0\n",
      "Val Loss: 0.01419791608879512 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0134475483233747 // Train Acc: 0.0\n",
      "Val Loss: 0.013917490738359364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.013230153095810653 // Train Acc: 0.0\n",
      "Val Loss: 0.013708415305749937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.013047489201539456 // Train Acc: 0.0\n",
      "Val Loss: 0.013536803585223176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.012886225097051495 // Train Acc: 0.0\n",
      "Val Loss: 0.013387017222968015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.012740523781899583 // Train Acc: 0.0\n",
      "Val Loss: 0.013252172534438696 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.012607180460283881 // Train Acc: 0.0\n",
      "Val Loss: 0.013128583852879025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.01248404501669415 // Train Acc: 0.0\n",
      "Val Loss: 0.01301385292106054 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.012369454296127166 // Train Acc: 0.0\n",
      "Val Loss: 0.012906200455671007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.012262017187581622 // Train Acc: 0.0\n",
      "Val Loss: 0.012804173957556486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.012160547277528674 // Train Acc: 0.0\n",
      "Val Loss: 0.012706551946360957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.01206406016970164 // Train Acc: 0.0\n",
      "Val Loss: 0.012612331091341647 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.011971765846337221 // Train Acc: 0.0\n",
      "Val Loss: 0.012520729640329426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.011883062777241275 // Train Acc: 0.0\n",
      "Val Loss: 0.012431211566383188 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.01179751768797534 // Train Acc: 0.0\n",
      "Val Loss: 0.012343509596857158 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.011714830446018749 // Train Acc: 0.0\n",
      "Val Loss: 0.01225757332180034 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.011634780281142556 // Train Acc: 0.0\n",
      "Val Loss: 0.012173491055992516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.01155716941062548 // Train Acc: 0.0\n",
      "Val Loss: 0.012091348015449263 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.011481772019574631 // Train Acc: 0.0\n",
      "Val Loss: 0.012011130890724335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.011408320522854386 // Train Acc: 0.0\n",
      "Val Loss: 0.011932674693790349 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.011336530803290269 // Train Acc: 0.0\n",
      "Val Loss: 0.011855730651454493 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.011266132802851329 // Train Acc: 0.0\n",
      "Val Loss: 0.011779998361387036 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.011196898968388502 // Train Acc: 0.0\n",
      "Val Loss: 0.011705231073905122 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.011128668327811578 // Train Acc: 0.0\n",
      "Val Loss: 0.011631247452036902 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.01106134548753534 // Train Acc: 0.0\n",
      "Val Loss: 0.011557977604256435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.010994888452991639 // Train Acc: 0.0\n",
      "Val Loss: 0.01148543715984984 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.010929302953515355 // Train Acc: 0.0\n",
      "Val Loss: 0.011413730765608225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.010864622145678559 // Train Acc: 0.0\n",
      "Val Loss: 0.011343034873293205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.010800890833964428 // Train Acc: 0.0\n",
      "Val Loss: 0.011273574041710659 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0107381630095666 // Train Acc: 0.0\n",
      "Val Loss: 0.01120559246363965 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.01067648417758649 // Train Acc: 0.0\n",
      "Val Loss: 0.011139342104169456 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.010615901438919105 // Train Acc: 0.0\n",
      "Val Loss: 0.011075071355497295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.010556462725845579 // Train Acc: 0.0\n",
      "Val Loss: 0.011013017069887032 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [24:56<02:46, 166.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1691258841929915 // Train Acc: 0.0\n",
      "Val Loss: 0.133287783508951 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03918053754966825 // Train Acc: 0.0\n",
      "Val Loss: 0.03763263506645506 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.025713694709263844 // Train Acc: 0.0\n",
      "Val Loss: 0.025144055621190504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.02226396448076724 // Train Acc: 0.0\n",
      "Val Loss: 0.021699795457111162 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.019468344826693542 // Train Acc: 0.0\n",
      "Val Loss: 0.019333617414601826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.018458257971510086 // Train Acc: 0.0\n",
      "Val Loss: 0.018557474652135916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.01782721497934975 // Train Acc: 0.0\n",
      "Val Loss: 0.01801292615180666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.017299491227455607 // Train Acc: 0.0\n",
      "Val Loss: 0.017534091950140215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.01684480578081521 // Train Acc: 0.0\n",
      "Val Loss: 0.017103423491459 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.016392350877256698 // Train Acc: 0.0\n",
      "Val Loss: 0.016665033115582033 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.015930109215075296 // Train Acc: 0.0\n",
      "Val Loss: 0.016218243386935106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.015473180431362293 // Train Acc: 0.0\n",
      "Val Loss: 0.015783847140317612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.015046477292294372 // Train Acc: 0.0\n",
      "Val Loss: 0.015386918144808574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.014665564492378878 // Train Acc: 0.0\n",
      "Val Loss: 0.015035351433537222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.014327681340600393 // Train Acc: 0.0\n",
      "Val Loss: 0.014720353746617383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.014023571130585725 // Train Acc: 0.0\n",
      "Val Loss: 0.014432517075064507 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.013744909174901914 // Train Acc: 0.0\n",
      "Val Loss: 0.014165412389080633 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.013486726198889758 // Train Acc: 0.0\n",
      "Val Loss: 0.013915577928789637 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.01324699376071867 // Train Acc: 0.0\n",
      "Val Loss: 0.013681936365636912 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.013025138249671827 // Train Acc: 0.0\n",
      "Val Loss: 0.013464690482413227 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.012820712313030476 // Train Acc: 0.0\n",
      "Val Loss: 0.013264136465097016 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.012632757402982002 // Train Acc: 0.0\n",
      "Val Loss: 0.013079984155906872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.012459891733445534 // Train Acc: 0.0\n",
      "Val Loss: 0.012911291056397286 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.012300622130671865 // Train Acc: 0.0\n",
      "Val Loss: 0.012756769062781876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.012153506395938598 // Train Acc: 0.0\n",
      "Val Loss: 0.01261501388454979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.012017156919209312 // Train Acc: 0.0\n",
      "Val Loss: 0.012484624211422421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.011890210536602016 // Train Acc: 0.0\n",
      "Val Loss: 0.012364235427230596 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.01177132467752042 // Train Acc: 0.0\n",
      "Val Loss: 0.012252519220452417 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.01165922607197405 // Train Acc: 0.0\n",
      "Val Loss: 0.012148222606629133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.011552771965900785 // Train Acc: 0.0\n",
      "Val Loss: 0.01205018830739639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.011451012914209333 // Train Acc: 0.0\n",
      "Val Loss: 0.011957453402944587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.011353245248473738 // Train Acc: 0.0\n",
      "Val Loss: 0.011869317394765941 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.011259026087599393 // Train Acc: 0.0\n",
      "Val Loss: 0.011785396586426279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.01116813264136547 // Train Acc: 0.0\n",
      "Val Loss: 0.011705578880553897 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.011080472888510044 // Train Acc: 0.0\n",
      "Val Loss: 0.011629929080266844 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.010995961418460425 // Train Acc: 0.0\n",
      "Val Loss: 0.01155854230746627 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.010914422025198125 // Train Acc: 0.0\n",
      "Val Loss: 0.011491405354304747 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.010835577609425742 // Train Acc: 0.0\n",
      "Val Loss: 0.01142835273322734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.010759077695109052 // Train Acc: 0.0\n",
      "Val Loss: 0.011369078733365644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.01068456994888445 // Train Acc: 0.0\n",
      "Val Loss: 0.011313182589682665 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.010611755512952464 // Train Acc: 0.0\n",
      "Val Loss: 0.011260245164686983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.01054043141214951 // Train Acc: 0.0\n",
      "Val Loss: 0.011209872796792876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.010470487974737196 // Train Acc: 0.0\n",
      "Val Loss: 0.011161729490215128 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.01040190908436974 // Train Acc: 0.0\n",
      "Val Loss: 0.011115528930994597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.01033474267545458 // Train Acc: 0.0\n",
      "Val Loss: 0.01107106265527281 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.010269068845985755 // Train Acc: 0.0\n",
      "Val Loss: 0.011028169087049636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.01020498353494629 // Train Acc: 0.0\n",
      "Val Loss: 0.010986729567362503 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.010142564894759084 // Train Acc: 0.0\n",
      "Val Loss: 0.010946668599816885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.010081875453741378 // Train Acc: 0.0\n",
      "Val Loss: 0.01090795738961209 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.010022952230902363 // Train Acc: 0.0\n",
      "Val Loss: 0.010870565084571187 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [27:42<00:00, 166.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1057, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1056, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.09735318772167893 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07183713153662051 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01867873889395807 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.014317738419563016 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01544338252252373 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.011163595940589029 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.014200483031666256 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010343768606510232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.01366499888538715 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010134095038451692 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.013324881263223683 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009945345919250566 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.013035568739823875 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009718261385226952 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.01274403108801468 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009459611391374731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.012421463897972754 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009162190027863663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.012048610143441796 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008876737987841754 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.011679569423161243 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008765632998855674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 103\n",
      "INFO: Validation loss did not improve in epoch 104\n",
      "INFO: Validation loss did not improve in epoch 105\n",
      "INFO: Validation loss did not improve in epoch 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:24<12:37, 84.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 107\n",
      "Early stopping after 107 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.13600726080190056 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07829237603308524 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.018521672402469398 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.014351110809537418 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01623449603587702 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.012006960802382845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.014898859359446565 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010959919165436397 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.014431579832338354 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0105097933590193 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.014103138560541587 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010315726257805876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.013732333960636455 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010187685003449373 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.013209681972006584 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009946249267908143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.012568448731502079 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009522485382416668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.012051512718573553 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009169704725911073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.01169222825308121 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008952501661363332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.011405524534260143 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00877701341147151 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.011190855907747894 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008644628044053474 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.011049392880655661 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008580461493693292 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.01093980947946125 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008558309458963135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.010845141159923887 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00855268066173748 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.010759597517578936 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0085518101353527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.010680742066147544 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008550312157775112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.01060733596059635 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008545591358971946 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.010538584331580024 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008536521078306524 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.010473846988684381 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008522974696996458 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.010412546923516874 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008505444665548994 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.010354176501001162 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008484673926004153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.010298253863860922 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008461425320574027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.01024435547932172 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008436313741292585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.010192140005528927 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008409828043488018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.010141309734878269 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008382278590408318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.010091617415911115 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008354141363216674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.010042876034542592 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008326039656394106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.009994937773322315 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008298555587637512 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.009947706364866488 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008271874442203519 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.009901119827699782 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00824576273889226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.009855149763294069 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0082197693272439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.009809776761575894 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008193438482361244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.00976500940961181 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008166416589280261 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.009720856311707203 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.008138440378174624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.009677311265926256 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00810937220504617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.009634379097226482 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008079165359959006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.009592062580846999 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008047869107138146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.009550370627986965 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008015566649298896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.009509325876850737 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00798244190210586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.009468949501831731 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00794867196065538 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.009429277768324536 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007914548883598079 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.00939034750592304 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007880333790500812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.009352201101722595 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007846290868816568 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.00931489585781337 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007812683408915558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.009278475806215809 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007779719842159573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.009242996937379077 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007747555836377775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.009208511500287345 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00771628028494032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.009175059221083914 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007685920970021363 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [07:58<35:35, 266.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.12747127647210302 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07668702874113531 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.019450813448022526 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.016126024646355826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015600725422720587 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.011425501235541613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.014552984736566951 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010700367514372748 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.013678969482514987 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01024304689182078 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.012914539981161804 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009846154216896085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.012248044360856402 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009560757145449957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.011821631366009033 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0094591455109527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.011469429392253528 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009314235560048153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.011153157612511469 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009023773360668737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.010937385517725861 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008804737651884994 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.010780309196059308 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008644711342640221 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.010657123051851075 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00852262502645745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.010558040832404313 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008431796539191376 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.010475285984856678 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008361483444733656 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.01040372491360453 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008304237805799964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.010340181539019049 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008255711708710912 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.010282558605025508 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0082133767359397 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.01022934287503852 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008175563238396803 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0101793864941977 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008141108870725422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.010131806005141485 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008109262884211014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.010085896262388261 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008079517468371811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.010041050174293981 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008051377029486877 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.009996744710015612 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008024485994075589 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.009952532516756662 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007998526267542997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.009908044144002145 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007973376880673802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.009863006300080085 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007949144456206876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00981726227783627 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007925869431346655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.009770695319087662 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007903604658649248 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00972328236189516 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007882385179126525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.009675034923501094 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007862164310234435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.009626007818921692 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007842782617765753 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.009576288249404928 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007824030067936024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.009526005893957612 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007805629907285466 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.009475333889192089 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007787335766753291 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00942452878787453 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007769023732501356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.009373925448356153 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0077507385696448826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.009323925814860358 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007732736672658254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.009274955532876213 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007715478491531137 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.009227391327492947 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00769958516363712 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.009181494164617708 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007685769951957113 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.009137387033313567 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007674706675221815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.00909506523418981 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0076669313232688345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.009054429010382531 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007662768071681699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 438\n",
      "INFO: Validation loss did not improve in epoch 439\n",
      "INFO: Validation loss did not improve in epoch 440\n",
      "INFO: Validation loss did not improve in epoch 441\n",
      "Epoch: 441\n",
      "Train Loss: 0.009015333221142454 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0076622937888126165 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [13:47<35:29, 304.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 442\n",
      "Early stopping after 442 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.12230823028635172 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.06878088710500914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.018881589242374656 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.015485018991646083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.016746824850512147 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01242850614054238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.015173030658361163 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0109013540273094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.01430244604551269 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010610062156475204 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 45\n",
      "INFO: Validation loss did not improve in epoch 46\n",
      "INFO: Validation loss did not improve in epoch 47\n",
      "INFO: Validation loss did not improve in epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [14:26<19:56, 199.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 49\n",
      "Early stopping after 49 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.14021258564900874 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0758092466960935 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01875062743661554 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0153099549934268 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015607262507823365 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.011506928562405793 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.014758436618393229 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010681359316496289 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.014009878375623681 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010426125598742682 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.013492426999939956 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010224590064300335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.013047744023487639 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010111592381316073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.012582300655021106 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009889195371857461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.012096648721255947 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009578339063891154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.011606981820329832 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009288359104710467 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.011263240894500228 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009026609806289129 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.011019794750529626 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008819313932155423 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.010804700501939074 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00862338301479159 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.010593273453252693 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008389511960558593 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.01037624597862717 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008111500920837416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.010154223294291897 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007829384775558376 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00995210199885009 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007598000345751643 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.009792733753408209 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007437905289835352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00967099731047408 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007338465951547465 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.009574291771754513 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007281883202000137 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.009493958869637407 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007252660532043699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.009424637621515507 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007238816593171042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00936291835435123 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00723252839901868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.009306496649834121 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0072294065989006095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.009253776227456042 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00722737523371025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.009203651117184753 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007225409328170559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.009155387461519166 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007223087719039005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.009108500623687338 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00722034163910019 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.009062683041432236 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007217285373960347 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.009017734589366756 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007214239873813794 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00897351399228365 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007211622467045398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.008929912970099986 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007209977347348981 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 317\n",
      "INFO: Validation loss did not improve in epoch 318\n",
      "INFO: Validation loss did not improve in epoch 319\n",
      "INFO: Validation loss did not improve in epoch 320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [18:39<18:13, 218.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 321\n",
      "Early stopping after 321 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.2899841418867791 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.11230660481926273 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02114693403041409 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.017982327612116933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01575610814513928 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.011903794975403477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.014453839932498564 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01064603160108056 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.013565467118491612 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010062628685880233 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.012911460348753977 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009812971303129898 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.012437672684159832 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009589857394423555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.012059291287527932 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009367958478191319 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.011724525887771958 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009133583242895411 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.011437021142469422 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008903289563022554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.01119780267965069 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00866494609234745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.010988446740838792 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008415131179122803 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.010803386783337993 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00819761261535699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.010647753470660727 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008049627779709065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.010521948079823753 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007954727262532449 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.010416969664180329 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007881048190243104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.010325092459872188 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00781533948611468 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.01024259308904709 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007754344130208825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.010167558042211646 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007696310325306566 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.010098610010757443 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007639780127005104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.010034562487808998 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0075839593431309745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.009974432141448573 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007528828593957073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.009917473696322066 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007474757237907718 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.009863086709866752 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007422227652140838 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00981078780395825 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00737163210150731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.009760169485171872 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007323208401965744 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00971086789412046 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007276996620930731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.009662562679674755 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007232959094621679 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.00961497614851436 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007190933772910605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.009567868024948474 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00715071547721677 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.009521039100227642 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007112076245796154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.009474341053995807 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007074732668971752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.009427677694209246 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007038378807277802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.009381002779510315 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.007002686465378194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.009334331678424092 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0069673947428407915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.009287766665109592 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006932393757297713 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.009241593961214043 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006897885414004764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00919615587801034 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006864253390470848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.009151804316031254 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006832213907995645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.00910882868887465 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006802512421820532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.009067407096659517 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006775793023681378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.009027627837108669 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006752472521518083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.00898944978943602 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006732497782008175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.00895271257227508 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00671556157142143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.008917209248133412 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006701089914285523 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.008882709960597934 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006688498700147166 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.008849001355074224 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006677097716259167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.00881589436243838 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006666349733303136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.008783231611309106 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006655793212463751 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.00875090172989345 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006645023069508812 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [25:13<18:32, 278.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1915015218714746 // Train Acc: 0.0\n",
      "Val Loss: 0.08659922079566647 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01945889007489127 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.015164661159574547 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.017236099766151914 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01266552579780931 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.016225542108250793 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01169597388327341 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.015273326048474261 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010999935610658106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.014731472476438315 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01058672504116069 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.014441633245216468 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010312462800785023 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.014171595029469913 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010112548863296123 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.013842468719692795 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009968617054469445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.013505672530519692 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009886605261058053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.013203770612873873 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00981606514987481 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.012937629046523007 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00976589389200158 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.012706706503412978 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009711818454568 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.012504491141624408 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009622336447458057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.012321549402617966 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009497778318986735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.01215044368472175 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009354646797018015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.011987436739847037 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009210945449440795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.011831784649825292 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009075780419687577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.011683681646704169 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008947226766716032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.011541852898218106 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008819979841492194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.011400866732971787 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008696543655413039 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.011249560591853284 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008605214164537541 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.011092816104080657 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008563135045251864 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.010956206294057561 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008518769481109785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.010839870955302998 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008462371503222077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.010736603054883711 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00840535492170602 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.01064273226053592 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008355590959956102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.010557012688036552 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00831682626705836 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.010479189681955077 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008288849973777199 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.010408794937929448 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008268228773137225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.01034473875810782 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00825021104366683 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.010285647588967595 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008231082883225205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.010230276600621178 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008209301706622629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.010177679418226946 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008185091449002571 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.010127188771293998 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008159390684929402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.010078343734106005 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008133172756060958 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.01003082131160767 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008107202202903436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.009984395635515193 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008081990752971786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.009938889472971805 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008057801361086176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.009894174703488797 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008034659666009247 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.009850139679239086 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008012364748591447 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.009806682373792055 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007990508038988885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.00976369366853394 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007968539796660052 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.009721063290274884 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007945777334765914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.009678667694478341 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00792154814938412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.009636402269101734 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007895151879090597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00959417925529536 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007866003153407398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.009551952978029125 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00783361989648684 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.009509730090853678 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007797700202311663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.009467566055986329 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007758130415287965 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [31:45<15:47, 315.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.18223632878574528 // Train Acc: 0.0\n",
      "Val Loss: 0.07889716568238594 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01901059467141164 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.015033487798920012 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.016294578251205862 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.011921290814986123 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.014738337205334734 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01046939825584345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.014188112129399407 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010203837807399823 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 51\n",
      "Epoch: 51\n",
      "Train Loss: 0.013595091356935023 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010113876984071206 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 52\n",
      "INFO: Validation loss did not improve in epoch 53\n",
      "INFO: Validation loss did not improve in epoch 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [32:29<07:37, 228.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 55\n",
      "Early stopping after 55 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.17436547956380047 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.08541101688409553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.019096430905258633 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01557824160109329 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01535014877984576 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.011280078201226014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.014658349096669493 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010351403260274845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.013966780145255887 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009987591396031134 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 44\n",
      "INFO: Validation loss did not improve in epoch 45\n",
      "INFO: Validation loss did not improve in epoch 46\n",
      "INFO: Validation loss did not improve in epoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [33:07<02:49, 169.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 48\n",
      "Early stopping after 48 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.19043629964510528 // Train Acc: 0.0\n",
      "Val Loss: 0.08600574523648795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.019603029858845353 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.015929543495397356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01559816376358046 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010994619922712445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0147971844026509 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010376382879365017 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.014248820106878578 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010119444245527335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.01366863676416645 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009942218661308289 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.013113083709474019 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009597027982475565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.012656393447734239 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009270496324033421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.012277403847234395 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009030662894742015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0119747546149445 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008861144121243236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.011729064193840259 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008740555138929802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.011516944353742835 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008647147744127056 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.011326878515695318 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008570496259969385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.011155258358066298 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008502064704182832 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.011001121925089638 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008435626007506953 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.01086457461200423 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008370718822869308 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.01074641896049974 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008311205810648115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.01064680460427583 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00825855604318135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.010563554350155623 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008209412486073287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.010492739613275206 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008159847916377819 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.010430577968313636 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008108208209330984 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.010374205128649007 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008054551962927422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.010321481148686798 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007999470039709088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.01027074243091864 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007943491074804436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.010220621634186185 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007887021754868329 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.010169919841144731 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007830381448216298 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.010117547865148286 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0077739522093907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.010062493903603288 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007718554300749127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.010003855291897026 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007665716097964083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.009940982318885107 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0076178253409178815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.009873921748836417 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007577584714026135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.009804129277030772 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007546618923216182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00973461718418528 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007525029112858807 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00966869869803196 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007512492990559515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 341\n",
      "Epoch: 341\n",
      "Train Loss: 0.00960828821958848 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007509139133617282 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 342\n",
      "INFO: Validation loss did not improve in epoch 343\n",
      "INFO: Validation loss did not improve in epoch 344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [37:38<00:00, 225.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 345\n",
      "Early stopping after 345 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictive performance\n",
    "predictive_results = predictive_evaluation(\n",
    "    data_train_real=data_train_real_numpy, \n",
    "    data_test_real=data_test_real_numpy,\n",
    "    data_syn=data_syn_numpy, \n",
    "    hyperparameters=hyperparameters, \n",
    "    include_baseline=True, \n",
    "    verbose=True)\n",
    "\n",
    "# save results\n",
    "bidirectionality = \"bi\" if hyperparameters[\"bidirectional\"] else 'no_bi'\n",
    "predictive_results.to_csv(DATA_FOLDER / f\"results_{syn_data_type}_{hyperparameters['num_epochs']}_{hyperparameters['num_evaluation_runs']}_{bidirectionality}.csv\", index=False)\n",
    "\n",
    "# split in mse and mae results\n",
    "mse_results = predictive_results.loc[predictive_results['Metric'] == 'MSE']\n",
    "mae_results = predictive_results.loc[predictive_results['Metric'] == 'MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x19894257d70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIQAAAK9CAYAAABVd7dpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAACAwklEQVR4nOzdd3hUZeL28fvMJJkE0giEhBISOgKClAUBFRRYQEVR14rSVlZBkRUrFhBEcO0F1BUVRLGy6KurIogColgQECx0CDWhppNMMnPeP/gx65gEMmSSk+R8P9eVa3ee0+4JIZI7z3mOYZqmKQAAAAAAANiGw+oAAAAAAAAAqFwUQgAAAAAAADZDIQQAAAAAAGAzFEIAAAAAAAA2QyEEAAAAAABgMxRCAAAAAAAANkMhBAAAAAAAYDMUQgAAAAAAADZDIQQAAAAAAGAzFEIAAFQxc+fOlWEY5TrHQw89VO5z1FQpKSkaMWKE1TFOyTAMzZ071+oYlao8X7cn/t7s3LkzuKEAAKihKIQAALZy4odGwzC0cuXKYttN01RSUpIMw9DFF1/sty0nJ0eTJ09W+/btVbt2bdWtW1dnnXWWxo8fr3379vn2O/FDbWkfaWlpQXkveXl5euihh7Rs2bKgnA+l+/TTT/XQQw9ZHaNEmzZt0u23366ePXsqPDz8lKXIRx99pM6dOys8PFxNmjTR5MmTVVRUdNJrpKSknPRr+sSH3QqsE/78dz40NFQpKSm67bbblJGRYXU8AABKFGJ1AAAArBAeHq633npL55xzjt/48uXLtWfPHrlcLr/xwsJCnXfeedq4caOGDx+ucePGKScnR7/++qveeustXXbZZWrYsKHfMS+++KIiIyOLXTs2NjYo7yEvL09TpkyRJPXp08dv2wMPPKB77703KNfB8UJo1qxZVbIUWrVqlZ577jm1bdtWZ5xxhtatW1fqvp999pmGDBmiPn366Pnnn9eGDRs0bdo0HThwQC+++GKpxz3zzDPKycnxvf7000/19ttv6+mnn1a9evV84z179izXeynP1+0NN9yga665ptjf3cp04u98bm6uli5dqueff15r1qwpsXwGAMBqFEIAAFu68MIL9f777+u5555TSMj//nP41ltvqUuXLjp06JDf/h9++KHWrl2r+fPn67rrrvPblp+fL7fbXewaf/vb3/x+WK5MISEhfu8LNdcll1yijIwMRUVF6YknnjhpIXTnnXeqQ4cOWrx4se/rIzo6WtOnT9f48ePVpk2bEo8bMmSI3+u0tDS9/fbbGjJkiFJSUkq9Xm5urmrXrl3m91Ker1un0ymn03laxwbLH//O33TTTbrmmmv07rvv6ocfflC3bt0szQYAwJ9xyxgAwJauvfZaHT58WEuWLPGNud1uLViwoFjhI0nbtm2TJPXq1avYtvDwcEVHR1dc2BLs3LlT8fHxkqQpU6b4blU5MYOlpLVYDMPQrbfeqvfff19t27ZVRESEevTooQ0bNkiS/v3vf6tFixYKDw9Xnz59Srzt6Pvvv9fAgQMVExOjWrVqqXfv3vrmm2+K7bds2TJ17dpV4eHhat68uf7973+XmGnOnDm64IILVL9+fblcLrVt27bEmSopKSm6+OKLtXLlSnXr1k3h4eFq1qyZ5s2bdzqfPj+FhYWaMmWKWrZsqfDwcNWtW1fnnHOO72tjxIgRmjVrliT53RYkHf9zMAxDTzzxhGbNmqVmzZqpVq1a+utf/6rdu3fLNE09/PDDaty4sSIiInTppZfqyJEj5c78R3FxcYqKijrlfr/99pt+++03/eMf//ArXcaOHSvTNLVgwYJy5RgxYoQiIyO1bds2XXjhhYqKitLQoUMlSV9//bWuvPJKNWnSRC6XS0lJSbr99tt17Ngxv3Oc7Ov2ww8/VPv27eVyudSuXTstWrTIb7+S1hAK5Otm/fr16t27tyIiItS4cWNNmzZNc+bMKde6ROeee66k/33/OJGppDWs+vTp4zfTb9myZTIMQ++9954eeeQRNW7cWOHh4erbt6+2bt3qd+yWLVt0xRVXKDExUeHh4WrcuLGuueYaZWZmnlZuAIA98KtDAIAtpaSkqEePHnr77bc1aNAgScdvp8nMzNQ111yj5557zm//5ORkSdK8efP0wAMPlGnh25J+8A8JCQnKLWPx8fF68cUXNWbMGF122WW6/PLLJUkdOnQ46XFff/21PvroI91yyy2SpBkzZujiiy/W3XffrRdeeEFjx47V0aNH9dhjj2nUqFH68ssvfcd++eWXGjRokLp06aLJkyfL4XD4Cp2vv/7aNwNi7dq1GjhwoBo0aKApU6bI4/Fo6tSpvgLrj1588UW1a9dOl1xyiUJCQvTxxx9r7Nix8nq9vownbN26VX/729/097//XcOHD9drr72mESNGqEuXLmrXrt1pfy4feughzZgxQzfeeKO6deumrKwsrV69WmvWrFH//v110003ad++fVqyZIneeOONEs8xf/58ud1ujRs3TkeOHNFjjz2mq666ShdccIGWLVume+65R1u3btXzzz+vO++8U6+99tpp5z1da9eulSR17drVb7xhw4Zq3Lixb3t5FBUVacCAATrnnHP0xBNPqFatWpKk999/X3l5eRozZozq1q2rH374Qc8//7z27Nmj999//5TnXblypRYuXKixY8cqKipKzz33nK644grt2rVLdevWPemxZfm62bt3r84//3wZhqGJEyeqdu3aeuWVV8p9+9mJIqlOnTqnfY5HH31UDodDd955pzIzM/XYY49p6NCh+v777yUdL7IHDBiggoICjRs3TomJidq7d6/++9//KiMjQzExMeV6DwCAGswEAMBG5syZY0oyf/zxR3PmzJlmVFSUmZeXZ5qmaV555ZXm+eefb5qmaSYnJ5sXXXSR77i8vDyzdevWpiQzOTnZHDFihPnqq6+a6enpxa4xefJkU1KJH61bty5zxlM5ePCgKcmcPHlyqRn+SJLpcrnMHTt2+Mb+/e9/m5LMxMREMysryzc+ceJEU5JvX6/Xa7Zs2dIcMGCA6fV6ffvl5eWZTZs2Nfv37+8bGzx4sFmrVi1z7969vrEtW7aYISEhxTKd+Nz/0YABA8xmzZr5jSUnJ5uSzBUrVvjGDhw4YLpcLvOOO+4o4bNTuuTkZHP48OG+1x07dvT7sy7JLbfcUuKfyY4dO0xJZnx8vJmRkeEbP/H569ixo1lYWOgbv/baa82wsDAzPz//lDklmXPmzDn1G/qDxx9/3O/PraRtu3btKrbtL3/5i3n22WeX6zrDhw83JZn33ntvsf1L+nOeMWOGaRiGmZqa6hsr7es2LCzM3Lp1q2/s559/NiWZzz//vG/sxN+bP2Yq69fNuHHjTMMwzLVr1/rGDh8+bMbFxZX6+fyjE7k3bdpkHjx40Ny5c6f52muvmREREWZ8fLyZm5vrl+mPX38n9O7d2+zdu7fv9VdffWVKMs844wyzoKDAN/7ss8+akswNGzaYpmmaa9euNSWZ77///kkzAgDwZ9wyBgCwrauuukrHjh3Tf//7X2VnZ+u///1vibeLSVJERIS+//573XXXXZKO357y97//XQ0aNNC4ceNUUFBQ7Jj//Oc/WrJkid/HnDlzKvQ9nUrfvn391nzp3r27JOmKK67wu+3oxPj27dslSevWrdOWLVt03XXX6fDhwzp06JAOHTqk3Nxc9e3bVytWrJDX65XH49EXX3yhIUOG+C2y3aJFC99MrD+KiIjw/f/MzEwdOnRIvXv31vbt24vd7tK2bVvfLTjS8VlSrVu39mU8XbGxsfr111+1ZcuW0z7HlVde6TcT48Tn7/rrr/e7Pat79+5yu93au3fv6Qc+TSduzypp1kt4eHix27dO15gxY4qN/fHPOTc3V4cOHVLPnj1lmmaZZib169dPzZs3973u0KGDoqOjy/RnX5avm0WLFqlHjx4666yzfGNxcXG+W97KqnXr1oqPj1dKSopGjRqlFi1a6LPPPvPNlDodI0eOVFhYmO/1ifdyIv+Jr7vPP/9ceXl5p30dAID92LoQWrFihQYPHqyGDRvKMAx9+OGHAZ/DNE098cQTatWqlVwulxo1aqRHHnkk+GEBAEEXHx+vfv366a233tLChQvl8Xj0t7/9rdT9Y2Ji9Nhjj2nnzp3auXOnXn31VbVu3VozZ87Uww8/XGz/8847T/369fP76NGjR0W+pVNq0qSJ3+sTP0wmJSWVOH706FFJ8pUlw4cPV3x8vN/HK6+8ooKCAmVmZurAgQM6duyYWrRoUezaJY1988036tevn2rXrq3Y2FjFx8frvvvuk6RihdCfs0vHb8U5kfF0TZ06VRkZGWrVqpXOPPNM3XXXXVq/fn1A5zjdz2tlOlHKlFRe5ufn+5U2pyskJESNGzcuNr5r1y6NGDFCcXFxioyMVHx8vHr37i2p+J9zScrzZ1+WY1NTU8v8NXsyJ0rgt956S2effbYOHDhQ7s/rn/OfuP3sRP6mTZtqwoQJeuWVV1SvXj0NGDBAs2bNYv0gAMAp2XoNodzcXHXs2FGjRo3yrb0QqPHjx2vx4sV64okndOaZZ+rIkSNBXywSAFBxrrvuOo0ePVppaWkaNGhQmdf3SU5O1qhRo3TZZZepWbNmmj9/vqZNm1axYYOgtKcwlTZumqYkyev1SpIef/xxv1kUfxQZGan8/PwyZ9m2bZv69u2rNm3a6KmnnlJSUpLCwsL06aef6umnn/Zds6wZT9d5552nbdu26f/9v/+nxYsX65VXXtHTTz+tl156STfeeGOZznG6n9fK1KBBA0nS/v37ixVV+/fvD8pTsFwulxwO/983ejwe9e/fX0eOHNE999yjNm3aqHbt2tq7d69GjBhR7M+5JOX5PFbmn8F5553ne8rY4MGDdeaZZ2ro0KH66aeffJ+X0tYf83g8JWYtS/4nn3xSI0aM8H0N33bbbZoxY4a+++67Egs6AAAkmxdCgwYNKnH6+gkFBQW6//779fbbbysjI0Pt27fXv/71L98TIH7//Xe9+OKL+uWXX9S6dWtJx39LAwCoPi677DLddNNN+u677/Tuu+8GfHydOnXUvHlz/fLLLxWQ7uTKsrB1sJy4XSc6Olr9+vUrdb/69esrPDy82FOQJBUb+/jjj1VQUKCPPvrIbxbEV199FaTUZRcXF6eRI0dq5MiRysnJ0XnnnaeHHnrIVwhV5ue6opwo8lavXu1X/uzbt0979uzRP/7xjwq57oYNG7R582a9/vrrGjZsmG/8j0/4s1pycnKZvmYDERkZqcmTJ2vkyJF67733dM0110g6/j0jIyOj2P6pqalq1qzZaV/vzDPP1JlnnqkHHnhA3377rXr16qWXXnqpWhTVAABr2PqWsVO59dZbtWrVKr3zzjtav369rrzySg0cONA3bf7jjz9Ws2bN9N///ldNmzZVSkqKbrzxRmYIAUA1EhkZqRdffFEPPfSQBg8eXOp+P//8sw4dOlRsPDU1Vb/99pvvFwOV6cS6JCX9cBlsXbp0UfPmzfXEE08oJyen2PaDBw9KOj6boV+/fvrwww+1b98+3/atW7fqs88+8zvmxMyHP850yMzMrPR1lg4fPuz3OjIyUi1atPC7tap27dqSKudzXVHatWunNm3a6OWXX5bH4/GNv/jiizIM46S3S5ZHSX/Opmnq2WefrZDrnY4BAwZo1apVWrdunW/syJEjmj9/frnOO3ToUDVu3Fj/+te/fGPNmzfXd999J7fb7Rv773//q927d5/WNbKyslRUVOQ3duaZZ8rhcJR4eyAAACfYeobQyezatUtz5szRrl27fIti3nnnnVq0aJHmzJmj6dOna/v27UpNTdX777+vefPmyePx6Pbbb9ff/vY3v8f0AgCqtuHDh59ynyVLlmjy5Mm65JJLdPbZZysyMlLbt2/Xa6+9poKCAj300EPFjlmwYIEiIyOLjffv318JCQnlzh0REaG2bdvq3XffVatWrRQXF6f27durffv25T73nzkcDr3yyisaNGiQ2rVrp5EjR6pRo0bau3evvvrqK0VHR+vjjz+WdPwx7osXL1avXr00ZswYeTwezZw5U+3bt/f7gfuvf/2rwsLCNHjwYN10003KycnR7NmzVb9+fe3fvz/o76E0bdu2VZ8+fdSlSxfFxcVp9erVWrBggW699VbfPl26dJEk3XbbbRowYICcTqdvxofVMjMz9fzzz0s6viaTJM2cOVOxsbGKjY31ex+PP/64LrnkEv31r3/VNddco19++UUzZ87UjTfeqDPOOKNC8rVp00bNmzfXnXfeqb179yo6Olr/+c9/LFlHqTR333233nzzTfXv31/jxo3zPXa+SZMmOnLkyGnPEAsNDdX48eN11113adGiRRo4cKBuvPFGLViwQAMHDtRVV12lbdu26c033/RbNDsQX375pW699VZdeeWVatWqlYqKivTGG2/I6XTqiiuuOK1zAgDsgUKoFBs2bJDH41GrVq38xgsKClS3bl1Jx9dTKCgo0Lx583z7vfrqq+rSpYs2bdpkyW+LAQAV44orrlB2drYWL16sL7/8UkeOHFGdOnXUrVs33XHHHTr//POLHVPS05ak47dEBaMQkqRXXnlF48aN0+233y63263JkydXSCEkSX369NGqVav08MMPa+bMmcrJyVFiYqK6d++um266ybdfly5d9Nlnn+nOO+/Ugw8+qKSkJE2dOlW///67Nm7c6NuvdevWWrBggR544AHdeeedSkxM1JgxYxQfH69Ro0ZVyHsoyW233aaPPvpIixcvVkFBgZKTkzVt2jTfE+Uk6fLLL9e4ceP0zjvv6M0335RpmlWmEDp69KgefPBBv7Enn3xS0vFbof5YCF188cVauHChpkyZonHjxvkW8Z40aVKF5QsNDdXHH3/sW9cmPDxcl112mW699VZ17Nixwq4biKSkJH311Ve67bbbNH36dMXHx+uWW25R7dq1ddtttyk8PPy0z/2Pf/xD06ZN06OPPqqBAwdqwIABevLJJ/XUU0/pn//8p7p27ar//ve/uuOOO07r/B07dtSAAQP08ccfa+/evapVq5Y6duyozz77TGefffZp5wYA1HyGacWqhlWQYRj64IMPNGTIEEnSu+++q6FDh+rXX38ttphfZGSkEhMTNXnyZE2fPl2FhYW+bceOHVOtWrW0ePFi9e/fvzLfAgCghpg7d65GjhxpycLDFWnIkCHlfry7nRiGoTlz5mjEiBFWR7Gtf/7zn/r3v/+tnJycUhd3BgCgumKGUCk6deokj8ejAwcO6Nxzzy1xn169eqmoqEjbtm3zTfPdvHmzpOO/kQMAwK6OHTvm97jtLVu26NNPPy3T7XmAFf78NXv48GG98cYbOueccyiDAAA1kq0LoZycHL+nR+zYsUPr1q1TXFycWrVqpaFDh2rYsGF68skn1alTJx08eFBLly5Vhw4ddNFFF6lfv37q3LmzRo0apWeeeUZer1e33HKL+vfvX+xWMwAA7KRZs2YaMWKEmjVrptTUVL344osKCwvT3XffXWHXTEtLO+n2iIgIxcTEVNj1Ub316NFDffr00RlnnKH09HS9+uqrysrKKnY7HgAANYWtC6HVq1f7rfkwYcIESccXF507d67mzJmjadOm6Y477tDevXtVr149nX322br44oslHV9g8+OPP9a4ceN03nnnqXbt2ho0aJDvvn0AAOxq4MCBevvtt5WWliaXy6UePXpo+vTpatmyZYVds0GDBifdfuK/70BJLrzwQi1YsEAvv/yyDMNQ586d9eqrr+q8886zOhoAABWCNYQAAECN8MUXX5x0e8OGDdW2bdtKSgMAAFC1UQgBAAAAAADYjMPqAAAAAAAAAKhctltDyOv1at++fYqKipJhGFbHAQAAAAAACArTNJWdna2GDRvK4Tj5HCDbFUL79u1TUlKS1TEAAAAAAAAqxO7du9W4ceOT7mO7QigqKkrS8U9OdHS0xWkAAAAAAACCIysrS0lJSb7u42RsVwiduE0sOjqaQggAAAAAANQ4ZVkih0WlAQAAAAAAbIZCCAAAAAAAwGYohAAAAAAAAGzGdmsIAQAAAACAqsE0TRUVFcnj8VgdpdoIDQ2V0+ks93kohAAAAAAAQKVzu93av3+/8vLyrI5SrRiGocaNGysyMrJc56EQAgAAAAAAlcrr9WrHjh1yOp1q2LChwsLCyvRkLLszTVMHDx7Unj171LJly3LNFKIQAgAAAAAAlcrtdsvr9SopKUm1atWyOk61Eh8fr507d6qwsLBchRCLSgMAAAAAAEs4HNQSgQrWTCo+8wAAAAAAADZDIQQAAAAAAGAzFEIAAAAAAAA2QyEEAAAAAABQRiNGjJBhGLr55puLbbvllltkGIZGjBghSTp48KDGjBmjJk2ayOVyKTExUQMGDNA333zjOyYlJUWGYRT7ePTRRyv0ffCUMQAAAAAAUC2tW7dOn332mfbv368GDRpo0KBBOuussyr8uklJSXrnnXf09NNPKyIiQpKUn5+vt956S02aNPHtd8UVV8jtduv1119Xs2bNlJ6erqVLl+rw4cN+55s6dapGjx7tNxYVFVWh74FCCAAAAAAAVDvr1q3TSy+95Hudmpqql156STfffHOFl0KdO3fWtm3btHDhQg0dOlSStHDhQjVp0kRNmzaVJGVkZOjrr7/WsmXL1Lt3b0lScnKyunXrVux8UVFRSkxMrNDMf8YtYwAAAAAAoNr57LPPShxftGhRpVx/1KhRmjNnju/1a6+9ppEjR/peR0ZGKjIyUh9++KEKCgoqJVMgKIQAAAAAAEC1s3///hLH9+3bVynXv/7667Vy5UqlpqYqNTVV33zzja6//nrf9pCQEM2dO1evv/66YmNj1atXL913331av359sXPdc889vgLpxMfXX39dofkphAAAAAAAQLXToEGDEscbNmxYKdePj4/XRRddpLlz52rOnDm66KKLVK9ePb99rrjiCu3bt08fffSRBg4cqGXLlqlz586aO3eu33533XWX1q1b5/fRtWvXCs1PIQQAAAAAAKqdQYMGBTReEUaNGuWbBTRq1KgS9wkPD1f//v314IMP6ttvv9WIESM0efJkv33q1aunFi1a+H2cWKy6olhaCK1YsUKDBw9Ww4YNZRiGPvzww1MeM3/+fHXs2FG1atVSgwYNNGrUqGKrcwMAAAAAgJrtrLPO0s0336yUlBSFhYUpJSVFY8aMUceOHSstw8CBA+V2u1VYWKgBAwaU6Zi2bdsqNze3gpOdmqVPGcvNzVXHjh01atQoXX755afc/5tvvtGwYcP09NNPa/Dgwdq7d69uvvlmjR49WgsXLqyExAAAAAAAoKo466yzKuUx86VxOp36/fffff//jw4fPqwrr7xSo0aNUocOHRQVFaXVq1frscce06WXXuq3b3Z2ttLS0vzGatWqpejo6ArLbmkhNGjQoICmcq1atUopKSm67bbbJElNmzbVTTfdpH/9618VFREAAAAAAKBUpZU2kZGR6t69u55++mlt27ZNhYWFSkpK0ujRo3Xffff57Ttp0iRNmjTJb+ymm27SSy+9VGG5LS2EAtWjRw/dd999+vTTTzVo0CAdOHBACxYs0IUXXljqMQUFBX6Pd8vKyqqMqACAKio/P19ff/21fvvtN9WqVUvnnHOOzjjjDKtjAQAAoJr484LQf/bH5XBmzJihGTNmnHT/nTt3lj/UaahWhVCvXr00f/58XX311crPz1dRUZEGDx6sWbNmlXrMjBkzNGXKlEpMCQCoqtxut5588knt3r3bN/bTTz/pyiuvVN++fS1MBgAAAFSuavWUsd9++03jx4/XpEmT9NNPP2nRokXauXOnbr755lKPmThxojIzM30ff/whAABgL6tWrSrxvwMfffSR32xSAAAAoKarVjOEZsyYoV69eumuu+6SJHXo0EG1a9fWueeeq2nTpqlBgwbFjnG5XHK5XJUdFQBQgvz8fKWmplp2/W+++UY5OTnFxnNycrR8+XIlJydbkKp6SE5OVnh4uNUxAAAAECTVqhDKy8tTSIh/5BOreJumaUUkAEAAUlNTNXr0aMuun5eXV+pMoClTphR7MgT+Z/bs2WrdurXVMQAAABAklhZCOTk52rp1q+/1jh07tG7dOsXFxalJkyaaOHGi9u7dq3nz5kmSBg8erNGjR+vFF1/UgAEDtH//fv3zn/9Ut27d1LBhQ6veBgCgjJKTkzV79mzLrn/gwAHNnj1bXq/XbzwlJUU33HDDaZ83NTVV06ZN0wMPPFBjZxnV1PcFAACsxeSOwAXrc2ZpIbR69Wqdf/75vtcTJkyQJA0fPlxz587V/v37tWvXLt/2ESNGKDs7WzNnztQdd9yh2NhYXXDBBTx2HgCqifDwcEtnmbRu3VpRUVF69913dfToUUlS+/btNXz4cEVFRZX7/MnJycyiAQAAKIPQ0FBJx2dwR0REWJymenG73ZJU7tnthmmzOi4rK0sxMTHKzMxUdHS01XEAABbwer1KS0tTrVq1FBsbW+7zbdq0SaNHj+a2KgAAgADs379fGRkZql+/vmrVqiXDMKyOVOV5vV7t27dPoaGhatKkSbHPWSCdR7VaQwgAgGBwOBzcagwAAGCxxMREScdv60fZORyOEsugQFEIAQAAAACASmcYhho0aKD69eursLDQ6jjVRlhYmBwOR7nPQyEEAAAAAAAs43Q6edqrBcpfKQEAAAAAAKBaoRACAAAAAACwGQohAAAAAAAAm6EQAgAAAAAAsBkKIQAAAAAAAJuhEAIAAAAAALAZCiEAAAAAAACboRACAAAAAACwGQohAAAAAAAAm6EQAgAAAAAAsBkKIQAAAAAAAJuhEAIA4P8UFRUpMzNTHo/H6igAAABAhQqxOgAAAFXBokWLtGTJEuXm5io6OloDBw7UBRdcYHUsAAAAoEJQCAEAbO/LL7/Uhx9+6HudlZWl9957TxEREerRo4d1wQAAAIAKwi1jAADb+/LLL0scX7p0aSUnAQAAACoHhRAAwPaOHj0a0DgAAABQ3VEIAQBsLyUlJaBxAAAAoLqjEAIA2N7gwYPldDr9xkJDQ3XxxRdblAgAAACoWCwqDQCwvTZt2uiuu+7SkiVLlJ6erkaNGqlfv35KSkqyOhoAAABQISiEAADQ8dvDRo8ebXUMAAAAoFJwyxgAAAAAAIDNUAgBAAAAAADYDIUQAAAAAACAzVAIAQAAAAAA2AyFEAAAAAAAgM1QCAEAAAAAANgMhRAAAAAAAIDNUAgBAAAAAADYDIUQAAAAAACAzYRYHQAAgOrI7XZr1apV2rRpk3Jzc1VUVGR1JAAAAKDMKIQAAAiQ2+3W008/rR07dkiScnJylJ2drfXr16t169YWpwMAAABOjVvGAAAI0Lfffusrg/5oyZIlzBQCAABAtUAhBABAgH7//fcSx/Py8rRnz55KTgMAAAAEjlvGAKCKSk9PV0ZGhtUxqq2CggJt27ZNktSiRQuFhYUF7dy5ubnKycnxvT527Jjvf/fv36+CgoKgXQsVJzY2VgkJCVbHAAAAsIRhmqZpdYjKlJWVpZiYGGVmZio6OtrqOABQovT0dF0/dKgK3G6ro1RLbrdbeXl5OvGfOMMwVLt2bYWGhgbl/EVFRcrOzi42HhoaqsjIyKBcAxXPFRamN+fPpxQCAAA1RiCdBzOEAKAKysjIUIHbrTHtctWwtsfqONVKrtvU6z+75f3ThKAQR4ZGnBWm8BAjKNf59YD07W6PCjzHS6eGUQ4NaO5R7bCsoJwfFWtfrlMv/nr87xqFEAAAsCMKIQCowhrW9qhpNIVQIL7b41W4s+TJr3kFRTojLjjL5zWNlgY0dWhfjlQ7VKpXy5DkDcq5AQAAgIpGIQQAqFGKTnIjdFGQ+5pQp6HkmOCeEwAAAKgMPGUMAFCjtKlb8i1hhiG1LmVbVeU1TeUWmvLaa7k/AAAAVAJmCAEAapR6tQz1a+rQFzv8pwP9tZlTseHVpxD6drdXX+/2KsdtKirM0HlNHDq7Mb/HAQAAQHBQCAEAapzeyU61quvQrwe9MiSdWd+h+rWrTxn04z6vPtv2v7Wjst2mPtnqUViI1DmRUggAAADlRyEEAKiRGkQaahDptDrGafl2T8mLHX2720shBAAAgKDgX5UAAFQxGfklrxmUkV/JQQAAAFBjUQgBAFDFNIoq+fa2xtGVHAQAAAA1FoUQAABVzAUpDjn/1Ak5HdL5KdXzFjgAAABUPawhBABAFdOsjkN/72Tom91eHcg1Vb+2oXOSHGocXX0WxgYAAEDVRiEEAEAVlBRt6Jp2zAgCAABAxeCWMQAAAAAAAJuhEAIAAAAAALAZbhkDgCpsXy69PVAR+LsFAADsjkIIAKqwF3+NtDoCAAAAgBqIQggAqrAx7XLUsLbX6hhAjbMv10HhCgAAbI1CCACqsIa1vWoa7bE6BgAAAIAahhvoAQAAAAAAbIYZQgAAVLDtR71ascurA7lS/drSOUkOtYjjdzIAAACwjqX/Gl2xYoUGDx6shg0byjAMffjhh6c8pqCgQPfff7+Sk5PlcrmUkpKi1157reLDAgBwGrYe8Wrueo+2HTWV7Ta17aip19d7tPkwa0MBAADAOpbOEMrNzVXHjh01atQoXX755WU65qqrrlJ6erpeffVVtWjRQvv375fXyz+qAQBV0/JUr0yz+PhXqV61qsssIQAAAFjD0kJo0KBBGjRoUJn3X7RokZYvX67t27crLi5OkpSSklJB6QAAKL+03FLGc0poiQAAAIBKUq1+NfnRRx+pa9eueuyxx9SoUSO1atVKd955p44dO1bqMQUFBcrKyvL7AACgssRFlDZuVG4QAAAA4A+qVSG0fft2rVy5Ur/88os++OADPfPMM1qwYIHGjh1b6jEzZsxQTEyM7yMpKakSEwMA7O7cpJL/U1vaOAAAAFAZqtW/Rr1erwzD0Pz589WtWzddeOGFeuqpp/T666+XOkto4sSJyszM9H3s3r27klMDAOysfX2HrmjjVN3/mxEUF2HostZOnZVYrf4TDAAAgBqmWj12vkGDBmrUqJFiYmJ8Y2eccYZM09SePXvUsmXLYse4XC65XK7KjAkAgJ+zEh06K9Ehj9eU08GtYgAAALBetfr1ZK9evbRv3z7l5OT4xjZv3iyHw6HGjRtbmAwAgFOjDAIAAEBVYWkhlJOTo3Xr1mndunWSpB07dmjdunXatWuXpOO3ew0bNsy3/3XXXae6detq5MiR+u2337RixQrdddddGjVqlCIiSlm1EwAAAAAAAH4sLYRWr16tTp06qVOnTpKkCRMmqFOnTpo0aZIkaf/+/b5ySJIiIyO1ZMkSZWRkqGvXrho6dKgGDx6s5557zpL8AAAAAAAA1ZGlawj16dNHpmmWun3u3LnFxtq0aaMlS5ZUYCoAAAAAAICarVqtIQQAAAAAAIDyoxACACAI3B5THm/ps14BAACAqqRaPXYeAICqJjXT1KJtHu3JMhXqlDolODSguUNhTp4oBgAAgKqLQggAgNN05Jip19cXqdBz/HWhR/phn1e5hdI17ZzWhgMAAABOglvGAAA4TT/s8/rKoD/69aBXGfncPgYAAICqi0IIAIDTlJFf+rajFEIAAACowrhlDACAUmQVmPo53VReoalmdQy1qGPIMP63NlDDKEO/Hix+XIhDSqjNGkIAAACouiiEAAAowbajXs3f4FGh9/jrlbulNnUduqadQ07H8bKnawNDP+4zit0e1rOxQ7VCKYQAAABQdVEIAUAVti+XhYmtYJqm3tjgUbbbv9RZk24qrrahNvX+9+cyoIVTP+33aFemV+EhhtrGO9SinlM7sio7NQLB3y0AAGB3FEIAUAXFxsbKFRamF3+1Ook9FRUVKTs7u8Rt2zeEqXbt2qUe+3OOpB0VFAxB5QoLU2xsrNUxAAAALEEhBABVUEJCgt6cP18ZGRlWR7GlgwcP6qWXXipxW/v27XXZZZf5jaWmpmratGl64IEHlJycXBkREQSxsbFKSEiwOgYAAIAlKIQAoIpKSEjgh1WLtG7dWsuWLdOePXuKbbvooovUunXrEo9LTk4udRsAAABQlfDYeQAASnDjjTeqXr16vteGYWjgwIE688wzLUwFAAAABAczhAAAKEFiYqKmTp2q33//XdnZ2WrVqpXi4uKsjgUAAAAEBYUQAAClcDgcateundUxAAAAgKDjljEAgC0cOXJEaWlpMk3T6igAAACA5ZghBACo0Y4cOaK5c+dq8+bNkqT4+HgNHTpUbdq0sTgZAAAAYB1mCAEAarQXXnjBVwZJxx8p/8ILL+jIkSMWpgJQXeTm5uqLL77QvHnztHjxYuXk5FgdCQCAoGCGEACg0uTn5ys1NbXSrrd7925t3LixxG0LFy7UueeeG5TrnHhPlfneKltycrLCw8OtjgFUqsOHD+uJJ57Q0aNHfWNffPGF7rjjDiUkJFiYDACA8jNMmy2mkJWVpZiYGGVmZio6OtrqOABgK5s2bdLo0aMr7Xput1u5ubklbnO5XKpVq1alZanuZs+erdatW1sdA6hUc+bM0ffff19svFOnTrrpppssSAQAwMkF0nkwQwgAUGmSk5M1e/bsSrtedna2nn/+eXk8nmLbrrjiCrVt27bSslR3ycnJVkeAjVX27MITvv32Wx07dqzY+KpVq9SnT59Kz1MdMbsQAKouCiEAQKUJDw+v9Fkml19+uT777DO/sZYtW+qSSy6R0+ms1CwATk9qamqlzi48ITMzU16vt9i4w+GwJE91xOxCAKi6uGUMAFDj/fzzz/ruu++Un5+vM888U+ecc47CwsKsjgWgjKyaIbRixQotX7682HjPnj3Vt2/foFwjNTVV06ZN0wMPPBDUmXgZGRn6+eeflZ2draSkJLVr104hIZX/u2BmCAFA5eKWMQAA/qBjx47q2LGj1TEAnCYrZhdKUvPmzRUSEuK3jlDnzp01YsSIoJfKycnJQXuPGzdu1Pz58+V2uyVJW7Zs0fbt23X77bfL5XIF5RoAgOqPQggAAAAoQUhIiEaOHKnBgwdr3759SkxMVP369a2OdUpvv/22rww6YefOnVq2bJkGDBhgUSoAQFXjsDoAAAAAUJXVq1dPHTp0qBZl0KFDh5Senl7itl9++aWS0wAAqjIKIQAAAKCGONktYdwuBgD4IwohAAAAoIaIiopS+/btS9zWs2fPSk4DAKjKKIQAAACAGuSGG25Q06ZNfa+dTqcGDhyozp07W5gKAFDVsKg0AAAAUIPExMTonnvuUWpqqjIzM5WSknLKRw8DAOyHQggAAACogZKTk62OAACowrhlDAAAAAAAwGYohAAAAAAAAGyGQggAAAAAAMBmKIQAAAAAAABshkIIAAAAAADAZiiEAAAAAAAAbIZCCAAAAAAAwGYohAAAAAAAAGyGQggAAAAAAMBmKIQAAAAAAABshkIIAAAAAADAZiiEAAAAAAAAbIZCCAAAAAAAwGYohAAAAAAAAGyGQggAAAAAAMBmKIQAAAAAAABshkIIAAAAAADAZiiEAAAAgBokLy9PRUVFVscAAFRxIVYHAAAAAFB+v//+u/7zn/9oz549crlcOuecczRkyBCFhoZaHQ0AUAVRCAEAAADV3J49ezRz5kx5PB5JUkFBgZYuXapjx45p2LBhFqcDAFRF3DIGAAAAVHPLly/3lUF/9P333ysnJ8eCRACAqo5CCAAAAKjmDh06VOK4x+PR0aNHKzkNAKA6oBACAAAAqrnk5OQSxyMiIpSQkFDJaQAA1QGFEAAAAFDNnX/++YqJiSk2PnDgQIWFhVmQCABQ1bGoNAAAAFDNxcTE6O6779aiRYu0efNmRUdHq3fv3uratavV0QAAVRSFEAAAQA2Snp6ujIwMq2OgjFJTU/3+t7y6du3qVwJt2rQpKOfF/8TGxnIbHoAawTBN07Q6RGXKyspSTEyMMjMzFR0dbXUcAACAoElPT9fQ64fKXeC2OgpQY4W5wjT/zfmUQgCqpEA6D0tnCK1YsUKPP/64fvrpJ+3fv18ffPCBhgwZUqZjv/nmG/Xu3Vvt27fXunXrKjQnAABAdZCRkSF3gVvebl6Z0bb6nR9QKYwsQ+4f3MrIyKAQAlDtWVoI5ebmqmPHjho1apQuv/zyMh+XkZGhYcOGqW/fvkpPT6/AhAAAANWPGW1KdaxOAdQ8pihaAdQclhZCgwYN0qBBgwI+7uabb9Z1110np9OpDz/8MPjBAAAAAAAAarBq99j5OXPmaPv27Zo8eXKZ9i8oKFBWVpbfBwAAAAAAgJ1Vq0Joy5Ytuvfee/Xmm28qJKRsk5tmzJihmJgY30dSUlIFpwQAAAAAAKjaqk0h5PF4dN1112nKlClq1apVmY+bOHGiMjMzfR+7d++uwJQAAAAAAABVn6VrCAUiOztbq1ev1tq1a3XrrbdKkrxer0zTVEhIiBYvXqwLLrig2HEul0sul6uy4wIAAAAAAFRZ1aYQio6O1oYNG/zGXnjhBX355ZdasGCBmjZtalEyAAAAoGyKjhTJm+2VI9KhkLrV5p/iAIAayNL/CuXk5Gjr1q2+1zt27NC6desUFxenJk2aaOLEidq7d6/mzZsnh8Oh9u3b+x1fv359hYeHFxsHAAAAqhKzyFTej3kqOlzkGwuJC1Gtv9SSEWpYmAwAYFeWriG0evVqderUSZ06dZIkTZgwQZ06ddKkSZMkSfv379euXbusjAgAAACUW8GmAr8ySDo+Wyh/U75FiQAAdmfpDKE+ffrINM1St8+dO/ekxz/00EN66KGHghsKAAAACLLCfYUljhftLZKY7A4AsEC1ecoYAAAAUG2V8jvQk/1yFACAisRKdgAAlIPH49H69et1+PBh1a1bVx06dJDT6bQ6FoAqJiQxRO5d7mLjoQ1CLUgDAACFEACghtu2bZtWrFihzMxMtWzZUr1791ZkZGRQzr18+XLNmjVLaWlpvrHExETdcsst6t27d1CuAaBmcLV2yXPUI0+2xzfmjHTK1dplYSoAgJ1RCAEAaqzvvvvObz26jRs36rvvvtM999xT7lJo+fLlmjRpknr06KHJkyeradOm2rFjh9544w1NmjRJU6dOpRQC4ONwOVT73NoqSiuSJ9sjZ6RTIQ1CZDh4whgAwBqsIQQAqJE8Ho8++OCDYuMHDx7U8uXLy33uWbNmqUePHpo+fbratWunWrVqqV27dpo+fbp69OihF154QR6P59QnA2AbhsNQaMNQhbcOV2ijUMogAIClKIQAADXSgQMHlJmZWeK2zZs3l+vc69evV1pamm644QY5HP7/KXU4HLr++uu1f/9+rV+/vlzXAQAAACoKhRAAoEaKjIyUYZT82/fo6Ohynfvw4cOSpKZNm5a4vVmzZn77AQAAAFUNhRAAoEaKiopSly5dStx23nnnlevcdevWlSTt2LGjxO3bt2/32w8AAACoaiiEAAA11vXXX68uXbr4ZgpFR0dr+PDhatmyZbnO26FDByUmJuqNN96Q1+v12+b1evXmm2+qQYMG6tChQ7muAwAAAFQUnjIGAKixwsPDNXr0aGVnZys7O1v169dXSEj5/9PndDp1yy23aNKkSbrvvvt0/fXXq1mzZtq+fbvefPNNrVq1SlOnTpXT6QzCuwAAAACCj0IIAFDjRUVFKSoqKqjn7N27t6ZOnapZs2Zp7NixvvEGDRrwyHkAAABUeRRCAACcpt69e+ucc87R+vXrdfjwYdWtW1cdOnRgZhAAAACqPAohAADKwel0qlOnTlbHAAAAAALCotIAAAAAAAA2QyEEAAAAAABgMxRCAAAAAAAANkMhBAAAAAAAYDMUQgAAAAAAADZDIQQAAAAAAGAzFEIAAAAAAAA2QyEEAAAAAABgMxRCAAAAAAAANkMhBAAAAAAAYDMUQgAAAAAAADZDIQQAAAAAAGAzFEIAAAAAAAA2QyEEAAAAVFFmkSlPlkdmoWl1FABADRNidQAAAAAA/kzTVMGmArl3uGV6TBlOQ2EpYXK1cckwDKvjAQBqAGYIAQAAAFWMe6dbBVsLZHqOzwwyPaYKthXIvd1tcTIAQE1BIQQAAABUMYU7C0scd++kEAIABAeFEAAAAFDFeN3eEsfNAtYSAgAEB4UQAAAAUMWExJW81KezrrOSkwAAaioKIQAAAKCKcbV2yQjxXzzacBoKbx1uUSIAQE3DU8YAAABqmiyrA6C8nHIqskOk3Hvd8uR45KztVGijUDlNp3TU6nQ2xt8tADUIhRAAAEAN4/yB24pqAqecClXo8ReHJKWWvq9pmvJ6vTIMQw4HNwEAAE6NQggAAKCG8XTzSNFWp0BlKTpcpGNbjsmb75UhQ844pyJaR8jhohgKuiwKVwA1B4UQAABATRMtqY7VIVAZvLle5W3Ok+k1pVDJlKmi7CId23ZMtXvVtjoeAKAK49cGAAAAQDXl3u0+Xgb9SdHRInmyPBYkAgBUFxRCAAAAQDVlFhQvg3zb8kvfBgAAhRAAAABQTTnjSl7PxnAYcsTyT30AQOn4rwQAAABQTYU2DFVIbPFlQV2tXHKE8U99AEDpWFQaAAAAqKYMp6FaZ9eSe5dbRQeKZIQYCk0KVWhCqNXRAABVHIUQAAAAUI0ZIYZczVxyNXNZHQUAUI0wjxQAAAAAAMBmKIQAAACAKsyb75Un0yPTw1PDAADBwy1jAAAAQBVkFpk69vMxFe0vkilTjlCHXK1dCksJszoaAKAGYIYQAAAAUAXl/5Kvwv2FMnV8ZpC30KtjvxxT0YEii5MBAGoCCiEAAACgijELTRXuKyxxmzvVXclpAAA1EYUQAAAAUMWYhaZMb8lrBplu1hICAJQfhRAAAABQxRgRhhy1Sv6nurOus5LTAABqIgohAAAAoIoxDEPhbcNlGIbfuKOWQ2FNWVQaAFB+PGUMAAAAqIJCE0PlOMch9063vPlehcSFKDQ5VI4wfqcLACg/CiEAAACginLGOBXRMcLqGACAGohfLwAAAAAAANgMhRAAAAAAAIDNUAgBAAAAAADYDIUQAAAAAACAzVAIAQAAAAAA2IylhdCKFSs0ePBgNWzYUIZh6MMPPzzp/gsXLlT//v0VHx+v6Oho9ejRQ59//nnlhAUAAAAAAKghLC2EcnNz1bFjR82aNatM+69YsUL9+/fXp59+qp9++knnn3++Bg8erLVr11ZwUgAAAAAAgJojxMqLDxo0SIMGDSrz/s8884zf6+nTp+v//b//p48//lidOnUKcjoAAAAAAICaydJCqLy8Xq+ys7MVFxdX6j4FBQUqKCjwvc7KyqqMaAAAAAAAAFVWtV5U+oknnlBOTo6uuuqqUveZMWOGYmJifB9JSUmVmBAAAAAAAKDqqbaF0FtvvaUpU6bovffeU/369Uvdb+LEicrMzPR97N69uxJTAgAAAAAAVD3V8paxd955RzfeeKPef/999evX76T7ulwuuVyuSkoGAAAAAABQ9VW7GUJvv/22Ro4cqbffflsXXXSR1XEAAAAAAACqHUtnCOXk5Gjr1q2+1zt27NC6desUFxenJk2aaOLEidq7d6/mzZsn6fhtYsOHD9ezzz6r7t27Ky0tTZIUERGhmJgYS94DAAAAAABAdWPpDKHVq1erU6dOvkfGT5gwQZ06ddKkSZMkSfv379euXbt8+7/88ssqKirSLbfcogYNGvg+xo8fb0l+AAAAAACA6sjSGUJ9+vSRaZqlbp87d67f62XLllVsIAAAAAAAABuodmsIAQAAAAAAoHwCKoSKioo0depU7dmzp6LyAAAAAAAAoIIFVAiFhITo8ccfV1FRUUXlAQAAAAAAQAUL+JaxCy64QMuXL6+ILAAAAAAAAKgEAS8qPWjQIN17773asGGDunTpotq1a/ttv+SSS4IWDgAAAAAAAMEXcCE0duxYSdJTTz1VbJthGPJ4POVPBQAAAAAAgAoTcCHk9XorIgcAAAAAAAAqScCFEAAAAKo2I8uQKdPqGECNY2QZVkcAgKA5rUJo+fLleuKJJ/T7779Lktq2bau77rpL5557blDDAQAAoOxiY2MV5gqT+we31VGAGivMFabY2FirYwBAuRmmaQb066M333xTI0eO1OWXX65evXpJkr755ht98MEHmjt3rq677roKCRosWVlZiomJUWZmpqKjo62OAwAAEFTp6enKyMiwOgbKKDU1VdOmTdMDDzyg5ORkq+OgDGJjY5WQkGB1DAAoUSCdR8AzhB555BE99thjuv32231jt912m5566ik9/PDDVb4QAgAAqMkSEhL4YbUaSk5OVuvWra2OAQCwEUegB2zfvl2DBw8uNn7JJZdox44dQQkFAAAAAACAihNwIZSUlKSlS5cWG//iiy+UlJQUlFAAAAAAAACoOAHfMnbHHXfotttu07p169SzZ09Jx9cQmjt3rp599tmgBwQAAAAAAEBwBVwIjRkzRomJiXryySf13nvvSZLOOOMMvfvuu7r00kuDHhAAAACo7n766Sd98cUXOnTokJKTk3XhhReqWbNmVscCANhYQIVQUVGRpk+frlGjRmnlypUVlQkAAACoMVatWqXXX3/d9/qXX37Rxo0bdeedd1qYCgBgdwGtIRQSEqLHHntMRUVFFZUHAAAAqDFM09Qnn3xSbLyoqEiff/65BYkAADgu4EWl+/btq+XLl1dEFgAAAKBGKSgo0KFDh0rctmfPnkpOAwDA/wS8htCgQYN07733asOGDerSpYtq167tt/2SSy4JWjgAAACgOnO5XIqJiVFmZmaxbQkJCRYkAgDguIALobFjx0qSnnrqqWLbDMOQx+MpfyoAAACgBjAMQ3/961/1/vvvFxvv37+/RakAADiNQsjr9VZEDgAAAKBG6tu3r0JCQrR48WIdPnxYTZo00cUXX6zWrVtr06ZNVscDANhUQIVQYWGhIiIitG7dOrVv376iMgEAAAA1Su/evdW7d2+ZpinDMKyOAwBAYItKh4aGqkmTJtwWBgAAAJwGyiAAQFUR8FPG7r//ft133306cuRIReQBAAAAAABABQt4DaGZM2dq69atatiwoZKTk4s9ZWzNmjVBCwcAAABUVdz+BQCozgIuhIYMGVIBMQAAAICqz+Px6NNPP9WKFSuUnZ2tVq1aaciQIWrWrJnV0QAACEjAhdDkyZMrIgcAAABQ5b3//vtatmyZ7/XmzZv1zDPP6P7771dCQoJ1wQAACFCZ1xD64YcfTrqYdEFBgd57772ghAIAAACqmpycHK1cubLYuNvt9iuJAACoDspcCPXo0UOHDx/2vY6Ojtb27dt9rzMyMnTttdcGNx0AAABQRRw9elRFRUUlbktPT6/kNAAAlE+ZCyHTNE/6urQxAAAAoCaIj49XWFhYidsaN25cyWkAACifgB87fzI8ZQEAAAA1VXh4uPr27VtsvHbt2urTp0/lBwIAoBwCXlQaAAAAsKtLL71UsbGx+vrrr5WVlaVWrVrp4osvVlxcnNXRAAAISECF0G+//aa0tDRJx28P27hxo3JyciRJhw4dCn46AAAAoIrp3bu3evfubXUMAADKJaBCqG/fvn7rBF188cWSjt8qZpomt4wBAAAAAABUA2UuhHbs2FGROQAAAAAAAFBJylwIJScnV2QOAAAAAAAAVJKgPmUMAAAAAAAAVR+FEAAAAAAAgM1QCAEAAAAAANgMhRAAAAAAAIDNUAgBAAAAFsnPz1dBQYG++uorrV+/XqZpWh0JAGATZXrKWKdOnWQYRplOuGbNmnIFAgAAAOxgz549mjVrlvLy8rRy5UqtW7dOLVu21Lhx4xQWFmZ1PABADVemQmjIkCG+/5+fn68XXnhBbdu2VY8ePSRJ3333nX799VeNHTu2QkICAAAANc38+fOVl5fnN7ZlyxZ99dVXGjBggEWpAAB2UaZCaPLkyb7/f+ONN+q2227Tww8/XGyf3bt3BzcdAAAAbC8/P1+pqalWxwiq3NxcbdiwQceOHZMk3/9K0pdffqmUlBSLkgVXcnKywsPDrY4BACiBYQZ4o3JMTIxWr16tli1b+o1v2bJFXbt2VWZmZlADBltWVpZiYmKUmZmp6Ohoq+MAAADgFDZt2qTRo0dbHSOovF5vqf9uDgkJUVRUVCUnqhizZ89W69atrY4BALYRSOdRphlCfxQREaFvvvmmWCH0zTff0P4DAAAg6JKTkzV79myrYwTdW2+9pW3bthUbv/DCC9WlSxcLEgVfcnKy1REAAKUIuBD65z//qTFjxmjNmjXq1q2bJOn777/Xa6+9pgcffDDoAQEAAGBv4eHhNXKWyfjx4/X8889r//79vrGzzz5b1157bZkf6AIAwOkK+JYxSXrvvff07LPP6vfff5cknXHGGRo/fryuuuqqoAcMNm4ZAwAAQFVhmqY2btyoI0eOqFmzZmrQoIHVkQAA1VggncdpFULVGYUQAAAAAACoiQLpPBync4GMjAy98soruu+++3TkyBFJ0po1a7R3797TOR0AAAAAAAAqUcBrCK1fv179+vVTTEyMdu7cqRtvvFFxcXFauHChdu3apXnz5lVETgAAAAAAAARJwDOEJkyYoBEjRmjLli1+TxW78MILtWLFiqCGAwAAAAAAQPAFXAj9+OOPuummm4qNN2rUSGlpaUEJBQAAAAAAgIoTcCHkcrmUlZVVbHzz5s2Kj48PSigAAAAAAABUnIALoUsuuURTp05VYWGhJMkwDO3atUv33HOPrrjiiqAHBAAAAAAAQHAFXAg9+eSTysnJUf369XXs2DH17t1bLVq0UFRUlB555JGKyAgAAAAAAIAgCvgpYzExMVqyZIm++eYb/fzzz8rJyVHnzp3Vr1+/isgHAAAAAACAIAuoECosLFRERITWrVunXr16qVevXhWVCwAAAAAAABUkoFvGQkND1aRJE3k8nqBcfMWKFRo8eLAaNmwowzD04YcfnvKYZcuWqXPnznK5XGrRooXmzp0blCwAAAAAAAB2EfAaQvfff7/uu+8+HTlypNwXz83NVceOHTVr1qwy7b9jxw5ddNFFOv/887Vu3Tr985//1I033qjPP/+83FkAAAAAAADswjBN0wzkgE6dOmnr1q0qLCxUcnKyateu7bd9zZo1pxfEMPTBBx9oyJAhpe5zzz336JNPPtEvv/ziG7vmmmuUkZGhRYsWlek6WVlZiomJUWZmpqKjo08rKwAAAAAAQFUTSOcR8KLSJytsKtqqVauKLV49YMAA/fOf/yz1mIKCAhUUFPheZ2VlVVQ8AAAAAACAaiHgQmjy5MkVkaNM0tLSlJCQ4DeWkJCgrKwsHTt2TBEREcWOmTFjhqZMmVJZEQEAAAAAAKq8gNcQqm4mTpyozMxM38fu3butjgQAAAAAAGCpgGcIeTwePf3003rvvfe0a9cuud1uv+3BWGy6NImJiUpPT/cbS09PV3R0dImzgyTJ5XLJ5XJVWCYAAAAAAIDqJuAZQlOmTNFTTz2lq6++WpmZmZowYYIuv/xyORwOPfTQQxUQ8X969OihpUuX+o0tWbJEPXr0qNDrAgAAAAAA1CQBF0Lz58/X7NmzdccddygkJETXXnutXnnlFU2aNEnfffddQOfKycnRunXrtG7dOknHHyu/bt067dq1S9Lx272GDRvm2//mm2/W9u3bdffdd2vjxo164YUX9N577+n2228P9G0AAAAAAADYVsCFUFpams4880xJUmRkpDIzMyVJF198sT755JOAzrV69Wp16tRJnTp1kiRNmDBBnTp10qRJkyRJ+/fv95VDktS0aVN98sknWrJkiTp27Kgnn3xSr7zyigYMGBDo2wAAAAAAALCtgNcQaty4sfbv368mTZqoefPmWrx4sTp37qwff/wx4LV6+vTpI9M0S90+d+7cEo9Zu3ZtoLEBAAAAAADwfwKeIXTZZZf51vEZN26cHnzwQbVs2VLDhg3TqFGjgh4QAAAAAAAAwWWYJ5uiUwarVq3SqlWr1LJlSw0ePDhYuSpMVlaWYmJilJmZqejoaKvjAAAAAAAABEUgnUfAt4z9WY8ePXjKFwAAAAAAQDUScCE0b968k27/41PBAAAAAAAAUPUEfMtYnTp1/F4XFhYqLy9PYWFhqlWrlo4cORLUgMHGLWMAAAAAAKAmCqTzCHhR6aNHj/p95OTkaNOmTTrnnHP09ttvn3ZoAAAAAAAAVI6AC6GStGzZUo8++qjGjx8fjNMBAAAAAACgAgWlEJKkkJAQ7du3L1inAwAAAAAAQAUJeFHpjz76yO+1aZrav3+/Zs6cqV69egUtGAAAAAAAACpGwIXQkCFD/F4bhqH4+HhdcMEFevLJJ4OVCwAAAAAAABUk4ELI6/VWRA4AAAAAAABUkqCtIQQAAAAAAIDqIeAZQhMmTCjzvk899VSgpwcAAAAAAEAFC7gQWrt2rdauXavCwkK1bt1akrR582Y5nU517tzZt59hGMFLCQAAAAAAgKAJuBAaPHiwoqKi9Prrr6tOnTqSpKNHj2rkyJE699xzdccddwQ9JAAAAAAAAILHME3TDOSARo0aafHixWrXrp3f+C+//KK//vWv2rdvX1ADBltWVpZiYmKUmZmp6Ohoq+MAAAAAAAAERSCdR8CLSmdlZengwYPFxg8ePKjs7OxATwcAAAAAAIBKFnAhdNlll2nkyJFauHCh9uzZoz179ug///mP/v73v+vyyy+viIwAAAAAAAAIooDXEHrppZd055136rrrrlNhYeHxk4SE6O9//7sef/zxoAcEAAAAAABAcAW8htAJubm52rZtmySpefPmql27dlCDVRTWEAIAAAAAADVRha4hdELt2rXVoUMHxcTEKDU1VV6v93RPBQAAAAAAgEpU5kLotdde01NPPeU39o9//EPNmjXTmWeeqfbt22v37t1BDwgAAAAAAIDgKnMh9PLLL6tOnTq+14sWLdKcOXM0b948/fjjj4qNjdWUKVMqJCQAAAAAAACCp8yLSm/ZskVdu3b1vf5//+//6dJLL9XQoUMlSdOnT9fIkSODnxAAAAAAAABBVeYZQseOHfNbkOjbb7/Veeed53vdrFkzpaWlBTcdAAAAAAAAgq7MhVBycrJ++uknSdKhQ4f066+/qlevXr7taWlpiomJCX5CAAAAAAAABFWZbxkbPny4brnlFv3666/68ssv1aZNG3Xp0sW3/dtvv1X79u0rJCQAAAAAAACCp8yF0N133628vDwtXLhQiYmJev/99/22f/PNN7r22muDHhAAAAAAAADBZZimaVodojJlZWUpJiZGmZmZfmsiAQAAAAAAVGeBdB5lXkMIAAAAAAAANQOFEAAAAAAAgM1QCAEAAAAAANgMhRAAAAAAAIDNUAgBAAAAAADYTJkfO3+Cx+PR3LlztXTpUh04cEBer9dv+5dffhm0cAAAAAAAAAi+gAuh8ePHa+7cubrooovUvn17GYZREbkAAAAAAABQQQIuhN555x299957uvDCCysiDwAAAAAAACpYwGsIhYWFqUWLFhWRBQAAAAAAAJUg4ELojjvu0LPPPivTNCsiDwAAAAAAACpYwLeMrVy5Ul999ZU+++wztWvXTqGhoX7bFy5cGLRwAAAAAAAACL6AC6HY2FhddtllFZEFAAAAAAAAlSDgQmjOnDkVkQMAAAAAAACVJOA1hAAAAAAAAFC9BTxDSJIWLFig9957T7t27ZLb7fbbtmbNmqAEAwAAAAAAQMUIeIbQc889p5EjRyohIUFr165Vt27dVLduXW3fvl2DBg2qiIwAAAAAAAAIooALoRdeeEEvv/yynn/+eYWFhenuu+/WkiVLdNtttykzM7MiMgIAAAAAACCIAi6Edu3apZ49e0qSIiIilJ2dLUm64YYb9Pbbbwc3HQAAAAAAAIIu4EIoMTFRR44ckSQ1adJE3333nSRpx44dMk0zuOkAAAAAAAAQdAEXQhdccIE++ugjSdLIkSN1++23q3///rr66qt12WWXBT0gAAAAAAAAgsswA5zW4/V65fV6FRJy/AFl77zzjr799lu1bNlSN910k8LCwiokaLBkZWUpJiZGmZmZio6OtjoOAAAAAABAUATSeQRcCFV3FEIAAAAAAKAmCqTzCPiWMUn6+uuvdf3116tHjx7au3evJOmNN97QypUrT+d0AAAAAAAAqEQBF0L/+c9/NGDAAEVERGjt2rUqKCiQJGVmZmr69OlBDwgAAAAAAIDgCrgQmjZtml566SXNnj1boaGhvvFevXppzZo1QQ0HAAAAAACA4Au4ENq0aZPOO++8YuMxMTHKyMgIRiYAAAAAAABUoIALocTERG3durXY+MqVK9WsWbOghAIAAAAAAEDFCbgQGj16tMaPH6/vv/9ehmFo3759mj9/vu68806NGTOmIjICAAAAAAAgiEICPeDee++V1+tV3759lZeXp/POO08ul0t33nmnxo0bVxEZAQAAAAAAEEQBzxAyDEP333+/jhw5ol9++UXfffedDh48qIcffvi0Q8yaNUspKSkKDw9X9+7d9cMPP5x0/2eeeUatW7dWRESEkpKSdPvttys/P/+0rw8AAAAAAGAnAc8QOiEsLExt27Ytd4B3331XEyZM0EsvvaTu3bvrmWee0YABA7Rp0ybVr1+/2P5vvfWW7r33Xr322mvq2bOnNm/erBEjRsgwDD311FPlzgMAAAAAAFDTGaZpmmXZcdSoUWU64WuvvRZQgO7du+svf/mLZs6cKUnyer1KSkrSuHHjdO+99xbb/9Zbb9Xvv/+upUuX+sbuuOMOff/991q5cuUpr5eVlaWYmBhlZmYqOjo6oKwAAAAAAABVVSCdR5lvGZs7d66++uorZWRk6OjRo6V+BMLtduunn35Sv379/hfI4VC/fv20atWqEo/p2bOnfvrpJ99tZdu3b9enn36qCy+8sMT9CwoKlJWV5fcBAAAAAABgZ2W+ZWzMmDF6++23tWPHDo0cOVLXX3+94uLiynXxQ4cOyePxKCEhwW88ISFBGzduLPGY6667TocOHdI555wj0zRVVFSkm2++Wffdd1+J+8+YMUNTpkwpV04AAAAAAICapMwzhGbNmqX9+/fr7rvv1scff6ykpCRdddVV+vzzz1XGu86CYtmyZZo+fbpeeOEFrVmzRgsXLtQnn3xS6qLWEydOVGZmpu9j9+7dlZYVAAAAAACgKgpoUWmXy6Vrr71W1157rVJTUzV37lyNHTtWRUVF+vXXXxUZGRnQxevVqyen06n09HS/8fT0dCUmJpZ4zIMPPqgbbrhBN954oyTpzDPPVG5urv7xj3/o/vvvl8Ph33G5XC65XK6AcgEAAAAAANRkAT923negwyHDMGSapjwez2mdIywsTF26dPFbINrr9Wrp0qXq0aNHicfk5eUVK32cTqckVepMJQAAAAAAgOoqoEKooKBAb7/9tvr3769WrVppw4YNmjlzpnbt2hXw7KATJkyYoNmzZ+v111/X77//rjFjxig3N1cjR46UJA0bNkwTJ0707T948GC9+OKLeuedd7Rjxw4tWbJEDz74oAYPHuwrhgAAAAAAAFC6Mt8yNnbsWL3zzjtKSkrSqFGj9Pbbb6tevXrlDnD11Vfr4MGDmjRpktLS0nTWWWdp0aJFvoWmd+3a5Tcj6IEHHpBhGHrggQe0d+9excfHa/DgwXrkkUfKnQUAAAAAAMAODLOM91k5HA41adJEnTp1kmEYpe63cOHCoIWrCFlZWYqJiVFmZqaio6OtjgMAAAAAABAUgXQeZZ4hNGzYsJMWQQAAAAAAAKgeylwIzZ07twJjAAAAAAAAoLKc9lPGAAAAAAAAUD1RCAEAAAAAANgMhRAAAAAAAIDNUAgBAAAAAADYDIUQAAAAAACAzVAIAQAAAAAA2AyFEAAAAAAAgM1QCAEAAAAAANgMhRAAAAAAAIDNUAgBAAAAAADYDIUQAAAAAACAzVAIAQAAAAAA2AyFEAAAAAAAgM1QCAEAAAAAANgMhRAAAAAAAIDNUAgBAAAAAADYDIUQAAAAAACAzVAIAQAAAAAA2AyFEAAAAAAAgM1QCAEAAAAAANgMhRAAAAAAAIDNUAgBAAAAAADYDIUQAAAAAACAzVAIAQAAAAAA2AyFEAAAAAAAgM1QCAEAAAAAANgMhRAAAAAAAIDNUAgBAAAAAADYDIUQAAAAAACAzVAIAQAAAAAA2AyFEAAAAAAAgM1QCAEAAAAAANgMhRAAAAAAAIDNUAgBAAAAAADYDIUQAAAAAACAzVAIAQAAAAAA2AyFEAAAAAAAgM1QCAEAAAAAANgMhRAAAAAAAIDNUAgBAAAAAADYDIUQAAAAAACAzVAIAQAAAAAA2AyFEAAAAAAAgM1QCAEAAAAAANgMhRAAAAAAAIDNUAgBAAAAAADYDIUQAAAAAACAzVAIAQAAAAAA2AyFEAAAAAAAgM1QCAEAAAAAANgMhRAAAAAAAIDNUAgBAAAAAADYDIUQAAAAAACAzVAIAQAAAAAA2AyFEAAAAAAAgM1QCAEAAAAAANhMlSiEZs2apZSUFIWHh6t79+764YcfTrp/RkaGbrnlFjVo0EAul0utWrXSp59+WklpAQAAAAAAqrcQqwO8++67mjBhgl566SV1795dzzzzjAYMGKBNmzapfv36xfZ3u93q37+/6tevrwULFqhRo0ZKTU1VbGxs5YcHAAAAAACohgzTNE0rA3Tv3l1/+ctfNHPmTEmS1+tVUlKSxo0bp3vvvbfY/i+99JIef/xxbdy4UaGhoQFfLysrSzExMcrMzFR0dHS58wMAAAAAAFQFgXQelt4y5na79dNPP6lfv36+MYfDoX79+mnVqlUlHvPRRx+pR48euuWWW5SQkKD27dtr+vTp8ng8Je5fUFCgrKwsvw8AAAAAAAA7s7QQOnTokDwejxISEvzGExISlJaWVuIx27dv14IFC+TxePTpp5/qwQcf1JNPPqlp06aVuP+MGTMUExPj+0hKSgr6+wAAAAAAAKhOqsSi0oHwer2qX7++Xn75ZXXp0kVXX3217r//fr300ksl7j9x4kRlZmb6Pnbv3l3JiQEAAAAAAKoWSxeVrlevnpxOp9LT0/3G09PTlZiYWOIxDRo0UGhoqJxOp2/sjDPOUFpamtxut8LCwvz2d7lccrlcwQ8PAAAAAABQTVk6QygsLExdunTR0qVLfWNer1dLly5Vjx49SjymV69e2rp1q7xer29s8+bNatCgQbEyCAAAAAAAAMVZfsvYhAkTNHv2bL3++uv6/fffNWbMGOXm5mrkyJGSpGHDhmnixIm+/ceMGaMjR45o/Pjx2rx5sz755BNNnz5dt9xyi1VvAQAAAAAAoFqx9JYxSbr66qt18OBBTZo0SWlpaTrrrLO0aNEi30LTu3btksPxv94qKSlJn3/+uW6//XZ16NBBjRo10vjx43XPPfdY9RYAAAAAAACqFcM0TdPqEJUpKytLMTExyszMVHR0tNVxAAAAAAAAgiKQzsPyW8YAAAAAAABQuSiEAAAAAAAAbIZCCAAAAAAAwGYohAAAAAAAAGyGQggAAAAAAMBmKIQAAAAAAABshkIIAAAAAADAZiiEAAAAAAAAbIZCCAAAAAAAwGYohAAAAAAAAGyGQggAAAAAAMBmKIQAAAAAAABshkIIAAAAAADAZiiEAAAAAAAAbIZCCAAAAAAAwGYohAAAAAAAAGyGQggAAAAAAMBmKIQAAAAAAABshkIIAAAAAADAZiiEAAAAAAAAbIZCCAAAAAAAwGYohAAAAAAAAGyGQggAAAAAAMBmKIQAAAAAAABshkIIAAAAAADAZiiEAAAAAAAAbIZCCAAAAAAAwGYohAAAAAAAAGyGQggAAAAAAMBmKIQAAAAAAABshkIIAAAAAADAZiiEAAAAAAAAbIZCCAAAAAAAwGYohAAAAAAAAGyGQggAAAAAAMBmKIQAAAAAAABshkIIAAAAAADAZiiEAAAAAAAAbIZCCAAAAAAAwGYohAAAAAAAAGyGQggAAAAAAMBmQqwOAAAAAACA3RUVFWnFihVas2aNDMNQ586dde655yokhB/bUTH4ygIAAAAAwGIvv/yy1q9f73u9ZcsWbdy4UWPGjLEwFWoyCiEAAAAAACy0bds2rV69WmlpacrKypLD4VCdOnXk9Xq1bds2NW/e3OqIqIEohAAAAAAANVJ+fr5SU1OtjnFKK1as0KZNm1RYWOgb279/v7KysvT111+rqKhIklRQUKClS5fq119/VWFhoVq3bq2+ffsqNjbWouQVIzk5WeHh4VbHqPEM0zRNq0NUpqysLMXExCgzM1PR0dFWxwEAAAAAVJBNmzZp9OjRVsc4pdzcXOXm5pa4LSoqShEREZKknJwcv9JIkhwOh6Kjo2UYRoXnrCyzZ89W69atrY5RLQXSeTBDCAAAAABQIyUnJ2v27NlWxzilTz/9VAsWLChW9oSGhmr8+PHq2LGj9u7dq9dee02SdOzYMW3evFmtWrVSRESELr74YnXq1MmK6BUiOTnZ6gi2QCEEAAAAAKiRwsPDq8VMkwMHDmj9+vXas2ePb6ZQ7dq11bhxY/Xs2VONGzfW0aNHFRkZ6XdcRESEIiMjFRISUi3eJ6oWCiEAAAAAACzUrVs3ff7553K5XL71gkJCQtShQwc1btxYktSoUaNSjz+xDxAIh9UBAAAAAACwM5fLpTvuuEM9evRQTEyM4uPjNXDgQN14442+fZKSknTmmWcWOzYhIUGdO3euzLioIZghBAAAAAA2l56eroyMDKtj2N7ZZ5+ts88+2/d6x44dftt79+4tp9OpVatWyeFwKCUlRZdcckmx/VD1xMbGKiEhweoYfnjKGAAAAADYWHp6uq4fOlQFbrfVUYAayxUWpjfnz6/wUoinjAEAAAAAyiQjI0MFbrf+Jine6jBADXRQ0gK3WxkZGVVqlhCFEAAAAABA8ZIayrA6BlADVc0bs1hUGgAAAAAAwGYohAAAAAAAAGyGQggAAAAAAMBmKIQAAAAAAKjhTNNUkb0eMo5TqBKF0KxZs5SSkqLw8HB1795dP/zwQ5mOe+edd2QYhoYMGVKxAQEAAAAAqKbW5edrblamXsrI0JtZmdrsdlsdCVWA5YXQu+++qwkTJmjy5Mlas2aNOnbsqAEDBujAgQMnPW7nzp268847de6551ZSUgAAAAAAqpd1+flaeeyYcr3HZwdleLxanJurnYWFFieD1SwvhJ566imNHj1aI0eOVNu2bfXSSy+pVq1aeu2110o9xuPxaOjQoZoyZYqaNWtWiWkBAAAAAKg+1hUUlDi+tiC/kpOgqrG0EHK73frpp5/Ur18/35jD4VC/fv20atWqUo+bOnWq6tevr7///e+nvEZBQYGysrL8PgAAAAAAqOmKTFM5Xm+J2zI9JY/DPiwthA4dOiSPx6OEhAS/8YSEBKWlpZV4zMqVK/Xqq69q9uzZZbrGjBkzFBMT4/tISkoqd24AAAAAAKq6EMNQHWfJP/bHO52VnAZVjeW3jAUiOztbN9xwg2bPnq169eqV6ZiJEycqMzPT97F79+4KTgkAAAAAQNXQPTyi2JjTkLqGh1uQBlVJiJUXr1evnpxOp9LT0/3G09PTlZiYWGz/bdu2aefOnRo8eLBvzPt/099CQkK0adMmNW/e3O8Yl8sll8tVAekBAAAAAKjaWoSFabBhaF1BvjI8XtUPcaqzK1wJIZbWAagCLP0KCAsLU5cuXbR06VLfo+O9Xq+WLl2qW2+9tdj+bdq00YYNG/zGHnjgAWVnZ+vZZ5/ldjAAAAAAAP4kOTRUyaGhVsdAFWN5JThhwgQNHz5cXbt2Vbdu3fTMM88oNzdXI0eOlCQNGzZMjRo10owZMxQeHq727dv7HR8bGytJxcYBAAAAAABQMssLoauvvloHDx7UpEmTlJaWprPOOkuLFi3yLTS9a9cuORzVaqkjAAAAAACAKs3yQkiSbr311hJvEZOkZcuWnfTYuXPnBj8QAAAAAABADcbUGwAAAAAAAJuhEAIAAAAAALAZCiEAAAAAAACboRACAAAAAACwGQohAAAAAAAAm6EQAgAAAAAAsBkKIQAAAAAAAJuhEAIAAAAAALCZEKsDAAAAAABQk3hMU9sLC3XU61E9p1MpIaFyGIbVsQA/FEIAAAAAAARJjterD3KydaTIoxzTK7dpqq7TqeujoxXn5EdwVB3cMgYAAAAAQJCsPJanw0Ue7fMU6YjHqxyvqdTCIr2UkaGDRUVWxwN8qCcBAAAAADooSTItTlG9maap3woLddjrkdv0/1we8Xr1ef4xXRAZaVE6WOWg1QFKQSEEAAAAANACqwPUBIahDEmFplmsWjMkfVdYqI2mKYP1hFAFUAgBAAAAAPQ3SfFWh6gBVoWG6vvCQhX+aTzK4VBdw9CVlEG2c1BVs3ClEAIAAAAAKF5SQ1FWlNeFEbW03+3WjsL/rRcU4TBUz+FQpzAXn2Nbqpq3YrKoNAAAAAAAQRLhcGhsTKzOiYhQPadDDUKcSnSGqFVYmHpERFgdD/BhhhAAAAAAAEHkcDh0WVSUcr21dcTjUYzToWiH0+pYgB8KIQAAAAAAKkBth0O1HdyYg6qJr0wAAAAAAACboRACAAAAAACwGW4ZAwAAAAAAPgWmqe1ut9wylRwSqlgn6x/VRBRCAAAAAABAkrSnsFCf5ubKbR5/VPrXOqau4eE6myek1TjcMgYAAAAAgE3keb3a5C7QNrdbRf9X+pzgMU0tzvtfGXTC6vx87SsqrMyYqATMEAIAAAAAwAY2FBRo5bE8ef6v74lwGBpUO1INQ45XA/uKipTnNUs8dou7UA1DQisrKioBM4QAAAAAAKjhDns8Wp73vzJIko55TX2WmyOPWXIJ9Een3gPVDTOEAAAAAAA6KIkf+2uun90Fyv/Tn2+RaepAoVdzsrPUJCxMzUJDZToMFXi9xY6PCQ3RPr4+TstBqwOUgkIIAAAAAGwsNjZWrrAwLXC7rY6CCnRMUv4fXpumKY/HI9M0dbioSOs8HjkKChQeHq5jx47J/MOsofDwcC0M5Xax8nCFhSk2NtbqGH4ohAAAAADAxhISEvTm/PnKyMiwOgrKKDU1VdOmTdMDDzyg5OTkMh8zb9483+s9e/YoLy9PDodDzZo1k8NxfEWZDh06qH///vr9999VUFCgli1bKj4+vkLeh53ExsYqISHB6hh+KIQAAAAAwOYSEhKq3A+rOLXk5GS1bt26TPu2bt1aR44c0bJly2Saptxut0JDQ9W4cWNFR0f79jt8+LA6deqkTp06VVRsVBEUQgAAAAAA2MA111yjbt26af369crMzJTH41FeXp4KCgoUGxur8PBwRUZGWh0TlYSnjAEAAAAAYBPNmjXTpZdeqoSEBO3du1eHDx/WgQMHtGXLFh09elS9evWyOiIqCYUQAAAAAAA2sn79eh07dkx16tTxG/d4POrZs6dFqVDZuGUMAAAAAFAj5efnKzU11eoYQXfiPZ3ue/viiy+Ul5enOnXqKCoqSoWFhQoLC5PT6dSXX36p5s2bBzNuwJKTkxUeHm5pBjswzD8+S84GsrKyFBMTo8zMTL+FswAAAAAANcumTZs0evRoq2NUOSfWDSpJVFSUQkKsnTsye/bsMi+WDX+BdB7MEAIAAAAA1EjJycmaPXu21TGqnLS0NL3yyiv68/yQuLg4jR07VoZhWJTsuOTkZEuvbxcUQgAAAACAGik8PJyZJiVo3bq1DMPQggUL5Ha7JUnx8fG6+eab1ahRI4vTobJQCAEAAAAAYDPnnXeeunbtqi1btig8PFytWrWyfGYQKheFEAAAAAAANlSrVi117NjR6hiwCI+dBwAAAAAAsBkKIQAAAAAAAJuhEAIAAAAAALAZCiEAAAAAAACboRACAAAAAACwGQohAAAAAAAAm6EQAgAAAAAAsJkQqwMAVYXH49H69et1+PBh1a1bVx06dJDT6bQ6FgAAAAAAQUchhGopPT1da9euldPpVOfOnVW3bt1ynW/58uWaNWuW0tLSfGOJiYm65ZZb1Lt37/LGBQAAAACgSjFM0zStDlGZsrKyFBMTo8zMTEVHR1sdB6dh0aJF+vDDD32vDcPQDTfcoJ49e57W+ZYvX65JkyapR48euuGGG9S0aVPt2LFDb7zxhlatWqWpU6dSCgEAAAAAqrxAOg/WEEK1kpaW5lcGSZJpmnrrrbeUnZ0d8Pk8Ho9mzZqlHj16aPr06WrXrp1q1aqldu3aafr06erRo4deeOEFeTyeIL0DAAAAAACsRyGEamXNmjUljhcVFWnDhg0Bn2/9+vVKS0vTDTfcIIfD/6+Dw+HQ9ddfr/3792v9+vWnlRcAAAAAgKqIQgjVyskWef5zoVMWhw8fliQ1bdq0xO3NmjXz2w8AAAAAgJqAQgjVSpcuXUocDwsLU4cOHQI+34nFqHfs2FHi9u3bt/vtBwAAAABATUAhhGqlXr16Gjp0qN9MobCwMI0aNUq1atUK+HwdOnRQYmKi3njjDXm9Xr9tXq9Xb775pho0aHBaZRMAAAAAAFUVTxlDtZSVlaUNGzbI4XCoY8eOp1UGnfDHp4xdf/31atasmbZv364333yTp4wBAAAAAKqNQDoPCiFAx0uhWbNmKS0tzTfWoEEDjR07ljIIAAAAAFAtUAidBIUQSuPxeLR+/XodPnxYdevWVYcOHU66iDUAAAAAAFVJIJ1HSCVlAqo8p9OpTp06WR0DAAAAAIAKx6LSAAAAAAAANlMlCqFZs2YpJSVF4eHh6t69u3744YdS9509e7bOPfdc1alTR3Xq1FG/fv1Ouj8AAAAAAAD8WV4Ivfvuu5owYYImT56sNWvWqGPHjhowYIAOHDhQ4v7Lli3Ttddeq6+++kqrVq1SUlKS/vrXv2rv3r2VnBwAAAAAAKB6snxR6e7du+svf/mLZs6cKUnyer1KSkrSuHHjdO+9957yeI/Hozp16mjmzJkaNmzYKfdnUenqKS8vT8uWLdPGjRtVu3ZtnXPOOWrXrl2l5zhy5IhCQ0MVFRVV6dcGAAAAAOBkqs2i0m63Wz/99JMmTpzoG3M4HOrXr59WrVpVpnPk5eWpsLBQcXFxJW4vKChQQUGB73VWVlb5QqPS5efn64knntC+fft8Y2vXrtWVV16pvn37qrCwUKtWrdIvv/wil8ulHj16qG3btkHNsH37dr311lvas2ePJKl9+/a6/vrrFRsbG9TrAAAAAABQGSy9ZezQoUPyeDxKSEjwG09ISFBaWlqZznHPPfeoYcOG6tevX4nbZ8yYoZiYGN9HUlJSuXOjcq1cudKvDDrh448/Vl5enmbOnKm33npL69ev148//qjnnntOn332WdCun5WVpeeee85XBknSL7/8olmzZgXtGgAAAAAAVKZq/dj5Rx99VO+8846WLVum8PDwEveZOHGiJkyY4HudlZVV6aVQenq6MjIyKvWaFa2goKDMpV15LV68WAcPHixx29NPP601a9YUG3/ttdfkdrsVERFR7uuvW7dOu3fvLjZ+8OBBzZs3T4mJieW+RmVJTEyUy+WyOkbQxcbGFiuWAQAAAACls7QQqlevnpxOp9LT0/3G09PTT/lD9hNPPKFHH31UX3zxhTp06FDqfi6Xy9IfgNPT0zV06PVyuwtOvTNKlJeX53fb3x/t2LFDhYWFJW579NFHFRoaWqHXf+GFFxQWFlbua6B8wsJcmj//TUohAAAAACgjSwuhsLAwdenSRUuXLtWQIUMkHV9UeunSpbr11ltLPe6xxx7TI488os8//1xdu3atpLSnJyMjQ253gfKb95EZEWt1nODxFskoyKmca+XnyrtzrfSn9c+dtevIGxEp76His3ckyd2kozy1yr9wuDfroLx7N5a4rajFX+QNLXl2WlVkuiIlR7WeGFiMcSxD2rZMGRkZFEIAAAAAUEaW/2Q4YcIEDR8+XF27dlW3bt30zDPPKDc3VyNHjpQkDRs2TI0aNdKMGTMkSf/61780adIkvfXWW0pJSfHdthQZGanIyEjL3sepmBGx8tauZ3WM4KrEB22F1q6nwtS1MgtyJRly1mmkkGZdZRYVyszOkEyP3/6OWrFSk87ylHSyQMU1k3EsT96cQ37DIYmtZDZoX+I1TK9XnkM75DmyV4bDIWfdZDnrsn5VRbB0ITQAAAAAqKYsL4SuvvpqHTx4UJMmTVJaWprOOussLVq0yPeb/l27dsnh+N+PfC+++KLcbrf+9re/+Z1n8uTJeuihhyozOiqRs04jOWIbyszPlhESJuP/ZuUYIS6Fteypwp0/yXTnSZIckfUU2uJs37GmacqTvkWeQ6kyvUVyxjZQSMMzZISU7VZCw+FQWJve8qRvlSdjn+RwylkvWSH1Ukrc3zRNFW5ZKU/Gft+Y5+heheS0Umhyp9P8DAAAAAAAEDyWF0KSdOutt5Z6i9iyZcv8Xu/cubPiA6FKMgxDRkTxW8CcdRrKiEmUmX1ACq0l559uEyvcsVpFaZulwnxJkjfnqLwZaQpr31+Gw3nK63qyDsiTvlWm+5gcUfEKadDKV0iVxJuZ5lcGnVCUtkXOhJZyhFfdmWwAAAAAAHuoEoWQHRjHMri1pYIUHdmrwrQtMguPSY5QmfWaKDSxpQzDkLcgT0Wpa2Xm50g6sQZRpjzuHJl7N8gZ1/jU5969wXesJ3OfvOmbFd6yh4zQkmcYFR3aLsPjlukpkuk+JtPrkeEMkREWIR3cKscpronAGMcyrI4AAAAAANUOhVAlCd+2zOoINVJhYaHyc3JkSDL+b8ybsVvGnp8UERGh/Px8mTlZxQ/MKZBjy3JF1K5d6rlN01RWVpYcXm+xbUbu/lIfaW/k58uTlydPUdH/ziVJeRkK8+bKta/mPfYdAAAAAFC9UAhVkoLGXWS6KnEVZpvI3/WLvObRYuPHnCFSs+5yp22T8n4ttt2U5I5pImfymaWe2+s+pqJtq0vcVlArWkZyxxK3mUWF8qxfIsntP244dSymicxGZ5T+hhAwoyBbrj0/WR0DAAAAAKoVCqEKFhsbq7Awl8QPrBXCnZUl01Pys8RcW5fJcLvl9hTK/NMj6w3DUETOXoVvO1zquU3TlDsvs9ixkhRamKXwbcWLqBPHHfPky+Px+I41DENOp1dG+maF56eX9e2hjMLCXIqNjbU6BgAAAABUGxRCFSwhIUHz57+pjIwMq6MEVWpqqqZNm2Z1DDmdTnlKKIScTqcMw1BYWJhCQkLk8Xjk/b9bvwzDUEhIiMLCwk56bsMwFBoaKrfbXWyby1X6bV/Hy5/j1zdNU4Zh+Lb98Yl5VnjggQeUnJxsaYaKEBsb63syIQAAAADg1AyzpOkPNVhWVpZiYmKUmZmp6OjiT6xC2eTn5ys1NdXqGDp06JDmzJmj/Px835hhGLr88svVtm1bSdLPP/+sTz75REVFRTJNUy6XS5deeqnOOOPUt24VFhbqs88+0y+//CKPx6OoqChdcMEF6tChw0mPW7Zsmb7++uti4xdffLE6dbLu0fPJyckKDy/9CWkAAAAAgOorkM6DQgjVXnp6uhYtWqTU1FTVrVtXffv2VZs2bfz2yc7O1vr16+VwONShQwfVPsli0iXJy8tTTk6O6tWrV6ZZPh6PRwsWLNDKlStVWFio8PBw9e/fXxdddFFA1wUAAAAAoKwohE6CQgiVKS8vT0eOHFF8fPxJbzMDAAAAAKC8Auk8WEMIqEC1atVSrVq1rI4BAAAAAIAfa1e4BQAAAAAAQKWjEAIAAAAAALAZCiEAAAAAAACboRACAAAAAACwGQohAAAAAAAAm6EQAgAAAAAAsBkKIQAAAAAAAJuhEAIAAAAAALAZCiEAAAAAAACboRACAAAAAACwGQohAAAAAAAAm6EQAgAAAAAAsBkKIQAAAAAAAJuhEAIAAAAAALAZCiEAAAAAAACboRACAAAAAACwGQohAAAAAAAAm6EQAgAAAAAAsBkKIQAAAAAAAJuhEAIAAAAAALAZCiEAAAAAAACboRACAAAAAACwmRCrA1Q20zQlSVlZWRYnAQAAAAAACJ4TXceJ7uNkbFcIZWdnS5KSkpIsTgIAAAAAABB82dnZiomJOek+hlmW2qgG8Xq92rdvn6KiomQYhtVxUMVkZWUpKSlJu3fvVnR0tNVxAFQTfO8AcLr4/gHgdPC9A6UxTVPZ2dlq2LChHI6TrxJkuxlCDodDjRs3tjoGqrjo6Gi+sQIIGN87AJwuvn8AOB1870BJTjUz6AQWlQYAAAAAALAZCiEAAAAAAACboRAC/sDlcmny5MlyuVxWRwFQjfC9A8Dp4vsHgNPB9w4Eg+0WlQYAAAAAALA7ZggBAAAAAADYDIUQAAAAAACAzVAIAQAAAAAA2AyFEKq8Pn366J///Kdl1x8xYoSGDBlSZfIAAAAAsJedO3fKMAytW7eu1H2WLVsmwzCUkZFheRZUDxRCQIAWLlyohx9+2OoYAILIMIyTfjz00EO+f/yc+IiLi1Pv3r319ddfS5JSUlJOeo4RI0ZIkpYvX64LLrhAcXFxqlWrllq2bKnhw4fL7XZb+BkAcDrK8r1Dkj744AOdffbZiomJUVRUlNq1a+f75VKfPn1Oeo4+ffpI8v8eU6tWLZ155pl65ZVXrHnjAKqknj17av/+/YqJibE6CqqJEKsDANVNXFyc1REABNn+/ft9///dd9/VpEmTtGnTJt9YZGSkDh06JEn64osv1K5dOx06dEiPPPKILr74Ym3evFk//vijPB6PJOnbb7/VFVdcoU2bNik6OlqSFBERod9++00DBw7UuHHj9NxzzykiIkJbtmzRf/7zH9+xAKqPsnzvWLp0qa6++mo98sgjuuSSS2QYhn777TctWbJE0vFfNJ0ohHfv3q1u3br5vs9IUlhYmO98U6dO1ejRo5WXl6f3339fo0ePVqNGjTRo0KDKeLsAqriwsDAlJiZaHQPVCDOEUC0UFRXp1ltvVUxMjOrVq6cHH3xQpmlKkt544w117dpVUVFRSkxM1HXXXacDBw74jj169KiGDh2q+Ph4RUREqGXLlpozZ45v++7du3XVVVcpNjZWcXFxuvTSS7Vz585Ss/z5lrGUlBRNnz5do0aNUlRUlJo0aaKXX37Z75hArwGgciUmJvo+YmJiZBiG31hkZKRv37p16yoxMVHt27fXfffdp6ysLH3//feKj4/37X+iOK5fv77feRcvXqzExEQ99thjat++vZo3b66BAwdq9uzZioiIsOrtAzhNZfne8fHHH6tXr16666671Lp1a7Vq1UpDhgzRrFmzJB3/RdOJ/ePj4yX97/vMH7+fSPL9W6dZs2a65557FBcX5yuWAFQur9erxx57TC1atJDL5VKTJk30yCOPSJI2bNigCy64QBEREapbt67+8Y9/KCcnx3fsiSUppk+froSEBMXGxmrq1KkqKirSXXfdpbi4ODVu3NjvZ5YTNm7cqJ49eyo8PFzt27fX8uXLfdv+fMvY3LlzFRsbq88//1xnnHGGIiMjNXDgQL8yW5JeeeUVnXHGGQoPD1ebNm30wgsv+G3/4Ycf1KlTJ4WHh6tr165au3ZtsD6NsBiFEKqF119/XSEhIfrhhx/07LPP6qmnnvJNky4sLNTDDz+sn3/+WR9++KF27tzpuzVDkh588EH99ttv+uyzz/T777/rxRdfVL169XzHDhgwQFFRUfr666/1zTff+L5RBnL7xpNPPun75jh27FiNGTPG9xvCYF0DQNVy7NgxzZs3T5L/b/BPJjExUf+/vbuPybr6/zj+usRABCmQOyMgA1RwYN6ksbKr5gzvmFZuzrxB0UqHSeVdzCQdJZfOWYucmhqQijjDTMVNM2UkhTNFU0QTMJlLl1O7uVRE5Pz+6Of1/RLoFxNvr+djY+PzOedzzvv8cX12nffOOdepU6dUWFh4O0MDcA8JDAxUaWmpDh061Gxt1tXVKS8vT+fPn2/y+wdA80pJSZHNZnPMNXJychQQEKALFy4oLi5O3t7e2rNnj9atW6ft27dr0qRJ9Z7fsWOHfv31VxUWFmrhwoV6//33NWjQIHl7e2v37t2aMGGC3njjDZ08ebLec9OmTdOUKVNUUlKi2NhYxcfH6+zZs9eN8+LFi1qwYIFWrlypwsJCVVVVaerUqY7y1atXKzU1VR9++KHKyso0d+5czZo1S9nZ2ZIku92uQYMGKSoqSnv37tXs2bPrPY/7nAHucVar1URGRpq6ujrHvRkzZpjIyMhG6+/Zs8dIMn/99Zcxxpj4+HgzduzYRuuuXLnSdOzYsV7bly9fNu7u7mbr1q3GGGMSEhLM4MGD68WTnJzsuA4NDTUjR450XNfV1Rl/f3+zePHiJvcB4N6RmZlpHn744Qb3jx8/biQZd3d34+HhYSwWi5FkunfvbmpqaurV3blzp5Fkzp8/X+9+bW2tGTNmjJFkAgMDzZAhQ0xGRob5448/buOIANwJ13t32O12M2DAACPJhIaGmmHDhpkVK1aY6urqBnWvvWdKSkoalIWGhhpXV1fj4eFhWrZsaSQZHx8fc+zYsdswGgA38ueffxo3NzezbNmyBmWfffaZ8fb2Nna73XEvPz/ftGjRwpw+fdoY8/f8IjQ01Fy9etVRp2PHjqZ3796O69raWuPh4WHWrFljjPnP+8FmsznqXLlyxTz22GNm3rx5xpiG3z8yMzONJFNeXu54ZtGiRSYgIMBxHRYWZnJycuqNIS0tzcTGxhpjjFm6dKlp27atuXTpkqN88eLF131X4f7CCiHcF55++mlZLBbHdWxsrI4dO6arV69q7969io+PV0hIiNq0aSOr1SpJqqqqkiRNnDhRubm5evLJJzV9+nR9//33jnYOHDig8vJytWnTRp6envL09JSPj4+qq6tVUVHR5PhiYmIc/19bLn5t21pz9QHg3rB27VqVlJQoLy9P4eHhysrK0kMPPdSkZ11cXJSZmamTJ09q/vz5CgoK0ty5c9W5c+cGy7cBPBg8PDyUn5+v8vJyvffee/L09NSUKVPUs2dPXbx48abamjZtmvbv368dO3aoV69e+uijjxQeHn6bIgdwPWVlZbp8+bL69OnTaFmXLl3k4eHhuPfMM8+orq6u3hljnTt3VosW/5mOBwQEKDo62nHt4uKitm3b1jsKQ/p7HnRNy5Yt1aNHD5WVlV031tatWyssLMxx3a5dO0ebFy5cUEVFhcaNG+eYp3h6euqDDz5wzFPKysoUExOjVq1aNRoD7m8cKo37WnV1teLi4hQXF6fVq1fLz89PVVVViouLc2zH6t+/v06cOKEtW7bom2++UZ8+fZSUlKQFCxbIbrere/fuWr16dYO2r+3jb4p/TgYtFovq6uokqdn6AHBvCA4OVkREhCIiIlRbW6uXXnpJhw4dkpubW5PbCAoK0qhRozRq1CilpaWpQ4cOWrJkiebMmXMbIwdwN4WFhSksLEzjx4/XzJkz1aFDB61du1Zjx45tchu+vr4KDw9XeHi41q1bp+joaPXo0UNRUVG3MXIA/9Qc5/41Nn+40ZyiOfsx/38W67VzjZYtW6ZevXrVq+fi4nJL/eL+wAoh3Bd2795d77q4uFgRERE6cuSIzp49K5vNpt69e6tTp04NsujS34mXhIQErVq1Sh9//LHj0Odu3brp2LFj8vf3d3zBuvbXXD/XeCf6AHB3DB06VC1btmxw+OLN8Pb2Vrt27XThwoVmjAzAvezxxx9X69atb+lzHxwcrGHDhiklJaUZIwPQFBEREXJ3d9e3337boCwyMlIHDhyo9/kuKipSixYt1LFjx1vuu7i42PF/bW2t9u7dq8jIyH/VVkBAgB599FFVVlY2mKe0b99e0t/j+emnn1RdXd1oDLi/kRDCfaGqqkrvvPOOjh49qjVr1igjI0PJyckKCQmRq6urMjIyVFlZqY0bNyotLa3es6mpqfr6669VXl6u0tJSbd682fHSHDFihHx9fTV48GB99913On78uAoKCjR58uQGB7j9W3eiDwB3h8Vi0eTJk2Wz2Zq09WPp0qWaOHGitm3bpoqKCpWWlmrGjBkqLS1VfHz8HYgYwJ02e/ZsTZ8+XQUFBTp+/LhKSkqUmJioK1euqG/fvrfUdnJysjZt2qQff/yxmaIF0BStWrXSjBkzNH36dH3xxReqqKhQcXGxVqxYoREjRqhVq1ZKSEjQoUOHtHPnTr355psaNWqUAgICbrnvRYsW6auvvtKRI0eUlJSk8+fPKzEx8V+3N2fOHKWnp+uTTz7Rzz//rIMHDyozM1MLFy6UJL366quyWCx67bXXdPjwYW3ZskULFiy45XHg3kBCCPeF0aNH69KlS+rZs6eSkpKUnJys119/XX5+fsrKytK6desUFRUlm83W4AXl6uqqlJQUxcTE6LnnnpOLi4tyc3Ml/b2ntrCwUCEhIXr55ZcVGRmpcePGqbq6Wl5eXs0S+53oA8Ddk5CQoCtXrujTTz/9n3V79uwpu92uCRMmqHPnzrJarSouLtaGDRsc558BeLBYrVZVVlZq9OjR6tSpk/r376/Tp09r27Ztt7xaICoqSi+++KJSU1ObKVoATTVr1ixNmTJFqampioyM1LBhw/Tbb7+pdevW2rp1q86dO6ennnpKQ4cOVZ8+fZr0PaEpbDabbDabunTpol27dmnjxo2OX1D+N8aPH6/ly5crMzNT0dHRslqtysrKcqwQ8vT01KZNm3Tw4EF17dpVM2fO1Lx585plLLj7LObaBkIAAAAAAAA4BVYIAQAAAAAAOBkSQgAAAAAAAE6GhBAAAAAAAICTISEEAAAAAADgZEgIAQAAAAAAOBkSQgAAAAAAAE6GhBAAAAAAAICTISEEAAAAAADgZEgIAQAA3EMsFos2bNhwt8MAAAAPOBJCAAAA/zBmzBhZLBZNmDChQVlSUpIsFovGjBnTpLYKCgpksVj0+++/N6n+qVOn1L9//5uIFgAA4OaREAIAAGhEcHCwcnNzdenSJce96upq5eTkKCQkpNn7q6mpkSQFBgbKzc2t2dsHAAD4bySEAAAAGtGtWzcFBwdr/fr1jnvr169XSEiIunbt6rhXV1en9PR0tW/fXu7u7urSpYu+/PJLSdIvv/yiF154QZLk7e1db2XR888/r0mTJumtt96Sr6+v4uLiJDXcMnby5EkNHz5cPj4+8vDwUI8ePbR79+7bPHoAAPCga3m3AwAAALhXJSYmKjMzUyNGjJAkff755xo7dqwKCgocddLT07Vq1SotWbJEERERKiws1MiRI+Xn56dnn31WeXl5euWVV3T06FF5eXnJ3d3d8Wx2drYmTpyooqKiRvu32+2yWq0KCgrSxo0bFRgYqH379qmuru62jhsAADz4SAgBAABcx8iRI5WSkqITJ05IkoqKipSbm+tICF2+fFlz587V9u3bFRsbK0l64okntGvXLi1dulRWq1U+Pj6SJH9/fz3yyCP12o+IiND8+fOv239OTo7OnDmjPXv2ONoJDw9v5lECAABnREIIAADgOvz8/DRw4EBlZWXJGKOBAwfK19fXUV5eXq6LFy+qb9++9Z6rqampt63serp3737D8v3796tr166OZBAAAEBzISEEAABwA4mJiZo0aZIkadGiRfXK7Ha7JCk/P19BQUH1yppyMLSHh8cNy/97exkAAEBzIiEEAABwA/369VNNTY0sFovj4OdroqKi5ObmpqqqKlmt1kafd3V1lSRdvXr1pvuOiYnR8uXLde7cOVYJAQCAZsWvjAEAANyAi4uLysrKdPjwYbm4uNQra9OmjaZOnaq3335b2dnZqqio0L59+5SRkaHs7GxJUmhoqCwWizZv3qwzZ844VhU1xfDhwxUYGKghQ4aoqKhIlZWVysvL0w8//NCsYwQAAM6HhBAAAMD/4OXlJS8vr0bL0tLSNGvWLKWnpysyMlL9+vVTfn6+2rdvL0kKCgrSnDlz9O677yogIMCx/awpXF1dtW3bNvn7+2vAgAGKjo6WzWZrkJgCAAC4WRZjjLnbQQAAAAAAAODOYYUQAAAAAACAkyEhBAAAAAAA4GRICAEAAAAAADgZEkIAAAAAAABOhoQQAAAAAACAkyEhBAAAAAAA4GRICAEAAAAAADgZEkIAAAAAAABOhoQQAAAAAACAkyEhBAAAAAAA4GRICAEAAAAAADiZ/wNFY9GcHK1yzAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mse_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mse_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MSE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mae_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mae_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MAE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2*1e06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_data_augmentation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
