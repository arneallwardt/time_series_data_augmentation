{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Füge das übergeordnete Verzeichnis zu sys.path hinzu\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "from utilities import split_data_into_sequences, load_sequential_time_series, reconstruct_sequential_data, Scaler, extract_features_and_targets_reg, get_discriminative_test_performance\n",
    "from data_evaluation.visual.visual_evaluation import visual_evaluation\n",
    "from predictive_evaluation import predictive_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../../data\")\n",
    "REAL_DATA_FOLDER = DATA_FOLDER / \"real\"\n",
    "SYNTHETIC_DATA_FOLDER = DATA_FOLDER / \"synthetic\" / \"usable\" / \"1y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of loading data\n",
    "- Laden der Originaldaten: als pd dataframe \n",
    "- Laden der synthetischen, sequentiellen Daten: als np array (GAN, (V)AE)\n",
    "- Laden der synthetischen, sequentiellen Daten: als pd dataframe (brownian, algorithmit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible types: 'timegan_lstm', 'timegan_gru', 'jitter', 'timewarp', 'autoencoder', 'vae'\n",
    "syn_data_type = 'autoencoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " syn data:\n",
      "\n",
      "       traffic_volume           temp   rain_1h       snow_1h     clouds_all\n",
      "count   104712.000000  104712.000000  104712.0  1.047120e+05  104712.000000\n",
      "mean      3196.964351     279.349870       0.0  1.266347e-09      40.697540\n",
      "std       1877.608938      14.884194       0.0  4.027184e-07      39.007478\n",
      "min          0.000000     247.150009       0.0  0.000000e+00       0.000000\n",
      "25%       1297.783539     271.447609       0.0  0.000000e+00       0.000000\n",
      "50%       3544.542114     281.756729       0.0  0.000000e+00      31.727083\n",
      "75%       4784.060547     291.352676       0.0  0.000000e+00      86.484615\n",
      "max       7716.058105     307.195068       0.0  1.302962e-04     110.427353\n",
      "\n",
      "\n",
      "real train data:\n",
      "\n",
      "       traffic_volume         temp      rain_1h      snow_1h   clouds_all\n",
      "count     8759.000000  8759.000000  8759.000000  8759.000000  8759.000000\n",
      "mean      3244.668912   282.208136     0.086792     0.000233    44.397306\n",
      "std       1946.247953    12.114907     0.901360     0.006145    39.195308\n",
      "min          0.000000   243.390000     0.000000     0.000000     0.000000\n",
      "25%       1252.500000   273.605500     0.000000     0.000000     1.000000\n",
      "50%       3402.000000   283.650000     0.000000     0.000000    40.000000\n",
      "75%       4849.500000   292.060000     0.000000     0.000000    90.000000\n",
      "max       7260.000000   307.330000    42.000000     0.250000   100.000000\n",
      "\n",
      "\n",
      "real test data:\n",
      "\n",
      "       traffic_volume         temp  rain_1h  snow_1h   clouds_all\n",
      "count     2135.000000  2135.000000   2135.0   2135.0  2135.000000\n",
      "mean      3325.263700   270.553730      0.0      0.0    45.065105\n",
      "std       1996.851023     7.864566      0.0      0.0    40.781402\n",
      "min        216.000000   248.660000      0.0      0.0     0.000000\n",
      "25%       1222.500000   265.735000      0.0      0.0     1.000000\n",
      "50%       3563.000000   271.550000      0.0      0.0    40.000000\n",
      "75%       4946.000000   275.680000      0.0      0.0    90.000000\n",
      "max       7280.000000   290.150000      0.0      0.0    92.000000\n"
     ]
    }
   ],
   "source": [
    "# Load real time series\n",
    "data_train_real_df = pd.read_csv(REAL_DATA_FOLDER/'mitv_prep_1y.csv')\n",
    "data_train_real_numpy = dc(data_train_real_df).to_numpy()\n",
    "\n",
    "data_test_real_df = pd.read_csv(REAL_DATA_FOLDER/'mitv_prep_3mo.csv')\n",
    "data_test_real_numpy = dc(data_test_real_df).to_numpy()\n",
    "\n",
    "if syn_data_type == 'timegan_lstm':\n",
    "    # load sequential data (which should already be scaled)\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_lstm_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'timegan_gru':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_gru_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'autoencoder':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'8726_12_5_lstm_autoencoder.csv', shape=(8726, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'vae':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28511_12_5_lstm_vae_unscaled.csv', shape=(28511, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'jitter':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'jittered_01.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "elif syn_data_type == 'timewarp':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'time_warped.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "# Loot at real and syn data\n",
    "df = pd.DataFrame(data_syn_numpy.reshape(-1, data_syn_numpy.shape[-1]), columns=data_train_real_df.columns)\n",
    "\n",
    "print('\\n\\n syn data:\\n')\n",
    "print(df.describe())\n",
    "\n",
    "print('\\n\\nreal train data:\\n')\n",
    "print(data_train_real_df.describe())\n",
    "\n",
    "print('\\n\\nreal test data:\\n')\n",
    "print(data_test_real_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Predictive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"seq_len\": 12,\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 32,\n",
    "    \"hidden_size\": 12,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"num_evaluation_runs\": 10,\n",
    "    \"num_epochs\": 500,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETERS:\n",
      "seq_len :  12\n",
      "lr :  0.0001\n",
      "batch_size :  32\n",
      "hidden_size :  12\n",
      "num_layers :  1\n",
      "bidirectional :  True\n",
      "num_evaluation_runs :  10\n",
      "num_epochs :  500\n",
      "device :  cpu\n",
      "Synthetic Data is sequential: True\n",
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1057, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1056, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.22210548995878465 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.1790074733657735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.034015577905097585 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0348062379669179 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014117488672831061 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.015417480943998432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011157282343028915 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012148211258189642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009685477176868105 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010396832001039429 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008719537041343775 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00921684383715996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008070867788216548 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008443164577543297 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007605147669533689 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007874167695477167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007212883716340374 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0074044295572949686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006895387509963509 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007091972066144294 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006637156671626888 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006827706895659075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006376053622573695 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006543730734847486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006133731406175253 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006286180290557882 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005950095249683236 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006119978032074869 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00582286357636611 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006048189209061949 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005727824110479985 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006031225257388809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 152\n",
      "INFO: Validation loss did not improve in epoch 153\n",
      "INFO: Validation loss did not improve in epoch 154\n",
      "INFO: Validation loss did not improve in epoch 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive\\predictive_evaluation.py:277: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([{'Model': evaluation_method, 'Metric': 'MAE', 'Error': mae}])], ignore_index=True)\n",
      " 10%|█         | 1/10 [01:06<09:54, 66.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 156\n",
      "Early stopping after 156 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.160780620400923 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.10367059085846823 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02480298150534721 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.02697919076308608 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012489895287044618 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013580722046112093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010546062049884213 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011257396100143738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00906693335388019 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009747682474827504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008232508380693403 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009058137629728983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007585533089038447 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008155398618649034 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007118714460508259 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00731309421141358 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006884945983944094 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006940332378315575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00670084557292072 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00669617742053507 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006544110835121985 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006505930460715557 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006410513515074323 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006351804846952505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006294271095917581 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00621806547347018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006189218862067201 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006091631503830499 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006097012368136215 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005972953294129933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006021495498495897 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005871541763874976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005960942751194357 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005793247762245729 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005910584038160877 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005736412905047045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00586641443864422 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0056952835841323525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0058259111671898865 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005663774099529666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005787668856758162 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005637082736939192 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005751112896191079 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005612111814758357 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005716114526743708 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0055875112713478945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005682574645837048 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0055630333475111165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005650305478106232 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005538749535951544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0056190880348593215 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005514716291252305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00558871065774579 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0054909511304953515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005558983554101692 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005467443795436446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005529745135211596 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005444162853938692 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0055008709050857725 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005421076038414065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005472267853830309 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005398137090892038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005443875030376507 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005375331123907338 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0054156630678972514 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005352651303195778 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005387610499662123 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005330097642453278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005359686645985085 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005307695985881283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005331837483783708 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005285476265913423 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005303968367826221 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005263461683438543 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005275921114120483 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005241655032424366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0052474356742055715 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005220099297516486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0052181177672311445 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005198913701700375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005187471134861741 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.005178511382409316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005155018237623365 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.005159638897406266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005120450458922729 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0051431203986901574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0050837590283099015 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.005128951346063439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005045581506828635 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0051155075129559816 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0050074871691932945 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.005100270726389307 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004971457006074964 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.005080713863101076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004938851043294748 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.005055418169564184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004909822256746192 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0050245739642859385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004883684989500002 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004989244988845552 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [04:32<19:49, 148.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.16187112863060005 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.1243111441624077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0202860610481406 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.023113673203624785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012585175634926036 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014175390944514862 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00960677035002507 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01076837396591573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008389263383255605 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009222359239014195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007343720432488167 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007882400662364328 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006885662790044571 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00756770559340058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006714483705199711 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007334268251297009 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0065907806142770356 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007171657429460217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006485319279565945 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00705722118864822 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006390199096861506 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006971641361494274 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006302602040618794 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006902495851082837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006219247360518338 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006843007477822111 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006136670938512841 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006790699259213665 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006052593557405401 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00674659397918731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00596680955997036 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0067133299924214095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005881174011189953 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006689041400985683 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005798170788308782 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0066680382690666356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0057194845170550805 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006647348047836739 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005646094182730537 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006624387440216892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005578894417114338 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00659073530422414 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005518660701328657 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006532541359774768 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0054654240783263185 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006443588419214767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005418273055225774 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006337713463889325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005376053248985881 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006233769825057071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005337933478214146 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0061406971790882595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005303390234892331 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006060227679828291 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00527206921854811 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0059911697171628475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005243734944353465 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005931673835113864 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005218133522812821 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005879113666357144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0051948940162597915 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005830675709362635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005173576030630387 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005783931574756827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005153778426767238 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005736994269468328 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005135203367965907 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005689563541262246 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005117548499630261 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005641479620381314 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005100573593645227 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005593120219761177 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0050840896871246835 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005545080379199456 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005067951130692637 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005498269398231059 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005052040808835835 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005453992278917748 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005036283921523455 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005413066455409588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005020642723424197 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005376209754852907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005005102439989152 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00534395977977992 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004989670882304029 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005316437567885527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004974369909257431 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005293530271635117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004959229944974021 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005274854641517296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004944273607591235 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005259918453007498 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0049295277518719895 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005248082390822032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004915010277309326 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00523864445345038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.00490073221531717 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005230991299921537 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004886701998783507 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005224463368957753 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [07:57<20:21, 174.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.22886953982150685 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.17974434130797712 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.022791502271595335 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.024727845126215148 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011040898988679404 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012487099242314477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008364362908564644 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009430998111116317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.006930770855957819 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007959624177173656 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006359612670332792 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007219152607242851 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006111765619414947 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006691417150089846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.005980324100390295 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006354258950416218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.005890360273706326 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006139885271241998 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005822427258537878 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006010796874761581 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005766078887548107 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005937777630820432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00571594439339858 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005898052045856328 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0056697022854724395 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005877229830195361 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0056262560678224495 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005869250655557741 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 135\n",
      "INFO: Validation loss did not improve in epoch 136\n",
      "INFO: Validation loss did not improve in epoch 137\n",
      "INFO: Validation loss did not improve in epoch 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [08:55<12:49, 128.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 139\n",
      "Early stopping after 139 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.10500474046426315 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0921382558915545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.020692232336821784 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.023085927029195076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011267573623680067 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012599214363624068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009450400125381475 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01016018950544736 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008463698929604007 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008864300246761344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007977629105057431 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008399488777607022 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007690017420907308 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008211254395599313 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007462147565730202 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008036569332676557 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00728372573392826 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007862914990469375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007137853005793911 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00769909515785163 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007003272801805899 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007526908747797065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006862483943734373 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007317590575172182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006694861695367544 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007042288999347126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0064806273100000335 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00672435070908464 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0062393180309645285 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006465743631398415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00602934742218944 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006326281527221641 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005865566387014586 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006286244106698124 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 163\n",
      "INFO: Validation loss did not improve in epoch 164\n",
      "INFO: Validation loss did not improve in epoch 165\n",
      "INFO: Validation loss did not improve in epoch 166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [10:04<08:54, 106.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 167\n",
      "Early stopping after 167 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.14785252414970068 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.10867415127508781 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.024278532812222295 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.02677100551698138 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012320382784402855 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013575275942190167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010267976039245615 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011195445063469164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008748925791715143 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009586442446829203 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008260238341465389 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009049451773000114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007979671855756237 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008635316619320828 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007708007478270761 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008220239494488957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007415424163935937 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007817017606130856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007100669281764541 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007421619553283295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00680666483801035 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007012502755969763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006540417129065107 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006571401102358804 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006202222748802309 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006200904190978583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0059405345661756 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006049639100263662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 136\n",
      "INFO: Validation loss did not improve in epoch 137\n",
      "INFO: Validation loss did not improve in epoch 138\n",
      "INFO: Validation loss did not improve in epoch 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [11:02<06:00, 90.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 140\n",
      "Early stopping after 140 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.402111201621864 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.2777495950131732 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.034108392642742964 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.038929433534469676 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013723369303710051 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.015274199095609434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010342441955972191 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011324266314177829 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008453163416481774 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009137410735574496 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0073733065548407285 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008028230533989914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006923372243330508 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007422570289824815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0066780938051409855 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00708323810249567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006451034594673908 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006756568108411396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006166006800001419 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006328962476211874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005888153820546738 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005911067032310016 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005695979009776709 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0056170414900407195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0055705088816997836 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005435429109424791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005474430993692202 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005299891208243721 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005392775305777951 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005179619813776191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005320225960411893 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005068504840464276 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005254504309939968 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00496631400550113 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005194441073346393 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004873298916637021 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005139370504970642 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004789510474759428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005088853060070724 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00471484859008342 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005042504625654963 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004648956182577154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004999934726464392 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004591127665822997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004960739541752943 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004540335893740549 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004924513625633651 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0044953732080209785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004890862950693177 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0044550221372286185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004859395833544936 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00441815352122135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004829718657415768 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004383820547338794 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004801429731875019 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004351284033071031 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004774126446641044 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004320015596664127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004747420807290877 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004289685981348157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0047209467770732315 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004260132993187974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004694372844027464 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004231288405957029 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004667408078191734 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004203183590160573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004639800177822501 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004175895571653896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004611341781526749 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004149543480975006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0045818732252657195 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004124311100253288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0045512893418169645 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004100416410330902 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004519565778284803 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004078097668859889 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004486778785855285 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004057570155162145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.00445313802151261 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004038968469317564 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004419010955531042 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004022302242982037 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004384901940085021 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0040074903072844096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004351426636116974 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.003994335543692988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004319236423254325 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0039825882516143956 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.00428893426562792 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.003971987540888435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004260986132345869 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.003962293517200605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004235656511159206 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.003953253412071396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004213001340904687 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0039446722606525705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004192890180869667 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00393634384004947 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004175066490666975 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.003928126534447074 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [14:29<06:25, 128.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2870382928819715 // Train Acc: 0.0\n",
      "Val Loss: 0.20197592716773644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03546545294922416 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03904545471510466 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015252231016506978 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01682694207685178 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011280930550288015 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01212804315967814 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009361704691214636 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009623033453381676 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008374145157409179 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008473469026605873 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00788814832973056 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008080269066233407 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007575649188161848 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007834526895107153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007290694105479676 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007597571108764147 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006908239220841754 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007285311968777986 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006445475980166999 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006774937535416992 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006125381981603882 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006424323956975166 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00590630909546965 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006244286196306348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005758408663954151 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0061449447016724765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005653283916529349 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006068791091606459 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005571385124531731 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006002535461448133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005504628676958518 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0059510447043815955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005448797212940825 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005912828697439502 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0053992847160837295 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005879461614634185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0053532319100927156 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005846988307038213 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005309425602857377 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005814684383735499 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005267412533446548 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005782293525579221 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005227029483755166 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0057495602009379684 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005188200580734542 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005716260755434632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0051508592832561605 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0056822790956015095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005114917318127628 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005647620597087285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00508025542443727 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005612396272173261 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005046744432587544 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005576758357860586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005014260221327091 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00554087566321387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004982696160594219 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005504875430179869 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004951970615206681 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005468898904783761 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00492202969782911 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00543302357169416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0048928348072226685 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005397281575235812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0048643742266525755 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005361697485889582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004836635025945513 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0053262679960907385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004809613162218287 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005290938096175737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004783322616143791 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005255591295495191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004757779668022395 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005220184671035146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004733000492360986 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005184652318027528 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004708996123335168 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005148929612272803 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004685763670955819 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005112983005614404 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004663290011094896 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005076778977287605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004641549343737794 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0050402870949576885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004620508829991666 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005003534551874241 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004600142873887374 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004966576015302802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0045804307697490405 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004929558168548872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00456138048883966 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004892670412493103 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.00454301419924948 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004856178269940703 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004525359732950324 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004820313582005089 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004508442568327606 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004785275117576341 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [17:53<05:05, 152.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.19753476507268355 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.15085787146295737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02987731278391324 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03226102086003212 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012906493129205285 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014458552804118133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010817387969365657 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01193694130736677 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009741514870203543 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010519461651497027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008946076857204801 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009481617300908136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00836954746279784 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008839001584633747 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008002159743991266 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00848991280722925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007754052805639532 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008248132126241484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007556013427300882 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008042569374939537 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007365470088042836 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007833888439242454 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007171354732003716 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007608935182147166 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006997018757643327 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007380680865882074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006843848308177841 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007169664864811827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006691360470573724 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006973091081496985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006517229744498312 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00678201109472224 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006318515617855872 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006587390692028052 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0061335005511930826 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006369187664377558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005979096122259397 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006168714758720906 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005844234761248349 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006005105131086619 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005719335095440275 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005861468168030328 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005599528166591224 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005711990737301462 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0054836344019761375 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005540855584994835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005373102512523314 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053480024696053825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005270406929691098 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005145378499839674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005177537263345558 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004948510897948462 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005095392770862996 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004769597856519634 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005023909561262962 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004615389539942364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004962413752771044 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004488101355758879 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004909915380081311 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004386676704812357 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004865340395859463 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004307953520979294 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00482760733065712 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004247853090740083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004795553157595496 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004201928985661224 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0047680608134540005 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004166178617571645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004744143155573384 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0041375881204765074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004722999978171539 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004114136520964915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0047040272769212285 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004094473716309842 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004686781979143878 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004077718644539881 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004670939079815535 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004063192686504301 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004656251196377319 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004050397930447669 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004642523515860747 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004038933322162312 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004629603046651957 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004028515431427342 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004617362087728442 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004018913975040264 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004605710024207858 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004009937641539556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.00459455653628246 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00400145463850897 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004583830853845979 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0039933537002926804 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004573475143126123 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.003985528700628921 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004563435686312031 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.003977941585403374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004553667496529537 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.003970516331986908 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004544130076117215 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.003963235871838953 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [21:18<02:48, 168.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.15119626334983938 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.1248246837160824 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.028642419718858534 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.030800637159058276 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012938806682614351 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014368503683191888 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00950837907309977 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010676753416191787 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00834480587632674 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009454017354394583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007211602977426029 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008609131386037916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0068436036456587055 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007876974369590992 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006697070234018399 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007575644406161326 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006581760583068142 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007384368745774469 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006480727289305374 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007223257918239516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00639093189286136 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007076937171137508 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006311177923157132 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006949005799148889 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006239239236061209 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006844887345591012 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006172660049934783 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006763763068353429 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006109562271655313 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006700716433389222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006048592752403151 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0066505536395946845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0059890665647566975 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006608679856392829 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005931082181207645 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0065719774472253285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005875435472129552 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0065389278769383535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005823221646103138 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006509745924952714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0057752613652919 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006486261660671409 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005731759479024658 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006470097509651061 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005692319353077098 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006460738970952875 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005656266032656941 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006456025896648712 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005622953850772558 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006453355260686401 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0055918168411658135 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006450534599614055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005562392118672439 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006446483257390997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005534287433337121 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006441035321639741 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005507154533823095 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006432839847334167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005480739821248219 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006421749452676843 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005454845799993125 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00640782924807247 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0054293110891054965 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006391110816313063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005404083435100768 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006371570162146407 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005378884989870237 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0063488284906591565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005353596090359518 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006322692785247722 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005328228036502553 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006293441148420029 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0053028061775232315 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006261166513842695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005277251377769739 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006225208402611315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005251527868422007 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0061851328868857205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005225724503049546 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006140469251584043 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005199936970798163 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006091429940972696 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005174389284488194 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006038385546108817 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005149190527825432 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005981869303950053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005124508394744166 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005922807496972382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005100535737634399 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005862075431436738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005077439578068205 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005800723399528686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005055349218840949 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005739322425249745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0050343429239765895 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005678069548380068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0050144137374472115 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005619098007788553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004995533863182799 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005562618306344923 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [24:43<00:00, 148.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (6996, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1741, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.18400539021479756 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.11322531474923546 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.028744381388913005 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02452534359287132 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014244632024135118 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012249933191659776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011330542563924302 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009245870292017405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009833029948162196 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008016934207725254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008901164433217212 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0073631739540194925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007832977514645961 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006627881209450689 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007272428579917645 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00616895479827442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006942489004234643 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00580927944234149 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0067149797490623575 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005520583100786263 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006566062943848301 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005314007397233085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006457810836618757 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005171376233920455 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00636773500602148 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005062245912003246 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006287460130869015 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004969698749482632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006213198184699126 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004885431992906061 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006142665469406607 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004805104383690791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006074244793840433 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004726545593108643 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006006807696444225 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004648482291535898 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005939813319074849 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004570593794977123 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005873366397779996 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004493632607839324 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005808128871674305 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004419146169146354 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0057450840871602505 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004349048139358109 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005685262345351687 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004285202035680414 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005629515963751126 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004228927313604138 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005578334602744203 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004180869485505603 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005531830106092166 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004140811987136575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005489819700145014 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004107923790897158 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005451927450591843 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004081138915551657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005417691149268316 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004059466861442409 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005386636220343989 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004041918911124495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005358315643335621 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004027642758393829 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005332341450096436 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004015935322439128 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005308380048534914 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004006223799660802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0052861511249086696 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003998028419234536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005265413457102496 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003990969186733392 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0052459670637934105 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003984739997593517 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005227639233017315 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003979092891412702 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005210289065555997 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003973845155401663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005193789726650538 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003968844196589833 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005178046011957063 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003963973138227382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005162971284023601 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0039591518124904145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005148494542261679 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003954308613372797 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005134556748687405 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003949393550018695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005121112791566204 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0039443622999401256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005108116189821636 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003939192016101019 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005095539319001512 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003933860998685387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005083360309504211 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00392837687395513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005071558093345941 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003922751609405333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0050601015477675145 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003916980197060514 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0050489672265104845 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0039110718104480345 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:48<25:20, 168.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.46435427618081165 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.3194805631583387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.035419166500607856 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02968446785076098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01498730512352907 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012420032528991048 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011012147651713121 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008984813411635432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00902037007508791 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0073046235494654286 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008188979710839246 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006503539363091642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007696099813759633 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006094269005751068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007365884617625919 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00591745874421163 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007155636738513737 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005812545997005972 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007009622526087173 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00572085508399389 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006885058848297896 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0056237926579673185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006761536091464023 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005517345141958107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00662779987717396 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005395697806538506 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006482681249412942 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005255433527583426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006337574915960431 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00510494813577018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006200934068159494 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004959219277159057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00607530361777129 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004828695561313494 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005965407185583933 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004719772702082991 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005873418046846203 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004633905461312018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005796977419291355 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004567944650030272 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005732788491704965 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00451760852328417 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005678213671111336 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004479165591130202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005631097273733126 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004449385671283711 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005589647417926319 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004425587864931334 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00555247697579466 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004405739748935131 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005518586200462084 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004388447895392098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005487274801939062 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004372822044586593 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0054580570160321995 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004358347179368139 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.00543058288552624 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043447415767745535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005404588413694831 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004331835880028931 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00537987894720938 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004319552668708969 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00535630292448398 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043078774586319925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005333734267459306 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004296807180666788 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005312064964777139 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004286343896422874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005291197287369536 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004276476628993722 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005271048343540531 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004267182894347405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005251552943500677 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004258467559702694 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005232650016383459 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004250328995245085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005214281045150259 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004242748371325433 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005196394955103613 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004235707174732603 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005178945888308 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004229169715703888 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005161892230538481 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004223108898043971 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.00514518446607949 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004217471294528381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005128794668808996 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004212230485228991 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005112690060504725 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004207334907683121 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005096841812161431 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042027357330715115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005081224513875924 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004198390264487402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005065818520163964 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004194252839608287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005050604461429675 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004190274899486791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005035564208378777 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004186413751449436 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [05:38<22:32, 169.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2926192426858427 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.1945219493725083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.028522935844787725 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02447462732141668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014254023827034045 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012053683527152647 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010782321078328577 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009164112344892188 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009160915751129253 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007743249787017703 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008135791619725049 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006843358346007087 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007468518846749103 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006176171955567869 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007172519807304047 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005651438972828063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007037518252554821 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0054473616987128146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006924226818410637 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005335007400505922 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006788078971955602 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005226137848909605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006578769443926644 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005081540828740055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006369562982262609 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004900922465392135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006224445136451082 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004802689125592058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006130302207412615 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004755638273094188 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006066530990227206 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004721567140553485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006015176914066723 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004688480225476352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005970142307036373 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004656817484647036 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005929235772433856 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004627169059081511 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005891290384336252 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004599376966838133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0058555568453587895 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004573011334816164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005821510563608787 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004547613897276196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0057887763622722944 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004522821711460975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005757076963442101 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004498305095529015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005726214648717717 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004473854305053299 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005696039866936738 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004449289234947752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0056664506696402755 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004424526127563282 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005637379508831254 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004399541536854072 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0056087818208307915 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004374347898093137 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005580632291951102 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004349013242277909 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005552916473402858 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00432362228166312 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005525626024745881 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042982748730785465 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0054987573089316216 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004273075212470509 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005472305730478541 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004248118523338979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005446264964154152 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004223490396345204 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00542063268484148 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004199270777065646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005395404921130162 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004175522451458329 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0053705794605975075 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004152285915122114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00534616351089351 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004129619032821872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005322161390282401 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004107560795223848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005298584333108525 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004086161680011587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0052754412180335865 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004065464064478874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005252742668885959 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004045503205535087 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0052304969001221195 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004026309201832522 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005208712178027283 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004007915083572946 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005187394121899929 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0039903373436324975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005166548716763383 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003973601320335134 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0051461772487677426 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003957716989415613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005126279914064852 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003942701415243474 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005106850522176945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003928569283082404 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [08:27<19:43, 169.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.23675295868335658 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.14281660955060613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03522171839629406 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.029989977401088583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.016213022596649314 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.014371064356104895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012216730176186248 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01039585719223727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009838786886762572 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008394414402375167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007976243612101327 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006981064324182543 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007439275894135441 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006222233383662321 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007145754620326538 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005841628661040555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006919067433278276 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005614399465478279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006720419333116513 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0054634878708218985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006541323748132273 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005342348364435813 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0063738945103994655 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052195248744365845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006222932924278352 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005090674918822266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0061005813688397 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004970056216486476 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0060026521308636765 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048662200781770725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005918484408148994 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004778164866465059 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005841834580666092 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004701883204027333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005770152530390514 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004633434216322546 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005702404301617822 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004569871105592359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00563804006425145 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004509464863010428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005576690578658865 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004451543759469959 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005518133707129775 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004396115844561295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005462251316817673 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043434758649461645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005408943960066782 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004294007804922082 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005358072721769344 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004248161144046621 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005309444144378803 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004206285407682034 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005262837272677486 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004168486072343182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005218058674368736 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004134649696590548 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0051750167765756685 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004104535292241383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005133745310504891 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004077847445891662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005094380348391371 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004054234182165766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005057076149573337 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004033242573495954 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0050219414106115155 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004014347230126573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004989004505481401 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003997010328087278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004958207677565005 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003980778510131957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004929424413707379 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003965332235251977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004902498021809019 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003950483759399504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004877259728624209 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003936136001721024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004853543251610391 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003922272543422878 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004831193884373239 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003908943746831607 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004810059567903845 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038961809869347648 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0047900081785198 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003884026936297728 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004770925640235932 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038725618982094934 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004752704051597528 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0038618505699560047 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004735249575210488 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0038519676002165813 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004718483588206433 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003842970750040629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00470233428651854 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0038348826812580226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004686734237103217 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003827703854238445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004671622482319809 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0038214173062111846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004656940830720495 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0038159777071665635 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [11:15<16:53, 168.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.22884174253468373 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.13745062158189036 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03757984493965428 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03231658954173326 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015475821974751067 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.013683828787708824 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012126137730537926 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01000988651490347 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.01028143997106395 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008388530573045666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00928540398275318 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007622994893145832 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00884806255216723 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007180989741093733 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00857862688470249 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006789591819555922 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008313367322680528 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006421747494658286 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007940941007826987 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006014618137851357 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007446432401921566 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00555755768860267 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007115642981897172 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00527382660233839 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006928457933814014 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005159564142708074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006795003236756239 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005103417998179793 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006687699678379481 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005062458456747911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006595799928003726 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005020138062536717 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006515176450093724 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0049737106797031385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00644364228820828 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004925529205832969 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006379197439344677 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004878222307359631 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006319924823293403 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048328941899605776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006264363180050737 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00478944650597193 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006211629662594703 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004747501024129716 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006161281729508276 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004706815541298552 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006113146645969554 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004667334906248884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006067155945416727 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004629154571078041 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006023254092957093 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004592450674284588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005981364562792797 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004557414674623446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005941394457091752 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00452422464097088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0059032451072239985 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004493022231723775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005866822772808203 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004463928781280464 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00583202644680801 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044370273424481806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005798761574466411 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044123701116239485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005766930230829405 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004389976096254858 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005736435505191553 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004369834886694496 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0057071805229946355 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004351909936998378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005679067759561994 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004336134941232475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005652007881815643 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043224250516769565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00562591765864782 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004310666316781532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00560072162475641 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004300710929303684 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005576355969129326 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004292378350245682 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005552764229803903 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004285439095375212 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0055299002281709074 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004279659017497166 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005507710956985378 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042747852117331194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005486144179788728 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042705787261101335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005465146592716138 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004266828315501864 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005444673331747827 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004263366513292898 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005424673760471415 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004260098301297562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005405097210845701 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004256954636763443 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005385899002244909 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00425391608256508 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005367042132404518 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042509868360039865 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [14:04<14:04, 168.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.19657927031998765 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.11499797466465018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03364280202600373 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02932199869643558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.016205470024535783 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.014632169936190952 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012264260402923135 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010337928420102055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010244880454215012 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008455888084559278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008950575220836624 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007321376256136732 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008152668714920935 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006578883723440496 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007347740098721344 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006023875522342595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00691815321206128 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005598478467965668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00665336119789273 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0053001655943014404 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006431133591582718 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005075466374612667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006267698859630076 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004868764244019985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006171690750884139 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004727223591709679 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006095917532369324 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004621420669454065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006029284725573816 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00453225227326832 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0059685061534773135 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004452361340041865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00591201779005207 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004379491846669804 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005858936029184423 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004313336697999727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005808681048682495 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004254110369153998 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005760825999276755 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00420197562568567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005715055388534552 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004156866201876917 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00567114369127236 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004118470199914141 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005628928733504799 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004086276073940099 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005588276513754369 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004059710404412313 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005549065175478997 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004038222714073279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005511165951106476 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004021309307691726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005474441718271844 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004008508998561989 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005438755157926719 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0039994326906956056 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005403989209680814 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003993748757056892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005370075828512731 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003991188638081605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 296\n",
      "INFO: Validation loss did not improve in epoch 297\n",
      "INFO: Validation loss did not improve in epoch 298\n",
      "INFO: Validation loss did not improve in epoch 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [15:46<09:44, 146.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 300\n",
      "Early stopping after 300 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.208334372927473 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.13295403036542913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.041758046983039546 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03594226610254158 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015977838100040477 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01337253366258334 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011363590959051293 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00937700823626735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00916606179740467 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007583963350308212 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008081531718524267 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006617052654143084 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007464641339430526 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006047353901985017 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007173172567681873 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005751397617330605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0070054564416646686 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005567537243901329 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006892326067320867 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005438976463946429 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006802681407712604 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005343091157688336 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006722324702157278 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005267565447667783 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006645111848715029 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052042608234015375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006569558115608004 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005147939890792424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0064970819469856 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005096010076390071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006430018212131679 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005048172239383513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0063696434273763 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050049320244314995 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00631554680025911 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004965742609717629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006266425882006855 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004928724933415651 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006221047707846108 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048919409555806355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006178596752831823 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004854472502219406 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006138693050642978 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004816570238802921 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006101228135688971 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004779077630320734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006066127110923296 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004742767234248194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006033252919364942 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004708192886954004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00600243700704363 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046756273643537 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005973517405694206 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046451607295735315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005946338385105405 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004616770343008367 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005920742536877966 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004590417999266224 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0058965723983917945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004566034344448285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005873664966401682 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004543537553399801 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005851854691409493 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004522776425900784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0058309815305212865 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004503567106175152 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005810877106250285 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004485694759271361 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005791372688674287 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004468912842937491 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005772286580054762 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004452977761287581 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005753426923157145 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004437628341838718 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005734587735687829 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004422645198858597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005715540318418141 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00440779610706324 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005696033555525249 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004392872530628335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005675763361683311 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004377654546194456 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005654359316627695 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004361879084767266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005631331521294773 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004345234724777666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0056060066757102804 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043272254311225635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005577452021510634 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00430707149207592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005544392017277647 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00428348967212845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005505275343795685 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004254516349597411 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005458964913079864 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004217924713157117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005406824313210468 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041741394861177965 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005354361200373466 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004130186705680734 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [18:35<07:40, 153.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.424161145506112 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.30449343459172684 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03663805828825252 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.031805709512396294 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014529223286984786 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01178774856538935 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011104886664064883 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009024386027489196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009346150409336708 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007628068560734391 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00862381793824386 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006930665438994765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008256848874931399 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0064752283971756695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007996753382296664 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006181185739114881 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0077815340650076534 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006002832077104937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007575263939032408 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005896000830795277 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007373588117685798 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0058219804174520755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007184535418594571 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00574352845380252 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006998010319402467 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005632205083119598 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006801144238540129 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005482071794738824 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006608833092133 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00533128370615569 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006441068030217637 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005218421841378916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006294022191987802 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00513881588016044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00616355619731372 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005075676057657057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006049970077716461 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005012951431457292 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005952009061015462 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004947243207557635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005867887339363359 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004883754922246391 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0057952761002624895 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004826148269190029 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005731579862517989 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004774817832830278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005674538405114555 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004728564025241543 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005622481253767837 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.00468592933294448 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005574276864839097 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004645957520485602 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005529173984209323 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0046081337315792385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005486665126127638 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004572233807464892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0054463721459867545 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004538230356675658 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005407991969519929 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0045061645479026165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00537127040892193 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004476113473488526 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005335991089572475 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004448120994493365 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005301975585056733 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004422179467722096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005269086419511161 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0043982547961852764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0052372223062900035 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004376264250921932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005206334940476777 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004356119231405583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.00517640157696285 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004337668598798866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0051474248008062575 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004320763610303402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0051194151707172186 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0043052423406731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005092385941031963 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004290964124216275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.00506634367300308 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004277809341014786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005041283381503229 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004265659056942571 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005017188080526956 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0042544034669514405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.00499402493749826 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0042439275378869335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004971748600144058 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004234115287280557 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004950307380327249 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004224859497679228 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004929638711722848 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004216049454936927 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004909676191616021 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004207598602145233 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.00489035910675855 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004199431788980622 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0048716202424520195 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0041914899770001115 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [21:24<05:16, 158.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1698846858522119 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.11446016996421597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03722438081401532 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03208153627135537 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014928363349190892 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0139654476957565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011940088516449955 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010514959316192703 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010547505549450732 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009284670735624704 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009463236410995801 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008362165868112987 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008454378266908126 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007303854437883605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0078289332554025 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006459807439453223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0074961407808586955 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006038698537105864 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007265391132845352 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0058196364266967235 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007077141111550776 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0056633417875590645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006905425989524074 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005525202549655329 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006742253027788308 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005392607381906021 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006592821535020726 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00526626431806521 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006464915960329717 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00515297844751992 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006359203353409388 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005056486452337016 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006271959295747248 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004976079088043083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006198871731928222 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00491018929857422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006135795484977396 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004855663388628851 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006079416810301001 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004808707882396199 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006027499568376469 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004766445170918649 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005978670771782325 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004727170523256064 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005932077501271107 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004689953988417983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005887141825469662 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00465434784154323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0058434144822353306 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004620141203684563 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0058005180153527965 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004587298390370878 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005758135939935503 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004555917222222144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005716039909190205 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004526253273202614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005674127380355138 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004498778736557473 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00563245967506818 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00447420061735267 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0055912838112265216 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004453414924104105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005551005743409945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004437355711971494 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005512110162026687 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004426673547872765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005474979748426337 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004421259244819256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005439703323512623 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044198524079878225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 343\n",
      "INFO: Validation loss did not improve in epoch 344\n",
      "INFO: Validation loss did not improve in epoch 345\n",
      "INFO: Validation loss did not improve in epoch 346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [23:21<02:25, 145.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 347\n",
      "Early stopping after 347 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.36188310258022455 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.25098984851078554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03316138341988875 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.028222278302366084 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014349981874533712 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011951019301671873 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011532040256539096 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009482452201403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00949172309565796 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007804614453661171 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008622521730182377 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007096632472662763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007763220125396808 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006337057693268765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007187849716028224 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005820835678076202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006835870318259347 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005486718628724868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006558708065151147 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052669951946220615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006379923138971606 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051090073898773306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006254254479373733 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004965689359232783 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006152572060327075 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004849505102769895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006063871002186209 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004758773032914509 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005984057665411298 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004686581961471926 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005912050055701405 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004627887394533238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005847619305318009 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004579082563180815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0057901449726903 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045371515168385074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005738343227522864 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004499641365625641 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005690660641037489 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004464814318767325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005645664685488717 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004431575371629812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005602193278681538 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004399335009723225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005559385648915825 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004367880783551796 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005516671633547742 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004337263553911312 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005473786209641901 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004307683449323204 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005430809831348796 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004279361466284503 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0053882456411709625 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004252426252192395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005347010381328426 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004226886851459064 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005308152468275432 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004202747643417256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005272412906798479 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041800879386507655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005239942439323606 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041590077866038135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0052103733082544314 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004139491807754067 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005183136190572694 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041213608495044435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005157700709235264 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004104346013627946 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005133675835541871 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004088220019316809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005110793432314374 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004072802350856363 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005088881438504479 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040579658784818924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005067822984823761 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004043660484339026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0050475325466763045 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00402986473954198 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005027943688921086 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004016580589284951 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005009004629863485 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004003816269422797 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004990670654325231 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003991598669778217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.00497290260662414 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003979963565837931 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004955669591250111 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003968950009650805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004938940517338563 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003958581510761922 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004922692277511267 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00394887420645153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00490690155175927 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003939842587252232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004891547393829872 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003931491115045818 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004876611500470283 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003923812049271708 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004862075405765206 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003916790842247958 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [26:10<00:00, 157.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.26621266648377456 // Train Acc: 0.0\n",
      "Val Loss: 0.1772501060908491 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.030165125864672744 // Train Acc: 0.0\n",
      "Val Loss: 0.02714417651295662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.009853936598389559 // Train Acc: 0.0\n",
      "Val Loss: 0.00829773748039522 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.006461702762958359 // Train Acc: 0.0\n",
      "Val Loss: 0.005261326251043515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.004672202924064458 // Train Acc: 0.0\n",
      "Val Loss: 0.0038416347318244253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0030304439144993637 // Train Acc: 0.0\n",
      "Val Loss: 0.00266238122924485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0016280991289994284 // Train Acc: 0.0\n",
      "Val Loss: 0.001625040788415142 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.000847037295881578 // Train Acc: 0.0\n",
      "Val Loss: 0.0009086538418407806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0004549146183411863 // Train Acc: 0.0\n",
      "Val Loss: 0.00047016234523405065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00028965358052787595 // Train Acc: 0.0\n",
      "Val Loss: 0.0002793877303536812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00021591506392474937 // Train Acc: 0.0\n",
      "Val Loss: 0.0002061245693105527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0001776807391447266 // Train Acc: 0.0\n",
      "Val Loss: 0.00017860080009665002 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0001589520335700852 // Train Acc: 0.0\n",
      "Val Loss: 0.00016210982959654013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00014555595475836385 // Train Acc: 0.0\n",
      "Val Loss: 0.00014926856555658478 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0001349213524166331 // Train Acc: 0.0\n",
      "Val Loss: 0.00013954115585709752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00012621652450480023 // Train Acc: 0.0\n",
      "Val Loss: 0.00013202918132513083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00011905301578561393 // Train Acc: 0.0\n",
      "Val Loss: 0.00012601764709953303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00011308343256468892 // Train Acc: 0.0\n",
      "Val Loss: 0.00012096230434095063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00010794978803443642 // Train Acc: 0.0\n",
      "Val Loss: 0.00011643667709325779 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0001033572061050478 // Train Acc: 0.0\n",
      "Val Loss: 0.00011213511765659364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 9.912097692443376e-05 // Train Acc: 0.0\n",
      "Val Loss: 0.00010788294140101326 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 9.514951911878764e-05 // Train Acc: 0.0\n",
      "Val Loss: 0.0001036149210862773 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 9.140462331232226e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.932994646739892e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 8.78776323696658e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.506114220130258e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 8.457505670307017e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.086425316953947e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 8.150055469890853e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.681357629195025e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 7.864667570183095e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.298663239904933e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 7.60023916691311e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.944371163003697e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 7.35579844464263e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.62199240191628e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 7.130603336262119e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.332106374220033e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 6.92387530018685e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.073196006372613e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 6.734763244564324e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.842787535623013e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 6.56221789906182e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.63788991170639e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 6.40499882088266e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.455837313048753e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 6.26178180899755e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.294269071738447e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 6.131189762593803e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.151102352305315e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 6.011909616761043e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.02416007909183e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 5.9026902851228153e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.9112893756140364e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 5.802450988961455e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.810334435161415e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 5.710159743327108e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.719329737158577e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 5.62489590078731e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.636342291175176e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 5.545811159021942e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.5597518538971516e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 5.472120066363015e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.488178212544881e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 5.4030934543292764e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.420459796865047e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 5.338113715800539e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.355686599531592e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 5.276623945990891e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.293172210010446e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 5.21812807077467e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.232317277907648e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 5.162246549291644e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.172815882278056e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 5.108639231353937e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.114368035389237e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 5.057034635700133e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.056881134144284e-05 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:48<25:12, 168.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1279630790906852 // Train Acc: 0.0\n",
      "Val Loss: 0.07733871066434816 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.044944368889682916 // Train Acc: 0.0\n",
      "Val Loss: 0.03982118446041237 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013569467003589079 // Train Acc: 0.0\n",
      "Val Loss: 0.011307127981192686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007597177797613759 // Train Acc: 0.0\n",
      "Val Loss: 0.005960591363889927 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.005044946049129565 // Train Acc: 0.0\n",
      "Val Loss: 0.0038932734983973204 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0031631011093252222 // Train Acc: 0.0\n",
      "Val Loss: 0.0025335252041589805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00188175703553838 // Train Acc: 0.0\n",
      "Val Loss: 0.0015674749193501404 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.001106677545204261 // Train Acc: 0.0\n",
      "Val Loss: 0.0009134565834590996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0006498193125318647 // Train Acc: 0.0\n",
      "Val Loss: 0.0004998210500079122 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0003754202217064568 // Train Acc: 0.0\n",
      "Val Loss: 0.00025915522485526956 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0002498336147437761 // Train Acc: 0.0\n",
      "Val Loss: 0.00016638257008956068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00019482688755849504 // Train Acc: 0.0\n",
      "Val Loss: 0.00014392698014324362 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00016717435537263876 // Train Acc: 0.0\n",
      "Val Loss: 0.00013718687438416633 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00015146364744624536 // Train Acc: 0.0\n",
      "Val Loss: 0.00013133847374278545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00013965731641832105 // Train Acc: 0.0\n",
      "Val Loss: 0.00012555224205822345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00012944651284275288 // Train Acc: 0.0\n",
      "Val Loss: 0.0001197915138618555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00012017748563204356 // Train Acc: 0.0\n",
      "Val Loss: 0.00011406607657342896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00011180394861571274 // Train Acc: 0.0\n",
      "Val Loss: 0.00010847113246034662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00010459668519449543 // Train Acc: 0.0\n",
      "Val Loss: 0.00010324825694128362 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 9.872289055358505e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.860728777394714e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 9.405757240421079e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.459813161149875e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 9.032263357962267e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.115072329719127e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 8.725108839672475e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.815860412803225e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 8.464719309676471e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.552823720409916e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 8.23810393520026e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.319252609164158e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 8.037003849847796e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.110744969268456e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 7.855876675134761e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.923470108256548e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 7.690643963554701e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.753266313970512e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 7.538096628253631e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.595906933023468e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 7.395785421594982e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.448095367925073e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 7.26195331360559e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.307821185597938e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 7.135314920822267e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.173982736606955e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 7.014937407337661e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.045930942695122e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 6.900054591558513e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.923297809077088e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 6.790057897771608e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.805708127847703e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 6.684393404665898e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.692733348410745e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 6.58259728157172e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.583978187832558e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 6.484194128548452e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.478921049248047e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 6.388757431618636e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.37704215478152e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 6.295812810804657e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.277766888987654e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 6.204856576312431e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.18038478106345e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 6.115281617587335e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.084094516567844e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 6.026426256884402e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.987932037731463e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 5.937548464470592e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.890684994731353e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 5.848185305881524e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.791525257915385e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 5.7590092365820276e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.691199634880335e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 5.672941713027239e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.594911278669976e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 5.593189159203555e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.509630960163618e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 5.519218212533126e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.43509139375105e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 5.4487973202039066e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.3662954682791184e-05 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [05:34<22:18, 167.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.07458380750299656 // Train Acc: 0.0\n",
      "Val Loss: 0.05305663181299513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.014130067770246025 // Train Acc: 0.0\n",
      "Val Loss: 0.011394837371666322 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0072885816757350345 // Train Acc: 0.0\n",
      "Val Loss: 0.00572245663497597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.005204701718298046 // Train Acc: 0.0\n",
      "Val Loss: 0.004156607719646259 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00410839502625784 // Train Acc: 0.0\n",
      "Val Loss: 0.0032964870857540517 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0030888285656400093 // Train Acc: 0.0\n",
      "Val Loss: 0.002519500513815067 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0020282868766087663 // Train Acc: 0.0\n",
      "Val Loss: 0.0018451017306440256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0015097405891982439 // Train Acc: 0.0\n",
      "Val Loss: 0.0013191641832236201 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0013110223111960204 // Train Acc: 0.0\n",
      "Val Loss: 0.0011282064057674937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0011887402826820214 // Train Acc: 0.0\n",
      "Val Loss: 0.0010203200979793275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.001081364186928229 // Train Acc: 0.0\n",
      "Val Loss: 0.0009303142999256538 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0009732872951237834 // Train Acc: 0.0\n",
      "Val Loss: 0.000848573985488408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0008640997466624854 // Train Acc: 0.0\n",
      "Val Loss: 0.0007761608031366698 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0007869558564076821 // Train Acc: 0.0\n",
      "Val Loss: 0.000715395855315199 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.000734201331235379 // Train Acc: 0.0\n",
      "Val Loss: 0.0006657544401770627 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0006878831052113979 // Train Acc: 0.0\n",
      "Val Loss: 0.0006215591184710237 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0006433637864163707 // Train Acc: 0.0\n",
      "Val Loss: 0.000580597528658638 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0005992251574184074 // Train Acc: 0.0\n",
      "Val Loss: 0.0005417976708320732 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0005547922688268431 // Train Acc: 0.0\n",
      "Val Loss: 0.0005040652467869222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0005078481227349805 // Train Acc: 0.0\n",
      "Val Loss: 0.00046469443001445724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0004452601247004629 // Train Acc: 0.0\n",
      "Val Loss: 0.0004146950610447675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00036214700951581164 // Train Acc: 0.0\n",
      "Val Loss: 0.0003661487087480385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00031535739808574053 // Train Acc: 0.0\n",
      "Val Loss: 0.0003381027027816427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00028606800805138356 // Train Acc: 0.0\n",
      "Val Loss: 0.0003110248983086256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00026435764780487084 // Train Acc: 0.0\n",
      "Val Loss: 0.00029088172939902343 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0002471655212527728 // Train Acc: 0.0\n",
      "Val Loss: 0.00027488246255829424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00023277905794497384 // Train Acc: 0.0\n",
      "Val Loss: 0.00026099262600341304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00022028836561008102 // Train Acc: 0.0\n",
      "Val Loss: 0.0002483001771245406 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.00020917729856276282 // Train Acc: 0.0\n",
      "Val Loss: 0.00023637756953989579 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00019913572445870632 // Train Acc: 0.0\n",
      "Val Loss: 0.0002250313502207229 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00018996620691553095 // Train Acc: 0.0\n",
      "Val Loss: 0.00021416067479135978 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00018153886075400572 // Train Acc: 0.0\n",
      "Val Loss: 0.00020373221949822353 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00017376358332242667 // Train Acc: 0.0\n",
      "Val Loss: 0.0001937347935216332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0001665723212602863 // Train Acc: 0.0\n",
      "Val Loss: 0.00018418321818568405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.00015990681783548667 // Train Acc: 0.0\n",
      "Val Loss: 0.00017509567458711734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00015371484845025108 // Train Acc: 0.0\n",
      "Val Loss: 0.00016650434633695775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0001479431841920872 // Train Acc: 0.0\n",
      "Val Loss: 0.00015842691546739927 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00014253775291817294 // Train Acc: 0.0\n",
      "Val Loss: 0.00015087019359767015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00013744195002053024 // Train Acc: 0.0\n",
      "Val Loss: 0.00014381766635448333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0001326005329238676 // Train Acc: 0.0\n",
      "Val Loss: 0.00013723342047094114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.00012795845550374918 // Train Acc: 0.0\n",
      "Val Loss: 0.00013106845832556825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0001234671777206235 // Train Acc: 0.0\n",
      "Val Loss: 0.00012527000111400743 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.00011909085031668974 // Train Acc: 0.0\n",
      "Val Loss: 0.00011979176025223834 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.00011481266355404596 // Train Acc: 0.0\n",
      "Val Loss: 0.00011460521021614444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0001106442320122693 // Train Acc: 0.0\n",
      "Val Loss: 0.0001097020038948606 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.00010662488472572481 // Train Acc: 0.0\n",
      "Val Loss: 0.0001050950889872514 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00010281739662176522 // Train Acc: 0.0\n",
      "Val Loss: 0.00010081257205456496 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 9.928983453304781e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.688399156650782e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 9.608909074390109e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.332928582973016e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 9.322553337764284e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.013763265102171e-05 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [08:22<19:31, 167.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.15789164493039046 // Train Acc: 0.0\n",
      "Val Loss: 0.10773552889851007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.027620522475473957 // Train Acc: 0.0\n",
      "Val Loss: 0.023041584884578532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008478300964361966 // Train Acc: 0.0\n",
      "Val Loss: 0.0064589079998602925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.005249837637755629 // Train Acc: 0.0\n",
      "Val Loss: 0.004125881700945849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.003873630474666006 // Train Acc: 0.0\n",
      "Val Loss: 0.0031619484451684086 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0029101947165141094 // Train Acc: 0.0\n",
      "Val Loss: 0.002425147355957465 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0023048338767345227 // Train Acc: 0.0\n",
      "Val Loss: 0.001987093665891073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0019378958912630094 // Train Acc: 0.0\n",
      "Val Loss: 0.001671678477644243 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.001699673503873334 // Train Acc: 0.0\n",
      "Val Loss: 0.0014485490529543975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0015436899151916714 // Train Acc: 0.0\n",
      "Val Loss: 0.0013035999939099631 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0014288735451850802 // Train Acc: 0.0\n",
      "Val Loss: 0.0012009734954600308 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0013302679247537534 // Train Acc: 0.0\n",
      "Val Loss: 0.0011160095222294331 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0012376941022884153 // Train Acc: 0.0\n",
      "Val Loss: 0.0010390073955270716 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.001147751111274619 // Train Acc: 0.0\n",
      "Val Loss: 0.0009664567256219347 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0010591673063535295 // Train Acc: 0.0\n",
      "Val Loss: 0.00089652650534514 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0009712531799067781 // Train Acc: 0.0\n",
      "Val Loss: 0.0008280665248590098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0008835502571668731 // Train Acc: 0.0\n",
      "Val Loss: 0.0007603417865042998 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0007958367777766115 // Train Acc: 0.0\n",
      "Val Loss: 0.0006926397848556834 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0007082942417279506 // Train Acc: 0.0\n",
      "Val Loss: 0.0006240461532301693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0006217653054470775 // Train Acc: 0.0\n",
      "Val Loss: 0.000554160540658896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0005381748655153923 // Train Acc: 0.0\n",
      "Val Loss: 0.00048432916254651816 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0004607462830528974 // Train Acc: 0.0\n",
      "Val Loss: 0.0004182511969702318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0003931603269727548 // Train Acc: 0.0\n",
      "Val Loss: 0.00036003533023176715 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0003377267343048474 // Train Acc: 0.0\n",
      "Val Loss: 0.00031172706790543586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0002941644937451185 // Train Acc: 0.0\n",
      "Val Loss: 0.0002728307274413634 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00026031171402800404 // Train Acc: 0.0\n",
      "Val Loss: 0.00024182362130059946 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0002336551354691433 // Train Acc: 0.0\n",
      "Val Loss: 0.0002173426352039149 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00021211224613837753 // Train Acc: 0.0\n",
      "Val Loss: 0.00019799907749984413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.00019423018217144735 // Train Acc: 0.0\n",
      "Val Loss: 0.00018244449767569843 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0001791469997413548 // Train Acc: 0.0\n",
      "Val Loss: 0.00016970552420719865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0001663845236113903 // Train Acc: 0.0\n",
      "Val Loss: 0.0001591500613854309 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0001555550716895521 // Train Acc: 0.0\n",
      "Val Loss: 0.00015022557451051068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00014622425435567552 // Train Acc: 0.0\n",
      "Val Loss: 0.00014238995949695396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00013796148739018122 // Train Acc: 0.0\n",
      "Val Loss: 0.00013520205188383857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.00013041614301858328 // Train Acc: 0.0\n",
      "Val Loss: 0.00012838475575501269 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00012335677729840083 // Train Acc: 0.0\n",
      "Val Loss: 0.00012181088333653117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.00011668990896707752 // Train Acc: 0.0\n",
      "Val Loss: 0.00011548512444609183 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00011045994245470874 // Train Acc: 0.0\n",
      "Val Loss: 0.00010953257250631313 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00010481589782032074 // Train Acc: 0.0\n",
      "Val Loss: 0.00010415763162282846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 9.991502827596872e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.953321367158258e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 9.582067219966137e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.569421712182124e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 9.24626325684909e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.253180948938031e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 8.969691218857464e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.98835523747204e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 8.73731477685766e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.761140984874642e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 8.536880084889017e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.562003429026596e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 8.359349382385828e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.384487866185902e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 8.198371170738235e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.223648837883957e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 8.049511004202907e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.075971920995719e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 7.909903773051483e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.938637837386605e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 7.777561199354045e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.809827045341742e-05 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [11:09<16:43, 167.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2583575837280108 // Train Acc: 0.0\n",
      "Val Loss: 0.15367069562727756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02802023785954265 // Train Acc: 0.0\n",
      "Val Loss: 0.024327773397619074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.009310032153336254 // Train Acc: 0.0\n",
      "Val Loss: 0.007256092998961156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.006592986297785181 // Train Acc: 0.0\n",
      "Val Loss: 0.005116208198226311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0049769769350415495 // Train Acc: 0.0\n",
      "Val Loss: 0.0039716073100201105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0036666638647912062 // Train Acc: 0.0\n",
      "Val Loss: 0.003143920412879776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0025762277535698696 // Train Acc: 0.0\n",
      "Val Loss: 0.002386670280247927 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0018835978332128734 // Train Acc: 0.0\n",
      "Val Loss: 0.001675330189226026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0014165664420220745 // Train Acc: 0.0\n",
      "Val Loss: 0.0011628156527876854 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0009705476413271496 // Train Acc: 0.0\n",
      "Val Loss: 0.0007499367353210057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0006499486530442046 // Train Acc: 0.0\n",
      "Val Loss: 0.0004779871195999228 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00044597271165638296 // Train Acc: 0.0\n",
      "Val Loss: 0.0003175277503694154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0003062056034131477 // Train Acc: 0.0\n",
      "Val Loss: 0.00022798284798310222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00023526196603859418 // Train Acc: 0.0\n",
      "Val Loss: 0.00019801846620711412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0001973284350441807 // Train Acc: 0.0\n",
      "Val Loss: 0.00017672260186042297 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0001696314426209697 // Train Acc: 0.0\n",
      "Val Loss: 0.0001560896106556439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00014987611914514437 // Train Acc: 0.0\n",
      "Val Loss: 0.0001401567475775003 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00013533613024123724 // Train Acc: 0.0\n",
      "Val Loss: 0.00012842041700919667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0001227832380703493 // Train Acc: 0.0\n",
      "Val Loss: 0.00011862104147439823 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00011150153024000123 // Train Acc: 0.0\n",
      "Val Loss: 0.00010956773648186672 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00010316142531864815 // Train Acc: 0.0\n",
      "Val Loss: 0.00010269400987784717 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 9.683696783018048e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.732388895248401e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 9.187309234266857e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.297822772086048e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 8.78461197071914e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.933105474914721e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 8.445525570208054e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.61535351181043e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 8.148259249417224e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.328180050408594e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 7.876944276023141e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.059865394898225e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 7.619424735879809e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.801262515634086e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 7.365635026863735e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.543844092817215e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 7.109399745566398e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.280689136347395e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 6.854602895341133e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.012626827583351e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 6.610897256589822e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.751043937666427e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 6.38318611292985e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.506787021035879e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 6.173203751152715e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.283001594883072e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 5.9817865778861906e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.0788655338745396e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 5.80905587113405e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.893410651134962e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 5.654233052350366e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.72621648230283e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 5.51585807812801e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.5766244027340274e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 5.3919077970217234e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.4432945779064376e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 5.280239855639103e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.3243801151190634e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 5.178790798214887e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.217816853308415e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 5.085746477262758e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.121692247485043e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 4.999619146333088e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.034350704756269e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 4.9192578290831284e-05 // Train Acc: 0.0\n",
      "Val Loss: 4.954404407313136e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 4.843769827600131e-05 // Train Acc: 0.0\n",
      "Val Loss: 4.880781112164682e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 4.772507558607594e-05 // Train Acc: 0.0\n",
      "Val Loss: 4.812591302073666e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 4.7049730509151064e-05 // Train Acc: 0.0\n",
      "Val Loss: 4.749133040604647e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 4.6407809252463324e-05 // Train Acc: 0.0\n",
      "Val Loss: 4.689787451339229e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 4.579642234511578e-05 // Train Acc: 0.0\n",
      "Val Loss: 4.634110127054472e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 4.521317053385574e-05 // Train Acc: 0.0\n",
      "Val Loss: 4.581677200225055e-05 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [13:56<13:55, 167.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.15722697556495122 // Train Acc: 0.0\n",
      "Val Loss: 0.09980421763929453 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.028797634756064987 // Train Acc: 0.0\n",
      "Val Loss: 0.02483731782571836 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008066986447208685 // Train Acc: 0.0\n",
      "Val Loss: 0.006284761272201484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0047811768993152705 // Train Acc: 0.0\n",
      "Val Loss: 0.003894601487131281 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0032797458182412228 // Train Acc: 0.0\n",
      "Val Loss: 0.002821328735444695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0023939400896978605 // Train Acc: 0.0\n",
      "Val Loss: 0.0020639612201855265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0017407591995298353 // Train Acc: 0.0\n",
      "Val Loss: 0.0014781325960277834 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.001197255144510248 // Train Acc: 0.0\n",
      "Val Loss: 0.0009319781473922458 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0008035180090206832 // Train Acc: 0.0\n",
      "Val Loss: 0.0006241042804586785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0005981433637877161 // Train Acc: 0.0\n",
      "Val Loss: 0.0004911895968358625 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00047326060150192886 // Train Acc: 0.0\n",
      "Val Loss: 0.0003997919718544422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00038789658563755894 // Train Acc: 0.0\n",
      "Val Loss: 0.0003302933489480479 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00032517979315298154 // Train Acc: 0.0\n",
      "Val Loss: 0.0002755609530140646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00027721713339728114 // Train Acc: 0.0\n",
      "Val Loss: 0.00023243547210850837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00023837761668946602 // Train Acc: 0.0\n",
      "Val Loss: 0.0001972198331548663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0002040792368291282 // Train Acc: 0.0\n",
      "Val Loss: 0.000166137566504238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00017454273452608608 // Train Acc: 0.0\n",
      "Val Loss: 0.0001397745830631307 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00015349477792589832 // Train Acc: 0.0\n",
      "Val Loss: 0.00012206527608213947 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0001383736208776282 // Train Acc: 0.0\n",
      "Val Loss: 0.00011012797311361117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0001272281515245915 // Train Acc: 0.0\n",
      "Val Loss: 0.0001019263905949298 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00011903133640543146 // Train Acc: 0.0\n",
      "Val Loss: 9.63852907509797e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00011299068524359223 // Train Acc: 0.0\n",
      "Val Loss: 9.257684842768041e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00010847958864896138 // Train Acc: 0.0\n",
      "Val Loss: 8.982953268886459e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00010501786734024807 // Train Acc: 0.0\n",
      "Val Loss: 8.77181302877778e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0001022601098480813 // Train Acc: 0.0\n",
      "Val Loss: 8.599144692363387e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 9.997624105688282e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.450904894165102e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 9.80222388343422e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.319385101045059e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 9.630847351583567e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.200407001740215e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 9.477706183519929e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.091504979264838e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 9.338905044167396e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.99107094776859e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 9.211652432650765e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.89787904068362e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 9.093906795000019e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.811000885505399e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 8.984114436847881e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.729713596001437e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 8.88104529111257e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.653336367432281e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 8.783791237716727e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.581268992825327e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 8.691584497014301e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.513086146156473e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 8.60385055324572e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.448345250767571e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 8.52007992076847e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.386676112549718e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 8.439886634948782e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.327775754013352e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 8.362959153567745e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.271389027168467e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 8.288984031527575e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.217294742903587e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 8.217744487651399e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.165274219005369e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 8.149049091992738e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.115205679094123e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 8.082715442274447e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.066900639487853e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 8.018595419601393e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.020293689327611e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 7.956545358695425e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.975255204121243e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 7.896466373810683e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.931697607797105e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 7.838231251971343e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.889575852255802e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 7.781776032858681e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.848829692568292e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 7.726987426656407e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.809424858974208e-05 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [16:43<11:08, 167.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.08241737665412906 // Train Acc: 0.0\n",
      "Val Loss: 0.058827621388164435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01934713514555739 // Train Acc: 0.0\n",
      "Val Loss: 0.015595147089863365 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008677247868633943 // Train Acc: 0.0\n",
      "Val Loss: 0.006853595062751662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.005885763196955057 // Train Acc: 0.0\n",
      "Val Loss: 0.004630199890710752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.004597808408219214 // Train Acc: 0.0\n",
      "Val Loss: 0.0037645986208438196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0036819486636413285 // Train Acc: 0.0\n",
      "Val Loss: 0.0031119803665205837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.002894674836855688 // Train Acc: 0.0\n",
      "Val Loss: 0.0023951358585195107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.001751719142861035 // Train Acc: 0.0\n",
      "Val Loss: 0.0014077005611563271 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0012804595944918183 // Train Acc: 0.0\n",
      "Val Loss: 0.0011174434795975686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0010842326786645506 // Train Acc: 0.0\n",
      "Val Loss: 0.000981706742789935 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0009545864240738423 // Train Acc: 0.0\n",
      "Val Loss: 0.0008726315123071386 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.000837803115009343 // Train Acc: 0.0\n",
      "Val Loss: 0.0007728509123394774 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0007228008726586218 // Train Acc: 0.0\n",
      "Val Loss: 0.0006788249042901126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0006089685565167953 // Train Acc: 0.0\n",
      "Val Loss: 0.0005891873913986439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0004999296190578609 // Train Acc: 0.0\n",
      "Val Loss: 0.0005004799480295994 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0004045179972375135 // Train Acc: 0.0\n",
      "Val Loss: 0.0004146153252804652 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.000329081909421387 // Train Acc: 0.0\n",
      "Val Loss: 0.0003415035528384826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.000272228887127117 // Train Acc: 0.0\n",
      "Val Loss: 0.00028454041986895553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00022999183426969407 // Train Acc: 0.0\n",
      "Val Loss: 0.00024099637322995643 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0001982881988055332 // Train Acc: 0.0\n",
      "Val Loss: 0.00020718694233272055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00017375533688828332 // Train Acc: 0.0\n",
      "Val Loss: 0.00018023883934471417 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00015406366966924732 // Train Acc: 0.0\n",
      "Val Loss: 0.00015823469756552104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0001378176877975196 // Train Acc: 0.0\n",
      "Val Loss: 0.00014005283393833616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00012424537878056677 // Train Acc: 0.0\n",
      "Val Loss: 0.00012507477672029794 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00011289023953573948 // Train Acc: 0.0\n",
      "Val Loss: 0.00011286455357383767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00010341484347540677 // Train Acc: 0.0\n",
      "Val Loss: 0.00010298789473546838 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 9.552952622171899e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.501631965659644e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 8.898603226699798e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.85791393309112e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 8.357341052797149e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.337555521972139e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 7.90981364046536e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.91590932666705e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 7.5375064608793e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.572433278255631e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 7.22368840465336e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.289539872462311e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 6.95498643030219e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.052732802218419e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 6.721588953834858e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.850345889688469e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 6.51652618677735e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.673631055904976e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 6.334646992009562e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.515922723338008e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 6.171904471108837e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.372452153300401e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 6.025041013394725e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.239741166857791e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 5.891390267785343e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.115503443156327e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 5.768784200744735e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.998161382608073e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 5.6555244052550355e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.8866640888895334e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 5.550212941446648e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.780358151241671e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 5.451759530327572e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.678774124879221e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 5.359294414639204e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.581583705365615e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 5.272095373775342e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.48854322418761e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 5.1895561423054344e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.399395595304668e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 5.111189270107002e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.313921385756905e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 5.0365462886649566e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.231930099398596e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 4.965272573951539e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.153145325907231e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 4.8970847706082935e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.077404407943091e-05 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [19:30<08:21, 167.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.12731282830612572 // Train Acc: 0.0\n",
      "Val Loss: 0.08710693940520287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02060175855052934 // Train Acc: 0.0\n",
      "Val Loss: 0.01619297316805883 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008637057686411695 // Train Acc: 0.0\n",
      "Val Loss: 0.0069250598063015124 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00562113725641938 // Train Acc: 0.0\n",
      "Val Loss: 0.004422271137379787 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.004423475774859831 // Train Acc: 0.0\n",
      "Val Loss: 0.003457836563360285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0036628552928851857 // Train Acc: 0.0\n",
      "Val Loss: 0.0029045424519360745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.002868208066734685 // Train Acc: 0.0\n",
      "Val Loss: 0.002317111343357035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0017677961248525984 // Train Acc: 0.0\n",
      "Val Loss: 0.0014848078643395141 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0012712890442277912 // Train Acc: 0.0\n",
      "Val Loss: 0.0011013465499590067 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0009887801385197585 // Train Acc: 0.0\n",
      "Val Loss: 0.0008744527347682214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0008139482935354726 // Train Acc: 0.0\n",
      "Val Loss: 0.0007302398380654102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0007016881651853181 // Train Acc: 0.0\n",
      "Val Loss: 0.0006368775767358867 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.000620899214058451 // Train Acc: 0.0\n",
      "Val Loss: 0.0005687557494225489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0005557600692654513 // Train Acc: 0.0\n",
      "Val Loss: 0.0005119332601845434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0004987547436695052 // Train Acc: 0.0\n",
      "Val Loss: 0.0004603999359956519 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00044640692034109017 // Train Acc: 0.0\n",
      "Val Loss: 0.00041194931863256813 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0003972618328859247 // Train Acc: 0.0\n",
      "Val Loss: 0.00036598050157243215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.000351032959076695 // Train Acc: 0.0\n",
      "Val Loss: 0.00032264779702844944 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.000308102876738407 // Train Acc: 0.0\n",
      "Val Loss: 0.0002824165662538938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0002690132918459848 // Train Acc: 0.0\n",
      "Val Loss: 0.0002456699357770214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00023409434247094306 // Train Acc: 0.0\n",
      "Val Loss: 0.00021253100554035468 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0002033969681905625 // Train Acc: 0.0\n",
      "Val Loss: 0.0001829487263669514 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.000176822138869429 // Train Acc: 0.0\n",
      "Val Loss: 0.00015686752515400507 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00015418543353422703 // Train Acc: 0.0\n",
      "Val Loss: 0.0001342495333202268 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00013521714723729061 // Train Acc: 0.0\n",
      "Val Loss: 0.00011507931637672961 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00012072571423718042 // Train Acc: 0.0\n",
      "Val Loss: 0.00010119521254885265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00011081414642880913 // Train Acc: 0.0\n",
      "Val Loss: 9.288863247175785e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00010375083736385874 // Train Acc: 0.0\n",
      "Val Loss: 8.722798811504618e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 9.853746326383496e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.306926187783988e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 9.452604953695055e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.989585217067294e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 9.128764798262977e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.737030398195864e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 8.85568590100895e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.527613306592684e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 8.61728956839697e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.347690030159852e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 8.403791260406413e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.188587233858099e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 8.208882885411771e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.044526798629455e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 8.028519555015201e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.911674317962024e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 7.859909864689795e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.787221368127079e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 7.701081592201831e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.669346489202739e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 7.55060622703149e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.556722695346583e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 7.407428294347947e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.448426915581381e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 7.270817690011705e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.343917778973595e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 7.140173056956538e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.242832466589541e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 7.015153584676502e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.145019545907747e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 6.895415575539089e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.050431379249362e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 6.780839126580235e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.9591107690886766e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 6.671284306018303e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.871143317067022e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 6.566658959198811e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.7865999306986026e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 6.46685170582905e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.705559306079522e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 6.371745259299573e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.628029929622161e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 6.281151035261026e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.553931613659105e-05 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [22:17<05:34, 167.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3290827472324241 // Train Acc: 0.0\n",
      "Val Loss: 0.234293278103525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02674810598353477 // Train Acc: 0.0\n",
      "Val Loss: 0.023388570682568985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.009371005199088388 // Train Acc: 0.0\n",
      "Val Loss: 0.007864133924753828 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0060338664140241844 // Train Acc: 0.0\n",
      "Val Loss: 0.004866214430975643 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.004575572303521817 // Train Acc: 0.0\n",
      "Val Loss: 0.003623188716698099 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0035485581029206514 // Train Acc: 0.0\n",
      "Val Loss: 0.002851108081681146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.002606090460026841 // Train Acc: 0.0\n",
      "Val Loss: 0.002126246326687661 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.001692290095448681 // Train Acc: 0.0\n",
      "Val Loss: 0.0013936649023724552 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0009580030964238816 // Train Acc: 0.0\n",
      "Val Loss: 0.0007795812350443819 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0004869373054105414 // Train Acc: 0.0\n",
      "Val Loss: 0.00039286244110288946 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.000285948253563365 // Train Acc: 0.0\n",
      "Val Loss: 0.00025995889895553276 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00023639313825464607 // Train Acc: 0.0\n",
      "Val Loss: 0.0002182742662791332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00020925030342837982 // Train Acc: 0.0\n",
      "Val Loss: 0.00018881239904492924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00018767422254114372 // Train Acc: 0.0\n",
      "Val Loss: 0.0001675938741589727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00016964565575565104 // Train Acc: 0.0\n",
      "Val Loss: 0.00015109304936645045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00015410561870451864 // Train Acc: 0.0\n",
      "Val Loss: 0.00013765814517021434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00014024837140964565 // Train Acc: 0.0\n",
      "Val Loss: 0.00012638933410264805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00012791468937281672 // Train Acc: 0.0\n",
      "Val Loss: 0.00011698265522930094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00011757694440734356 // Train Acc: 0.0\n",
      "Val Loss: 0.0001094682363724463 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00010936454348927938 // Train Acc: 0.0\n",
      "Val Loss: 0.00010349715614045801 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00010274115765943578 // Train Acc: 0.0\n",
      "Val Loss: 9.836279833424752e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 9.720045080146387e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.367797819405414e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 9.25291911017795e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.947047744269102e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 8.861991566484667e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.58279942225305e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 8.533171863605047e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.268540808454749e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 8.251087552649617e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.991471793502569e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 8.003843730331779e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.742805610178038e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 7.783445391695539e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.517868943978101e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 7.584475494101782e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.313765275863592e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 7.403095709104155e-05 // Train Acc: 0.0\n",
      "Val Loss: 7.1283045029056e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 7.236431168802146e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.95949610830708e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 7.082247095207522e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.805541829221925e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 6.938792881290067e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.664812423299405e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 6.804612862921934e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.535832223810509e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 6.678589667305489e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.417287113436032e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 6.559783766479667e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.308053532186684e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 6.447447041665166e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.207057631399949e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 6.340979481349565e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.113335660881024e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 6.239907782828885e-05 // Train Acc: 0.0\n",
      "Val Loss: 6.026109956358348e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 6.143836153112458e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.944572896458505e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 6.052434040296219e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.868125005111903e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 5.965412886213664e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.796138168741229e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 5.882546024435688e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.728037074070678e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 5.803601927097119e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.6634045838357205e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 5.728371914168639e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.6018171480073676e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 5.656644556372103e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.542883974902163e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 5.588232137323421e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.4863402858080174e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 5.5229400401070104e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.4318853488604705e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 5.460556786855285e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.3792912810670464e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 5.400897696155515e-05 // Train Acc: 0.0\n",
      "Val Loss: 5.3283007781084795e-05 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [25:05<02:47, 167.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1749766087882459 // Train Acc: 0.0\n",
      "Val Loss: 0.11038127900524573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02751800289045866 // Train Acc: 0.0\n",
      "Val Loss: 0.023841241527010094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008666235285908937 // Train Acc: 0.0\n",
      "Val Loss: 0.006833514643155716 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0063834436494032065 // Train Acc: 0.0\n",
      "Val Loss: 0.0049382136817174875 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.004867546085999709 // Train Acc: 0.0\n",
      "Val Loss: 0.0038872605782340874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0036323455488734527 // Train Acc: 0.0\n",
      "Val Loss: 0.0030952931466427715 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.002789806771798051 // Train Acc: 0.0\n",
      "Val Loss: 0.002497673230457374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0021920508780031885 // Train Acc: 0.0\n",
      "Val Loss: 0.001977147451940585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0017081654514186084 // Train Acc: 0.0\n",
      "Val Loss: 0.0015153402071022852 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0012936326649867118 // Train Acc: 0.0\n",
      "Val Loss: 0.001122676675864072 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0009418467264920133 // Train Acc: 0.0\n",
      "Val Loss: 0.0008078093878628517 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0006638926599540463 // Train Acc: 0.0\n",
      "Val Loss: 0.0005780911586374383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00047089058269808267 // Train Acc: 0.0\n",
      "Val Loss: 0.00042788356050467967 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.000349874674399983 // Train Acc: 0.0\n",
      "Val Loss: 0.00032473596670156855 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00027534569138272974 // Train Acc: 0.0\n",
      "Val Loss: 0.0002532873146595772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00023191369149422477 // Train Acc: 0.0\n",
      "Val Loss: 0.0002100876306278885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0002070490433017194 // Train Acc: 0.0\n",
      "Val Loss: 0.00018654531569071962 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0001910694526607058 // Train Acc: 0.0\n",
      "Val Loss: 0.00017286164154277437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00017921453144228018 // Train Acc: 0.0\n",
      "Val Loss: 0.00016351888189092278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00016961010496161844 // Train Acc: 0.0\n",
      "Val Loss: 0.00015622541289882395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00016138314129744835 // Train Acc: 0.0\n",
      "Val Loss: 0.00014997790898302232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00015406714824145793 // Train Acc: 0.0\n",
      "Val Loss: 0.00014435469903136518 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00014743116091815627 // Train Acc: 0.0\n",
      "Val Loss: 0.00013923484617738393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00014136257619221194 // Train Acc: 0.0\n",
      "Val Loss: 0.0001345957999794998 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0001358002348889099 // Train Acc: 0.0\n",
      "Val Loss: 0.00013041710573650727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00013070104682706956 // Train Acc: 0.0\n",
      "Val Loss: 0.0001266607522583482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00012602931811882514 // Train Acc: 0.0\n",
      "Val Loss: 0.00012328000348960896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00012175333427693737 // Train Acc: 0.0\n",
      "Val Loss: 0.00012022655130501582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.00011784187285495352 // Train Acc: 0.0\n",
      "Val Loss: 0.000117462542535081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00011426361758311356 // Train Acc: 0.0\n",
      "Val Loss: 0.00011494629652588628 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00011098859020371679 // Train Acc: 0.0\n",
      "Val Loss: 0.0001126412625705019 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00010798614453279825 // Train Acc: 0.0\n",
      "Val Loss: 0.00011050664417614991 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0001052276869539701 // Train Acc: 0.0\n",
      "Val Loss: 0.00010850114366886291 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00010268678896319968 // Train Acc: 0.0\n",
      "Val Loss: 0.00010658627685519274 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.00010033936393471142 // Train Acc: 0.0\n",
      "Val Loss: 0.00010472749001399444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 9.816513060178548e-05 // Train Acc: 0.0\n",
      "Val Loss: 0.00010290417419376106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 9.61460144391664e-05 // Train Acc: 0.0\n",
      "Val Loss: 0.00010109810490658591 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 9.426775541713092e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.930272089232776e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 9.251776965900875e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.751551842782647e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 9.088524756693976e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.574264153012668e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 8.936026775808133e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.399099329708736e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 8.793361035655901e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.227239338956265e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 8.659620445259405e-05 // Train Acc: 0.0\n",
      "Val Loss: 9.059458957794546e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 8.533904957033525e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.896874642232433e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 8.415311151202747e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.739911146139176e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 8.302961048925536e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.588818362279033e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 8.196034428011572e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.443606983002444e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 8.093781891405438e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.304181825554803e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 7.995460944244379e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.170159995973914e-05 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 7.90044121616589e-05 // Train Acc: 0.0\n",
      "Val Loss: 8.040981205836446e-05 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [27:52<00:00, 167.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1057, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1056, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.14975550012676628 // Train Acc: 0.0\n",
      "Val Loss: 0.07418660808573752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.009319678165267638 // Train Acc: 0.0\n",
      "Val Loss: 0.011682132782880217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.005133006875926775 // Train Acc: 0.0\n",
      "Val Loss: 0.007611056522685377 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00421502664309613 // Train Acc: 0.0\n",
      "Val Loss: 0.006380569456857355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0038712001400867772 // Train Acc: 0.0\n",
      "Val Loss: 0.006019733880427392 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.003577735450477816 // Train Acc: 0.0\n",
      "Val Loss: 0.0058096172835897 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0032913469848404146 // Train Acc: 0.0\n",
      "Val Loss: 0.005672393733060316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.003086767358719714 // Train Acc: 0.0\n",
      "Val Loss: 0.005589424824232564 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0029725804351002252 // Train Acc: 0.0\n",
      "Val Loss: 0.005502844996312086 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0028973933741298355 // Train Acc: 0.0\n",
      "Val Loss: 0.005390877883388277 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0028377464806220035 // Train Acc: 0.0\n",
      "Val Loss: 0.005277326142223661 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.002786199003798543 // Train Acc: 0.0\n",
      "Val Loss: 0.005170811416225179 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.002740789929664927 // Train Acc: 0.0\n",
      "Val Loss: 0.005072742491713999 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.002700800763153032 // Train Acc: 0.0\n",
      "Val Loss: 0.004982218714704847 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0026655771879028527 // Train Acc: 0.0\n",
      "Val Loss: 0.0048972283565329715 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0026343000634908385 // Train Acc: 0.0\n",
      "Val Loss: 0.0048156806904658235 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.002606001978326577 // Train Acc: 0.0\n",
      "Val Loss: 0.0047355758382336185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0025796849082721654 // Train Acc: 0.0\n",
      "Val Loss: 0.004654930548413712 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.002554505158768468 // Train Acc: 0.0\n",
      "Val Loss: 0.004572096972397584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.002529922658972352 // Train Acc: 0.0\n",
      "Val Loss: 0.004486241258050808 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0025058137800818407 // Train Acc: 0.0\n",
      "Val Loss: 0.004398004141608801 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00248249246994454 // Train Acc: 0.0\n",
      "Val Loss: 0.004309988361509407 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0024605099977674286 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004226366263430785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0024402899603985957 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004150969469371964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0024219074487001713 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004085478557766799 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0024051640963508964 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004029231868437766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.002389758295886422 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00398039969149977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0023753969878821516 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003936994005925953 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.002361841880895369 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0038973716197206695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0023489183171730363 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0038603021975551896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.002336493734207764 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0038249593652675256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00232449561642737 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0037908362950581837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.002312876508067663 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0037576351006624892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.002301613465839528 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003725182114388136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.002290696387933767 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0036933606675387744 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0022801219540263163 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003662101931267363 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0022698794826256515 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0036313371349345237 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.002259949414467931 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0036010126316207737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.002250307404757643 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0035710766370517805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0022409233924702075 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00354146770105752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.002231761282923358 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003512153107508579 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0022227882154184178 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0034831092686063666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0022139599968130585 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0034543336334857434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.00220524261035604 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0034258083289708287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0021966045725265123 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003397563725764699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0021880159732593878 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003369563355055802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0021794503759146903 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0033418290378690206 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0021708876480241147 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003314366002621896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.002162319953354706 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003287203116890262 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0021537493631468025 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0032604529679862455 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [06:35<59:16, 395.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.08375596817475582 // Train Acc: 0.0\n",
      "Val Loss: 0.061064453905119616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.008387789447692699 // Train Acc: 0.0\n",
      "Val Loss: 0.01104611878091579 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.005706742629810011 // Train Acc: 0.0\n",
      "Val Loss: 0.008394082340494017 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.004642807388843345 // Train Acc: 0.0\n",
      "Val Loss: 0.00707216581042089 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.004215186349434652 // Train Acc: 0.0\n",
      "Val Loss: 0.0063807440182084545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.003877811071344173 // Train Acc: 0.0\n",
      "Val Loss: 0.005519724471549339 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.003478464140278803 // Train Acc: 0.0\n",
      "Val Loss: 0.005138810073463794 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.003296275309012893 // Train Acc: 0.0\n",
      "Val Loss: 0.0049728868419633195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0031590658129138404 // Train Acc: 0.0\n",
      "Val Loss: 0.004814267833032371 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0030265785772083122 // Train Acc: 0.0\n",
      "Val Loss: 0.00468626941609032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 97\n",
      "INFO: Validation loss did not improve in epoch 98\n",
      "INFO: Validation loss did not improve in epoch 99\n",
      "INFO: Validation loss did not improve in epoch 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [07:55<27:59, 209.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 101\n",
      "Early stopping after 101 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.09309038365646256 // Train Acc: 0.0\n",
      "Val Loss: 0.06430441897143335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.008819708781692925 // Train Acc: 0.0\n",
      "Val Loss: 0.011330292416352998 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.004998788094500052 // Train Acc: 0.0\n",
      "Val Loss: 0.007259120938696843 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0037640217274309933 // Train Acc: 0.0\n",
      "Val Loss: 0.006025559618137777 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 41\n",
      "Epoch: 41\n",
      "Train Loss: 0.0034337883414632523 // Train Acc: 0.0\n",
      "Val Loss: 0.005960657227072208 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 42\n",
      "INFO: Validation loss did not improve in epoch 43\n",
      "INFO: Validation loss did not improve in epoch 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [08:30<15:12, 130.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 45\n",
      "Early stopping after 45 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.10352233257076586 // Train Acc: 0.0\n",
      "Val Loss: 0.06932306256802644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.008451589265705462 // Train Acc: 0.0\n",
      "Val Loss: 0.010962053660905975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.005004079963357896 // Train Acc: 0.0\n",
      "Val Loss: 0.007378769079771112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.004316604143028595 // Train Acc: 0.0\n",
      "Val Loss: 0.006640136899317012 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0038702141013531905 // Train Acc: 0.0\n",
      "Val Loss: 0.006082287838901667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0035501503762516654 // Train Acc: 0.0\n",
      "Val Loss: 0.0056718101053882175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.003340097963463062 // Train Acc: 0.0\n",
      "Val Loss: 0.005404840353602434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.003170761145974622 // Train Acc: 0.0\n",
      "Val Loss: 0.005207836155450958 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0030195490934431016 // Train Acc: 0.0\n",
      "Val Loss: 0.005116724083884893 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 83\n",
      "INFO: Validation loss did not improve in epoch 84\n",
      "INFO: Validation loss did not improve in epoch 85\n",
      "INFO: Validation loss did not improve in epoch 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [09:39<10:36, 106.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 87\n",
      "Early stopping after 87 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.11127483466609468 // Train Acc: 0.0\n",
      "Val Loss: 0.06924112141132355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.008635097853699465 // Train Acc: 0.0\n",
      "Val Loss: 0.011324105921256192 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0048775333677924205 // Train Acc: 0.0\n",
      "Val Loss: 0.007265059386982638 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.004423429614117699 // Train Acc: 0.0\n",
      "Val Loss: 0.006505433285115834 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.004042465740753512 // Train Acc: 0.0\n",
      "Val Loss: 0.006096621064524002 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00365960663281359 // Train Acc: 0.0\n",
      "Val Loss: 0.005738278779009467 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0033262485130779425 // Train Acc: 0.0\n",
      "Val Loss: 0.005479409934624153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 67\n",
      "INFO: Validation loss did not improve in epoch 68\n",
      "INFO: Validation loss did not improve in epoch 69\n",
      "INFO: Validation loss did not improve in epoch 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [10:36<07:20, 88.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 71\n",
      "Early stopping after 71 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.08641572474115537 // Train Acc: 0.0\n",
      "Val Loss: 0.06757624562391464 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.008330214898684275 // Train Acc: 0.0\n",
      "Val Loss: 0.011068000667043687 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.005836411341368238 // Train Acc: 0.0\n",
      "Val Loss: 0.008352521125583308 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.004267822946875119 // Train Acc: 0.0\n",
      "Val Loss: 0.006756392536986181 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0038439769273256913 // Train Acc: 0.0\n",
      "Val Loss: 0.006427687966018258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 48\n",
      "INFO: Validation loss did not improve in epoch 49\n",
      "INFO: Validation loss did not improve in epoch 50\n",
      "INFO: Validation loss did not improve in epoch 51\n",
      "Epoch: 51\n",
      "Train Loss: 0.0034989560073708957 // Train Acc: 0.0\n",
      "Val Loss: 0.006390101091443177 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [11:17<04:48, 72.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 52\n",
      "Early stopping after 52 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.09184693162698959 // Train Acc: 0.0\n",
      "Val Loss: 0.06365245531367905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.008695900146217433 // Train Acc: 0.0\n",
      "Val Loss: 0.011239560293581556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.00564669391860329 // Train Acc: 0.0\n",
      "Val Loss: 0.008083940093295978 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.004670045762013183 // Train Acc: 0.0\n",
      "Val Loss: 0.006771822956472854 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.004281433355077034 // Train Acc: 0.0\n",
      "Val Loss: 0.006330754392205135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.003844498796729027 // Train Acc: 0.0\n",
      "Val Loss: 0.006018102730569594 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 60\n",
      "INFO: Validation loss did not improve in epoch 61\n",
      "Epoch: 61\n",
      "Train Loss: 0.0035919361515077074 // Train Acc: 0.0\n",
      "Val Loss: 0.0059307236292892515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 62\n",
      "INFO: Validation loss did not improve in epoch 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [12:08<03:15, 65.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 64\n",
      "Early stopping after 64 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.09847451130292588 // Train Acc: 0.0\n",
      "Val Loss: 0.05951713239226271 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.008338244862748494 // Train Acc: 0.0\n",
      "Val Loss: 0.010795941285323352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.005006912233598145 // Train Acc: 0.0\n",
      "Val Loss: 0.007006158418816459 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.004242680294646062 // Train Acc: 0.0\n",
      "Val Loss: 0.006454611265593592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0038499565124877778 // Train Acc: 0.0\n",
      "Val Loss: 0.006241122527760179 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0033115616174762934 // Train Acc: 0.0\n",
      "Val Loss: 0.005853098015958334 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.002943995785693552 // Train Acc: 0.0\n",
      "Val Loss: 0.005537340301088989 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0028304764525142285 // Train Acc: 0.0\n",
      "Val Loss: 0.00525673372356002 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0027494756515933604 // Train Acc: 0.0\n",
      "Val Loss: 0.005017191359965021 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0026832428602990184 // Train Acc: 0.0\n",
      "Val Loss: 0.004804568913053064 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0026281608280164345 // Train Acc: 0.0\n",
      "Val Loss: 0.004620409146060839 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0025823955701653875 // Train Acc: 0.0\n",
      "Val Loss: 0.004466807825819534 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0025439338050651794 // Train Acc: 0.0\n",
      "Val Loss: 0.004341053334749578 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.002510894642590751 // Train Acc: 0.0\n",
      "Val Loss: 0.0042378290296148724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0024817577879457696 // Train Acc: 0.0\n",
      "Val Loss: 0.004151775200358208 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00245538774529525 // Train Acc: 0.0\n",
      "Val Loss: 0.004078265296651379 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.002431010271629431 // Train Acc: 0.0\n",
      "Val Loss: 0.004013615763088798 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0024081339924068444 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003955137946725111 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0023863993850369576 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003901147684219348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00236557661560629 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003850587273893111 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.002345517714161984 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003802846226950779 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0023261595681564875 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0037576035110225134 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.002307489128086662 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003714717017240165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0022894993313453908 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003674139220760587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.002272189473082593 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0036358811365276135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0022555615423482177 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003599945332526284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.002239624948844282 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003566322519498713 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0022243993803078223 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0035349721540970836 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.002209908815421995 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0035058759967796504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0021961675325552874 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.003478961520800915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.002183180216327022 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0034541135777116697 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0021709415618258623 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.003431220117764657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0021594445898721246 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0034101508996065926 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.002148675109654894 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0033908021458260275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0021386054462779413 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.003373074081644197 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0021291984553857266 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0033568619264234953 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0021204107761083717 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.003342054085806012 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.002112192220699267 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0033285316826282617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0021044920930857374 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.003316169303348836 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.00209726151287242 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0033048485467374765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0020904521047577372 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0032944516895119756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.002084019738648794 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.003284845847006449 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0020779251392788792 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0032759185312041905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0020721320262558885 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0032675705630989637 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.002066609086937563 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0032596856369363036 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.002061328118282272 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0032521955025217987 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.002056265886102261 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0032450342585113555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.002051399239603955 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.003238140375983408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.002046709912149923 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0032314799301435843 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.002042179359718489 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0032250114155056723 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [18:44<05:41, 170.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.058259280810517414 // Train Acc: 0.0\n",
      "Val Loss: 0.059420687420403254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.008310544743235402 // Train Acc: 0.0\n",
      "Val Loss: 0.011034261914627516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.004525079480910817 // Train Acc: 0.0\n",
      "Val Loss: 0.007129419314236764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 27\n",
      "INFO: Validation loss did not improve in epoch 28\n",
      "INFO: Validation loss did not improve in epoch 29\n",
      "INFO: Validation loss did not improve in epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [19:08<02:04, 124.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 31\n",
      "Early stopping after 31 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.09080404177532826 // Train Acc: 0.0\n",
      "Val Loss: 0.06524478978313067 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.007273971702447212 // Train Acc: 0.0\n",
      "Val Loss: 0.010155025626505342 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.004672579490388122 // Train Acc: 0.0\n",
      "Val Loss: 0.007821998750681387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.003741056709192775 // Train Acc: 0.0\n",
      "Val Loss: 0.006960604004287983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0032314503266092157 // Train Acc: 0.0\n",
      "Val Loss: 0.006359100095270311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0030218432025596164 // Train Acc: 0.0\n",
      "Val Loss: 0.00605051636504119 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0029196892284521735 // Train Acc: 0.0\n",
      "Val Loss: 0.005861390436835149 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.002855302009478695 // Train Acc: 0.0\n",
      "Val Loss: 0.005730402183390278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0028038546184403056 // Train Acc: 0.0\n",
      "Val Loss: 0.0056169165900963195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0027602166427187435 // Train Acc: 0.0\n",
      "Val Loss: 0.005509698876481065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0027226871746793132 // Train Acc: 0.0\n",
      "Val Loss: 0.0054119931438061245 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.002689994723649723 // Train Acc: 0.0\n",
      "Val Loss: 0.005327323359190761 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0026614592456168595 // Train Acc: 0.0\n",
      "Val Loss: 0.005254393316777971 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0026366707753788907 // Train Acc: 0.0\n",
      "Val Loss: 0.00518986488254193 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0026149178252400495 // Train Acc: 0.0\n",
      "Val Loss: 0.005131924785125782 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0025953586538434215 // Train Acc: 0.0\n",
      "Val Loss: 0.005079768913085847 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.002577368308093239 // Train Acc: 0.0\n",
      "Val Loss: 0.005032345789539463 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0025605745531903355 // Train Acc: 0.0\n",
      "Val Loss: 0.0049885209197835885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.002544758584382484 // Train Acc: 0.0\n",
      "Val Loss: 0.004947348196497735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0025297721191964895 // Train Acc: 0.0\n",
      "Val Loss: 0.004908061329521896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0025155020123945678 // Train Acc: 0.0\n",
      "Val Loss: 0.004870110092109398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.002501847819315348 // Train Acc: 0.0\n",
      "Val Loss: 0.004833128577207818 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0024887110756348797 // Train Acc: 0.0\n",
      "Val Loss: 0.004796873127548572 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0024760027680325986 // Train Acc: 0.0\n",
      "Val Loss: 0.004761192368973485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0024636206485269766 // Train Acc: 0.0\n",
      "Val Loss: 0.00472601386033656 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.002451475233613629 // Train Acc: 0.0\n",
      "Val Loss: 0.004691258490578655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0024394765379998264 // Train Acc: 0.0\n",
      "Val Loss: 0.004656878210987677 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0024275385925884253 // Train Acc: 0.0\n",
      "Val Loss: 0.004622772853027153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.002415584842493533 // Train Acc: 0.0\n",
      "Val Loss: 0.004588743979933069 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0024035571052474047 // Train Acc: 0.0\n",
      "Val Loss: 0.004554488737245693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0023914182674539 // Train Acc: 0.0\n",
      "Val Loss: 0.00451965000042144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.002379162649503165 // Train Acc: 0.0\n",
      "Val Loss: 0.004483804777812432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0023668016599105173 // Train Acc: 0.0\n",
      "Val Loss: 0.004446603864540949 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.002354348453128467 // Train Acc: 0.0\n",
      "Val Loss: 0.004407838822397239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0023418052059053862 // Train Acc: 0.0\n",
      "Val Loss: 0.004367493654546493 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0023291612542945196 // Train Acc: 0.0\n",
      "Val Loss: 0.004325666848350973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0023164044621847893 // Train Acc: 0.0\n",
      "Val Loss: 0.0042825196287594736 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.002303526526391702 // Train Acc: 0.0\n",
      "Val Loss: 0.0042382400681483835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.002290529293172875 // Train Acc: 0.0\n",
      "Val Loss: 0.004193070187123821 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0022774360070713343 // Train Acc: 0.0\n",
      "Val Loss: 0.004147297815721044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0022642847954085304 // Train Acc: 0.0\n",
      "Val Loss: 0.004101305374848273 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0022511397828217095 // Train Acc: 0.0\n",
      "Val Loss: 0.004055514914558872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0022380831907178747 // Train Acc: 0.0\n",
      "Val Loss: 0.00401039474972469 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0022251901327314406 // Train Acc: 0.0\n",
      "Val Loss: 0.003966390429174199 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.002212559910333831 // Train Acc: 0.0\n",
      "Val Loss: 0.003923862617846359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0022002831060714744 // Train Acc: 0.0\n",
      "Val Loss: 0.0038831224425367136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.002188436877218367 // Train Acc: 0.0\n",
      "Val Loss: 0.003844328330802348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0021770818349888285 // Train Acc: 0.0\n",
      "Val Loss: 0.0038075094446813795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0021662514959951483 // Train Acc: 0.0\n",
      "Val Loss: 0.0037726222688113063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0021559605845275728 // Train Acc: 0.0\n",
      "Val Loss: 0.0037395426804912 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [25:43<00:00, 154.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictive performance\n",
    "predictive_results = predictive_evaluation(\n",
    "    data_train_real=data_train_real_numpy, \n",
    "    data_test_real=data_test_real_numpy,\n",
    "    data_syn=data_syn_numpy, \n",
    "    hyperparameters=hyperparameters, \n",
    "    include_baseline=True, \n",
    "    verbose=True)\n",
    "\n",
    "# save results\n",
    "bidirectionality = \"bi\" if hyperparameters[\"bidirectional\"] else 'no_bi'\n",
    "predictive_results.to_csv(DATA_FOLDER / f\"results_{syn_data_type}_{hyperparameters['num_epochs']}_{hyperparameters['num_evaluation_runs']}_{bidirectionality}.csv\", index=False)\n",
    "\n",
    "# split in mse and mae results\n",
    "mse_results = predictive_results.loc[predictive_results['Metric'] == 'MSE']\n",
    "mae_results = predictive_results.loc[predictive_results['Metric'] == 'MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x24bd109c260>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mse_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mse_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MSE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mae_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mae_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MAE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2*1e06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_data_augmentation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
