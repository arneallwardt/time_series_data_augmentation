{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Füge das übergeordnete Verzeichnis zu sys.path hinzu\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "from utilities import split_data_into_sequences, load_sequential_time_series, reconstruct_sequential_data, Scaler, extract_features_and_targets_reg, get_discriminative_test_performance\n",
    "from visual.visual_evaluation import visualize\n",
    "from predictive_evaluation import predictive_evaluation\n",
    "\n",
    "from discriminative.discriminative_model import LSTMClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../../data\")\n",
    "REAL_DATA_FOLDER = DATA_FOLDER / \"real\"\n",
    "SYNTHETIC_DATA_FOLDER = DATA_FOLDER / \"synthetic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of loading data\n",
    "- Laden der Originaldaten: als pd dataframe \n",
    "- Laden der synthetischen, sequentiellen Daten: als np array (GAN, (V)AE)\n",
    "- Laden der synthetischen, sequentiellen Daten: als pd dataframe (brownian, algorithmit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible types: 'timegan_lstm', 'timegan_gru', 'jitter', 'timewarp', 'autoencoder'\n",
    "syn_data_type = 'jitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " syn data:\n",
      "\n",
      "       traffic_volume          temp       rain_1h       snow_1h    clouds_all\n",
      "count    28511.000000  28511.000000  28511.000000  28511.000000  28511.000000\n",
      "mean      3314.609915    282.697251      0.087762      0.000585     42.550345\n",
      "std       1979.334229     12.438217      0.677296      0.008304     38.958564\n",
      "min          0.000000    242.892694      0.000000      0.000000      0.000000\n",
      "25%       1307.756109    273.430078      0.000000      0.000000      2.276123\n",
      "50%       3519.132346    284.438376      0.003428      0.000000     37.938644\n",
      "75%       4957.645301    292.805025      0.052643      0.000568     86.159244\n",
      "max       7499.889454    310.882360     42.007573      0.510301    107.027592\n",
      "\n",
      "\n",
      "real data:\n",
      "\n",
      "       traffic_volume          temp       rain_1h       snow_1h    clouds_all\n",
      "count     28511.00000  28511.000000  28511.000000  28511.000000  28511.000000\n",
      "mean       3313.74238    282.688768      0.061611      0.000250     42.122795\n",
      "std        1971.53206     12.367361      0.678185      0.008298     39.316195\n",
      "min           0.00000    243.390000      0.000000      0.000000      0.000000\n",
      "25%        1289.00000    273.480000      0.000000      0.000000      1.000000\n",
      "50%        3507.00000    284.550000      0.000000      0.000000     40.000000\n",
      "75%        4948.00000    292.790000      0.000000      0.000000     90.000000\n",
      "max        7280.00000    310.070000     42.000000      0.510000    100.000000\n"
     ]
    }
   ],
   "source": [
    "# Load real time series\n",
    "data_real_df = pd.read_csv(REAL_DATA_FOLDER/'metro_interstate_traffic_volume_label_encoded_no_categorical.csv')\n",
    "data_real_numpy = dc(data_real_df).to_numpy()\n",
    "\n",
    "if syn_data_type == 'timegan_lstm':\n",
    "    # load sequential data (which should already be scaled)\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_lstm_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'timegan_gru':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_gru_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'autoencoder':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28478_12_5_autoencoder_unscaled.csv', shape=(28478, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'jitter':\n",
    "    jitter_factor = 0.1\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'mitv_jittered_{str(jitter_factor).replace(\".\", \"\")}.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "elif syn_data_type == 'timewarp':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'mitv_time_warped.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "# Loot at real and syn data\n",
    "df = pd.DataFrame(data_syn_numpy.reshape(-1, data_syn_numpy.shape[-1]), columns=data_real_df.columns)\n",
    "\n",
    "print('\\n\\n syn data:\\n')\n",
    "print(df.describe())\n",
    "\n",
    "print('\\n\\nreal data:\\n')\n",
    "print(data_real_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Predictive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"seq_len\": 12,\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 32,\n",
    "    \"hidden_size\": 4,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"num_evaluation_runs\": 10,\n",
    "    \"num_epochs\": 200,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETERS:\n",
      "seq_len :  12\n",
      "lr :  0.0001\n",
      "batch_size :  32\n",
      "hidden_size :  4\n",
      "num_layers :  1\n",
      "bidirectional :  True\n",
      "num_evaluation_runs :  10\n",
      "num_epochs :  200\n",
      "device :  cpu\n",
      "Synthetic Data is sequential: False\n",
      "Shape of the data after splitting into sequences: (22797, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2841, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2840, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Loss: 0.4690896761417389\n",
      "Training Loss: 0.3858056369423866\n",
      "Training Loss: 0.28619490027427674\n",
      "Validation Loss: 0.1788847628101874\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.1387615136243403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.09119359521195293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# evaluate predictive performance\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictive_results \u001b[38;5;241m=\u001b[39m \u001b[43mpredictive_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_real_numpy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_syn_numpy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_baseline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# save results\u001b[39;00m\n\u001b[1;32m      5\u001b[0m predictive_results\u001b[38;5;241m.\u001b[39mto_csv(DATA_FOLDER \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/results_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msyn_data_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_evaluation_runs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/-Studium/Bachelorarbeit/time_series_data_augmentation/data_evaluation/predictive/predictive_evaluation.py:34\u001b[0m, in \u001b[0;36mpredictive_evaluation\u001b[0;34m(data_real, data_syn, hyperparameters, include_baseline, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m     baseline_train_data, baseline_test_data \u001b[38;5;241m=\u001b[39m train_test_split(data_real_dc, split_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m) \u001b[38;5;66;03m# split real data into train and test\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     baseline_data, baseline_scaler \u001b[38;5;241m=\u001b[39m get_distinct_data(train_data\u001b[38;5;241m=\u001b[39mbaseline_train_data, test_data\u001b[38;5;241m=\u001b[39mbaseline_test_data,\n\u001b[1;32m     30\u001b[0m                                                     evaluation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m                                                     syn_data_is_sequential\u001b[38;5;241m=\u001b[39mdata_syn_is_sequential,\n\u001b[1;32m     32\u001b[0m                                                     hyperparameters\u001b[38;5;241m=\u001b[39mhyperparameters)\n\u001b[0;32m---> 34\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mevaluation_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbaseline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m### TRTS ###\u001b[39;00m\n\u001b[1;32m     42\u001b[0m TRTS_data, TRTS_scaler \u001b[38;5;241m=\u001b[39m get_distinct_data(train_data\u001b[38;5;241m=\u001b[39mdata_real_dc, test_data\u001b[38;5;241m=\u001b[39mdata_syn_dc,\n\u001b[1;32m     43\u001b[0m                                         evaluation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRTS\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     44\u001b[0m                                         syn_data_is_sequential\u001b[38;5;241m=\u001b[39mdata_syn_is_sequential,\n\u001b[1;32m     45\u001b[0m                                         hyperparameters\u001b[38;5;241m=\u001b[39mhyperparameters)\n",
      "File \u001b[0;32m~/Documents/-Studium/Bachelorarbeit/time_series_data_augmentation/data_evaluation/predictive/predictive_evaluation.py:233\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(data, scaler, evaluation_method, hyperparameters, results, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# train model once\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_MSE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# evaluate model on test data and save results\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n",
      "File \u001b[0;32m~/Documents/-Studium/Bachelorarbeit/time_series_data_augmentation/data_evaluation/predictive/LSTM.py:148\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, device, verbose, patience, num_epochs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     current_train_loss, current_train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     current_validation_loss, current_validation_acc \u001b[38;5;241m=\u001b[39m validate_one_epoch(model, val_loader, criterion, device, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    151\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(current_train_loss)\n",
      "File \u001b[0;32m~/Documents/-Studium/Bachelorarbeit/time_series_data_augmentation/data_evaluation/predictive/LSTM.py:75\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device, verbose, log_interval)\u001b[0m\n\u001b[1;32m     71\u001b[0m running_train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m accuracy(y_true\u001b[38;5;241m=\u001b[39my_batch, y_pred\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mround(torch\u001b[38;5;241m.\u001b[39msigmoid(train_logits)))\n\u001b[1;32m     73\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# retain graph to access the current hidden state in the next forward pass\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# clip gradients to prevent exploding gradients\u001b[39;00m\n\u001b[1;32m     77\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     80\u001b[0m     \n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# log training loss \u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/-Studium/Bachelorarbeit/time_series_data_augmentation/time_series_data_augmentation_venv/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:20\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/-Studium/Bachelorarbeit/time_series_data_augmentation/time_series_data_augmentation_venv/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:73\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend([torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(g, norm_type) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_grads])\n\u001b[0;32m---> 73\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(torch\u001b[38;5;241m.\u001b[39mstack(\u001b[43m[\u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnorms\u001b[49m\u001b[43m]\u001b[49m), norm_type)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlogical_or(total_norm\u001b[38;5;241m.\u001b[39misnan(), total_norm\u001b[38;5;241m.\u001b[39misinf()):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe total norm of order \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for gradients from \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset `error_if_nonfinite=False`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/-Studium/Bachelorarbeit/time_series_data_augmentation/time_series_data_augmentation_venv/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:73\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend([torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(g, norm_type) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_grads])\n\u001b[0;32m---> 73\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_device\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m norm \u001b[38;5;129;01min\u001b[39;00m norms]), norm_type)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlogical_or(total_norm\u001b[38;5;241m.\u001b[39misnan(), total_norm\u001b[38;5;241m.\u001b[39misinf()):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe total norm of order \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for gradients from \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset `error_if_nonfinite=False`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# evaluate predictive performance\n",
    "predictive_results = predictive_evaluation(data_real_numpy, data_syn_numpy, hyperparameters, include_baseline=True, verbose=True)\n",
    "\n",
    "# save results\n",
    "predictive_results.to_csv(DATA_FOLDER / f\"results/results_{syn_data_type}_{hyperparameters['num_epochs']}_{hyperparameters['num_evaluation_runs']}.csv\", index=False)\n",
    "\n",
    "# split in mse and mae results\n",
    "mse_results = predictive_results.loc[predictive_results['Metric'] == 'MSE']\n",
    "mae_results = predictive_results.loc[predictive_results['Metric'] == 'MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mse_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mse_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MSE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs {\" | jitter factor = \" + str(jitter_factor) if syn_data_type == \"jitter\" else \"\"}')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mae_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mae_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MAE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs {\" | jitter factor = \" + str(jitter_factor) if syn_data_type == \"jitter\" else \"\"}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (28500, 12, 5)\n",
      "Shape of the data after splitting into sequences: (28500, 12, 5)\n"
     ]
    }
   ],
   "source": [
    "# split data before feeding into visual evaluation\n",
    "data_real_seq = split_data_into_sequences(data_real_numpy, seq_len=hyperparameters['seq_len'], shuffle_data=True)\n",
    "\n",
    "if data_syn_numpy.ndim == 3:\n",
    "    data_syn_seq = data_syn_numpy\n",
    "else:\n",
    "    data_syn_seq = split_data_into_sequences(data_syn_numpy, seq_len=hyperparameters['seq_len'], shuffle_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate visual performance\n",
    "visualize(data_real_seq, data_syn_seq, metric='pca')\n",
    "visualize(data_real_seq, data_syn_seq, metric='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispielaussage:\n",
    "PCA-Analyse von Realen und Synthetischen Daten\n",
    "Um die Ähnlichkeit zwischen den realen und synthetischen Daten zu bewerten, wurde eine Principal Component Analysis (PCA) durchgeführt. Die PCA reduziert die Dimensionalität der Daten und projiziert sie auf eine zweidimensionale Ebene, wobei die Hauptkomponenten beibehalten werden, die den größten Teil der Varianz erklären.\n",
    "\n",
    "Abbildung X zeigt den PCA-Plot der realen (rote Punkte) und synthetischen Daten (blaue Punkte). Die folgenden Beobachtungen können gemacht werden:\n",
    "\n",
    "Verteilung und Clusterbildung:\n",
    "\n",
    "Die roten Punkte, die die realen Daten repräsentieren, sind in einem spezifischen Bereich konzentriert.\n",
    "Die blauen Punkte, die die synthetischen Daten darstellen, zeigen eine größere Verteilung und decken einen breiteren Bereich ab.\n",
    "Ähnlichkeit und Unterschiede:\n",
    "\n",
    "Die Tatsache, dass die synthetischen Daten eine größere Variabilität aufweisen, könnte darauf hinweisen, dass sie eine breitere Vielfalt an Mustern generieren.\n",
    "Die Cluster der realen und synthetischen Daten überlappen sich teilweise, was darauf hindeutet, dass die synthetischen Daten einige der Eigenschaften der realen Daten gut nachahmen. Allerdings gibt es auch Bereiche, in denen die synthetischen Daten stark von den realen Daten abweichen, was auf Unterschiede in den zugrunde liegenden Verteilungen hinweist.\n",
    "Schlussfolgerung:\n",
    "\n",
    "Die PCA-Analyse zeigt, dass die synthetischen Daten in gewissem Maße die Struktur der realen Daten einfangen, jedoch eine größere Variabilität aufweisen.\n",
    "Weitere Untersuchungen und Anpassungen am Generierungsprozess der synthetischen Daten könnten notwendig sein, um deren Genauigkeit und Übereinstimmung mit den realen Daten zu verbessern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_data_augmentation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
