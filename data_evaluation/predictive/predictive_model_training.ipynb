{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Füge das übergeordnete Verzeichnis zu sys.path hinzu\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "from utilities import split_data_into_sequences, load_sequential_time_series, reconstruct_sequential_data, Scaler, extract_features_and_targets_reg, get_discriminative_test_performance\n",
    "from data_evaluation.visual.visual_evaluation import visual_evaluation\n",
    "from predictive_evaluation import predictive_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../../data\")\n",
    "REAL_DATA_FOLDER = DATA_FOLDER / \"real\"\n",
    "SYNTHETIC_DATA_FOLDER = DATA_FOLDER / \"synthetic\" / \"usable\" / \"1y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of loading data\n",
    "- Laden der Originaldaten: als pd dataframe \n",
    "- Laden der synthetischen, sequentiellen Daten: als np array (GAN, (V)AE)\n",
    "- Laden der synthetischen, sequentiellen Daten: als pd dataframe (brownian, algorithmit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible types: 'timegan_lstm', 'timegan_gru', 'jitter', 'timewarp', 'autoencoder', 'vae'\n",
    "syn_data_type = 'timewarp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " syn data:\n",
      "\n",
      "       traffic_volume         temp      rain_1h      snow_1h   clouds_all\n",
      "count     8759.000000  8759.000000  8759.000000  8759.000000  8759.000000\n",
      "mean      3309.222966   281.384549     0.076457     0.000191    46.013120\n",
      "std       1949.980766    11.293620     0.741707     0.005003    38.320736\n",
      "min          0.001673   243.393557     0.000000     0.000000     0.000000\n",
      "25%       1400.118164   273.729032     0.000000     0.000000     1.000000\n",
      "50%       3569.796796   281.848930     0.000000     0.000000    41.480458\n",
      "75%       4936.000000   290.388538     0.000000     0.000000    90.000000\n",
      "max       7216.981273   307.281342    39.612953     0.250000   100.000000\n",
      "\n",
      "\n",
      "real train data:\n",
      "\n",
      "       traffic_volume         temp      rain_1h      snow_1h   clouds_all\n",
      "count     8759.000000  8759.000000  8759.000000  8759.000000  8759.000000\n",
      "mean      3244.668912   282.208136     0.086792     0.000233    44.397306\n",
      "std       1946.247953    12.114907     0.901360     0.006145    39.195308\n",
      "min          0.000000   243.390000     0.000000     0.000000     0.000000\n",
      "25%       1252.500000   273.605500     0.000000     0.000000     1.000000\n",
      "50%       3402.000000   283.650000     0.000000     0.000000    40.000000\n",
      "75%       4849.500000   292.060000     0.000000     0.000000    90.000000\n",
      "max       7260.000000   307.330000    42.000000     0.250000   100.000000\n",
      "\n",
      "\n",
      "real test data:\n",
      "\n",
      "       traffic_volume         temp  rain_1h  snow_1h   clouds_all\n",
      "count     2135.000000  2135.000000   2135.0   2135.0  2135.000000\n",
      "mean      3325.263700   270.553730      0.0      0.0    45.065105\n",
      "std       1996.851023     7.864566      0.0      0.0    40.781402\n",
      "min        216.000000   248.660000      0.0      0.0     0.000000\n",
      "25%       1222.500000   265.735000      0.0      0.0     1.000000\n",
      "50%       3563.000000   271.550000      0.0      0.0    40.000000\n",
      "75%       4946.000000   275.680000      0.0      0.0    90.000000\n",
      "max       7280.000000   290.150000      0.0      0.0    92.000000\n"
     ]
    }
   ],
   "source": [
    "# Load real time series\n",
    "data_train_real_df = pd.read_csv(REAL_DATA_FOLDER/'mitv_prep_1y.csv')\n",
    "data_train_real_numpy = dc(data_train_real_df).to_numpy()\n",
    "\n",
    "data_test_real_df = pd.read_csv(REAL_DATA_FOLDER/'mitv_prep_3mo.csv')\n",
    "data_test_real_numpy = dc(data_test_real_df).to_numpy()\n",
    "\n",
    "if syn_data_type == 'timegan_lstm':\n",
    "    # load sequential data (which should already be scaled)\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_lstm_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'timegan_gru':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_gru_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'autoencoder':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28478_12_5_lstm_autoencoder_unscaled_15.csv', shape=(28478, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'vae':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28511_12_5_lstm_vae_unscaled.csv', shape=(28511, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'jitter':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'jittered_01.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "elif syn_data_type == 'timewarp':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'time_warped.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "# Loot at real and syn data\n",
    "df = pd.DataFrame(data_syn_numpy.reshape(-1, data_syn_numpy.shape[-1]), columns=data_train_real_df.columns)\n",
    "\n",
    "print('\\n\\n syn data:\\n')\n",
    "print(df.describe())\n",
    "\n",
    "print('\\n\\nreal train data:\\n')\n",
    "print(data_train_real_df.describe())\n",
    "\n",
    "print('\\n\\nreal test data:\\n')\n",
    "print(data_test_real_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Predictive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"seq_len\": 12,\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 32,\n",
    "    \"hidden_size\": 12,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"num_evaluation_runs\": 10,\n",
    "    \"num_epochs\": 500,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETERS:\n",
      "seq_len :  12\n",
      "lr :  0.0001\n",
      "batch_size :  32\n",
      "hidden_size :  12\n",
      "num_layers :  1\n",
      "bidirectional :  True\n",
      "num_evaluation_runs :  10\n",
      "num_epochs :  500\n",
      "device :  cpu\n",
      "Synthetic Data is sequential: False\n",
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1057, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1056, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.19514432603180626 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.15106541518017527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.019957637936080786 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.022123722564976883 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011270406694630718 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013312211965539437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008300442394739302 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00957849430714679 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00718077064456119 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008336547635468271 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006874675969559237 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008013035526827854 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006713953435533156 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007743304095450132 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0066027971652479174 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0075125576763906896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006513372647412883 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007316978977007025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006433309014227214 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007151802299845526 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006355293850961692 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007012363732107641 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006274428594616127 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006894775039708132 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006188020141995818 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006794071578256348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006096545381243782 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0067030650919632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006003945055128803 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006615111720748246 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005914857113958221 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006530161669669563 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005833001665424311 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006457974795279477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0057600637953932375 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006406585285303128 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00569608525660256 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006375549326869933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005640130289099485 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006358183009604759 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005590825825572748 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006346684634028112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005546789615071739 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006335226860006943 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0055068913336510155 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006320191232268424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005470244284892577 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.006300251958343913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005436178533883806 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.006275220219429363 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0054041909939357945 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.006245826121063574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005373926239298915 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0062133727444554955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005345157788213288 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0061796129414099545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005317769951040918 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0061461327919353015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005291753415405805 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0061142322374507785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005267138835501579 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.006084239918409902 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005243872481408482 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.006056095624123426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0052219235842170565 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.006029966239379171 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005201240196351615 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0060046096537810035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0051817038524759945 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005979549735510612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0051631741444529275 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0059544682187740415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0051455087337326805 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005929196150699521 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005128561409256578 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005903528205475167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005112195749262375 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005877363690695561 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005096294341250289 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00585058909457396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005080759983737511 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005823100136373849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005065517011650899 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005794796955716961 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005050510102645052 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005765721294344129 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005035708468266334 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005735677434131503 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.00502110158270021 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005704799358842566 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005006689851063501 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005672936488682514 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004992489043171144 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0056402534588842704 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004978516331557728 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005606752895640538 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004964793117735603 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005572517097735887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.00495133935078727 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005537771284306312 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive\\predictive_evaluation.py:277: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([{'Model': evaluation_method, 'Metric': 'MAE', 'Error': mae}])], ignore_index=True)\n",
      " 10%|█         | 1/10 [03:26<30:57, 206.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.19444283951468166 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.14876105966900735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03092825108540863 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03424732025493594 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013066436159832362 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014492619207457584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010378031812975332 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011162440922549543 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008879619106279863 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009173057637834811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007547382377495949 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007870439796105903 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007130888601285779 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0074987592764527485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0069376557371171235 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007400779169984162 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0067534083394286135 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007307829602402361 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006558359533048704 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007182349437190329 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0063693070432415955 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007040076628875206 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006205361694637958 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006904452094150817 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006064953135469262 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006774823407313842 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0059414758414104854 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006649962181280202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005832899328708703 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006530538564273978 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005740533828443039 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006414670154781026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005664775688473108 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006302619606311268 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00560397205000784 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006198942058664911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0055552977188252405 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006107225226621856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005515777269279733 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006028835873996072 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005482917089426523 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005962731138638714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0054548596092295855 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005907097584841882 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005430266667279775 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005859839983339257 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005408224076148204 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005819159484577968 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00538808464269786 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00578355896697544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005369380529652125 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005751783980111427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005351760383248397 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005722796318449956 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0053349551443098945 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005695827414884287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0053187538808072326 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005670242244377732 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005302993458972727 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00564563598292058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005287545163186104 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005621666663928944 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0052722925592186675 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0055979873355039775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005257125622269963 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005574381300796042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005241947846374998 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0055506780455984615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005226673486975641 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005526645739069756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005211232747138226 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0055021276382510274 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.00519555933322302 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00547713743906249 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005179592339197908 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005451538045342793 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005163273962731235 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005425228408592588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005146552623975275 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005398119005429394 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005129380658503459 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053700193461468995 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005111722811167706 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053407925468705155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005093555011613172 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005310220571974402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005074871761977577 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0052780989276738285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0050556952644472625 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005244274993481881 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0050360789780851695 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005208636295077775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005016114406821311 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005171156573958476 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004995947458354729 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005131955340723781 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004975751190221984 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005091412013451404 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004955723644429682 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005050045249791925 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [06:49<27:15, 204.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1125139510327012 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.10186716070508256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.028762531670721344 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.031957141808508074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011761490338869448 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01295426337952342 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009655454082594654 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010576434330740833 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008752502142417714 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009546085897668758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008312160789076048 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008869320188429864 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008023314149523439 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008377485648345421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00779994497888035 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008043269599404405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007598596223524642 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007821647333912551 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007267096565091425 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007523937917807523 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 99\n",
      "INFO: Validation loss did not improve in epoch 100\n",
      "Epoch: 101\n",
      "Train Loss: 0.006641370642005744 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007326109547113233 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006353936188286646 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007258598411948804 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 117\n",
      "INFO: Validation loss did not improve in epoch 118\n",
      "INFO: Validation loss did not improve in epoch 119\n",
      "INFO: Validation loss did not improve in epoch 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [07:38<15:34, 133.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 121\n",
      "Early stopping after 121 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.38050110512379093 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.2966071859221248 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04148944489059657 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.04645913855775314 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01584295495617183 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01814835091285846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011099087102440642 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012424292891998501 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009927736213736672 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010567941090009888 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009183097012335923 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009369687553878655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0086122956148172 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008555215351995738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008131300391506975 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008044177392388092 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007772716169286329 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007765465231119271 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007514457281655122 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007596966139424373 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007340818513983548 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007467087995096603 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007235930001288381 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007354432134889066 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007155623603918093 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0072782320066300385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007080128430014979 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007209402899367406 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.007002395564557672 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007132595987059176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00691949581322906 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007041293894872069 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006830144088535169 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0069310306639903604 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006734848611760395 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0068003505139666446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0066390084519125795 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006665164200753412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0065505312697071395 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006559699472478207 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006466843940259848 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006486731718349106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006378511459750198 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006423489129006425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006277047082387509 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0063537377042367175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006158056004010277 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006276957156574901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006031843423197576 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006208005587241668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005923140201765201 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0061534702750470705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005839830586503185 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006111683878664146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005774303984379627 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0060791007914196915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005720528907373711 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0060518358299946964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005675075016575899 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006028006654506659 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005635494629709716 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00600757124587236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005599851380136326 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005991213126381969 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005566658617004099 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0059795096954878635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005534875538475458 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0059725634534569345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.00550383064312942 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0059698248651864775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 346\n",
      "INFO: Validation loss did not improve in epoch 347\n",
      "INFO: Validation loss did not improve in epoch 348\n",
      "INFO: Validation loss did not improve in epoch 349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [10:00<13:41, 136.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 350\n",
      "Early stopping after 350 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.13850496937078935 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.10503136646002531 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.025416442764120816 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.02802684875752996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011762768030166626 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013051592690103194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009701329165726328 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01066952062811812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008629230955100693 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009771710049415775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008181525576378756 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009299359161078054 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007710875987277509 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008678050807622425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007157671607752079 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007798009120640071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006659068601855151 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007171326994841152 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006446637818249908 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0070520103429718055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006313079759481968 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007034165209487957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006196822814327277 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007015994181582595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006081044047231358 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00696208788876367 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005960798896696201 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006873251425157136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005843047615791582 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006767600932268097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005740812624869257 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006662046584738966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005661093253246339 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006567293168593417 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005600512409701699 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0064872602439102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005552657614767986 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0064211454420514845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00551286782307999 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0063660838572746695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005478339231093788 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006318960914059597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0054473641845159725 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0062772334101335965 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00541885592804773 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006239074145388954 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005392079378170621 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006203241817488828 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00536652159113199 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006168874658589416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005341808704649145 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006135396590894636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005317659077130897 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00610238585538943 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005293857464294908 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006069532513399334 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005270229875582549 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006036555695840541 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005246642602048475 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006003210211501402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0052229911040083035 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005969272426548688 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00519919165250349 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0059344943831948676 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00517519176036908 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005898703547085033 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005150945316515795 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005861738881589297 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005126432294746602 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005823468508691911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005101640679853132 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005783814163056805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005076546648969316 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0057427414405324005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005051181172607059 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005700334139606532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005025569565363065 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005656675908111912 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004999747202624601 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005611960659734905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004973764599931433 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00556638790979324 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004947680700825269 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005520205383243806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004921549619948763 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005473673853831475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004895421915966338 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0054270790184519305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.00486934801370728 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005380668465102858 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004843382563579651 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005334674805293188 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004817579093900761 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005289312229519162 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004791999044556198 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005244790949612199 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004766708585547528 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005201284184723216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0047417764112618445 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0051589516395538606 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [13:23<13:22, 160.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.40206413699762666 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.30276596655740456 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.029202290810644627 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03194764111300602 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013430579738813812 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014684373009539045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010781977205863562 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011674785137395649 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009172482486137862 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009662855173433748 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007356049308560136 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007723684477455476 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006954366121148813 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007352636428549886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006655755502946783 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007144237279563266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006421075922260265 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006941899465506568 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006241458262925301 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00673632141586174 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006095059309494648 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006582859547480065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005970298533113741 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006483070278430686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005857087639891237 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0064103464075528525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005745733930919673 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006338973892578746 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005627914882489365 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006257374428541344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0055047096871519395 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0061873311984955385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 161\n",
      "Epoch: 161\n",
      "Train Loss: 0.0053975617699580005 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0061674994443926745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 162\n",
      "INFO: Validation loss did not improve in epoch 163\n",
      "INFO: Validation loss did not improve in epoch 164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [14:29<08:34, 128.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 165\n",
      "Early stopping after 165 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.25214836894989995 // Train Acc: 0.0\n",
      "Val Loss: 0.1716707065317999 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.019357534623398943 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.020913527607369947 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012552783501599609 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013809164975742427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008939857241174165 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009588632133433266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007203213042447031 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007629064617075902 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006950860588988085 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007315966348602053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006818945627968867 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007146148243919015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006709177590427363 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0070048619390410535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006607163672838496 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006874390008092365 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006507203280313933 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006747630164575051 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0064062053634254875 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006621494471533772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006302026303971091 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006494795348878731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006192913618411216 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006367854027570609 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006077518635242498 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00624307494966642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005955373064485671 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006127351627904265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005828400189651815 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006037022466497386 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005702385137598608 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005994734374861068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 166\n",
      "INFO: Validation loss did not improve in epoch 167\n",
      "INFO: Validation loss did not improve in epoch 168\n",
      "INFO: Validation loss did not improve in epoch 169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [15:38<05:26, 108.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 170\n",
      "Early stopping after 170 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.3085200829757717 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.22650683593169293 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02239344591431211 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.02404177324462901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013501626580450548 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014643344151623109 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01001690882581021 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010615235057604663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008347705960865167 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00875764385358814 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007764808551908002 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008152198012206046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007317483163418344 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007660421789349879 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006990542261432992 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007354761521770235 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006806831361205881 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007250833277152304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006686318615607809 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007205638555152451 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006592989668936679 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007181856377214631 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0065138289104997565 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007166258593582932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006443635521058918 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007153436089591945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006379684153774305 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007141359917381231 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006320103471435906 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007128592498381348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006263321116447693 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007113344775622382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006207900588319086 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007093017735956784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0061527026271665074 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007064922836006564 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006096860224782169 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0070268874919480265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006039779979555467 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006977502955123782 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005981122427382065 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006916176301755887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005920786920843143 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0068431796423871726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005858893447596389 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006759629940942806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005795773220703984 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006667321013724979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005731952653681166 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006568923229625558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0056682068771295195 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006467560299343485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005605517979626319 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006366741323076627 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005544996342407607 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006269471480182427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005487633194777788 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006177678156424971 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005434082328583229 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006091949920279577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005384545023791002 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00601175327456611 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005338827315480007 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005936219315866337 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005296480148966104 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005864507368053584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0052569328576163215 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005795947803348741 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005219574975628188 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005729949663338416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005183884910100486 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005665979464538395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005149443781710345 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005603553241064005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005115956975993476 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005542101468616987 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005083233975664284 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0054813117295613185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0050511769366188885 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005421039593570372 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005019762777820243 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005361418164444759 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004989006124899118 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005302643841680358 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004958941483101977 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005245006663779563 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004929611607655257 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005188878687262973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004901058340944812 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0051346308455857284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004873324215138522 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005082612686023554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004846439691749911 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005033132092862883 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004820429528791485 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004986427306635853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004795317988448443 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004942670300164644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004771124186414042 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004901957561207169 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [18:59<04:36, 138.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.17558728970492082 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.13581013934077313 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02193663689736141 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.023407952462816062 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012018039623312788 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013389834110378562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009375808040788193 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010270270710701452 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008018611327627755 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008610615969005534 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0073410656584008685 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007675201186965055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006972058759458418 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007174826774965315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006704797921532317 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0069556151763262115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006492866980282413 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006878197446520276 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006321451393462527 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006829750505002106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006170599437556671 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006771853111465187 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006029243423793819 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006696291356895338 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005900062155403387 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006607358521946213 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005791632560537244 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0065169007558485165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005705634532258404 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006429786307682448 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0056367040026327935 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006345683058230754 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005579311543409407 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006265510880278752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0055297971465183005 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006190993816263098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005485655520881193 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006122813135495081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005445003034088108 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006060086498858736 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005406450386538109 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006000649526386577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0053690671417301345 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005941877531928613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005332355142144459 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0058814671150792175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005296162504874456 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00581796549479751 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0052605646169136015 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005750937490066623 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00522575032728194 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005680880940738408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00519186857302612 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005608861964634236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005158964720219849 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005536104299073272 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.00512699089221228 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005463728153913775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005095833446618074 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005392611088395557 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0050653584072464924 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00532337389064624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0050354360101020545 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005256433183710803 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0050059509269016235 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005191975711461376 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004976812164625493 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005130098194486517 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004947959325390181 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005070767755729749 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004919361958303998 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005013876170029535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004891025522565741 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004959230037296519 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004862983056971205 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004906603936379885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004835290920344542 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00485573333058068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004808012126514987 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004806353190146825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004781207745303366 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004758253590861226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004754935721161497 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0047112200409173965 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004729245239562851 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004665135683300083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004704180745667962 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0046199781902353555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004679810579731911 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004575716906829792 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004656244171732343 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004532389654367066 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004633600127638528 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004490129437352366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0046119818714172 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004449155075502966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004591444830357498 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004409752399249769 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0045720096333394245 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004372220893856138 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [22:20<02:38, 158.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1979968344935481 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.12769037515253706 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.031194182289560345 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03512632282560363 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013740671485030248 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.015561416936928736 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010876962911663917 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012044355218463084 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009158214586481016 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009656436600968899 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008049819879335807 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008425150511731557 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007240587830998982 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007656416448298842 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006572287003288766 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007074033574420302 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006219336538266282 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00678844700622208 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0059956685826012416 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00659407623221769 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005850774553403884 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00647121193298303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005759045338074602 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00639202248524217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0056910541033487845 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006321369259453872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00563333388868325 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006251935785471955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005580987204308112 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006184963064323015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0055323537765410696 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006122851148521637 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0054870615602831225 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006068161366890897 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0054452678555772255 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006023161428268342 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0054072188217716335 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005989018149728722 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0053730076093933665 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00596531731870902 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005342493915886204 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005950195434064988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005315330241089619 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005941059515701935 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005291057385616412 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005935423521270209 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005269193951716905 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005931286997271373 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00524929930102488 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005927440881564775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0052309962801879994 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005923195783158436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005213974494942511 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005918191328151699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005197995310023862 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005912366343716926 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005182867013614788 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005905728328370434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005168441431953769 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005898346117807224 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00515460644773867 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005890327991972513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005141269721525864 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005881785421960932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005128353910204567 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00587274251944002 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005115797170046989 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005863233955631799 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005103553287569352 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005853345960049945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0050915844460616205 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005843096977675005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005079861521724285 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0058325072271091976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0050683564019566195 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005821603665347485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005057051454086101 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005810411331956001 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005045931061039794 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005798934058368425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005034982557765405 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005787150222150718 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.00502419738646216 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005775098901187234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005013567735194602 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00576276340357521 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005003089877397815 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005750176513923661 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.00499276139214295 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00573731810950181 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004982578354024322 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005724202690865187 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00497253820700333 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0057109126629417434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004962572399025484 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005697276415851186 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004952792259890001 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005683384465930217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004943165143171706 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005669294969480047 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [25:42<00:00, 154.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (6996, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1741, 12, 5)\n",
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3108779012170284 // Train Acc: 0.0\n",
      "Val Loss: 0.2078266684304584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04594364623670981 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.039277892119505185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.016285107342407245 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.014027566987682472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01246897578739123 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011050945909863169 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010683255406207145 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009772335898808458 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00951014838519519 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009031445825133811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008826924668684533 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008458583831617779 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008417633491287595 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007950390358878807 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008110508113274495 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007510381378233432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007832349120686043 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007100907268679955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007538110478653585 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0066790966105393385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0072198196821399545 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00625273789787157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006909611275564182 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005866752353242853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006638242114658497 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005532704378393563 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006432116314977169 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005286331246183677 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00628221140574817 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005124443794854662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006165272434514237 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005010608482089909 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006071141665826922 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004923084611073136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005995287711425901 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004854394537820057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005933022837273641 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004799923878586428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005880369274817475 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004755407114597884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005834625776618991 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004717598597265103 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005794078306205673 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004684403906999664 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005757626188604255 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00465455637736754 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005724518122916964 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00462725441056219 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005694201688141855 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004601983258246698 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0056662262230261 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004578325209546496 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005640206865426238 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00455593030942096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005615802875137356 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004534485510719771 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005592709684161869 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004513694411566989 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005570663212581653 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00449331007601524 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0055494294060632315 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00447310573336753 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005528808464019742 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00445291141513735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0055086324698415045 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004432599671947008 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0054887567570254415 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004412093269638717 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005469067515507681 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043913517189635475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005449468631948 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043703915826468305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005429887533353718 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004349211791784249 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005410268528016973 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004327849779193374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0053905793455042384 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00430638488932428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005370803571786715 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004284937140023844 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.00535094676449997 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042636561334471815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005331031488548097 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004242722594856538 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005311092701485406 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004222321791828356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005291179333362712 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004202643265439706 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005271341866435118 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041838804217563435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0052516303369647835 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004166166494939137 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.00523207791002901 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041495753960175946 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005212712721988456 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004134150969118558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005193554446500935 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041199119444089854 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:45<24:49, 165.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.18855735072738503 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.12436303010379726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03838023535703143 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0325695461678234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013161199883404763 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011043269906870344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011588163531415131 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009708601092411713 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.01070568273343639 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008920695746994832 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009842822890618145 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008120867834341797 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0089730958323543 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007228775275871157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007984743413693998 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006253717746585607 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007383290661252252 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0058658612511036075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007143486231101941 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005676285194402391 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0069631902946448426 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005563480199568651 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006816786787255782 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005483169891786846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006689414606306311 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005411436095495116 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006576360328246299 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005336617356674238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006479242995708807 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005261554865335877 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006396411636899561 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005193320932713422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0063244576080060105 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051334107506342905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006260458658530151 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005080559485676614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006202421110701751 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005033353623002767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006149017338675009 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004990719030187889 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006099290296341799 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004951886735348539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006052488102854046 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004916287954388694 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006007971011365861 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00488348311575299 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00596518392066501 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004853097095408223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005923623273884835 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004824758905240081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005882823776526028 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004798110487701541 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005842345357409011 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004772796227850698 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005801742451507853 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047484535787423905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005760527891000588 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004724711495112967 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0057181584644535365 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004701170077632097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005674052119178519 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004677309506488117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0056276579938199555 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046525259735062715 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00557865806936061 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046262902017174794 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005527088371824184 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004598134578290311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005473480230839455 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004567822824570943 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005418851202434635 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004535515682602471 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005364524144743098 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045018675440753044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005311856978173159 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004468012158758939 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005261958882395501 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044352996882728556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005215564479553843 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004405067712915214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0051730028956971175 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004378214241428808 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005134273829155486 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004355154228820042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005099201428737985 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004336033511737531 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005067468095373056 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043205790242857554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005038692571273814 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004308264876123179 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005012514451880127 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004298607488586144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004988607067027933 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004291121310300448 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0049666736830987004 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004285309628837488 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004946448794219507 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00428075488995422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004927700505587676 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004277101620523767 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [05:30<22:02, 165.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.16783895911691396 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.10342784906652841 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.029856077518705365 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.025897913121364333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013416700425013457 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011751859084787694 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010451577252444713 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008729364955797791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008831095554308804 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007159253586591645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008056656754103629 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0063624034339392725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007669739775225674 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006003923828459599 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0074128603924716475 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005756342233243314 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007221099998181065 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0055705870798026975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0070704736642658575 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005428398464044386 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006947619591459003 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005320497653023763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006842758598233484 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005238084528933872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006749358812249244 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005172492220828479 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006663825715887764 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005117287359793078 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006584644431384391 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005068593006581068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006511548477156964 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005024276419796727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006444767801108086 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004983258255842058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006384335518346836 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004944783948700536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006329699634195871 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004908355325460434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006279884723278791 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004873604369773106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006233913126001857 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004840233773839744 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00619106063646898 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004808075425469063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006150891388273164 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047771275593814526 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006113161484404684 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004747467466884038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006077714757164557 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004719198854978789 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006044430820883749 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046924196649342775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.006013186761713906 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004667160689661449 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005983829930462964 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004643400275910443 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005956178702218416 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004621058334173127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005930039293842909 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004599997426637194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005905211867007627 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004580080521885644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005881507586492183 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004561158595606685 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005858749030030345 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004543090171434663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005836778530344945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004525773595510559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005815461120083418 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004509119109504602 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005794680602402005 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004493067692965269 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005774055247207823 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004477553437887268 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005753626829527795 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004462518881667744 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005733491914010858 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004448003613982688 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005713598429518816 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004434052448381077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005693913803682514 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004420698137784546 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005674411270014738 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004407964236187664 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005655074231092686 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004395876382477582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005635897970235307 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004384472815912556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0056168793357236815 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004373793886043131 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.00559802320796452 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004363859639587728 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005579346883404133 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004354750481434166 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005560850654941342 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004346434640782801 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005542550476259429 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004338919803161513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005524460104832077 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004332189367745411 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [08:15<19:16, 165.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.4129165396196385 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.2915367836301977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.038187662347913064 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.032706445455551146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01553742895987242 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012989454898475247 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012358995510656471 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010060314381156456 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010806130162933544 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008732281976633451 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009586844257861545 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0077053358863023195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008650392528355429 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006890638405457139 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0078001167828782745 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006107906468043273 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007264364217344212 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0056656658818775955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007142757796512313 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005526923561807383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007065727509139318 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005456001878800717 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007004146967572266 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005415799333290621 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006950114905508873 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005387445221739736 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006900076865490747 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005363020449030129 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006852143005570014 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005339589901268482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0068052293760404335 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005316174309700728 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006758676365562225 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00529248305935074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006712051978670741 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005268408959223466 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006665070609776431 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005243830831552093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00661754211279049 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005218586804006588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006569334326554345 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051924387289380486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006520363665930076 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051650876052338965 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006470595786271439 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051361988594924865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006420043527816325 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005105439416894859 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006368773776748681 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005072464933618903 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006316924171062977 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005037001245231791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.006264739919168934 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004998898493464698 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.006212595624840695 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004958257722583684 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.00616096311353028 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004915413730354472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.006110365616825312 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00487093613694676 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.006061294330039474 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004825557169335132 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.006014109880197793 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047800462290813975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005968986451200563 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00473500418857756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005925914270273881 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004690794947303154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005884753421616643 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004647553851827979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0058453131539398405 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004605273565870117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005807397344370785 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004563882180743597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005770836270267578 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00452331196601418 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0057354944800111765 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004483543285592036 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005701266985320695 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004444623526863077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.00566807527389573 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004406636625274339 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005635848171056406 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004369700355031951 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005604503621542869 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004333874045617201 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005573959915885982 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004299237547357651 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005544122345703038 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004265810818072747 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005514896059617162 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004233563153750517 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005486192602449475 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004202460537833924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005457935750337752 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004172409382987428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0054300679258053874 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004143308746543797 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005402553548024261 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004115063142539425 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [11:01<16:32, 165.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3015042369734479 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.19065366448326546 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.029476174625427756 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02481182519007813 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014859563377622056 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01236496068621901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010576893827988506 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008644002451646057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008661653634668944 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006987622117793018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007896057090330784 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006186413062228398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00739223465799741 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005706662705845454 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00707309217921556 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005443563794886524 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006804006708973007 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005245294057848779 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006573218660382341 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005069714433259585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006411076081623259 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004929361064833673 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006296264258060247 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004819550678472628 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006205074435958987 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004729645627296783 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006125993783198739 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004651656260036609 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006054130714539796 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004581274121830409 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005987428383876199 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004516501182859594 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005924983824003641 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004456478327682072 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005866386410971619 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004400820208882743 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005811335832586503 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004349388999187133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0057594985849985245 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004302116034721786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005710444173384373 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004258903522383083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005663716218319454 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042195393428714435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0056189815774262295 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041836071018637585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005576179906636579 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041505225489593366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005535462915846338 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004119633749889379 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005496950383132247 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004090459359047765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0054605900651252225 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004062832034700974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005426185906728619 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004036815472963182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005393484398960795 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004012489718893035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005362247719140256 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003989868755029006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0053322711756418895 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003968832658773119 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005303386217684881 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003949192869053646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0052754512587221995 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003930764636871489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005248352373869384 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0039133835160596805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005221988173088986 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003896909202872352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0051962683127755 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038812625158408825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005171119176982541 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038663811050355435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005146495289313841 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038522486320950768 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005122375382178518 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003838885575532913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005098753888904198 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003826349010606381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005075642560527885 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003814694480123845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005053063234756594 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038039895600046622 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005031042760471945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037942739394070074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005009612238106901 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037855743832716886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004988799807743157 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003777900875800035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004968630600471866 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003771245350468565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00494912346972782 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003765576108443466 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004930290723906311 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037608417026190597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004912138302580849 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037569773976098408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004894664735941548 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003753913043659519 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [13:46<13:46, 165.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2750418603539331 // Train Acc: 0.0\n",
      "Val Loss: 0.17858771715651858 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0348011981865995 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.029655090262266724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015066201785859996 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01211197329685092 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011572421999376047 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009267703947526488 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009575686192663056 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007694287035106258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00828530407802002 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006722172815352678 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007587980889379298 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00631004054183987 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00730715767314581 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006078620508990504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0070997363585791635 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00586208348501135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006910766409518682 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005655517276715149 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006720198074368479 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005447021672840823 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006539197704014815 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052314006003805185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006394298978417806 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005025117526846853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00628671248721971 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004861778185956857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006202668131788402 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047504850091751325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006131425339911456 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004677114587023177 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006067951917371896 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046276957495138046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006010277415243016 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004593310655433346 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0059575745934242015 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045685199385678225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005909328620570402 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00454991245218976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005865049170496019 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004535292899659412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005824191891800902 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004523224246010862 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005786173888716047 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004512681009840559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005750418438673972 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004502866808748381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005716406957131543 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004493124482475899 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005683718381032824 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044829685346816075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005652033560430912 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004472067027183419 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005621118313413896 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004460252190686085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005590786310880271 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004447469655001028 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005560875583767517 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004433728358708322 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0055312311553745 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004419075066901066 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005501703333405174 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004403582562438466 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005472154245439695 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004387352759526534 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005442462151617101 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004370526613836938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005412536009574164 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00435328773138198 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005382324246260163 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043358425461602484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.00535182669289946 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004318428249098361 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005321106884881565 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043012761024080895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005290294677119055 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042846537808972325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005259563465981178 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004268734595230357 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005229098197140786 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004253593527457931 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005199077274420694 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042391981231048705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005169662574920633 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042254687456244775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005140992286817073 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004212282930331474 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005113164322837745 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00419948678040369 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005086242727888535 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004186939930712635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005060252229062219 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004174527176655829 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.00503519538031959 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004162153345532715 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005011057274917898 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004149777827445756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0049878102806970885 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004137381983243606 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [16:32<11:01, 165.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.13658656839060185 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.08445062085308812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.034470520390529336 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02945856973528862 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01509079248612517 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01378906394608996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011883957604409863 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01027781938795339 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009775561206659397 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008201483497396111 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008592970321618328 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006965882348066026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008072129897262952 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006373786278577013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007689446606354354 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0060621299061246895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007243544500657106 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0057776178233325485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0068885636894398085 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005512242294339971 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006652421889971245 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005316817815500227 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006464126324238571 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005166042519902641 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006300030198569099 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005037780550562523 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0061570915843245306 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0049268163190307945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00603545942188551 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004827601284804669 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005929308748361761 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004734018044969575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005832251109773948 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004642607646316967 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005741183348074471 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004552606522867626 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005654863843090458 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004464828180657192 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00557441866082739 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004382514403286305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005502199958775992 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004312972669405016 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005438312842695629 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042602534977380525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005381383224800825 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004221624978394671 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00533015290025375 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004192320409823548 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00528380802522103 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004169077968055552 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00524185374448263 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004150277824903076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0052039146303203485 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004135089702057567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005169601302187822 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004122887215238403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005138464537810653 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041130063610828735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005110030964050935 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004104827136986635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00508384125920642 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004097811488265341 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005059490787273605 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004091531340964139 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005036648987477739 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004085691182196817 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005015057926257588 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004080110182985664 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004994518964521621 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004074693202379753 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004974869296505418 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040693907342343166 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004955980071710788 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004064180903052064 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00493774220579172 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004059054448523305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004920065922600953 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004054011410864239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004902875041599772 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004049057600257749 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004886103905609822 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004044196168384091 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004869698796136364 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004039434654722837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.00485361503786939 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00403477198401974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004837815803167363 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004030224069190974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004822275050613842 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004025792216204784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0048069710926722615 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004021480984308503 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004791893598944646 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004017302481753921 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004777030157378224 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004013264448043298 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004762375739256354 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004009374949700114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004747930403052485 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004005656244275583 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [19:17<08:16, 165.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1131856049066537 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.07826782099225305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.033485493188953566 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.028395752541043543 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014016230303058444 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011870873186059974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010735411255952124 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009005825335837223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009283753014679947 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007962063103067603 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008754081283610944 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007404615789313208 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008373886160233556 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00691735821522095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0080686012452009 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006493794570930979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007832941529553673 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006162926525046879 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007588576693428026 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0058637048287147825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007263180585556821 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005614837631583214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007035304851761964 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005467923040586439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00687810050091838 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005392293441532687 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006750778500288384 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005338401360098611 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006632586856602295 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005283526445484974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0065155925535380025 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005220591683279385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006397619998171823 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005148046683858741 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006279155101678143 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005067362014034933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006161925985188687 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004980669445781545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0060481155566556475 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004890113184228539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005939662155639976 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004798084819181399 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005837718534100478 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004706939910961823 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005742350031864153 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004618654683740301 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005652610694970426 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004534877351993187 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005566836908602126 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004456803378310393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005483100673164662 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043847481021657584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005400078942878044 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004318449421870437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0053186655128544804 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00425843743565069 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005242482279044614 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004206880484707654 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005175152837115948 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004165638324973935 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005117861944203984 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004134243667464364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005069738979819209 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004110606778836386 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005029177321089202 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004092483607713472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004994612783044747 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004078178133138202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004964776520673653 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004066587817347186 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004938715470972849 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00405708106700331 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004915683759912236 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040492092693139884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00489511390904509 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004042627737561071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004876568130580028 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004037102292799814 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004859693981781517 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00403243128252639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0048442108573372895 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004028447820085355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.00482988514837524 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004025002145631747 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004816527207824295 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004021968195130202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004803978918959093 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004019228232474151 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0047921127227473085 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004016686222431335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004780819078484235 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004014253638557751 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00477001158075118 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004011870842342349 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004759615838961446 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004009480555330149 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004749570972420122 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004007040806622668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004739826686252929 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004004525457805192 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [22:03<05:31, 165.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1820203378779567 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.11619521818039093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04132722441498275 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03593153404918584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014733450429845755 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012732611808248542 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01210871977533839 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0103118673263287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010762131164811518 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009040223028172147 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009928585920529653 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008241960173472762 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009291588308047247 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007654166937043721 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0088065057763294 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007157100559296933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008447832714438813 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006734891667623411 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008139705383771664 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006407569162547588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007860543762895036 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006184077076613903 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0076149201502207 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006055076280608774 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007424765768818149 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006006515656851909 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007277302325971118 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005974573773247275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.007150795388345917 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005923932731490243 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.007033622538198858 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0058528011876412414 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006918077552202051 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005765423288738186 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006797469551696285 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0056632081554694605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006666328803716933 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005542348748580976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006524497446961292 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005402396026660095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006379601296322361 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005253212632272731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006241477562378258 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005106078740209341 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006116680160364706 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004968454654921185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006007112580730014 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004846436864781109 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005910505210559764 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004742366879839789 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005823092886510403 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046541300856254315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005741834241288744 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004577853598377922 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0056649119865370395 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004509883073412559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005591442880271743 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004447396671060811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005521160408484507 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004388426654887471 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005454186016276955 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004331767631017349 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0053908252625058505 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00427681601987305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005331378607940541 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004223423095589335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00527602280957888 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004171704907308925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0052247607552118975 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004121866989457472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005177451720775943 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004074183612299913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005133847070785588 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004028906368396499 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005093654077850946 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003986249315891076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00505657410663744 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003946355893276632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005022325217309217 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003909314967776564 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004990661935136573 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003875189735977487 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004961362285869086 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038439697755331343 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004934229019957012 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038156140828505157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004909076678755876 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003790021893060343 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004885728319215942 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037670616250993177 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0048640135598058505 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003746569044464691 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00484377238952627 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037283621008761905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0048248536587633265 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037122485884041947 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004807114410564967 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0036980319662358275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004790423639420607 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0036855121460658584 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [24:48<02:45, 165.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.12269402352336063 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.07997866134074601 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0225257003343915 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02044856875118884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014338529857989724 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012441702229394154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011733438954667346 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009988949346271429 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009869949739600732 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008692871305075558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008631850469466214 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007635346664623781 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00801367544118298 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00694556759077717 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007577479949867412 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006503197241743857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007221067642163672 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006172641709616239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006921082534657188 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005888774682005698 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006666674672574927 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005650152431123636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006466519472946213 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0054570103191177955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006318999499563111 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005307051044126803 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006206365734908611 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005196774374185638 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006112914410160352 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005109601040286097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0060310521578739425 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005030880419706756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0059578326758269445 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004955464101989161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005891888048903821 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004883833132176237 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005832224143945429 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004817955009639263 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005777949101044945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047587297010150825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005728353104090582 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047062339049509985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005682923600872868 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004660057500851425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005641266467053971 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00461954227225347 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005603029080624076 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004583941328085281 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005567843513350644 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004552471307529645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005535320797456709 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004524373703382232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005505061966471255 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004498976684937423 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005476686423357372 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004475681627677246 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005449852042355188 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00445400955663486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005424259833302604 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004433599922975356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0053996599213428555 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004414162738248706 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005375840522021429 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004395459380678155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00535261779056105 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004377277165820653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00532983932043447 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004359430620785464 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005307369726735537 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004341736198826269 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005285095656556439 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004324036070399663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0052629188572260715 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043061771874570035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005240754336522696 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00428802457595752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005218536303044812 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004269461310468614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005196210232161445 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004250387087548998 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.00517373955829843 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00423073283481327 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005151105864445538 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004210459261031991 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005128311575656187 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004189570163461295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005105385309633837 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0041681119791147385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005082389382330794 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0041461760199374776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005059421136358659 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004123921599239111 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005036616441155848 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0041015618353743444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005014136491011675 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004079370405948297 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004992156281544227 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004057624188929119 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004970846405320856 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004036623070185835 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [27:33<00:00, 165.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (6996, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1741, 12, 5)\n",
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.18135045479013495 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.10744895647195253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03846409230080505 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03842370262877508 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014993830808315034 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.017503072727810254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011114911281516494 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012715638361193916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009561426070179646 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010509745395657692 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008563405242626295 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009071539992212573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007799282962056068 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008005210215395147 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007159983056778679 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007098019641654735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006585008817189054 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0063135595103217795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00606854288300848 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005630756010809405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005711788368479404 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005246858408843929 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005517529437255186 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005131544261662797 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0053591909785127715 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005003375018184836 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005219379005983969 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004857022454962135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005107349293124671 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004731707495044578 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005021300405789099 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004641158601523123 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004952059604613468 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004580351830968125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0048920659082048665 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004539541703310203 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004836947118281918 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00451142092616382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004784445263130152 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004491808309897103 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004733458237701785 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004478881129232997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004683546530501173 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004472201061435044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 218\n",
      "INFO: Validation loss did not improve in epoch 219\n",
      "INFO: Validation loss did not improve in epoch 220\n",
      "INFO: Validation loss did not improve in epoch 221\n",
      "Epoch: 221\n",
      "Train Loss: 0.004634745345867162 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004471782961098308 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:12<10:54, 72.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 222\n",
      "Early stopping after 222 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.24469267650077878 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.14625703929500147 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.031344768549762154 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03252912351692265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014106385313882386 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.016295352937992325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010405494398267335 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011761007301340049 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008877694760672492 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009651546112515709 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007825676408930168 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0081521228548478 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0069866934310130295 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006930800762281499 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006427065466135128 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0061516598417339 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00609735192462045 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005746998953294348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00585222183197809 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005517884318462827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005634240964778431 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005351718436841938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00545469533968887 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005251783447932791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005301972305360697 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005186526991680942 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00517043687714706 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005124103876931424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0050616439998832415 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00505847128065811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00497215701140608 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004989353685893796 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004896289416345381 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004921227164397185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004829948723898801 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004857980451461944 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004770776630883195 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004800349749116735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004717268589134519 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0047471943332559685 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004668302497147161 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004697004644284871 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004623006184486519 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0046486031962558625 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004580746015712415 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004601405006410046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004541125611405284 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004555537244728343 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004503966214243731 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0045118949566544456 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004469275732363094 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0044720165579664435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004437131082403078 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004437521809119393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004407484415274317 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004409486042674292 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004380000801438032 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004387729600156573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004354172797802385 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004370996853421357 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004329545178464465 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0043577923333611 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004305793594523252 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0043468704087321055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004282741935776981 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0043374486013569614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004260293445943272 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004329086158593947 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.00423840077136235 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004321561186489734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004217017568636167 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004314725725403564 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004196090007904645 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0043084769742563365 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004175549695764234 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004302692311731251 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004155316013541417 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004297241773879664 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004135312072969399 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004291941278444772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004115474992033017 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0042866514170203695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004095759712155381 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0042812262005596 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004076150177258224 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004275550547225232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004056639482623889 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004269550488838418 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004037238981005455 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004263171898624437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0040179630991318115 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042563836386596615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.003998835084252336 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004249192880127917 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0039798698536084205 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004241589219732718 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.003961087413159833 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004233597449704327 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.003942500872188779 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004225236461074515 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [03:57<16:53, 126.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3239037181892896 // Train Acc: 0.0\n",
      "Val Loss: 0.20936266048388047 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04880127978838606 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.04539666043763811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01730668642154159 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0191828797317364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011134889100935548 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012837904314933852 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009954460011609111 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011039867480708794 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00923115402097563 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009955182544548403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008533970971265382 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008946617595343428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007616961036397348 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007783376916565678 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006816650897484553 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007047681328417225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006327930523973536 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0066570802888071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005947001678810655 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006282939793626693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00565277331428428 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005954504326324571 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005431336576792041 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005705914561721412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005267599346403236 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0055402608228508725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005146475488609171 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005433417812243781 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005055097245120073 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0053588105153969745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0049836112871515185 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052997820047577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004925033682000453 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005247587219557979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004874750998505863 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005197923026174646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004829911167598351 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00514947462361306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004788784014797859 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005102440005760978 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004750227303798076 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005057311516297473 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004713435665163164 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005014270075215874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004677832595069497 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0049731804714114826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004643031731529638 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004933750904588537 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004608827149527377 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048956593985415315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004575171230943474 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004858640880874274 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004542138643050354 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004822505003010685 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004509880746497888 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004787110518240793 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004478575791483192 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004752420327118175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004448391297926752 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004718454726125029 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004419457044061189 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004685340095734731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004391836555308947 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004653200989758427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004365519472965588 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046221216018735006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004340442295928303 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004592117135920985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004316515126185849 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00456313458694653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004293652056938582 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004535086726007814 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004271773645626678 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004507870512845164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0042508180298657505 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004481388321569697 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004230730913602853 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004455544230189513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0042114640357765925 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004430272948758846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004192967191216277 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004405508399941027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0041751909657375096 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004381214537319135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004158082466251451 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004357345965267582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004141593474089955 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004333882510069419 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.00412567344892315 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.00431080650834536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00411028059034132 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004288113661195067 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004095374112914223 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004265817447396164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0040809236225415844 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0042439338179643855 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004066906937187512 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004222507774829865 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [06:41<16:47, 143.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.09891324908766028 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.06488844257864085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03045465048300485 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03147966827858578 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013541284313884704 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.015029855131764304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010305178785201196 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011404256962917067 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008649789575130155 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009273136720399965 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007606564404216454 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007841862160289152 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006965149354139666 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006870375711216846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0065852842328498 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0062851640141823074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00631262989782252 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005892564923587171 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005983728402747122 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005494654290801422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0056235894484931315 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005178824077698995 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005379233084372925 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004974282668395476 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005223754112522083 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004852093002674254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00511318418693295 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048129750860177655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 134\n",
      "INFO: Validation loss did not improve in epoch 135\n",
      "INFO: Validation loss did not improve in epoch 136\n",
      "INFO: Validation loss did not improve in epoch 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [07:26<10:29, 104.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 138\n",
      "Early stopping after 138 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.2301961523076715 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.13538469245487994 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.030473041405038034 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03166199446740475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014082005584493415 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.015382199124856428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010651064835984226 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011774520241570744 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00881294424716834 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00948303380600092 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007627276849500265 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00791352788748389 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006865553883064887 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0068661694944074205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0064204953035199375 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0062365027885376055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006155353602177153 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005891644194806841 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005954359256456177 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0056876034221865915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0057794740604296254 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0055541291303763335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0056282142464275636 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005465971780094233 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0055050125829854205 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005405762956731699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005405313863613553 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0053508276553739876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0053193457393410516 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005284043237439949 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005240521174374453 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005204466637223959 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005166030286776208 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005119317238727076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0050949118814147265 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005035307100677694 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0050269874817076175 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004956378779289397 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004962479821993698 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004884466270662167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004901779859476153 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004820467914793302 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004845340838339541 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004764554192396728 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004793576208861825 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047162785004316405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0047466132188371075 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004674764062193307 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004704269150896083 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004638922982849181 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004666123532431761 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004607678505338051 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004631616034506199 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004580128976059231 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004600116746105705 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004555545935661278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004571120193564082 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004533344877629795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00454421520367909 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00451314060694792 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004519064686650232 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004494736932048744 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004495406506137101 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004478027239780535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004473034603897053 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044629775411026045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00445179124494332 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004449571120891381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0044315447335792745 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004437780496664346 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00441218765354694 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004427579616789114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0043936252876261425 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004418925250965086 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004375778980522319 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004411761140958829 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004358580629139305 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004406032899648629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004341972053458183 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004401658924127167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004325904901650815 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004398546287451278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004310332700082836 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004396565878679129 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004295216799930554 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00439561692235822 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 427\n",
      "INFO: Validation loss did not improve in epoch 428\n",
      "INFO: Validation loss did not improve in epoch 429\n",
      "INFO: Validation loss did not improve in epoch 430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [09:48<09:51, 118.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 431\n",
      "Early stopping after 431 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.561348966335597 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.3472111945802515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.052270513100200866 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0482811380516399 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.023358146750397945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.025108197822489523 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012746016880322978 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.013950360227714885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010496768537221994 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01172248651696877 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009194641083279055 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01014764895239337 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00816950121573938 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008826899274506353 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007379580637706937 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007817693789150905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00680908120155428 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00706170102242719 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006375975969442077 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006469001665457406 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006069245136244069 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006030591369860552 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005849875695525018 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005714338158511302 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005683974225497188 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005494715694592081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005563326286030805 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005352413025684654 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00547093408768061 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005254604887555946 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005392600249708524 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005174838353625753 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005320707447699284 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005100109580565583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005252561774758673 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005026319417679175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005187877189404157 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004954381136816333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0051265059240464305 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004886185553517532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005067488373300809 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004822457351044498 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005009463764849378 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047629188936711715 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0049512117342825675 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004706884577701038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004891952794307277 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004653827323239636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004831279213946912 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004603387762538411 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004769105084939992 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004555573030798273 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004705708101617713 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004510980785231699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004641883222949181 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044711011250249365 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004579078362439759 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044382224325090645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004519219647935639 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00441434086215767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0044642348559020455 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004399732474915006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004415539629145028 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004392697270536287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004373497284601175 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043907596539197995 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 323\n",
      "INFO: Validation loss did not improve in epoch 324\n",
      "INFO: Validation loss did not improve in epoch 325\n",
      "INFO: Validation loss did not improve in epoch 326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [11:36<07:38, 114.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 327\n",
      "Early stopping after 327 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.1789396186465542 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.10577878982506016 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03693121173057586 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03648820448328148 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01358930312855101 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.014836736260489983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010302712004334075 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011457852720790966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00898503768895689 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009833626182411204 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008003671334489763 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008489598621698943 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007302353989314841 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007511309362863275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006809753027067362 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006834293070079928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006472820734507383 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006374590198340063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006232231008989277 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006050793062472208 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006030889724744867 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0057851457777856425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0058424965456951 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005536726965907623 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005653986698359392 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0053202602144500075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005468341662786629 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005180773682976988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005333221137292947 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050854251931675455 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0052327104399887605 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005001438473647629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005145323508167661 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004929710102309896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005065464688019538 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004865969819101421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00499227456256777 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004807196917350997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004926457627039599 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047539727910506455 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0048684719216384705 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004707833064127375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004818122491100165 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046693173237144945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004774558518031935 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004637719500301914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004736532592326275 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004611667612863874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004702776751122714 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004589777705471285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004672228603742881 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004570887964853848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004644103940165937 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004554132993375375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00461785858029236 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004538886649110778 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004593137553074401 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004524681093806232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004569703344942483 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004511194762943143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004547405378126484 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004498184054285627 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004526138927343144 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004485454083293337 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0045058269491716875 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004472863080445677 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0044864103359916595 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004460292582569475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004467839654083215 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004447658391754058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0044500674739039875 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044349219343117014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004433051433300464 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004422052543271672 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004416746136543158 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004409059491643513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004401107589404852 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004395962198561227 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004386093433320174 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004382798140233552 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0043716564972259815 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004369614177002487 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.00435775148493055 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004356438514183868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004344329848321001 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004343311049425128 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0043313450865196616 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004330282368358564 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004318754379685381 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004317372510294346 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004306504877787041 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004304612551773475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0042945537179257705 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004292042887854305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004282855554738988 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004279680956493725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004271368393924334 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004267565292221579 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004260059961061457 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004255732808219777 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [14:20<06:32, 130.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3419538167513669 // Train Acc: 0.0\n",
      "Val Loss: 0.2165659641677683 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.036708030428686374 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03549213893711567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013691723749113852 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01546706146645275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010637584618679736 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011820875612002882 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009779657974794558 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0105648128146475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00925321301570817 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009843417511067607 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008848539877977417 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009318999340757727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008475306001996842 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008843673367730596 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008089963271501624 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008335553962652656 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0076796000222165185 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007766433739611371 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00726607201390232 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007166459436782382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006897923512279043 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006623481753790243 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00659315464151683 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006200673566623168 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006311465616627133 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005887262061746283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006027199481039831 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0056867098837921565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005743334116493926 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005595641672103242 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005484369802504 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00552929773376408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0053203152164393724 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005426926633597098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005231878403204783 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005312537340531972 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005176108722658998 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005225200684402477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005135932578516541 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005167398500171574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005104048862669523 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005129856601442126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005076890994186543 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051046710842373695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005052621911314941 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005086751339364458 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005030205268912511 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005072935846295546 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005008969286783092 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005061136584051631 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004988377087895192 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005049762713976882 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004967915798701159 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005037433558821001 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004947041613065753 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005022829152982343 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004925195168922804 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005004751117138022 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004901869904136149 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004982519291595302 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004876812151048468 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0049566038439727645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004850241965183682 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004929179325699806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004822954778680706 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004903775123371319 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004796180682342934 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004883893640627238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004771092772105343 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004870986352166669 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0047480971022183715 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004863100477748296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004726562497778009 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004855563531799073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004705331454395857 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004843495566059243 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004683394705851192 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004823825889351693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.00466041886922797 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004796586822803047 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0046367742350860814 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004764939829791812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004613125221017051 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047336443983526395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004590010921392148 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004706701081754132 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004567716306566631 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004685986205004156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004546338469045211 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046716033248230815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004525883113577341 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00466277152299881 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004506309442971642 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0046584750652651895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 479\n",
      "INFO: Validation loss did not improve in epoch 480\n",
      "INFO: Validation loss did not improve in epoch 481\n",
      "Epoch: 481\n",
      "Train Loss: 0.004487548555703579 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0046577802965078845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [16:59<04:39, 139.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 483\n",
      "Early stopping after 483 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.18320103679447686 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.1106382606043057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04556296046261918 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.04091778390786865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.016748478472154146 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01839681350710717 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010948920208236764 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012384497611360117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009505523970992767 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010502936267717318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008855501670639145 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009599396463652904 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008251198259799966 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008747099071148444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007713124128090447 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007989115168509836 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007232135610674087 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007308992229148068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006785041483445518 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006658469506708736 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006375272983162064 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006033718566918238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0060819468634271115 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0055948677642101595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005922368950753783 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0054197461174970325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0058148558539674365 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005360580460083755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005719987886849382 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005327816570008343 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005630413334780547 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005300215051763437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005546287343569938 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005275037702680989 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005468330097867954 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005251979459585114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0053968002205580625 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005231392483057624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005331689016638349 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005213633020916445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0052728476605487596 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005198714958334511 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005219973212644377 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005186345809223977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0051726031363792885 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051760449942032044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005130130640277852 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051672638097608635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005091874782882682 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051594617459076375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005057166956182246 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005152167989449068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005025388579789047 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005145036801695824 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004996000200517467 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005137830142947761 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004968546937723734 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051303977675905285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004942648774055449 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005122631767087362 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00491800199084188 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005114471696486527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004894364002207507 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005105870485898446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004871551573891022 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050968000178479335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0048494257689900185 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005087231461551379 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004827909562290225 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050771311856806275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004806965926808802 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005066516244022007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004786606996052273 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00505544202698564 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004766889243412202 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005044032099910758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004747896829059722 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050324820130216805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004729731748896953 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005021075233393772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004712489275841178 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005010143074799668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004696229961009687 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005000018573958765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004680976800149578 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004990981110710312 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004666697625421651 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004983226029964333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004653320866785773 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004976800575175069 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004640750508977645 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004971665874208239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004628881626309546 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004967689649625258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004617616829391118 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004964691095731475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004606870912439637 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004962476683695885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004596569800625032 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004960860030471601 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [19:43<02:27, 147.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.183717803808448 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.11589062986048786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03413972064223328 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03499303097752007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014899609602996272 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.016251815482974052 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010998188058705363 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012164282002909617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009039130903085158 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009717252998697487 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007660893302359809 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007887518134984103 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0068602232014245805 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006808868289755827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006428139572367752 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006288924595256421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006120842304872774 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006028265658427369 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005876747442124471 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005850031593052501 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005687292927842094 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005688634167679332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005543928681306656 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005546438801948997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005437417526210586 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0054375406523997135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005357274751284704 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00536165412473069 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005294319437593477 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005310117184523154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005241986001856608 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005273359268903733 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005196131641320569 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005244084993715991 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005154294105282279 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005217602036216042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0051150744720829915 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005191268238492987 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005077710181562927 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005163819552399218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005041807034764158 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005134906850501218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005007171919532163 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005104760633019561 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004973743369374382 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005073997899043289 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004941524031144656 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005043386902914128 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004910576370168375 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005013740994036198 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004880974118408564 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004985729825090279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004852780202865073 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004959832967936315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004826017580878576 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004936331548643383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004800648255205658 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0049153217622502285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00477656412784108 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048967942891811785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004753585669613915 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048806300619617105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004731481436764413 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004866645237515596 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0047099896370152595 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004854610585607588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004688842095712149 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004844273997216739 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.00466778514453329 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004835403909568083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004646589531808811 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004827762458642775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004625067859303342 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004821071500720625 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004603089478725916 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0048149992923506284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004580598355368733 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004809075621464713 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0045576218885612 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004802725436589258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0045342565435473915 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004795305371623148 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004510636041929614 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0047861949604173955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004486872691269879 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004774987176907333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.00446303033022227 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004761613763614812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004439123945398953 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004746390366926789 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004415162518986216 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004730013304982673 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004391183403557694 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004713375538333573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004367270187573661 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004697407470931384 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004343529665347223 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004682830966670405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004320060425107346 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.00467014003502713 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [22:28<00:00, 134.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1057, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1056, 12, 5)\n",
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2109754094224035 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.08643086440861225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01260317189570421 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01409763763384784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008740951894249144 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009869303184506647 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007608412842009169 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008504479568835129 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.006907016770055235 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0075484727344968736 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006593078354990129 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007068496443988646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006404298067159297 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006778715952189968 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006277530526752342 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006571870810790535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006184850265558211 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006399111048428013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006110642257005435 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006251334493486758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00604693342259467 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006139884066001019 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005988602201076364 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00606753014247207 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005932318720785889 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006026090204934864 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005876136741115051 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006002863759503645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005819096305815995 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005985975710620337 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005761027192756308 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005968120900968856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005702071731797951 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005946355454130646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005642385784434657 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0059203834252377205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005582123472288479 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005890712169382502 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005521441341458243 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005857525605653577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005460458806317072 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005820936598705456 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005399312064996785 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0057816704228410825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005338309420545337 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005741058328353307 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0052780915550294655 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005700216563555467 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0052197844438266324 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005659873799338718 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005164828133906393 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005621285311302499 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0051143567585775805 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005586989145205521 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005068992696390905 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005557976187179413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005028718028378663 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005532530184342142 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00499302341720967 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005507983424810364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0049611532000162495 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0054823591729954765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004932368007207407 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005454829366713324 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004906087302788198 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00542533300433527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00488189931259677 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0053940950550467655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004859506408173175 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005361309672212776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0048386981739044115 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005327031221788596 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004819301816943856 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005291123401976246 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00480117684323213 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005253281252568259 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004784199652233088 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005213095796267118 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004768256943293428 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005170124326147796 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004753240095756878 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0051239181090803705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004739037009197205 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005074133456218988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004725504865930747 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005020634288324372 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004712435608100609 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00496359565295279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004699517716791981 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004903733983690686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004686281549797727 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004842663632349714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004672154382814465 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004783365854938679 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004656618151884403 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0047308233025593355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004639439885899334 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004692439979407936 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004620804115115763 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004677263348746826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 492\n",
      "INFO: Validation loss did not improve in epoch 493\n",
      "INFO: Validation loss did not improve in epoch 494\n",
      "INFO: Validation loss did not improve in epoch 495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [06:27<58:10, 387.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 496\n",
      "Early stopping after 496 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.20661666542223536 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.06895603529889793 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01214582639426279 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.013863300887511714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.009275520446844794 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01041423118508914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007844413490785062 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008348291335791787 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.006893454283544831 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00723319048719371 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006498092743843731 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006765896947506596 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006357200249328944 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006593889156219494 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006266657710452079 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006511284877299605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00619272645450622 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006460970483364209 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006125918434249872 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006425092825392152 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006063098472295545 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006397380750379799 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006003101607540944 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00637536429841181 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005944696207914094 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0063573253710809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005885496416893852 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006341791711747646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00582130398323344 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006328359968713759 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0057462093333494075 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0063182724406942725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005656155666357274 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006304499867837876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005552213109990964 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006262419885709225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005426739670474157 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006203338702428429 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0052774130224469045 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006169728612910737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005160523169315156 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006145112593110432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005086435431517045 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006075113436535877 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005037053101704587 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006005687526125899 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004999066766842449 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005954433626303559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004966893624451558 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005918393610045314 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004938275138250556 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005893080981503076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0049120959823632316 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0058752314653247595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004887691613519139 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005861905896488358 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004864632468624849 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005850312436985619 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0048426034951242454 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005838093056809157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004821353393864998 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005823694224305013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004800680467959577 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005806447475847295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0047804245553311414 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005786468778216445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004760462130783628 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005764411666485316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004740683560533546 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005741341635310913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00472098042539017 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00571849342668429 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004701253601112164 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005697229877114296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004681445539822986 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005678952993441592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004661557335470176 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005665011662880287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004641653913698096 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005656747907564482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 399\n",
      "INFO: Validation loss did not improve in epoch 400\n",
      "INFO: Validation loss did not improve in epoch 401\n",
      "Epoch: 401\n",
      "Train Loss: 0.004621833305412408 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005655144081067513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [11:43<46:02, 345.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 403\n",
      "Early stopping after 403 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.11148139503758939 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.08112404166775591 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01155854216342625 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.013015885460738312 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008955036808644682 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00986938935238868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00775932982761687 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008264981446486405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0068557999069462795 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00714424647294971 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006544649385816504 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006771677040138885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006418699567407111 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0066535564564058885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006333651620624595 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0065797612358651615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006263993714582363 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006531441228135544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006201875294608505 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006502170643002233 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00614413972559872 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006484478068373659 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006089321937540995 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006471334490925074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006036869893129319 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006457376451341107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005986574343750441 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006439862512599896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005938192971550452 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006418620701879263 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005891342192779293 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006394990945837516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005845470213656053 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0063709492439075425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005799783499727547 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0063486462801366165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005753148631260848 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006329972023034797 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005704119869359297 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006315166774370214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005651267404179387 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006300731767516802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005594153180667344 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0062782195643247924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005534544427995357 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0062372324481496915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005475613644690945 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006172717624234364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005419022367423965 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00608912921127151 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005364529745982299 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005995983442784671 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005311524134211862 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005902167190523709 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005259697971664628 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005814434515367097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005209034315867595 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005736785810714697 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00515973674864687 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005670576308415655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005112096640580185 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0056149922585224405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00506633862811975 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005567973397453041 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005022485891056555 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005526883233174244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004980378871178339 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005489079808980665 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004939781349985324 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005452278707012096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004900487054052543 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005414666724391282 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004862392704048104 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0053749814396724105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0048255476424039784 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0053325595251996726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00479014358227652 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005287398402716088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004756468108377821 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0052400947897695005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004724824695419724 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005191790926105836 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004695432098672625 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005143858002745272 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004668356660717096 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005097654072896522 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0046434995963953694 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005054222378770218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.00462063643749347 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005014114657088238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004599487209285888 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004977498696569134 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004579774849318986 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004944213042857454 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004561259239046686 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0049139303566121005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004543740153453148 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0048863047439440645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004527063002355287 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004860985503696343 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [18:14<42:44, 366.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.20121420136017817 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.09060780063052387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.012165155530765899 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.013754912042606841 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0086907617803331 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009829904541701955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007914038305373019 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008558525916134171 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007341208946846436 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00797474873594611 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006901233301985791 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007516037255032536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006691784958671812 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007187182813033681 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006518216194511328 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0069269589970217035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006332864924134998 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006697185138952644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006172170688331568 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006510756115483887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006034468174754545 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006364674143054906 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005909452448891462 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006245657055200461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005801739654313497 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006098376395290389 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005702974883460873 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005965482943472178 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0056018055371365085 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00588655865559464 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 143\n",
      "INFO: Validation loss did not improve in epoch 144\n",
      "INFO: Validation loss did not improve in epoch 145\n",
      "INFO: Validation loss did not improve in epoch 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [20:09<26:43, 267.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 147\n",
      "Early stopping after 147 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.21552479944074393 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.08045639349695514 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.013145117457126909 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01485878206542967 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.00843361641047948 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008782778085921617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007601940555361158 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007559588199536151 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007058087028059691 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007228227133643539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006683213971904266 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006990763125941157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006441019676366933 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0067926543792161875 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006262063443410879 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006693368120228543 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0061202432011710425 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006631137091009056 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005993525584873622 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006563873680801515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005873906046079329 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006485906676115359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005765510766517534 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0064025711035355926 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005674860467329805 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006318702430957381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005598886777694512 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006237223737544435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005533048144737054 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006157159230069202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005474330773170598 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006077475044602419 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00542092402053825 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00599902868270874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005371849116508745 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005923676153864054 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005326588430728804 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005853058592251995 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0052848589212643885 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00578817947055487 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005246465599953522 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005729384576518308 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005211237904888542 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005676505758481867 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005178972139956269 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005629118785316891 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005149415846134262 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0055866169893895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005122264689030347 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00554829353110536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0050971812463895 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005513395716929261 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005073814372729146 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005481149463722592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005051807165594935 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005450766067951918 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005030815374345384 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005421521419467514 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005010528231737284 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005392815547940486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004990672891541559 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005364243588273358 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004971042687670963 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005335732034462339 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004951495831690548 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005307510207571527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0049319527788412075 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00528010558542412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004912370262368304 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005254144831697512 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0048927321824265586 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00523034724584945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0048730258665590715 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005209416856386644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00485322310245967 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005192183752489441 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00483326528565507 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005179622320129591 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004813048943727817 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005172824404914589 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 397\n",
      "INFO: Validation loss did not improve in epoch 398\n",
      "INFO: Validation loss did not improve in epoch 399\n",
      "INFO: Validation loss did not improve in epoch 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [25:23<23:39, 283.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 401\n",
      "Early stopping after 401 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.15060874768166754 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.08164265981930144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.012619348478689108 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.014239136218641172 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008546203488670885 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009191469125011387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007544834212096957 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007760639747549944 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007185219983073118 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0075632502900107815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006720675595908747 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007156247471678345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006407745122284691 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006811332135625622 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006206184836907714 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006651671226684223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0060717732797953995 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0065388377509353795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005970528757026619 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006455808069885653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005888462182826284 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006389682277050965 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005818713984863579 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006331383339677225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005757023104788077 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006279533194816288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005700011623863061 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0062367935692343645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00564489155440375 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006207370343070258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005590007783147214 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006195984780788422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 153\n",
      "INFO: Validation loss did not improve in epoch 154\n",
      "INFO: Validation loss did not improve in epoch 155\n",
      "INFO: Validation loss did not improve in epoch 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [27:25<15:16, 229.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 157\n",
      "Early stopping after 157 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.20180268789993996 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.07305747747202129 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.011696114082343135 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.013584914541912867 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.009355982361081306 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010444823104669066 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007972293666054637 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00836676559137071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007400076397076604 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007603088489678853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00711384959431864 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007394232629688785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0068528266835164355 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0072320206296246714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006650609611625097 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007081517388167626 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006546658441642303 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006990537882837302 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006480094613643598 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006923761704991407 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006422964789414305 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006868640049908529 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006368520031509695 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006819176364361364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006314603216344681 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006770256865660057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006260358017073966 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006718406891998123 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006205303412158735 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006661228982813875 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006148939449428428 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00659705713555655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006090727569816304 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006525420665960102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006030348221803556 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006448021405102576 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005968111217075361 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006368677107863785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005905148913580295 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0062918049721595115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005843145807319017 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006220257683547542 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00578363320343405 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006154886868042762 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005727154301483502 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006096146949429941 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005672993491349701 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006046012184336124 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005619882079866295 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0060086767908240505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005566956603284959 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005988400720525533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 258\n",
      "INFO: Validation loss did not improve in epoch 259\n",
      "INFO: Validation loss did not improve in epoch 260\n",
      "INFO: Validation loss did not improve in epoch 261\n",
      "Epoch: 261\n",
      "Train Loss: 0.005514416282592594 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005986340224112877 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [30:50<11:03, 221.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 262\n",
      "Early stopping after 262 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.1447402506954355 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07585170207654729 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.012211450706985209 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.013793835861553602 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.00868066997029793 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009743347038131426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007431037987389141 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008346201078144504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.006688303636047322 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007179918757174164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00651103929505629 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006897358894895981 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006386432927098658 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006750775186125846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006279437518823327 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006642665850984699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006183818800454036 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006543909665197134 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006093999307457497 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0064438768347506135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006004653188635781 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006339036318583085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005912985929623808 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006227703354157069 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005818683473115371 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0061115254442591005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005724463710166391 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0059981691404519715 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005632821305985247 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005895051598439322 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005543679013525662 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005812253946821918 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0054581217566003655 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0057702948448850825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 165\n",
      "INFO: Validation loss did not improve in epoch 166\n",
      "INFO: Validation loss did not improve in epoch 167\n",
      "INFO: Validation loss did not improve in epoch 168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [33:03<06:25, 192.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 169\n",
      "Early stopping after 169 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.14936456540037607 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.06818425063701238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.013110746979207768 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.015041958184584099 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008424495774463 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00913309067597284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007568049244742272 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007682639639824629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007132868505058763 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007213993737583651 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006767693086847802 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006936335675966214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006475033388146194 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006725561188753037 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006290579477956746 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006565097733126844 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006175358395313101 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00646832155282883 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006084160756587282 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0064109423033454835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00600046987717795 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0063740542528274305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005920849006444895 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006352823974071618 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 121\n",
      "Epoch: 121\n",
      "Train Loss: 0.005846250234141952 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0063469348790343195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 122\n",
      "INFO: Validation loss did not improve in epoch 123\n",
      "INFO: Validation loss did not improve in epoch 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [34:41<02:43, 163.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 125\n",
      "Early stopping after 125 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.10098435943588696 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07373984300476663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01290717536870431 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.014770186136421911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008608473994036187 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009549215319566429 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00713582635075884 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007146933526896378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00670581865418103 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006589833150535603 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006510467949487018 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006459701313253711 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006386441890253742 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006407947252121042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006291922161014456 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006365407141856849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006217433232287331 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006325307932189282 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006157110558078501 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006294739117626758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006102966754158816 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006274522953283261 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006048849184948004 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006260433672543834 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005991260295120235 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006247874602730221 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005930019792271914 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006232297951903413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005869835735776699 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006204670658061171 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005812064880367719 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006151156897163566 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005749740621398103 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006081600577625281 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0056785570926963435 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006005246877012884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005599016744653665 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005918498411226799 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0055144681864808835 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005823462468791096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005429528294053691 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00572680082062588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005348681308845419 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0056323913843644895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005274525088297673 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005542663261568283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005207050372284566 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005459676298093708 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005145070368532173 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005385211785323918 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005087311758280526 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005320220014683026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005032763839986642 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005264442468829015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004980817037433219 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005216678364805001 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0049313250492825774 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005175467238158864 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004884491292527797 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005139418480479542 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004840647674966314 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005107416863799752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004800011684202505 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005078538088127971 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004762577025455609 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0050520722631036356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00472814270964112 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00502738482592737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004696404121278136 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005003935380099232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004667030095328769 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0049813340404344835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004639719629390387 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00495926384567557 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004614221250725198 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004937500021803905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004590342968918972 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00491592102095156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004568138247442226 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004894153543693178 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004547038177227077 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004871835760936579 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004526858656965979 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0048495261135565884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004507679111650725 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004827016183887334 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004489414576931398 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004804212363738129 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004471953151612653 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004780963169383433 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004455220797465232 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004757414448677617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0044391559546735045 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004733784836443032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004423692952864931 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004710209377876976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004408765811634027 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0046868191532078475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004394316686771919 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004663726040983901 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [41:12<00:00, 247.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictive performance\n",
    "predictive_results = predictive_evaluation(\n",
    "    data_train_real=data_train_real_numpy, \n",
    "    data_test_real=data_test_real_numpy,\n",
    "    data_syn=data_syn_numpy, \n",
    "    hyperparameters=hyperparameters, \n",
    "    include_baseline=True, \n",
    "    verbose=True)\n",
    "\n",
    "# save results\n",
    "bidirectionality = \"bi\" if hyperparameters[\"bidirectional\"] else 'no_bi'\n",
    "predictive_results.to_csv(DATA_FOLDER / f\"results_{syn_data_type}_{hyperparameters['num_epochs']}_{hyperparameters['num_evaluation_runs']}_{bidirectionality}.csv\", index=False)\n",
    "\n",
    "# split in mse and mae results\n",
    "mse_results = predictive_results.loc[predictive_results['Metric'] == 'MSE']\n",
    "mae_results = predictive_results.loc[predictive_results['Metric'] == 'MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1daafb9de50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mse_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mse_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MSE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mae_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mae_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MAE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2*1e06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_data_augmentation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
