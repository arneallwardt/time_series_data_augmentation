{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Füge das übergeordnete Verzeichnis zu sys.path hinzu\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "from utilities import split_data_into_sequences, load_sequential_time_series, reconstruct_sequential_data, Scaler, extract_features_and_targets_reg, get_discriminative_test_performance\n",
    "from data_evaluation.visual.visual_evaluation import visual_evaluation\n",
    "from predictive_evaluation import predictive_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../../data\")\n",
    "REAL_DATA_FOLDER = DATA_FOLDER / \"real\"\n",
    "SYNTHETIC_DATA_FOLDER = DATA_FOLDER / \"synthetic\" / \"usable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of loading data\n",
    "- Laden der Originaldaten: als pd dataframe \n",
    "- Laden der synthetischen, sequentiellen Daten: als np array (GAN, (V)AE)\n",
    "- Laden der synthetischen, sequentiellen Daten: als pd dataframe (brownian, algorithmit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible types: 'timegan_lstm', 'timegan_gru', 'jitter', 'timewarp', 'autoencoder'\n",
    "syn_data_type = 'autoencoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " syn data:\n",
      "\n",
      "       traffic_volume           temp   rain_1h   snow_1h     clouds_all\n",
      "count   341736.000000  341736.000000  341736.0  341736.0  341736.000000\n",
      "mean      3297.371241     282.815572       0.0       0.0      37.300382\n",
      "std       1943.009003      12.227262       0.0       0.0      39.207268\n",
      "min          0.000000     243.858856       0.0       0.0       0.000000\n",
      "25%       1351.575958     273.691193       0.0       0.0       0.000000\n",
      "50%       3630.303101     284.637863       0.0       0.0      20.491416\n",
      "75%       4942.487427     292.859795       0.0       0.0      85.235302\n",
      "max       7696.179688     309.678741       0.0       0.0     104.347099\n",
      "\n",
      "\n",
      "real data:\n",
      "\n",
      "       traffic_volume          temp       rain_1h       snow_1h    clouds_all\n",
      "count     28511.00000  28511.000000  28511.000000  28511.000000  28511.000000\n",
      "mean       3313.74238    282.688768      0.061611      0.000250     42.122795\n",
      "std        1971.53206     12.367361      0.678185      0.008298     39.316195\n",
      "min           0.00000    243.390000      0.000000      0.000000      0.000000\n",
      "25%        1289.00000    273.480000      0.000000      0.000000      1.000000\n",
      "50%        3507.00000    284.550000      0.000000      0.000000     40.000000\n",
      "75%        4948.00000    292.790000      0.000000      0.000000     90.000000\n",
      "max        7280.00000    310.070000     42.000000      0.510000    100.000000\n"
     ]
    }
   ],
   "source": [
    "# Load real time series\n",
    "data_real_df = pd.read_csv(REAL_DATA_FOLDER/'metro_interstate_traffic_volume_label_encoded_no_categorical.csv')\n",
    "data_real_numpy = dc(data_real_df).to_numpy()\n",
    "\n",
    "if syn_data_type == 'timegan_lstm':\n",
    "    # load sequential data (which should already be scaled)\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_lstm_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'timegan_gru':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_gru_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'autoencoder':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28478_12_5_fc_autoencoder_unscaled.csv', shape=(28478, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'jitter':\n",
    "    jitter_factor = 0.1\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'mitv_jittered_{str(jitter_factor).replace(\".\", \"\")}.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "elif syn_data_type == 'timewarp':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'mitv_time_warped.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "# Loot at real and syn data\n",
    "df = pd.DataFrame(data_syn_numpy.reshape(-1, data_syn_numpy.shape[-1]), columns=data_real_df.columns)\n",
    "\n",
    "print('\\n\\n syn data:\\n')\n",
    "print(df.describe())\n",
    "\n",
    "print('\\n\\nreal data:\\n')\n",
    "print(data_real_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Predictive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"seq_len\": 12,\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 32,\n",
    "    \"hidden_size\": 4,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"num_evaluation_runs\": 10,\n",
    "    \"num_epochs\": 200,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETERS:\n",
      "seq_len :  12\n",
      "lr :  0.0001\n",
      "batch_size :  32\n",
      "hidden_size :  4\n",
      "num_layers :  1\n",
      "bidirectional :  True\n",
      "num_evaluation_runs :  10\n",
      "num_epochs :  200\n",
      "device :  cpu\n",
      "Synthetic Data is sequential: True\n",
      "Shape of the data after splitting into sequences: (22797, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2841, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2840, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Loss: 0.13145430358126758\n",
      "Training Loss: 0.08327196711674333\n",
      "Training Loss: 0.06545656248927116\n",
      "Validation Loss: 0.05184060883488548\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.05466958869248629\n",
      "Training Loss: 0.050813169591128825\n",
      "Training Loss: 0.04952523828484118\n",
      "Validation Loss: 0.04513409897015336\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.04647692705504596\n",
      "Training Loss: 0.04315120765008032\n",
      "Training Loss: 0.04083655645139515\n",
      "Validation Loss: 0.0361481648608205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.03657150295563042\n",
      "Training Loss: 0.032896511731669305\n",
      "Training Loss: 0.030397462584078314\n",
      "Validation Loss: 0.026489053151748154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.026988303042016923\n",
      "Training Loss: 0.024579732338897885\n",
      "Training Loss: 0.022911302070133387\n",
      "Validation Loss: 0.02030413319471847\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.021225266600959004\n",
      "Training Loss: 0.01999746712157503\n",
      "Training Loss: 0.01877307333284989\n",
      "Validation Loss: 0.016858764275322468\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.018069726412650196\n",
      "Training Loss: 0.017518415423110126\n",
      "Training Loss: 0.016475130633916705\n",
      "Validation Loss: 0.014863212483024664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.016266928541008384\n",
      "Training Loss: 0.01608341363724321\n",
      "Training Loss: 0.01510839904891327\n",
      "Validation Loss: 0.013592778621346092\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.0151249960064888\n",
      "Training Loss: 0.015134713747538626\n",
      "Training Loss: 0.014179322714917363\n",
      "Validation Loss: 0.012658923393471187\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.01428096940042451\n",
      "Training Loss: 0.014390098802978172\n",
      "Training Loss: 0.013426680685952305\n",
      "Validation Loss: 0.011838193480553252\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.013526859157718719\n",
      "Training Loss: 0.013675834850873798\n",
      "Training Loss: 0.012677812493639067\n",
      "Validation Loss: 0.010962993858821607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.012718075355514884\n",
      "Training Loss: 0.01288253277540207\n",
      "Training Loss: 0.011857107959222048\n",
      "Validation Loss: 0.010018827372722411\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.011875911622773856\n",
      "Training Loss: 0.012097869054414331\n",
      "Training Loss: 0.011103644287213683\n",
      "Validation Loss: 0.009217169674720322\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.011181747368536889\n",
      "Training Loss: 0.011482239522738382\n",
      "Training Loss: 0.010530352889327332\n",
      "Validation Loss: 0.008604875242525942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.010657679432770238\n",
      "Training Loss: 0.011011117623420432\n",
      "Training Loss: 0.010081252630334348\n",
      "Validation Loss: 0.008097939389454347\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.010233999945921824\n",
      "Training Loss: 0.010624216212891042\n",
      "Training Loss: 0.009708268651738763\n",
      "Validation Loss: 0.007661293124717273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.009876841029617936\n",
      "Training Loss: 0.010296441600657999\n",
      "Training Loss: 0.009392160467104987\n",
      "Validation Loss: 0.007283277184378063\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.009571880472358317\n",
      "Training Loss: 0.01001624600845389\n",
      "Training Loss: 0.009122687936760486\n",
      "Validation Loss: 0.006957053897504726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.009310513634700328\n",
      "Training Loss: 0.00977606995496899\n",
      "Training Loss: 0.008892527436837555\n",
      "Validation Loss: 0.006676548840791992\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.009086072678910569\n",
      "Training Loss: 0.009569911614526062\n",
      "Training Loss: 0.008695670567685738\n",
      "Validation Loss: 0.006435840228425987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008892919566715136\n",
      "Training Loss: 0.009392685493221507\n",
      "Training Loss: 0.008526932294480503\n",
      "Validation Loss: 0.006229218048451657\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.008726163282990456\n",
      "Training Loss: 0.009239954472286626\n",
      "Training Loss: 0.008381755928276106\n",
      "Validation Loss: 0.00605133926401731\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008581544144544751\n",
      "Training Loss: 0.009107828572159632\n",
      "Training Loss: 0.008256167438812554\n",
      "Validation Loss: 0.005897401005364536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.00845539545523934\n",
      "Training Loss: 0.008992921826429665\n",
      "Training Loss: 0.008146757371723651\n",
      "Validation Loss: 0.005763239645807261\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.00834461412508972\n",
      "Training Loss: 0.008892337830038741\n",
      "Training Loss: 0.008050672723911703\n",
      "Validation Loss: 0.005645361488287369\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008246632068185135\n",
      "Training Loss: 0.00880365754594095\n",
      "Training Loss: 0.007965581471798941\n",
      "Validation Loss: 0.0055409227707161664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008159355769166723\n",
      "Training Loss: 0.008724896273342893\n",
      "Training Loss: 0.00788961075944826\n",
      "Validation Loss: 0.005447638647505239\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008081104909069835\n",
      "Training Loss: 0.00865445144358091\n",
      "Training Loss: 0.007821275156456977\n",
      "Validation Loss: 0.005363713688311282\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008010538155212998\n",
      "Training Loss: 0.00859104267321527\n",
      "Training Loss: 0.007759406123077497\n",
      "Validation Loss: 0.005287725860357619\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.007946584229357541\n",
      "Training Loss: 0.008533648321172222\n",
      "Training Loss: 0.007703076617326587\n",
      "Validation Loss: 0.005218556671392884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007888382393866777\n",
      "Training Loss: 0.00848145152325742\n",
      "Training Loss: 0.007651552188908681\n",
      "Validation Loss: 0.005155308682241299\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007835233486257493\n",
      "Training Loss: 0.00843379137921147\n",
      "Training Loss: 0.007604242372326553\n",
      "Validation Loss: 0.005097264021090912\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.00778656316222623\n",
      "Training Loss: 0.008390129341278226\n",
      "Training Loss: 0.007560661217430606\n",
      "Validation Loss: 0.005043828357638938\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007741888425080105\n",
      "Training Loss: 0.0083500179706607\n",
      "Training Loss: 0.007520406272960827\n",
      "Validation Loss: 0.00499450389212102\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.0077007975662127135\n",
      "Training Loss: 0.008313077150378377\n",
      "Training Loss: 0.007483133756322786\n",
      "Validation Loss: 0.004948870775949085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007662936666747555\n",
      "Training Loss: 0.008278983612544834\n",
      "Training Loss: 0.007448551011038944\n",
      "Validation Loss: 0.004906565369514937\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007627995592774823\n",
      "Training Loss: 0.008247456449316815\n",
      "Training Loss: 0.007416403118986637\n",
      "Validation Loss: 0.004867274643790521\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.0075957016099710015\n",
      "Training Loss: 0.008218251095386223\n",
      "Training Loss: 0.007386466911993921\n",
      "Validation Loss: 0.004830720517889\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007565814499976113\n",
      "Training Loss: 0.008191149721387774\n",
      "Training Loss: 0.00735854544211179\n",
      "Validation Loss: 0.00479665497485339\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007538117412477732\n",
      "Training Loss: 0.008165963161736726\n",
      "Training Loss: 0.007332464460050687\n",
      "Validation Loss: 0.004764862133587679\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007512421049177646\n",
      "Training Loss: 0.008142522413982079\n",
      "Training Loss: 0.00730807050364092\n",
      "Validation Loss: 0.004735142740766319\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007488553700968623\n",
      "Training Loss: 0.00812067482038401\n",
      "Training Loss: 0.00728522727265954\n",
      "Validation Loss: 0.004707320640386825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007466361350379884\n",
      "Training Loss: 0.008100286463741214\n",
      "Training Loss: 0.0072638096136506645\n",
      "Validation Loss: 0.004681233000114895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007445705740246921\n",
      "Training Loss: 0.008081234628334641\n",
      "Training Loss: 0.007243708912283182\n",
      "Validation Loss: 0.004656738190901246\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.00742646096390672\n",
      "Training Loss: 0.008063405667198821\n",
      "Training Loss: 0.007224821286508813\n",
      "Validation Loss: 0.004633698795576778\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007408510245149955\n",
      "Training Loss: 0.008046695241937414\n",
      "Training Loss: 0.007207052289741114\n",
      "Validation Loss: 0.004611990213122094\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007391744650667533\n",
      "Training Loss: 0.008031004972290248\n",
      "Training Loss: 0.007190311937592924\n",
      "Validation Loss: 0.004591505017047853\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007376062906114384\n",
      "Training Loss: 0.008016242368612439\n",
      "Training Loss: 0.0071745159872807565\n",
      "Validation Loss: 0.004572136126186573\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007361369746504352\n",
      "Training Loss: 0.008002320706145838\n",
      "Training Loss: 0.007159585041226819\n",
      "Validation Loss: 0.004553793973990538\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007347575090825557\n",
      "Training Loss: 0.007989157871343196\n",
      "Training Loss: 0.007145444273483008\n",
      "Validation Loss: 0.004536394793749525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007334595998981968\n",
      "Training Loss: 0.007976679768180475\n",
      "Training Loss: 0.0071320240735076364\n",
      "Validation Loss: 0.004519862588495016\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007322354000061751\n",
      "Training Loss: 0.007964816997991874\n",
      "Training Loss: 0.007119259891333059\n",
      "Validation Loss: 0.0045041285920888186\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007310779963154346\n",
      "Training Loss: 0.007953506056219339\n",
      "Training Loss: 0.007107093083905056\n",
      "Validation Loss: 0.004489133764958281\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007299807936651632\n",
      "Training Loss: 0.00794269255711697\n",
      "Training Loss: 0.007095470228232443\n",
      "Validation Loss: 0.004474822376865182\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007289381151786074\n",
      "Training Loss: 0.00793232667609118\n",
      "Training Loss: 0.007084342539310455\n",
      "Validation Loss: 0.004461146072285731\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007279446922475472\n",
      "Training Loss: 0.007922363702673465\n",
      "Training Loss: 0.0070736678165849295\n",
      "Validation Loss: 0.004448055025462186\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0072699582506902515\n",
      "Training Loss: 0.007912766170920803\n",
      "Training Loss: 0.007063405538210645\n",
      "Validation Loss: 0.0044355129121980644\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007260874136118218\n",
      "Training Loss: 0.007903498300584033\n",
      "Training Loss: 0.007053521649213508\n",
      "Validation Loss: 0.00442347734161977\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007252156048780307\n",
      "Training Loss: 0.007894530651392416\n",
      "Training Loss: 0.007043983113253489\n",
      "Validation Loss: 0.004411912170039971\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007243771231733262\n",
      "Training Loss: 0.007885834987973794\n",
      "Training Loss: 0.007034763640258461\n",
      "Validation Loss: 0.004400783969256817\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007235689390217886\n",
      "Training Loss: 0.007877389932982624\n",
      "Training Loss: 0.007025836876127869\n",
      "Validation Loss: 0.004390062133813005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.0072278845414984975\n",
      "Training Loss: 0.007869172906503082\n",
      "Training Loss: 0.007017179965041578\n",
      "Validation Loss: 0.004379722896670358\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007220332942670211\n",
      "Training Loss: 0.007861166081856935\n",
      "Training Loss: 0.007008771839318797\n",
      "Validation Loss: 0.004369730736767308\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.0072130125225521625\n",
      "Training Loss: 0.007853352372767403\n",
      "Training Loss: 0.007000596063444391\n",
      "Validation Loss: 0.004360069421360667\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007205904548754916\n",
      "Training Loss: 0.007845718007301912\n",
      "Training Loss: 0.006992633594200015\n",
      "Validation Loss: 0.004350711895006426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.0071989907638635485\n",
      "Training Loss: 0.00783824836369604\n",
      "Training Loss: 0.006984870673622936\n",
      "Validation Loss: 0.004341637720929437\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007192256476264447\n",
      "Training Loss: 0.007830932611832396\n",
      "Training Loss: 0.0069772932981140914\n",
      "Validation Loss: 0.00433282726025732\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007185687420424074\n",
      "Training Loss: 0.007823758581653238\n",
      "Training Loss: 0.006969889407046139\n",
      "Validation Loss: 0.004324263804121299\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007179270159685985\n",
      "Training Loss: 0.007816718202084303\n",
      "Training Loss: 0.006962647936306894\n",
      "Validation Loss: 0.004315928597882223\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007172993938438594\n",
      "Training Loss: 0.0078098010760732\n",
      "Training Loss: 0.006955557721666992\n",
      "Validation Loss: 0.004307810494446018\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007166847770567983\n",
      "Training Loss: 0.0078030001430306585\n",
      "Training Loss: 0.006948609692044556\n",
      "Validation Loss: 0.004299888604456622\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007160822433652356\n",
      "Training Loss: 0.0077963083784561606\n",
      "Training Loss: 0.00694179545273073\n",
      "Validation Loss: 0.004292153763804543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007154908175580204\n",
      "Training Loss: 0.00778971822350286\n",
      "Training Loss: 0.0069351066090166565\n",
      "Validation Loss: 0.004284593036000648\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.00714909877628088\n",
      "Training Loss: 0.007783223509904929\n",
      "Training Loss: 0.006928536908235401\n",
      "Validation Loss: 0.004277192629099394\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007143384992377833\n",
      "Training Loss: 0.00777681887615472\n",
      "Training Loss: 0.006922078400384635\n",
      "Validation Loss: 0.004269947590871474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.0071377616666723044\n",
      "Training Loss: 0.007770498638856225\n",
      "Training Loss: 0.006915724974824115\n",
      "Validation Loss: 0.004262840851429808\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007132221844512969\n",
      "Training Loss: 0.007764259298564866\n",
      "Training Loss: 0.006909471283433959\n",
      "Validation Loss: 0.004255870485343457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007126760098617524\n",
      "Training Loss: 0.007758095047320239\n",
      "Training Loss: 0.006903312025824562\n",
      "Validation Loss: 0.004249025363830871\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007121372141409665\n",
      "Training Loss: 0.007752001990447752\n",
      "Training Loss: 0.006897241922561079\n",
      "Validation Loss: 0.004242296704729454\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007116052102064714\n",
      "Training Loss: 0.007745977636659518\n",
      "Training Loss: 0.006891256521921605\n",
      "Validation Loss: 0.004235680029319411\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0071107967058196666\n",
      "Training Loss: 0.007740017246687785\n",
      "Training Loss: 0.006885351096279919\n",
      "Validation Loss: 0.004229167133590646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007105601358925923\n",
      "Training Loss: 0.007734117933432571\n",
      "Training Loss: 0.006879521951777861\n",
      "Validation Loss: 0.004222750937137125\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007100462371017784\n",
      "Training Loss: 0.00772827641107142\n",
      "Training Loss: 0.006873765528434887\n",
      "Validation Loss: 0.004216431264681846\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.0070953768468461935\n",
      "Training Loss: 0.007722490725573152\n",
      "Training Loss: 0.006868077839026227\n",
      "Validation Loss: 0.004210196999215594\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.0070903415640350435\n",
      "Training Loss: 0.007716756942099892\n",
      "Training Loss: 0.006862455111695453\n",
      "Validation Loss: 0.0042040429923034615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007085352804278955\n",
      "Training Loss: 0.007711073736427352\n",
      "Training Loss: 0.006856894101947546\n",
      "Validation Loss: 0.00419796832962736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007080408313777297\n",
      "Training Loss: 0.007705437316326424\n",
      "Training Loss: 0.006851392514072359\n",
      "Validation Loss: 0.004191969442861468\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.00707550655468367\n",
      "Training Loss: 0.0076998469431418925\n",
      "Training Loss: 0.0068459469336085025\n",
      "Validation Loss: 0.0041860367375520175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007070643396582454\n",
      "Training Loss: 0.00769429903361015\n",
      "Training Loss: 0.006840554367518053\n",
      "Validation Loss: 0.004180172281706015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007065818106057122\n",
      "Training Loss: 0.007688792896806263\n",
      "Training Loss: 0.006835212933365256\n",
      "Validation Loss: 0.004174370847098278\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007061028344323859\n",
      "Training Loss: 0.007683325987309217\n",
      "Training Loss: 0.006829919378506019\n",
      "Validation Loss: 0.0041686317025359425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007056271098554135\n",
      "Training Loss: 0.007677896817331203\n",
      "Training Loss: 0.006824671648209915\n",
      "Validation Loss: 0.0041629459232803475\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007051545685390011\n",
      "Training Loss: 0.007672503892681561\n",
      "Training Loss: 0.006819467609748244\n",
      "Validation Loss: 0.0041573151025209535\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007046850328333676\n",
      "Training Loss: 0.007667144228471443\n",
      "Training Loss: 0.006814305237494409\n",
      "Validation Loss: 0.004151736942036182\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0070421833184082065\n",
      "Training Loss: 0.0076618186297127975\n",
      "Training Loss: 0.006809182154247537\n",
      "Validation Loss: 0.004146207295347717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.0070375428814440965\n",
      "Training Loss: 0.0076565241906791925\n",
      "Training Loss: 0.006804096371633932\n",
      "Validation Loss: 0.004140727198421118\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007032928073313087\n",
      "Training Loss: 0.007651259631384164\n",
      "Training Loss: 0.0067990462749730795\n",
      "Validation Loss: 0.004135292448532464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007028337185038254\n",
      "Training Loss: 0.007646023093257099\n",
      "Training Loss: 0.006794030425371602\n",
      "Validation Loss: 0.0041298999848744175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0070237690967042\n",
      "Training Loss: 0.0076408144953893494\n",
      "Training Loss: 0.006789045917103067\n",
      "Validation Loss: 0.004124546747947677\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.0070192222844343635\n",
      "Training Loss: 0.007635631744051352\n",
      "Training Loss: 0.0067840923159383235\n",
      "Validation Loss: 0.004119237025498591\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007014696266269311\n",
      "Training Loss: 0.007630473938188515\n",
      "Training Loss: 0.006779167632339522\n",
      "Validation Loss: 0.004113963049093491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007010189256398007\n",
      "Training Loss: 0.00762534056790173\n",
      "Training Loss: 0.006774270333116874\n",
      "Validation Loss: 0.0041087281306828845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.007005700852023438\n",
      "Training Loss: 0.007620228916639462\n",
      "Training Loss: 0.006769398477626964\n",
      "Validation Loss: 0.004103527900114069\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.007001229502493516\n",
      "Training Loss: 0.0076151400659000505\n",
      "Training Loss: 0.006764551447704434\n",
      "Validation Loss: 0.004098362430637137\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006996774672297761\n",
      "Training Loss: 0.007610071284580045\n",
      "Training Loss: 0.006759727171156555\n",
      "Validation Loss: 0.004093230636581109\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.00699233450810425\n",
      "Training Loss: 0.007605021953349933\n",
      "Training Loss: 0.006754924851702526\n",
      "Validation Loss: 0.004088130606903454\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.0069879097223747525\n",
      "Training Loss: 0.007599991294555366\n",
      "Training Loss: 0.0067501437733881176\n",
      "Validation Loss: 0.0040830604240214554\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006983498103218153\n",
      "Training Loss: 0.007594978884444572\n",
      "Training Loss: 0.006745381932705641\n",
      "Validation Loss: 0.0040780224502505215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006979099400341511\n",
      "Training Loss: 0.007589983865036629\n",
      "Training Loss: 0.00674063797807321\n",
      "Validation Loss: 0.004073009488769294\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006974712664959953\n",
      "Training Loss: 0.007585004817810841\n",
      "Training Loss: 0.0067359118931926785\n",
      "Validation Loss: 0.0040680275944825475\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006970337406964972\n",
      "Training Loss: 0.007580040778848342\n",
      "Training Loss: 0.006731201946968213\n",
      "Validation Loss: 0.004063073943337697\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.00696597239933908\n",
      "Training Loss: 0.007575091591570526\n",
      "Training Loss: 0.006726506476989016\n",
      "Validation Loss: 0.004058144573289691\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006961616778280586\n",
      "Training Loss: 0.0075701555918203664\n",
      "Training Loss: 0.006721825207350775\n",
      "Validation Loss: 0.004053243497397039\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006957270158454776\n",
      "Training Loss: 0.007565231964690611\n",
      "Training Loss: 0.0067171564931049945\n",
      "Validation Loss: 0.004048363773905662\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006952931944979355\n",
      "Training Loss: 0.007560320368502289\n",
      "Training Loss: 0.00671249941806309\n",
      "Validation Loss: 0.0040435103642011295\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.0069486006291117515\n",
      "Training Loss: 0.007555419774726033\n",
      "Training Loss: 0.006707854385022074\n",
      "Validation Loss: 0.00403868108909326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006944276710273698\n",
      "Training Loss: 0.0075505299237556755\n",
      "Training Loss: 0.006703218915499747\n",
      "Validation Loss: 0.00403387229915792\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.0069399584317579865\n",
      "Training Loss: 0.007545649212552234\n",
      "Training Loss: 0.006698593304026872\n",
      "Validation Loss: 0.004029086364558741\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.0069356460368726405\n",
      "Training Loss: 0.007540778119582683\n",
      "Training Loss: 0.006693975486559794\n",
      "Validation Loss: 0.004024321624088321\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006931337574496865\n",
      "Training Loss: 0.007535914365435019\n",
      "Training Loss: 0.006689366061473265\n",
      "Validation Loss: 0.004019578092135071\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006927033906104043\n",
      "Training Loss: 0.007531058026361279\n",
      "Training Loss: 0.006684762898366899\n",
      "Validation Loss: 0.0040148542945409255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.0069227336184121665\n",
      "Training Loss: 0.007526208381750621\n",
      "Training Loss: 0.0066801657783798875\n",
      "Validation Loss: 0.004010151248958925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006918435823172331\n",
      "Training Loss: 0.007521365051507019\n",
      "Training Loss: 0.006675573787651956\n",
      "Validation Loss: 0.004005466787971221\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006914140317821876\n",
      "Training Loss: 0.0075165268447017295\n",
      "Training Loss: 0.006670985968085006\n",
      "Validation Loss: 0.004000798851419031\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.0069098462013062086\n",
      "Training Loss: 0.007511692777625285\n",
      "Training Loss: 0.006666401971597224\n",
      "Validation Loss: 0.003996149924573269\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0069055528368335214\n",
      "Training Loss: 0.007506861527217552\n",
      "Training Loss: 0.006661820869194344\n",
      "Validation Loss: 0.003991519475062744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006901259899605066\n",
      "Training Loss: 0.007502034605131484\n",
      "Training Loss: 0.0066572413686662914\n",
      "Validation Loss: 0.003986905058165699\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006896965974010527\n",
      "Training Loss: 0.007497208260465413\n",
      "Training Loss: 0.0066526636516209695\n",
      "Validation Loss: 0.00398230732920883\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.0068926714931149035\n",
      "Training Loss: 0.007492385046207346\n",
      "Training Loss: 0.00664808664820157\n",
      "Validation Loss: 0.003977724820574264\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006888375675771385\n",
      "Training Loss: 0.0074875626107677815\n",
      "Training Loss: 0.006643509326968342\n",
      "Validation Loss: 0.003973160468805791\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006884077412541956\n",
      "Training Loss: 0.007482740193954669\n",
      "Training Loss: 0.006638931060442701\n",
      "Validation Loss: 0.003968609026057667\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006879775818670168\n",
      "Training Loss: 0.007477916256757453\n",
      "Training Loss: 0.006634352366672829\n",
      "Validation Loss: 0.003964072841564926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006875470743980258\n",
      "Training Loss: 0.007473091737483628\n",
      "Training Loss: 0.0066297706961631776\n",
      "Validation Loss: 0.0039595533437043265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.0068711627938319\n",
      "Training Loss: 0.007468265251955018\n",
      "Training Loss: 0.006625186826568097\n",
      "Validation Loss: 0.003955043624730759\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006866849513025955\n",
      "Training Loss: 0.007463436183170416\n",
      "Training Loss: 0.006620599053567275\n",
      "Validation Loss: 0.003950548290243644\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.0068625309935305264\n",
      "Training Loss: 0.007458602869301103\n",
      "Training Loss: 0.0066160074342042205\n",
      "Validation Loss: 0.003946067503747645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006858206419274211\n",
      "Training Loss: 0.007453766075777821\n",
      "Training Loss: 0.006611411208286882\n",
      "Validation Loss: 0.003941597499403308\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006853875759989023\n",
      "Training Loss: 0.0074489242193521935\n",
      "Training Loss: 0.006606809409568086\n",
      "Validation Loss: 0.00393713783247794\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006849537824746221\n",
      "Training Loss: 0.007444077617255971\n",
      "Training Loss: 0.006602201394271106\n",
      "Validation Loss: 0.003932690540893694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006845192959299311\n",
      "Training Loss: 0.007439223584369757\n",
      "Training Loss: 0.006597586611751467\n",
      "Validation Loss: 0.003928254022304848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006840839417418465\n",
      "Training Loss: 0.007434363187639974\n",
      "Training Loss: 0.006592964594019577\n",
      "Validation Loss: 0.003923826508244939\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.0068364775262307375\n",
      "Training Loss: 0.007429495027172379\n",
      "Training Loss: 0.006588333927793428\n",
      "Validation Loss: 0.0039194060000329375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.0068321064044721425\n",
      "Training Loss: 0.0074246176815358925\n",
      "Training Loss: 0.006583696010056883\n",
      "Validation Loss: 0.0039150012065813445\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006827725962502882\n",
      "Training Loss: 0.007419732709531672\n",
      "Training Loss: 0.006579048722051084\n",
      "Validation Loss: 0.003910604284898368\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006823334716027602\n",
      "Training Loss: 0.00741483781952411\n",
      "Training Loss: 0.006574391125468537\n",
      "Validation Loss: 0.0039062152467563413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006818933526519686\n",
      "Training Loss: 0.007409932485898025\n",
      "Training Loss: 0.006569723747670651\n",
      "Validation Loss: 0.0039018337651459353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006814520318293944\n",
      "Training Loss: 0.007405016053817235\n",
      "Training Loss: 0.006565044918097555\n",
      "Validation Loss: 0.0038974620611145257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006810096185654401\n",
      "Training Loss: 0.007400087797432207\n",
      "Training Loss: 0.006560354656539857\n",
      "Validation Loss: 0.0038930954414241937\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006805659165838734\n",
      "Training Loss: 0.0073951473401393745\n",
      "Training Loss: 0.0065556525636930015\n",
      "Validation Loss: 0.0038887372677308624\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.0068012096942402425\n",
      "Training Loss: 0.0073901945847319435\n",
      "Training Loss: 0.006550937317078933\n",
      "Validation Loss: 0.003884385036451093\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.0067967464495450255\n",
      "Training Loss: 0.007385227145277895\n",
      "Training Loss: 0.006546208772342652\n",
      "Validation Loss: 0.00388003943038037\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006792269637808204\n",
      "Training Loss: 0.007380246161483228\n",
      "Training Loss: 0.006541467068018392\n",
      "Validation Loss: 0.0038756996869329322\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006787778995931149\n",
      "Training Loss: 0.007375250236946158\n",
      "Training Loss: 0.006536710563814267\n",
      "Validation Loss: 0.003871362962435638\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.0067832729080691935\n",
      "Training Loss: 0.007370238426374271\n",
      "Training Loss: 0.0065319388429634275\n",
      "Validation Loss: 0.003867032771584777\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006778752246173098\n",
      "Training Loss: 0.00736521092359908\n",
      "Training Loss: 0.006527151956688613\n",
      "Validation Loss: 0.0038627045990355063\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006774215345503762\n",
      "Training Loss: 0.007360165950958617\n",
      "Training Loss: 0.006522348448634148\n",
      "Validation Loss: 0.0038583781387070927\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006769662882434204\n",
      "Training Loss: 0.007355104592279531\n",
      "Training Loss: 0.006517529018456116\n",
      "Validation Loss: 0.0038540579451854977\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006765093181747943\n",
      "Training Loss: 0.007350024191546254\n",
      "Training Loss: 0.0065126919338945296\n",
      "Validation Loss: 0.003849741667927651\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006760506975697353\n",
      "Training Loss: 0.007344925810466521\n",
      "Training Loss: 0.006507837609387934\n",
      "Validation Loss: 0.003845424537829469\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006755903040757402\n",
      "Training Loss: 0.007339808067772537\n",
      "Training Loss: 0.006502964912215248\n",
      "Validation Loss: 0.0038411099466317323\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006751280723838135\n",
      "Training Loss: 0.00733467026904691\n",
      "Training Loss: 0.006498073861002922\n",
      "Validation Loss: 0.003836796792967015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.0067466409970074895\n",
      "Training Loss: 0.007329513093573041\n",
      "Training Loss: 0.006493162717670202\n",
      "Validation Loss: 0.0038324852560364295\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006741982138482854\n",
      "Training Loss: 0.007324334030854516\n",
      "Training Loss: 0.0064882319746539\n",
      "Validation Loss: 0.0038281706988476634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006737303227419033\n",
      "Training Loss: 0.00731913341965992\n",
      "Training Loss: 0.006483280792599544\n",
      "Validation Loss: 0.003823859002336525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006732605430297554\n",
      "Training Loss: 0.007313910751254298\n",
      "Training Loss: 0.006478308521909639\n",
      "Validation Loss: 0.003819543494204624\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006727886330336333\n",
      "Training Loss: 0.007308665582095273\n",
      "Training Loss: 0.006473314869217575\n",
      "Validation Loss: 0.0038152301574146813\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006723147726152092\n",
      "Training Loss: 0.007303396512288601\n",
      "Training Loss: 0.006468298942781985\n",
      "Validation Loss: 0.0038109120554447678\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006718387682922184\n",
      "Training Loss: 0.00729810391261708\n",
      "Training Loss: 0.006463261387543753\n",
      "Validation Loss: 0.0038065942726359606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.0067136069480329755\n",
      "Training Loss: 0.007292787584010512\n",
      "Training Loss: 0.006458200557390228\n",
      "Validation Loss: 0.003802276304085854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006708804593654349\n",
      "Training Loss: 0.007287446380942129\n",
      "Training Loss: 0.006453116531483829\n",
      "Validation Loss: 0.0037979513152994297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006703979746671394\n",
      "Training Loss: 0.007282079470460303\n",
      "Training Loss: 0.006448008877923712\n",
      "Validation Loss: 0.003793624414162438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006699132182402537\n",
      "Training Loss: 0.007276687023113482\n",
      "Training Loss: 0.006442876295186579\n",
      "Validation Loss: 0.0037892941356730764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006694261617958545\n",
      "Training Loss: 0.007271267767064274\n",
      "Training Loss: 0.006437719230307266\n",
      "Validation Loss: 0.003784958764994412\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006689367819344625\n",
      "Training Loss: 0.007265821538167075\n",
      "Training Loss: 0.006432536678621545\n",
      "Validation Loss: 0.0037806203662093434\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006684450476896018\n",
      "Training Loss: 0.00726034824794624\n",
      "Training Loss: 0.006427328130230308\n",
      "Validation Loss: 0.0037762713893264364\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.00667950912611559\n",
      "Training Loss: 0.007254847392323427\n",
      "Training Loss: 0.00642209334182553\n",
      "Validation Loss: 0.003771923566132449\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006674543160479516\n",
      "Training Loss: 0.007249318089452572\n",
      "Training Loss: 0.006416832274990156\n",
      "Validation Loss: 0.0037675688809746606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.0066695525823161\n",
      "Training Loss: 0.007243759826524183\n",
      "Training Loss: 0.006411544460570439\n",
      "Validation Loss: 0.003763207081401867\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006664536328753456\n",
      "Training Loss: 0.007238172416109591\n",
      "Training Loss: 0.0064062275574542586\n",
      "Validation Loss: 0.0037588392844779437\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.0066594947630073875\n",
      "Training Loss: 0.0072325556690339\n",
      "Training Loss: 0.0064008836378343405\n",
      "Validation Loss: 0.0037544630363248706\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006654427791945636\n",
      "Training Loss: 0.007226908696466126\n",
      "Training Loss: 0.006395510743604973\n",
      "Validation Loss: 0.0037500782506121847\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006649333991808817\n",
      "Training Loss: 0.007221231054863893\n",
      "Training Loss: 0.006390108991181478\n",
      "Validation Loss: 0.0037456850005899755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.0066442142857704315\n",
      "Training Loss: 0.00721552217961289\n",
      "Training Loss: 0.006384677953319624\n",
      "Validation Loss: 0.0037412847120189266\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.00663906708243303\n",
      "Training Loss: 0.007209782818099484\n",
      "Training Loss: 0.00637921660207212\n",
      "Validation Loss: 0.0037368770134164377\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0066338929790072144\n",
      "Training Loss: 0.007204011013382115\n",
      "Training Loss: 0.0063737254519946876\n",
      "Validation Loss: 0.0037324570820488957\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006628690739162267\n",
      "Training Loss: 0.007198207403416745\n",
      "Training Loss: 0.00636820363230072\n",
      "Validation Loss: 0.0037280259578059733\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.0066234606702346354\n",
      "Training Loss: 0.007192371287965215\n",
      "Training Loss: 0.0063626502326224\n",
      "Validation Loss: 0.0037235859925387783\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006618201918900013\n",
      "Training Loss: 0.007186502105323598\n",
      "Training Loss: 0.0063570664345752445\n",
      "Validation Loss: 0.003719136105808482\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006612915464211255\n",
      "Training Loss: 0.007180599988205358\n",
      "Training Loss: 0.006351450707297772\n",
      "Validation Loss: 0.003714675808409124\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006607599752023816\n",
      "Training Loss: 0.007174664948834107\n",
      "Training Loss: 0.006345802757423371\n",
      "Validation Loss: 0.0037101992154807855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006602254948811606\n",
      "Training Loss: 0.007168695740983821\n",
      "Training Loss: 0.0063401224405970425\n",
      "Validation Loss: 0.0037057160575131184\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006596881019650027\n",
      "Training Loss: 0.007162692294223234\n",
      "Training Loss: 0.006334408962866292\n",
      "Validation Loss: 0.003701217887201085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.00659147648490034\n",
      "Training Loss: 0.007156654606224038\n",
      "Training Loss: 0.006328662552405149\n",
      "Validation Loss: 0.0036967086325807685\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006586042596027255\n",
      "Training Loss: 0.007150582110043615\n",
      "Training Loss: 0.006322882975218817\n",
      "Validation Loss: 0.0036921870300881147\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006580578505527228\n",
      "Training Loss: 0.007144475287059322\n",
      "Training Loss: 0.006317069673677907\n",
      "Validation Loss: 0.003687650394322497\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006575084094656632\n",
      "Training Loss: 0.007138333424227312\n",
      "Training Loss: 0.006311222439398989\n",
      "Validation Loss: 0.0036831001170356285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006569558612536639\n",
      "Training Loss: 0.007132156498846598\n",
      "Training Loss: 0.006305340612307191\n",
      "Validation Loss: 0.003678533951019387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006564002127852291\n",
      "Training Loss: 0.007125943621504121\n",
      "Training Loss: 0.006299424810567871\n",
      "Validation Loss: 0.003673958915202052\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.0065584153076633815\n",
      "Training Loss: 0.007119695778237655\n",
      "Training Loss: 0.006293474953854457\n",
      "Validation Loss: 0.003669364868370251\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0065527963533531875\n",
      "Training Loss: 0.007113411542377435\n",
      "Training Loss: 0.006287489451933652\n",
      "Validation Loss: 0.003664759361823456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive\\predictive_evaluation.py:272: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([{'Model': evaluation_method, 'Metric': 'MAE', 'Error': mae}])], ignore_index=True)\n",
      " 10%|█         | 1/10 [03:25<30:49, 205.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Loss: 0.2292486983910203\n",
      "Training Loss: 0.15347233604639768\n",
      "Training Loss: 0.11345677968114615\n",
      "Validation Loss: 0.08436936131688987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.07819454370066524\n",
      "Training Loss: 0.06370444763451814\n",
      "Training Loss: 0.059538753498345615\n",
      "Validation Loss: 0.053321970773212025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.05609009247273207\n",
      "Training Loss: 0.05318434115499258\n",
      "Training Loss: 0.05116597623564303\n",
      "Validation Loss: 0.045391756455215176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.04796953008510172\n",
      "Training Loss: 0.04458001608029008\n",
      "Training Loss: 0.041501038316637276\n",
      "Validation Loss: 0.03556717505280891\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.03745094634592533\n",
      "Training Loss: 0.03369025973603129\n",
      "Training Loss: 0.030661351550370454\n",
      "Validation Loss: 0.02557138040703669\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.027357364809140562\n",
      "Training Loss: 0.024417733438313008\n",
      "Training Loss: 0.02224414434283972\n",
      "Validation Loss: 0.018416045495214757\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.020485028494149446\n",
      "Training Loss: 0.018683754729572684\n",
      "Training Loss: 0.017180723594501613\n",
      "Validation Loss: 0.014452679956520206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.016610275097191332\n",
      "Training Loss: 0.015600471911020576\n",
      "Training Loss: 0.014459596564993262\n",
      "Validation Loss: 0.01230196709734168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.014468178343959153\n",
      "Training Loss: 0.013887351481243967\n",
      "Training Loss: 0.012901587199885399\n",
      "Validation Loss: 0.01094455080724248\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.01313046868192032\n",
      "Training Loss: 0.012796906840521842\n",
      "Training Loss: 0.01188107834663242\n",
      "Validation Loss: 0.009953493252396584\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.012181046823970974\n",
      "Training Loss: 0.012016027541831136\n",
      "Training Loss: 0.011136111784726382\n",
      "Validation Loss: 0.0091635921206116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.01144560013897717\n",
      "Training Loss: 0.011410235690418631\n",
      "Training Loss: 0.010552385875489562\n",
      "Validation Loss: 0.008502478577887242\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.010845048879273235\n",
      "Training Loss: 0.010915712746791541\n",
      "Training Loss: 0.010075073288753629\n",
      "Validation Loss: 0.007935009446790379\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.010340606370009482\n",
      "Training Loss: 0.010500049169640988\n",
      "Training Loss: 0.009675589407561348\n",
      "Validation Loss: 0.007442987513508689\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.009911840041168034\n",
      "Training Loss: 0.01014583108248189\n",
      "Training Loss: 0.009337662098696456\n",
      "Validation Loss: 0.00701583571093722\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.009546605891082437\n",
      "Training Loss: 0.009842707072384655\n",
      "Training Loss: 0.009050673363963141\n",
      "Validation Loss: 0.0066459142691926675\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.009236063874559476\n",
      "Training Loss: 0.009583339954260736\n",
      "Training Loss: 0.008806454149307683\n",
      "Validation Loss: 0.006326613165542818\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.008972561463015154\n",
      "Training Loss: 0.009361544351559132\n",
      "Training Loss: 0.00859798323130235\n",
      "Validation Loss: 0.006051856864850675\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.00874901857925579\n",
      "Training Loss: 0.009171653538942337\n",
      "Training Loss: 0.00841905113775283\n",
      "Validation Loss: 0.005816090956665157\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.008558925411198287\n",
      "Training Loss: 0.009008436674484983\n",
      "Training Loss: 0.008264258592389525\n",
      "Validation Loss: 0.005614236308512896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008396489769220352\n",
      "Training Loss: 0.00886722449096851\n",
      "Training Loss: 0.00812910437467508\n",
      "Validation Loss: 0.005441676150337698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.008256769075524062\n",
      "Training Loss: 0.00874402905232273\n",
      "Training Loss: 0.00800998511374928\n",
      "Validation Loss: 0.005294214667271028\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008135690753115342\n",
      "Training Loss: 0.008635561815463006\n",
      "Training Loss: 0.00790407668799162\n",
      "Validation Loss: 0.005168041583213411\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008029953530058264\n",
      "Training Loss: 0.008539155260659754\n",
      "Training Loss: 0.007809162843041122\n",
      "Validation Loss: 0.005059755713354503\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0079369054059498\n",
      "Training Loss: 0.008452655771980061\n",
      "Training Loss: 0.007723479233682155\n",
      "Validation Loss: 0.0049663517634604085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007854407150298358\n",
      "Training Loss: 0.008374329686630518\n",
      "Training Loss: 0.007645602532429621\n",
      "Validation Loss: 0.004885238909235831\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.007780723909381777\n",
      "Training Loss: 0.008302798903314397\n",
      "Training Loss: 0.0075743877212516965\n",
      "Validation Loss: 0.004814217976304922\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007714454862289131\n",
      "Training Loss: 0.008237003326648847\n",
      "Training Loss: 0.007508938678074628\n",
      "Validation Loss: 0.004751480223736569\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.007654468429973349\n",
      "Training Loss: 0.00817616059910506\n",
      "Training Loss: 0.007448566990206018\n",
      "Validation Loss: 0.004695522834368971\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.007599845053628087\n",
      "Training Loss: 0.008119714080821723\n",
      "Training Loss: 0.007392755523324013\n",
      "Validation Loss: 0.0046451479834870675\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.0075498315168078985\n",
      "Training Loss: 0.008067256899084895\n",
      "Training Loss: 0.0073410920344758775\n",
      "Validation Loss: 0.004599384081895265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007503800619160756\n",
      "Training Loss: 0.008018461485626176\n",
      "Training Loss: 0.007293223235756159\n",
      "Validation Loss: 0.004557440622469012\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007461222658166662\n",
      "Training Loss: 0.007973023478407414\n",
      "Training Loss: 0.00724881881265901\n",
      "Validation Loss: 0.0045186879552817074\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007421650956384838\n",
      "Training Loss: 0.007930640997365117\n",
      "Training Loss: 0.007207559861708433\n",
      "Validation Loss: 0.004482622183807027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007384703286224976\n",
      "Training Loss: 0.007891009571030736\n",
      "Training Loss: 0.007169136242009699\n",
      "Validation Loss: 0.004448838057153429\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007350052682450041\n",
      "Training Loss: 0.007853830363601447\n",
      "Training Loss: 0.0071332523121964185\n",
      "Validation Loss: 0.004417014074135112\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007317419113824144\n",
      "Training Loss: 0.00781882219715044\n",
      "Training Loss: 0.007099634417099879\n",
      "Validation Loss: 0.004386883062896517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007286554002203047\n",
      "Training Loss: 0.007785726304864511\n",
      "Training Loss: 0.007068032671231777\n",
      "Validation Loss: 0.004358227572696848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007257244281936437\n",
      "Training Loss: 0.0077543105219956485\n",
      "Training Loss: 0.007038220177637413\n",
      "Validation Loss: 0.004330869498558008\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007229300037724897\n",
      "Training Loss: 0.007724368270137347\n",
      "Training Loss: 0.00700999534339644\n",
      "Validation Loss: 0.004304664328516451\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007202556518604979\n",
      "Training Loss: 0.007695721470518038\n",
      "Training Loss: 0.006983179975068196\n",
      "Validation Loss: 0.0042794737585537725\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007176868633832783\n",
      "Training Loss: 0.007668214365257882\n",
      "Training Loss: 0.00695761555689387\n",
      "Validation Loss: 0.004255202410512426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007152111384784803\n",
      "Training Loss: 0.007641711008036509\n",
      "Training Loss: 0.0069331644556950775\n",
      "Validation Loss: 0.004231753545876048\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007128175318939611\n",
      "Training Loss: 0.007616098357830197\n",
      "Training Loss: 0.006909705221187323\n",
      "Validation Loss: 0.004209053015932859\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.00710496865096502\n",
      "Training Loss: 0.00759127716126386\n",
      "Training Loss: 0.006887134411372244\n",
      "Validation Loss: 0.004187046325231871\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.00708241208922118\n",
      "Training Loss: 0.007567166921799071\n",
      "Training Loss: 0.006865360852098093\n",
      "Validation Loss: 0.004165672555858834\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0070604400034062565\n",
      "Training Loss: 0.007543698018416763\n",
      "Training Loss: 0.006844306994462385\n",
      "Validation Loss: 0.004144897715847814\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007038995396578684\n",
      "Training Loss: 0.0075208158156601715\n",
      "Training Loss: 0.006823906567879021\n",
      "Validation Loss: 0.0041246806700410464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007018033986678347\n",
      "Training Loss: 0.0074984731758013365\n",
      "Training Loss: 0.0068041035649366675\n",
      "Validation Loss: 0.004104997163502436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006997518059797585\n",
      "Training Loss: 0.007476634815102443\n",
      "Training Loss: 0.006784848417155444\n",
      "Validation Loss: 0.004085822310821896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006977417706511915\n",
      "Training Loss: 0.007455270959762856\n",
      "Training Loss: 0.006766101847169921\n",
      "Validation Loss: 0.004067139857019601\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.0069577088742516936\n",
      "Training Loss: 0.007434360682382248\n",
      "Training Loss: 0.006747827664949\n",
      "Validation Loss: 0.004048926852580704\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006938373172888532\n",
      "Training Loss: 0.007413887202274054\n",
      "Training Loss: 0.006729998853988946\n",
      "Validation Loss: 0.004031176243259917\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006919397555757314\n",
      "Training Loss: 0.007393838056013919\n",
      "Training Loss: 0.006712590460665524\n",
      "Validation Loss: 0.004013878157299556\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006900771845830604\n",
      "Training Loss: 0.00737420599674806\n",
      "Training Loss: 0.006695582882966847\n",
      "Validation Loss: 0.0039970181762042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.006882488847477362\n",
      "Training Loss: 0.007354986996506341\n",
      "Training Loss: 0.006678957670228556\n",
      "Validation Loss: 0.003980585545290973\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006864543498959392\n",
      "Training Loss: 0.007336175175732933\n",
      "Training Loss: 0.006662701626773924\n",
      "Validation Loss: 0.0039645737008285825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.0068469318805728105\n",
      "Training Loss: 0.007317769820801913\n",
      "Training Loss: 0.006646800993476063\n",
      "Validation Loss: 0.003948974577458889\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006829652381129563\n",
      "Training Loss: 0.0072997691977070645\n",
      "Training Loss: 0.006631245340686292\n",
      "Validation Loss: 0.003933775543465457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006812701023882255\n",
      "Training Loss: 0.007282172426348552\n",
      "Training Loss: 0.006616023789392784\n",
      "Validation Loss: 0.003918965971045029\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006796077076578513\n",
      "Training Loss: 0.007264976255246438\n",
      "Training Loss: 0.006601127911126241\n",
      "Validation Loss: 0.003904536090466832\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006779777156189084\n",
      "Training Loss: 0.007248181201866828\n",
      "Training Loss: 0.0065865486883558335\n",
      "Validation Loss: 0.0038904788278650198\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006763798699248582\n",
      "Training Loss: 0.007231780709698796\n",
      "Training Loss: 0.006572277795057744\n",
      "Validation Loss: 0.0038767843834239613\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.0067481387662701306\n",
      "Training Loss: 0.007215774562791921\n",
      "Training Loss: 0.006558307734085247\n",
      "Validation Loss: 0.003863440415811505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006732792858965695\n",
      "Training Loss: 0.0072001548466505486\n",
      "Training Loss: 0.006544630029238761\n",
      "Validation Loss: 0.0038504340617886083\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006717753380071372\n",
      "Training Loss: 0.007184915974503383\n",
      "Training Loss: 0.006531235651345923\n",
      "Validation Loss: 0.003837753900881396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.006703015933744609\n",
      "Training Loss: 0.007170050027780235\n",
      "Training Loss: 0.006518117033410817\n",
      "Validation Loss: 0.0038253825767426176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.006688572178827599\n",
      "Training Loss: 0.007155548474984243\n",
      "Training Loss: 0.00650526384008117\n",
      "Validation Loss: 0.0038133108977939974\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006674414535518735\n",
      "Training Loss: 0.007141400115215219\n",
      "Training Loss: 0.0064926688035484405\n",
      "Validation Loss: 0.0038015254932780114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006660533144604415\n",
      "Training Loss: 0.007127593839541078\n",
      "Training Loss: 0.006480321477865801\n",
      "Validation Loss: 0.0037900147491312596\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.00664691976387985\n",
      "Training Loss: 0.007114117510500364\n",
      "Training Loss: 0.00646821208880283\n",
      "Validation Loss: 0.00377876201403861\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006633562847273424\n",
      "Training Loss: 0.00710095765942242\n",
      "Training Loss: 0.006456331500085071\n",
      "Validation Loss: 0.003767755760267126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006620452532079071\n",
      "Training Loss: 0.007088102192501538\n",
      "Training Loss: 0.006444670654600486\n",
      "Validation Loss: 0.003756987229198887\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006607578380499035\n",
      "Training Loss: 0.007075535471085459\n",
      "Training Loss: 0.006433218154124915\n",
      "Validation Loss: 0.00374643507235673\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006594926654361189\n",
      "Training Loss: 0.0070632441993802786\n",
      "Training Loss: 0.006421964592300355\n",
      "Validation Loss: 0.00373609505954795\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006582488521235064\n",
      "Training Loss: 0.0070512119831983\n",
      "Training Loss: 0.006410900574410334\n",
      "Validation Loss: 0.0037259475610564264\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006570250570075586\n",
      "Training Loss: 0.007039425490074791\n",
      "Training Loss: 0.006400014146929606\n",
      "Validation Loss: 0.003715983730625738\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.006558202323503792\n",
      "Training Loss: 0.007027871257741935\n",
      "Training Loss: 0.006389296930283308\n",
      "Validation Loss: 0.0037061899659757533\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006546331880381331\n",
      "Training Loss: 0.007016532279085368\n",
      "Training Loss: 0.0063787387078627945\n",
      "Validation Loss: 0.0036965523940626155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006534629319794476\n",
      "Training Loss: 0.007005397638422437\n",
      "Training Loss: 0.006368330563418567\n",
      "Validation Loss: 0.003687062908979028\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006523083589272573\n",
      "Training Loss: 0.006994451151695102\n",
      "Training Loss: 0.006358062393264845\n",
      "Validation Loss: 0.0036777079659985022\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.00651168565498665\n",
      "Training Loss: 0.006983682151767426\n",
      "Training Loss: 0.006347926412709057\n",
      "Validation Loss: 0.0036684772041573953\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006500424981350079\n",
      "Training Loss: 0.006973077001748607\n",
      "Training Loss: 0.006337913181632757\n",
      "Validation Loss: 0.0036593593050087436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006489293253980577\n",
      "Training Loss: 0.006962625642772764\n",
      "Training Loss: 0.00632801768137142\n",
      "Validation Loss: 0.003650350763012519\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006478283518226817\n",
      "Training Loss: 0.006952318532858044\n",
      "Training Loss: 0.006318231166806072\n",
      "Validation Loss: 0.003641433850338871\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.0064673870895057915\n",
      "Training Loss: 0.006942144299391657\n",
      "Training Loss: 0.006308548114029691\n",
      "Validation Loss: 0.00363260781879045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006456599024822936\n",
      "Training Loss: 0.006932094455114566\n",
      "Training Loss: 0.006298961497377604\n",
      "Validation Loss: 0.0036238572793081403\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006445911709452048\n",
      "Training Loss: 0.006922161754337139\n",
      "Training Loss: 0.006289468088652939\n",
      "Validation Loss: 0.0036151828793704174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.00643532159156166\n",
      "Training Loss: 0.006912339709233492\n",
      "Training Loss: 0.006280062034493312\n",
      "Validation Loss: 0.003606573410405453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.00642482461873442\n",
      "Training Loss: 0.006902620249893516\n",
      "Training Loss: 0.0062707400554791095\n",
      "Validation Loss: 0.003598028312573272\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006414416034240276\n",
      "Training Loss: 0.006893001322750934\n",
      "Training Loss: 0.006261497961822897\n",
      "Validation Loss: 0.0035895369188294985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0064040938054677095\n",
      "Training Loss: 0.006883474958594888\n",
      "Training Loss: 0.00625233409460634\n",
      "Validation Loss: 0.0035811007713501372\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006393855715868995\n",
      "Training Loss: 0.006874039316899143\n",
      "Training Loss: 0.006243245714576915\n",
      "Validation Loss: 0.0035727166379749607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006383700692094862\n",
      "Training Loss: 0.006864690958755091\n",
      "Training Loss: 0.0062342318193987015\n",
      "Validation Loss: 0.0035643790759606643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006373626966960728\n",
      "Training Loss: 0.006855426701367833\n",
      "Training Loss: 0.006225289781577885\n",
      "Validation Loss: 0.0035560837557037056\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.0063636345975100995\n",
      "Training Loss: 0.006846245159977116\n",
      "Training Loss: 0.006216419019037858\n",
      "Validation Loss: 0.0035478316804687126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.00635372249642387\n",
      "Training Loss: 0.006837143357261084\n",
      "Training Loss: 0.006207619593478739\n",
      "Validation Loss: 0.003539622956206708\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006343891056021675\n",
      "Training Loss: 0.006828121870639734\n",
      "Training Loss: 0.006198890132363886\n",
      "Validation Loss: 0.0035314547732535205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0063341412879526614\n",
      "Training Loss: 0.006819178472505882\n",
      "Training Loss: 0.00619023057166487\n",
      "Validation Loss: 0.0035233297280632377\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006324472511187196\n",
      "Training Loss: 0.006810313343303278\n",
      "Training Loss: 0.0061816408613231035\n",
      "Validation Loss: 0.003515243527657363\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006314886554609984\n",
      "Training Loss: 0.0068015244620619345\n",
      "Training Loss: 0.006173120884923265\n",
      "Validation Loss: 0.0035072006782244764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006305382130667567\n",
      "Training Loss: 0.006792813537758775\n",
      "Training Loss: 0.0061646708648186175\n",
      "Validation Loss: 0.0034991986774891783\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006295963438460604\n",
      "Training Loss: 0.006784178040688858\n",
      "Training Loss: 0.006156291628722102\n",
      "Validation Loss: 0.003491238749774403\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006286629135720432\n",
      "Training Loss: 0.006775620629196055\n",
      "Training Loss: 0.006147983547998592\n",
      "Validation Loss: 0.0034833230559578104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0062773806939367205\n",
      "Training Loss: 0.006767139359144494\n",
      "Training Loss: 0.006139745413092896\n",
      "Validation Loss: 0.0034754516405126686\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006268219241173938\n",
      "Training Loss: 0.006758734853356145\n",
      "Training Loss: 0.006131579017965123\n",
      "Validation Loss: 0.003467627756527803\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006259145167423413\n",
      "Training Loss: 0.006750406596111134\n",
      "Training Loss: 0.006123483098344877\n",
      "Validation Loss: 0.003459848334039614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006250158422626555\n",
      "Training Loss: 0.006742153816157952\n",
      "Training Loss: 0.006115459323627875\n",
      "Validation Loss: 0.0034521206797445926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.0062412612093612554\n",
      "Training Loss: 0.006733980156714097\n",
      "Training Loss: 0.00610750715364702\n",
      "Validation Loss: 0.0034444420729251054\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006232453844277188\n",
      "Training Loss: 0.006725882981554605\n",
      "Training Loss: 0.006099626590730623\n",
      "Validation Loss: 0.0034368150812584194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006223735720850527\n",
      "Training Loss: 0.006717862028162927\n",
      "Training Loss: 0.006091818255372345\n",
      "Validation Loss: 0.0034292392416993217\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006215108537580818\n",
      "Training Loss: 0.006709918042179197\n",
      "Training Loss: 0.006084081319859252\n",
      "Validation Loss: 0.0034217178256491596\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.00620657220017165\n",
      "Training Loss: 0.006702049353625625\n",
      "Training Loss: 0.006076415708521381\n",
      "Validation Loss: 0.00341424955384743\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006198124433867633\n",
      "Training Loss: 0.006694256790215149\n",
      "Training Loss: 0.00606882160063833\n",
      "Validation Loss: 0.00340684100702991\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006189767518080771\n",
      "Training Loss: 0.006686539310030639\n",
      "Training Loss: 0.006061297614360228\n",
      "Validation Loss: 0.003399483991650802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006181498703081161\n",
      "Training Loss: 0.006678894947399385\n",
      "Training Loss: 0.006053843541303649\n",
      "Validation Loss: 0.00339218930294214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.00617332078400068\n",
      "Training Loss: 0.006671325740753673\n",
      "Training Loss: 0.006046459391945973\n",
      "Validation Loss: 0.0033849505576817843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006165230461629107\n",
      "Training Loss: 0.006663827658048831\n",
      "Training Loss: 0.006039141936926171\n",
      "Validation Loss: 0.0033777722725226137\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006157226503128186\n",
      "Training Loss: 0.006656402552616783\n",
      "Training Loss: 0.0060318942635785786\n",
      "Validation Loss: 0.0033706559124664307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006149311354383826\n",
      "Training Loss: 0.006649047520477324\n",
      "Training Loss: 0.0060247132985387\n",
      "Validation Loss: 0.00336359842063001\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006141481380909681\n",
      "Training Loss: 0.0066417640284635125\n",
      "Training Loss: 0.006017597627360374\n",
      "Validation Loss: 0.003356603528843837\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.0061337356979493055\n",
      "Training Loss: 0.006634547732537612\n",
      "Training Loss: 0.006010547346668318\n",
      "Validation Loss: 0.0033496705438481288\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006126074123894796\n",
      "Training Loss: 0.006627398843411356\n",
      "Training Loss: 0.006003560352837667\n",
      "Validation Loss: 0.0033427984584541467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006118493443354964\n",
      "Training Loss: 0.00662031703221146\n",
      "Training Loss: 0.005996634857729078\n",
      "Validation Loss: 0.003335988132042413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006110994585324079\n",
      "Training Loss: 0.006613299361197278\n",
      "Training Loss: 0.005989771665772423\n",
      "Validation Loss: 0.003329242298410933\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006103574130684138\n",
      "Training Loss: 0.0066063445538748055\n",
      "Training Loss: 0.005982967469608411\n",
      "Validation Loss: 0.0033225572636623063\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006096230803523213\n",
      "Training Loss: 0.006599452141672373\n",
      "Training Loss: 0.00597622157074511\n",
      "Validation Loss: 0.0033159347949549556\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006088964013615623\n",
      "Training Loss: 0.006592619448783807\n",
      "Training Loss: 0.005969532353337854\n",
      "Validation Loss: 0.0033093731238224198\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006081770659657195\n",
      "Training Loss: 0.006585845910012722\n",
      "Training Loss: 0.00596289822133258\n",
      "Validation Loss: 0.0033028742711823642\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006074651021044701\n",
      "Training Loss: 0.006579128149314784\n",
      "Training Loss: 0.005956317886011675\n",
      "Validation Loss: 0.0032964370558770857\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006067600635578856\n",
      "Training Loss: 0.006572467225487344\n",
      "Training Loss: 0.0059497902705334125\n",
      "Validation Loss: 0.003290063920012267\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006060620668577031\n",
      "Training Loss: 0.006565860721166245\n",
      "Training Loss: 0.005943312614690512\n",
      "Validation Loss: 0.0032837442593293244\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006053706160746515\n",
      "Training Loss: 0.006559305507689715\n",
      "Training Loss: 0.005936884023249149\n",
      "Validation Loss: 0.0032774903092093848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006046858084155247\n",
      "Training Loss: 0.006552801338839345\n",
      "Training Loss: 0.005930503150448203\n",
      "Validation Loss: 0.003271295261318178\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006040074118645862\n",
      "Training Loss: 0.006546346587711014\n",
      "Training Loss: 0.005924168864148669\n",
      "Validation Loss: 0.0032651618023643667\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.00603335154009983\n",
      "Training Loss: 0.006539940062211826\n",
      "Training Loss: 0.005917877399479039\n",
      "Validation Loss: 0.0032590830115224708\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006026688114507124\n",
      "Training Loss: 0.006533579030656256\n",
      "Training Loss: 0.005911630219197832\n",
      "Validation Loss: 0.0032530657782250745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006020084281917661\n",
      "Training Loss: 0.006527262661256828\n",
      "Training Loss: 0.00590542487683706\n",
      "Validation Loss: 0.003247107165928386\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.00601353713660501\n",
      "Training Loss: 0.0065209900354966524\n",
      "Training Loss: 0.005899260346195661\n",
      "Validation Loss: 0.0032412096088898653\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006007045493461192\n",
      "Training Loss: 0.006514760070713237\n",
      "Training Loss: 0.0058931332960492\n",
      "Validation Loss: 0.0032353654838679882\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006000606392044574\n",
      "Training Loss: 0.006508570135338232\n",
      "Training Loss: 0.005887044245610013\n",
      "Validation Loss: 0.003229574996224615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.00599421800347045\n",
      "Training Loss: 0.006502418088493869\n",
      "Training Loss: 0.005880989563302137\n",
      "Validation Loss: 0.003223840093627321\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005987880389438942\n",
      "Training Loss: 0.006496303539024666\n",
      "Training Loss: 0.0058749694796279076\n",
      "Validation Loss: 0.003218160538013313\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005981589867733419\n",
      "Training Loss: 0.006490225815214217\n",
      "Training Loss: 0.005868982864776626\n",
      "Validation Loss: 0.003212531995854937\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005975346476770937\n",
      "Training Loss: 0.006484181886189617\n",
      "Training Loss: 0.005863026656443253\n",
      "Validation Loss: 0.003206956825543488\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005969148501753807\n",
      "Training Loss: 0.006478171069175005\n",
      "Training Loss: 0.005857101008296013\n",
      "Validation Loss: 0.003201432632062626\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005962993315188214\n",
      "Training Loss: 0.006472192454966716\n",
      "Training Loss: 0.0058512033568695184\n",
      "Validation Loss: 0.003195959714952898\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.0059568813117221\n",
      "Training Loss: 0.00646624434797559\n",
      "Training Loss: 0.005845335621852427\n",
      "Validation Loss: 0.0031905376870352566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.005950809608330019\n",
      "Training Loss: 0.006460326667292975\n",
      "Training Loss: 0.005839492022641934\n",
      "Validation Loss: 0.0031851647170574476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005944777324330062\n",
      "Training Loss: 0.0064544377796119075\n",
      "Training Loss: 0.005833673511515372\n",
      "Validation Loss: 0.0031798379456498818\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005938783019664697\n",
      "Training Loss: 0.006448575520189479\n",
      "Training Loss: 0.005827880486613139\n",
      "Validation Loss: 0.0031745623237338294\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005932826062198728\n",
      "Training Loss: 0.006442740677739494\n",
      "Training Loss: 0.005822107946733013\n",
      "Validation Loss: 0.003169330903015026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005926904260995798\n",
      "Training Loss: 0.006436930111376569\n",
      "Training Loss: 0.005816358961747028\n",
      "Validation Loss: 0.003164142447398201\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005921014574123546\n",
      "Training Loss: 0.006431142872897908\n",
      "Training Loss: 0.005810628673643805\n",
      "Validation Loss: 0.003158996096194795\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005915159072028473\n",
      "Training Loss: 0.006425378586864099\n",
      "Training Loss: 0.00580491877743043\n",
      "Validation Loss: 0.003153895016163169\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005909335276228376\n",
      "Training Loss: 0.006419636039063334\n",
      "Training Loss: 0.005799225984956138\n",
      "Validation Loss: 0.003148838991477165\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.0059035415755352\n",
      "Training Loss: 0.00641391534882132\n",
      "Training Loss: 0.005793550999369472\n",
      "Validation Loss: 0.003143819025456068\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.00589777659624815\n",
      "Training Loss: 0.006408214117982425\n",
      "Training Loss: 0.005787892208900303\n",
      "Validation Loss: 0.0031388400690211507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005892040915205144\n",
      "Training Loss: 0.006402531606145203\n",
      "Training Loss: 0.005782248448813334\n",
      "Validation Loss: 0.003133899993995686\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005886332287918777\n",
      "Training Loss: 0.006396866476279683\n",
      "Training Loss: 0.0057766186585649845\n",
      "Validation Loss: 0.0031289979697759736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005880649295286275\n",
      "Training Loss: 0.006391218744101934\n",
      "Training Loss: 0.00577100274444092\n",
      "Validation Loss: 0.0031241367968699234\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.00587499237852171\n",
      "Training Loss: 0.006385585563257337\n",
      "Training Loss: 0.005765397545765154\n",
      "Validation Loss: 0.003119306059376326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005869357337942347\n",
      "Training Loss: 0.0063799694395856935\n",
      "Training Loss: 0.005759804343688302\n",
      "Validation Loss: 0.003114510258251613\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005863747963448986\n",
      "Training Loss: 0.006374366524978541\n",
      "Training Loss: 0.005754221093957313\n",
      "Validation Loss: 0.003109749335942141\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0058581590256653725\n",
      "Training Loss: 0.0063687778275925665\n",
      "Training Loss: 0.0057486463279929016\n",
      "Validation Loss: 0.0031050196273273295\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.00585259128769394\n",
      "Training Loss: 0.006363201051135548\n",
      "Training Loss: 0.005743081264663488\n",
      "Validation Loss: 0.0031003230460657834\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.00584704534325283\n",
      "Training Loss: 0.0063576350762741644\n",
      "Training Loss: 0.005737523350398987\n",
      "Validation Loss: 0.003095657242922468\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005841519182431512\n",
      "Training Loss: 0.006352082443772816\n",
      "Training Loss: 0.005731972351204604\n",
      "Validation Loss: 0.003091019591358438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.0058360096096294\n",
      "Training Loss: 0.006346538168145344\n",
      "Training Loss: 0.005726427308400161\n",
      "Validation Loss: 0.003086411816674923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005830518398433924\n",
      "Training Loss: 0.006341003822162747\n",
      "Training Loss: 0.005720886259805411\n",
      "Validation Loss: 0.00308182584566556\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005825043269433081\n",
      "Training Loss: 0.00633547811768949\n",
      "Training Loss: 0.005715348875964992\n",
      "Validation Loss: 0.0030772672924265433\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.0058195851143682375\n",
      "Training Loss: 0.006329959222348407\n",
      "Training Loss: 0.0057098148268414665\n",
      "Validation Loss: 0.003072731701782748\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005814139886642806\n",
      "Training Loss: 0.006324445544742048\n",
      "Training Loss: 0.005704282104270533\n",
      "Validation Loss: 0.0030682218677018968\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005808709304546938\n",
      "Training Loss: 0.006318939015036449\n",
      "Training Loss: 0.00569875072396826\n",
      "Validation Loss: 0.0030637351845736417\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005803292716154829\n",
      "Training Loss: 0.006313437432399951\n",
      "Training Loss: 0.005693220974062569\n",
      "Validation Loss: 0.003059268133777581\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005797888179076835\n",
      "Training Loss: 0.006307939573889598\n",
      "Training Loss: 0.00568768848257605\n",
      "Validation Loss: 0.003054822458927467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005792494885390624\n",
      "Training Loss: 0.006302445698529482\n",
      "Training Loss: 0.005682155919494107\n",
      "Validation Loss: 0.003050390895184004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005787111427052878\n",
      "Training Loss: 0.006296955597936176\n",
      "Training Loss: 0.005676618567667902\n",
      "Validation Loss: 0.0030459831625725448\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005781737150391564\n",
      "Training Loss: 0.0062914661382092165\n",
      "Training Loss: 0.005671078859595582\n",
      "Validation Loss: 0.003041585419442128\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005776371517567895\n",
      "Training Loss: 0.006285975479404442\n",
      "Training Loss: 0.005665534752188251\n",
      "Validation Loss: 0.0030372050980608284\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005771014063502662\n",
      "Training Loss: 0.006280487022013404\n",
      "Training Loss: 0.005659984686644748\n",
      "Validation Loss: 0.0030328376595391317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005765662506455556\n",
      "Training Loss: 0.006274996778229252\n",
      "Training Loss: 0.005654428207781166\n",
      "Validation Loss: 0.003028480987472648\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005760316523956135\n",
      "Training Loss: 0.006269504586816766\n",
      "Training Loss: 0.005648863415117376\n",
      "Validation Loss: 0.0030241373146810894\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.0057549753109924495\n",
      "Training Loss: 0.00626400871318765\n",
      "Training Loss: 0.00564329034765251\n",
      "Validation Loss: 0.0030198015201983327\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005749637057888322\n",
      "Training Loss: 0.006258509624749422\n",
      "Training Loss: 0.005637707086862065\n",
      "Validation Loss: 0.00301547763539541\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.0057443012914154675\n",
      "Training Loss: 0.0062530056620016695\n",
      "Training Loss: 0.005632113119936548\n",
      "Validation Loss: 0.0030111604804445185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005738967429497279\n",
      "Training Loss: 0.006247495642746799\n",
      "Training Loss: 0.005626506305998191\n",
      "Validation Loss: 0.003006848934357672\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.0057336340617621315\n",
      "Training Loss: 0.006241978862672113\n",
      "Training Loss: 0.005620886422693729\n",
      "Validation Loss: 0.0030025442253819173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005728300088667311\n",
      "Training Loss: 0.00623645416519139\n",
      "Training Loss: 0.005615252298302949\n",
      "Validation Loss: 0.002998240715876389\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005722963477601297\n",
      "Training Loss: 0.006230921063688583\n",
      "Training Loss: 0.005609602392651141\n",
      "Validation Loss: 0.0029939373188620703\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.0057176241528941315\n",
      "Training Loss: 0.006225375508074648\n",
      "Training Loss: 0.005603934397804551\n",
      "Validation Loss: 0.0029896382828441897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005712279843864962\n",
      "Training Loss: 0.006219820993719622\n",
      "Training Loss: 0.005598248757887631\n",
      "Validation Loss: 0.002985335426049286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.00570692952722311\n",
      "Training Loss: 0.006214252981590107\n",
      "Training Loss: 0.005592541852383875\n",
      "Validation Loss: 0.0029810293907236852\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005701572854304686\n",
      "Training Loss: 0.006208670768537559\n",
      "Training Loss: 0.005586814962443895\n",
      "Validation Loss: 0.0029767214338912557\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.0056962070713052525\n",
      "Training Loss: 0.006203074046643451\n",
      "Training Loss: 0.005581063422141597\n",
      "Validation Loss: 0.0029724062710812086\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005690830772509798\n",
      "Training Loss: 0.006197461663396098\n",
      "Training Loss: 0.005575287916581146\n",
      "Validation Loss: 0.002968081383013658\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005685444626142271\n",
      "Training Loss: 0.006191831430769525\n",
      "Training Loss: 0.005569486226886511\n",
      "Validation Loss: 0.0029637492785041923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005680044720647857\n",
      "Training Loss: 0.006186182434903458\n",
      "Training Loss: 0.005563656055601314\n",
      "Validation Loss: 0.0029594042126528837\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.00567462949315086\n",
      "Training Loss: 0.006180511033162475\n",
      "Training Loss: 0.005557796600041911\n",
      "Validation Loss: 0.002955051479086782\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005669199944823049\n",
      "Training Loss: 0.006174818924628198\n",
      "Training Loss: 0.005551905544707552\n",
      "Validation Loss: 0.0029506827482242096\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [06:52<27:31, 206.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Loss: 0.21534262388944625\n",
      "Training Loss: 0.15618117995560168\n",
      "Training Loss: 0.12399917986243963\n",
      "Validation Loss: 0.09104538252681829\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.09055724080651999\n",
      "Training Loss: 0.0778324843198061\n",
      "Training Loss: 0.07601877517998218\n",
      "Validation Loss: 0.0702668217460761\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.07334675639867783\n",
      "Training Loss: 0.07052981097251176\n",
      "Training Loss: 0.07012496864423155\n",
      "Validation Loss: 0.06556729260790214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.06803937789052725\n",
      "Training Loss: 0.06543364400044084\n",
      "Training Loss: 0.06479970060288906\n",
      "Validation Loss: 0.0605328766984886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.06260743752121925\n",
      "Training Loss: 0.06002537418156862\n",
      "Training Loss: 0.05908834304660559\n",
      "Validation Loss: 0.055008190443341654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.05664312781766057\n",
      "Training Loss: 0.05398425778374076\n",
      "Training Loss: 0.05264033717103302\n",
      "Validation Loss: 0.04867808516608196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.04982127036899328\n",
      "Training Loss: 0.04702402092516422\n",
      "Training Loss: 0.04522298023104668\n",
      "Validation Loss: 0.041374652970875246\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.04205105872824788\n",
      "Training Loss: 0.03919021902605891\n",
      "Training Loss: 0.037036863351240755\n",
      "Validation Loss: 0.033423376363817224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.03384278640151024\n",
      "Training Loss: 0.031228510476648808\n",
      "Training Loss: 0.029049290674738586\n",
      "Validation Loss: 0.025917994863029276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.026440665400587024\n",
      "Training Loss: 0.024476641677320003\n",
      "Training Loss: 0.022620314387604593\n",
      "Validation Loss: 0.02012830255867055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.021004233779385685\n",
      "Training Loss: 0.01984814094379544\n",
      "Training Loss: 0.018412169353105128\n",
      "Validation Loss: 0.016474909295610497\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.017721764033194632\n",
      "Training Loss: 0.01723537643905729\n",
      "Training Loss: 0.016105129746720195\n",
      "Validation Loss: 0.014501144826998201\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.016007640841417016\n",
      "Training Loss: 0.01593988518929109\n",
      "Training Loss: 0.014957283562980593\n",
      "Validation Loss: 0.01349024556765563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.015140630120877177\n",
      "Training Loss: 0.01529058939544484\n",
      "Training Loss: 0.014358579951804132\n",
      "Validation Loss: 0.012926794290333317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.014649286759085953\n",
      "Training Loss: 0.01490720764733851\n",
      "Training Loss: 0.013990746417548508\n",
      "Validation Loss: 0.012560055209219122\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.014319549405481666\n",
      "Training Loss: 0.014635937579441816\n",
      "Training Loss: 0.013727419435745104\n",
      "Validation Loss: 0.012289681367241265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.014070358311291784\n",
      "Training Loss: 0.014423320272471755\n",
      "Training Loss: 0.013521610042080284\n",
      "Validation Loss: 0.012074921866146367\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.013869537287391723\n",
      "Training Loss: 0.014248245240887627\n",
      "Training Loss: 0.013352640735683962\n",
      "Validation Loss: 0.011896049263730142\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.013701101366896182\n",
      "Training Loss: 0.014099261943483725\n",
      "Training Loss: 0.013208806841867045\n",
      "Validation Loss: 0.011741386449194691\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.013555152241606265\n",
      "Training Loss: 0.013968522561481223\n",
      "Training Loss: 0.013082155747106299\n",
      "Validation Loss: 0.01160281847932198\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.013424558907281607\n",
      "Training Loss: 0.013849916273029521\n",
      "Training Loss: 0.012966549936681986\n",
      "Validation Loss: 0.011473871401270454\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.013303517745807767\n",
      "Training Loss: 0.013738127565011382\n",
      "Training Loss: 0.01285659400629811\n",
      "Validation Loss: 0.0113484987392603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.013186575539875775\n",
      "Training Loss: 0.01362779301358387\n",
      "Training Loss: 0.012746691708453\n",
      "Validation Loss: 0.011219854885189052\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.013067590140271932\n",
      "Training Loss: 0.013512344150803983\n",
      "Training Loss: 0.012629674029303714\n",
      "Validation Loss: 0.011078158011555338\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.012937885781284422\n",
      "Training Loss: 0.013381609501084312\n",
      "Training Loss: 0.012493855417706073\n",
      "Validation Loss: 0.01090556654681483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.012781897038221359\n",
      "Training Loss: 0.013215993277262895\n",
      "Training Loss: 0.012316345579456538\n",
      "Validation Loss: 0.01066574068698153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.012569722065236419\n",
      "Training Loss: 0.01298042920534499\n",
      "Training Loss: 0.012061278888722882\n",
      "Validation Loss: 0.01031133379483742\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.012266405015252531\n",
      "Training Loss: 0.012644850594224408\n",
      "Training Loss: 0.011706212519202381\n",
      "Validation Loss: 0.009826539018913434\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.011862489215563982\n",
      "Training Loss: 0.012213784797349945\n",
      "Training Loss: 0.01126596940215677\n",
      "Validation Loss: 0.009245266996617063\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.011387038377579302\n",
      "Training Loss: 0.011730229130480439\n",
      "Training Loss: 0.010791597419884056\n",
      "Validation Loss: 0.008642544117076009\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.01089969528489746\n",
      "Training Loss: 0.011258783711818978\n",
      "Training Loss: 0.010345419094664975\n",
      "Validation Loss: 0.008096692050294427\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.010456027892651036\n",
      "Training Loss: 0.010844760563923046\n",
      "Training Loss: 0.009958312851376832\n",
      "Validation Loss: 0.007637179307534956\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.01007084144745022\n",
      "Training Loss: 0.010488551291637123\n",
      "Training Loss: 0.009620282988762482\n",
      "Validation Loss: 0.007245518761592802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.009728402846958488\n",
      "Training Loss: 0.010169346840120852\n",
      "Training Loss: 0.009311390315415337\n",
      "Validation Loss: 0.006895088059618399\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.009412153469165787\n",
      "Training Loss: 0.00987198343151249\n",
      "Training Loss: 0.0090206975012552\n",
      "Validation Loss: 0.006570465601143542\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.009114428989123553\n",
      "Training Loss: 0.009591144596925005\n",
      "Training Loss: 0.008745849482947961\n",
      "Validation Loss: 0.006266772592691391\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008834393195575102\n",
      "Training Loss: 0.009327760849846527\n",
      "Training Loss: 0.008489268999546766\n",
      "Validation Loss: 0.005985478310730685\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008575112002436071\n",
      "Training Loss: 0.009085896473843605\n",
      "Training Loss: 0.008255378374597057\n",
      "Validation Loss: 0.0057308431113076005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008341130808694287\n",
      "Training Loss: 0.008870294716907666\n",
      "Training Loss: 0.008048418626422062\n",
      "Validation Loss: 0.0055070048920140505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008136337099131197\n",
      "Training Loss: 0.00868433466530405\n",
      "Training Loss: 0.007870814538327976\n",
      "Validation Loss: 0.005316029935865925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.00796247051563114\n",
      "Training Loss: 0.008528872124152258\n",
      "Training Loss: 0.007722511108731851\n",
      "Validation Loss: 0.005157291140993324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007818733950844035\n",
      "Training Loss: 0.008402225468307734\n",
      "Training Loss: 0.007601307642180473\n",
      "Validation Loss: 0.005027974241549212\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007702331475447863\n",
      "Training Loss: 0.00830097373109311\n",
      "Training Loss: 0.007503718746593222\n",
      "Validation Loss: 0.004924053778830036\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007609374988824129\n",
      "Training Loss: 0.008220939774764702\n",
      "Training Loss: 0.007425818538758904\n",
      "Validation Loss: 0.0048411795398576206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007535712834214792\n",
      "Training Loss: 0.008157961732940748\n",
      "Training Loss: 0.007363838064484298\n",
      "Validation Loss: 0.004775243502612529\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007477463113609702\n",
      "Training Loss: 0.008108334018616006\n",
      "Training Loss: 0.007314459850313142\n",
      "Validation Loss: 0.004722663125525532\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007431269293883816\n",
      "Training Loss: 0.00806896636262536\n",
      "Training Loss: 0.00727491513825953\n",
      "Validation Loss: 0.004680446214356533\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.00739437086158432\n",
      "Training Loss: 0.008037392272381112\n",
      "Training Loss: 0.007242973844986409\n",
      "Validation Loss: 0.004646172297051113\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007364570426288992\n",
      "Training Loss: 0.008011693473672494\n",
      "Training Loss: 0.007216876352904364\n",
      "Validation Loss: 0.004617928129652243\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0073401567211840305\n",
      "Training Loss: 0.007990411256905645\n",
      "Training Loss: 0.007195255649276078\n",
      "Validation Loss: 0.004594220641184222\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007319820226402953\n",
      "Training Loss: 0.007972443762701004\n",
      "Training Loss: 0.007177060532849282\n",
      "Validation Loss: 0.004573901365720405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007302562586264685\n",
      "Training Loss: 0.007956973043037578\n",
      "Training Loss: 0.007161489413119853\n",
      "Validation Loss: 0.004556095791363231\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007287637061672285\n",
      "Training Loss: 0.007943389536812902\n",
      "Training Loss: 0.007147932717343792\n",
      "Validation Loss: 0.004540148752302015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007274484098888934\n",
      "Training Loss: 0.007931245788931847\n",
      "Training Loss: 0.007135930606164038\n",
      "Validation Loss: 0.00452556873127567\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007262685460736975\n",
      "Training Loss: 0.007920208153082057\n",
      "Training Loss: 0.00712513352627866\n",
      "Validation Loss: 0.0045119884654126145\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007251928033074364\n",
      "Training Loss: 0.007910031590145082\n",
      "Training Loss: 0.0071152757736854255\n",
      "Validation Loss: 0.004499138073911995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0072419781540520485\n",
      "Training Loss: 0.007900532339699567\n",
      "Training Loss: 0.007106153885833919\n",
      "Validation Loss: 0.004486816006534723\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007232660399749875\n",
      "Training Loss: 0.007891572894295677\n",
      "Training Loss: 0.007097615367965773\n",
      "Validation Loss: 0.004474875863819393\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007223842348903418\n",
      "Training Loss: 0.007883050187956542\n",
      "Training Loss: 0.007089540731394664\n",
      "Validation Loss: 0.004463210769960385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007215425006579608\n",
      "Training Loss: 0.007874884910415858\n",
      "Training Loss: 0.007081838472513482\n",
      "Validation Loss: 0.004451738650847771\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007207329333759845\n",
      "Training Loss: 0.007867016479140147\n",
      "Training Loss: 0.007074437270639464\n",
      "Validation Loss: 0.004440403153617563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007199498577974737\n",
      "Training Loss: 0.007859397858846933\n",
      "Training Loss: 0.007067280805204064\n",
      "Validation Loss: 0.004429162510021935\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007191887546796351\n",
      "Training Loss: 0.007851989184273407\n",
      "Training Loss: 0.0070603270770516246\n",
      "Validation Loss: 0.004417985952406955\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007184461137512699\n",
      "Training Loss: 0.00784476520260796\n",
      "Training Loss: 0.007053539787884802\n",
      "Validation Loss: 0.004406851985795277\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007177191216032952\n",
      "Training Loss: 0.00783769963425584\n",
      "Training Loss: 0.007046893204096705\n",
      "Validation Loss: 0.004395745754807016\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007170056066242978\n",
      "Training Loss: 0.007830774805042892\n",
      "Training Loss: 0.007040364594431594\n",
      "Validation Loss: 0.004384659917428671\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.00716304119094275\n",
      "Training Loss: 0.007823974089697004\n",
      "Training Loss: 0.0070339370588772\n",
      "Validation Loss: 0.004373586805945451\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007156130049843341\n",
      "Training Loss: 0.007817286351928488\n",
      "Training Loss: 0.007027596585685387\n",
      "Validation Loss: 0.004362522552490988\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007149314325070009\n",
      "Training Loss: 0.007810699969995767\n",
      "Training Loss: 0.007021333132870495\n",
      "Validation Loss: 0.0043514711034138885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007142585610272363\n",
      "Training Loss: 0.007804209776222706\n",
      "Training Loss: 0.0070151374000124635\n",
      "Validation Loss: 0.0043404302952204194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007135936887934804\n",
      "Training Loss: 0.007797804351430387\n",
      "Training Loss: 0.007009002707200125\n",
      "Validation Loss: 0.004329406787128596\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007129364401334897\n",
      "Training Loss: 0.007791482347529382\n",
      "Training Loss: 0.007002924156840891\n",
      "Validation Loss: 0.004318402619676643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007122863683616742\n",
      "Training Loss: 0.007785237793577835\n",
      "Training Loss: 0.006996897818753496\n",
      "Validation Loss: 0.004307424036126709\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007116432508919388\n",
      "Training Loss: 0.007779067159863189\n",
      "Training Loss: 0.006990920469397679\n",
      "Validation Loss: 0.004296478216295664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.00711006872355938\n",
      "Training Loss: 0.007772967938799411\n",
      "Training Loss: 0.006984989877091721\n",
      "Validation Loss: 0.004285573509385746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00710377249866724\n",
      "Training Loss: 0.007766938676359132\n",
      "Training Loss: 0.006979106840444729\n",
      "Validation Loss: 0.00427471584211407\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007097541640978307\n",
      "Training Loss: 0.007760977860307321\n",
      "Training Loss: 0.006973268109140918\n",
      "Validation Loss: 0.004263914470152741\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007091376724420115\n",
      "Training Loss: 0.00775508489808999\n",
      "Training Loss: 0.0069674750429112465\n",
      "Validation Loss: 0.004253177398690179\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.0070852777676191185\n",
      "Training Loss: 0.007749258492840454\n",
      "Training Loss: 0.006961726350709796\n",
      "Validation Loss: 0.004242515734271303\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007079245919594541\n",
      "Training Loss: 0.007743498722556978\n",
      "Training Loss: 0.0069560239522252236\n",
      "Validation Loss: 0.004231936038011329\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007073280545882881\n",
      "Training Loss: 0.007737805697834119\n",
      "Training Loss: 0.006950368279358372\n",
      "Validation Loss: 0.004221450377862607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.00706738407141529\n",
      "Training Loss: 0.007732180043822154\n",
      "Training Loss: 0.006944759646430611\n",
      "Validation Loss: 0.004211066030436771\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007061556138796732\n",
      "Training Loss: 0.007726621121400967\n",
      "Training Loss: 0.0069391985435504466\n",
      "Validation Loss: 0.004200792673901979\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007055798311484978\n",
      "Training Loss: 0.007721129875862971\n",
      "Training Loss: 0.006933687851997093\n",
      "Validation Loss: 0.004190642355281985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007050111750140786\n",
      "Training Loss: 0.007715707400348037\n",
      "Training Loss: 0.00692822540528141\n",
      "Validation Loss: 0.004180619793975454\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007044495915761217\n",
      "Training Loss: 0.007710353470174596\n",
      "Training Loss: 0.006922814571298659\n",
      "Validation Loss: 0.004170737367939581\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.007038954026065767\n",
      "Training Loss: 0.007705067259958014\n",
      "Training Loss: 0.006917455977527425\n",
      "Validation Loss: 0.004161003527095478\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.00703348619164899\n",
      "Training Loss: 0.007699851739453152\n",
      "Training Loss: 0.006912149235140532\n",
      "Validation Loss: 0.00415142542117516\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007028091402025894\n",
      "Training Loss: 0.0076947059016674755\n",
      "Training Loss: 0.006906895787687972\n",
      "Validation Loss: 0.004142011098532278\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007022771979682148\n",
      "Training Loss: 0.0076896293554455045\n",
      "Training Loss: 0.006901695834239945\n",
      "Validation Loss: 0.004132768871744027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007017528145806864\n",
      "Training Loss: 0.007684622313827276\n",
      "Training Loss: 0.006896549792727456\n",
      "Validation Loss: 0.004123706094596242\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0070123595232144\n",
      "Training Loss: 0.007679685590555891\n",
      "Training Loss: 0.006891457480378449\n",
      "Validation Loss: 0.004114829380525632\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007007266156724654\n",
      "Training Loss: 0.0076748184027383105\n",
      "Training Loss: 0.00688641894957982\n",
      "Validation Loss: 0.00410614229131783\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007002247964264825\n",
      "Training Loss: 0.007670019999495708\n",
      "Training Loss: 0.006881433791713789\n",
      "Validation Loss: 0.0040976531644027385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006997305416734889\n",
      "Training Loss: 0.007665288660209626\n",
      "Training Loss: 0.00687650210922584\n",
      "Validation Loss: 0.004089366389553617\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006992437372100539\n",
      "Training Loss: 0.007660625673015602\n",
      "Training Loss: 0.006871622276958078\n",
      "Validation Loss: 0.0040812854958551656\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.0069876431190641596\n",
      "Training Loss: 0.007656028603669256\n",
      "Training Loss: 0.0068667942029424014\n",
      "Validation Loss: 0.0040734152053221225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006982922503957525\n",
      "Training Loss: 0.007651496734470129\n",
      "Training Loss: 0.006862015820806846\n",
      "Validation Loss: 0.0040657578214082155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006978273140848614\n",
      "Training Loss: 0.007647026920458302\n",
      "Training Loss: 0.006857287914026528\n",
      "Validation Loss: 0.004058315664571658\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006973695101914927\n",
      "Training Loss: 0.007642619913676754\n",
      "Training Loss: 0.006852606098400429\n",
      "Validation Loss: 0.00405109159810615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0069691862165927885\n",
      "Training Loss: 0.007638271602336317\n",
      "Training Loss: 0.006847970873350278\n",
      "Validation Loss: 0.004044084559885387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006964746342273429\n",
      "Training Loss: 0.007633980881073512\n",
      "Training Loss: 0.006843380178324878\n",
      "Validation Loss: 0.00403729687559973\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006960371345048771\n",
      "Training Loss: 0.007629746466409415\n",
      "Training Loss: 0.0068388311529997735\n",
      "Validation Loss: 0.004030729569442403\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006956061110831797\n",
      "Training Loss: 0.007625564346089959\n",
      "Training Loss: 0.006834322570357472\n",
      "Validation Loss: 0.004024377618550083\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006951813049963676\n",
      "Training Loss: 0.007621434396714904\n",
      "Training Loss: 0.006829851402435452\n",
      "Validation Loss: 0.0040182418234416106\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.00694762502796948\n",
      "Training Loss: 0.00761735257692635\n",
      "Training Loss: 0.0068254153954330835\n",
      "Validation Loss: 0.0040123200297795154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006943494770675898\n",
      "Training Loss: 0.0076133151195244865\n",
      "Training Loss: 0.006821013307198882\n",
      "Validation Loss: 0.004006609621489149\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0069394211890175935\n",
      "Training Loss: 0.007609320267802104\n",
      "Training Loss: 0.00681664134375751\n",
      "Validation Loss: 0.00400110662475312\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.00693540018692147\n",
      "Training Loss: 0.007605365278432146\n",
      "Training Loss: 0.006812298126751557\n",
      "Validation Loss: 0.003995809467310567\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006931430825497955\n",
      "Training Loss: 0.007601448258501477\n",
      "Training Loss: 0.006807980053126812\n",
      "Validation Loss: 0.003990712512828661\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006927509751403704\n",
      "Training Loss: 0.007597564681200311\n",
      "Training Loss: 0.006803686290513724\n",
      "Validation Loss: 0.00398581185158384\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006923635554267093\n",
      "Training Loss: 0.007593712341040373\n",
      "Training Loss: 0.006799412936670706\n",
      "Validation Loss: 0.0039811001232500825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.0069198050245177\n",
      "Training Loss: 0.007589889380615205\n",
      "Training Loss: 0.006795158772729337\n",
      "Validation Loss: 0.003976575637843167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006916016233153641\n",
      "Training Loss: 0.0075860924215521664\n",
      "Training Loss: 0.0067909209942445155\n",
      "Validation Loss: 0.00397223033654514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.0069122665317263455\n",
      "Training Loss: 0.007582318271161057\n",
      "Training Loss: 0.006786696919007227\n",
      "Validation Loss: 0.003968059407086687\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006908553892280907\n",
      "Training Loss: 0.007578564627328888\n",
      "Training Loss: 0.006782484592404217\n",
      "Validation Loss: 0.003964056041133538\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006904875342734158\n",
      "Training Loss: 0.007574829926015809\n",
      "Training Loss: 0.006778281696606428\n",
      "Validation Loss: 0.003960213340096845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006901229760260321\n",
      "Training Loss: 0.007571110224816948\n",
      "Training Loss: 0.006774087562225759\n",
      "Validation Loss: 0.003956526589810095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006897614388726651\n",
      "Training Loss: 0.0075674047821667045\n",
      "Training Loss: 0.006769898254424334\n",
      "Validation Loss: 0.0039529882533622255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006894026773516089\n",
      "Training Loss: 0.007563709625392221\n",
      "Training Loss: 0.006765713130589574\n",
      "Validation Loss: 0.003949590806922551\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006890466283075511\n",
      "Training Loss: 0.0075600245269015435\n",
      "Training Loss: 0.006761531184893101\n",
      "Validation Loss: 0.00394632840487227\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.00688692950992845\n",
      "Training Loss: 0.007556346144992858\n",
      "Training Loss: 0.006757349965628236\n",
      "Validation Loss: 0.003943192639147465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006883415342308581\n",
      "Training Loss: 0.007552673596073874\n",
      "Training Loss: 0.006753167802235112\n",
      "Validation Loss: 0.003940178743260128\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006879921699874103\n",
      "Training Loss: 0.0075490054138936105\n",
      "Training Loss: 0.006748984338482842\n",
      "Validation Loss: 0.00393727942359414\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006876447310787626\n",
      "Training Loss: 0.007545340222422965\n",
      "Training Loss: 0.006744798318250105\n",
      "Validation Loss: 0.0039344870346713435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0068729907600209115\n",
      "Training Loss: 0.0075416752125602214\n",
      "Training Loss: 0.00674060893128626\n",
      "Validation Loss: 0.003931795526819115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0068695507827214895\n",
      "Training Loss: 0.007538010701537133\n",
      "Training Loss: 0.006736413940088824\n",
      "Validation Loss: 0.003929196761429226\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.00686612463730853\n",
      "Training Loss: 0.007534346671891398\n",
      "Training Loss: 0.0067322142119519415\n",
      "Validation Loss: 0.003926687406930612\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006862712500733324\n",
      "Training Loss: 0.00753068050427828\n",
      "Training Loss: 0.006728009012294933\n",
      "Validation Loss: 0.00392425624690322\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006859312548767775\n",
      "Training Loss: 0.007527012389036827\n",
      "Training Loss: 0.0067237975844182075\n",
      "Validation Loss: 0.003921900675736703\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.00685592491645366\n",
      "Training Loss: 0.007523340938496404\n",
      "Training Loss: 0.006719579092459753\n",
      "Validation Loss: 0.003919612620224695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.0068525465717539195\n",
      "Training Loss: 0.0075196681404486295\n",
      "Training Loss: 0.006715354394400492\n",
      "Validation Loss: 0.003917390481945587\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006849178150878288\n",
      "Training Loss: 0.007515992099652067\n",
      "Training Loss: 0.006711123408749699\n",
      "Validation Loss: 0.0039152216658079894\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006845819933805615\n",
      "Training Loss: 0.007512314092018642\n",
      "Training Loss: 0.006706886760657654\n",
      "Validation Loss: 0.003913104110345077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006842469949042425\n",
      "Training Loss: 0.007508632985409349\n",
      "Training Loss: 0.006702645267359912\n",
      "Validation Loss: 0.003911035727929282\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.0068391272891312836\n",
      "Training Loss: 0.007504950996953994\n",
      "Training Loss: 0.006698397137224674\n",
      "Validation Loss: 0.0039090067619102055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006835792283527553\n",
      "Training Loss: 0.0075012663716916\n",
      "Training Loss: 0.006694145388901234\n",
      "Validation Loss: 0.003907012498121332\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006832464574836194\n",
      "Training Loss: 0.0074975819350220265\n",
      "Training Loss: 0.006689890305278823\n",
      "Validation Loss: 0.0039050511523997518\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006829144225921482\n",
      "Training Loss: 0.007493899225955829\n",
      "Training Loss: 0.0066856328095309435\n",
      "Validation Loss: 0.0039031176731053195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006825830785091966\n",
      "Training Loss: 0.0074902183149242775\n",
      "Training Loss: 0.006681375036714598\n",
      "Validation Loss: 0.0039012059687082183\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006822524530580267\n",
      "Training Loss: 0.0074865407956531274\n",
      "Training Loss: 0.006677116564242169\n",
      "Validation Loss: 0.00389931348199548\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006819226160878315\n",
      "Training Loss: 0.007482868097722531\n",
      "Training Loss: 0.006672860805410892\n",
      "Validation Loss: 0.003897437496173583\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00681593548739329\n",
      "Training Loss: 0.00747920079738833\n",
      "Training Loss: 0.006668608436593786\n",
      "Validation Loss: 0.003895571197676106\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006812651866930537\n",
      "Training Loss: 0.0074755415972322225\n",
      "Training Loss: 0.006664361888542771\n",
      "Validation Loss: 0.0038937160135717706\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006809376880992204\n",
      "Training Loss: 0.007471894357004203\n",
      "Training Loss: 0.006660123091423884\n",
      "Validation Loss: 0.003891868505030452\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006806111828191206\n",
      "Training Loss: 0.007468257520813495\n",
      "Training Loss: 0.006655894161667675\n",
      "Validation Loss: 0.0038900262521831003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006802856212598272\n",
      "Training Loss: 0.00746463451650925\n",
      "Training Loss: 0.006651677148183808\n",
      "Validation Loss: 0.0038881866742720766\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006799610584275797\n",
      "Training Loss: 0.0074610271368874236\n",
      "Training Loss: 0.006647474571364001\n",
      "Validation Loss: 0.0038863448883940496\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006796376050915569\n",
      "Training Loss: 0.007457438387791626\n",
      "Training Loss: 0.006643288778141141\n",
      "Validation Loss: 0.0038845054229742354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.0067931536049582065\n",
      "Training Loss: 0.007453868638258428\n",
      "Training Loss: 0.006639123113127425\n",
      "Validation Loss: 0.0038826642217888925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006789944304036908\n",
      "Training Loss: 0.007450321881333366\n",
      "Training Loss: 0.006634978324873373\n",
      "Validation Loss: 0.0038808195438403427\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006786749011953361\n",
      "Training Loss: 0.007446797307929956\n",
      "Training Loss: 0.006630858794087544\n",
      "Validation Loss: 0.0038789713603517646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006783567138481886\n",
      "Training Loss: 0.007443297937861644\n",
      "Training Loss: 0.006626765736145898\n",
      "Validation Loss: 0.0038771197877384804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006780400511343032\n",
      "Training Loss: 0.007439825517358258\n",
      "Training Loss: 0.006622701880987733\n",
      "Validation Loss: 0.0038752630823867375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006777249542064965\n",
      "Training Loss: 0.007436381716397591\n",
      "Training Loss: 0.006618669163435698\n",
      "Validation Loss: 0.0038734013410912974\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006774116292363032\n",
      "Training Loss: 0.007432966799824498\n",
      "Training Loss: 0.006614670909475535\n",
      "Validation Loss: 0.003871537007265881\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006771000128937885\n",
      "Training Loss: 0.0074295817536767575\n",
      "Training Loss: 0.006610707317013294\n",
      "Validation Loss: 0.0038696670789648306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006767901303828694\n",
      "Training Loss: 0.007426228590193204\n",
      "Training Loss: 0.006606781113659963\n",
      "Validation Loss: 0.0038677930107256504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006764820575481281\n",
      "Training Loss: 0.007422905787243508\n",
      "Training Loss: 0.0066028943739365785\n",
      "Validation Loss: 0.003865916734519467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006761759428190999\n",
      "Training Loss: 0.007419613428646698\n",
      "Training Loss: 0.00659904773463495\n",
      "Validation Loss: 0.0038640382673507663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006758717206539586\n",
      "Training Loss: 0.007416353437583893\n",
      "Training Loss: 0.0065952436323277654\n",
      "Validation Loss: 0.0038621556537037486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006755694107268937\n",
      "Training Loss: 0.007413125025923364\n",
      "Training Loss: 0.00659148182021454\n",
      "Validation Loss: 0.0038602722068369556\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006752691601868719\n",
      "Training Loss: 0.007409927974804304\n",
      "Training Loss: 0.006587763593997807\n",
      "Validation Loss: 0.0038583873538300395\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006749708788120188\n",
      "Training Loss: 0.007406759583391249\n",
      "Training Loss: 0.006584089718526229\n",
      "Validation Loss: 0.0038565013183573826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006746744929696433\n",
      "Training Loss: 0.007403622469864785\n",
      "Training Loss: 0.00658046044409275\n",
      "Validation Loss: 0.0038546172096237037\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006743800793774426\n",
      "Training Loss: 0.007400514922337607\n",
      "Training Loss: 0.006576875272439792\n",
      "Validation Loss: 0.0038527307307263943\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006740875800605863\n",
      "Training Loss: 0.007397433831356466\n",
      "Training Loss: 0.006573335399152711\n",
      "Validation Loss: 0.003850849482670343\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006737970333779231\n",
      "Training Loss: 0.007394380766199902\n",
      "Training Loss: 0.0065698389383032916\n",
      "Validation Loss: 0.0038489686558023095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006735083900857717\n",
      "Training Loss: 0.007391351571423002\n",
      "Training Loss: 0.006566386623308063\n",
      "Validation Loss: 0.003847089542463171\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006732215519878082\n",
      "Training Loss: 0.007388348748791031\n",
      "Training Loss: 0.006562978532165289\n",
      "Validation Loss: 0.003845214164878629\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006729364522616379\n",
      "Training Loss: 0.0073853676521684975\n",
      "Training Loss: 0.006559611358679831\n",
      "Validation Loss: 0.003843343225464727\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006726530281594023\n",
      "Training Loss: 0.007382410913705826\n",
      "Training Loss: 0.00655628593172878\n",
      "Validation Loss: 0.0038414760479661687\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006723713158280588\n",
      "Training Loss: 0.007379473205073737\n",
      "Training Loss: 0.006553001147694886\n",
      "Validation Loss: 0.0038396147670998666\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006720912242890335\n",
      "Training Loss: 0.007376554753864184\n",
      "Training Loss: 0.006549755854066462\n",
      "Validation Loss: 0.003837756991773593\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006718125428305939\n",
      "Training Loss: 0.007373655891278759\n",
      "Training Loss: 0.006546547833131626\n",
      "Validation Loss: 0.003835904727208564\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006715354131301865\n",
      "Training Loss: 0.007370774436858483\n",
      "Training Loss: 0.0065433762781322005\n",
      "Validation Loss: 0.0038340593115269634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006712595534045249\n",
      "Training Loss: 0.007367908448213711\n",
      "Training Loss: 0.006540240346221253\n",
      "Validation Loss: 0.0038322184844402953\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006709849509061314\n",
      "Training Loss: 0.007365057318238542\n",
      "Training Loss: 0.006537137269042433\n",
      "Validation Loss: 0.0038303829379003037\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006707116468460299\n",
      "Training Loss: 0.007362219562637619\n",
      "Training Loss: 0.006534067409811542\n",
      "Validation Loss: 0.003828555896218992\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006704393762047403\n",
      "Training Loss: 0.007359394272789359\n",
      "Training Loss: 0.006531027837190777\n",
      "Validation Loss: 0.0038267337309008235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006701682304847054\n",
      "Training Loss: 0.007356580165796913\n",
      "Training Loss: 0.0065280180308036504\n",
      "Validation Loss: 0.003824920288883568\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006698979618959129\n",
      "Training Loss: 0.007353777713724412\n",
      "Training Loss: 0.00652503632591106\n",
      "Validation Loss: 0.003823114173183364\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006696285672369413\n",
      "Training Loss: 0.007350985183147713\n",
      "Training Loss: 0.006522080602589995\n",
      "Validation Loss: 0.003821314349142688\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006693599777063355\n",
      "Training Loss: 0.0073482005723053585\n",
      "Training Loss: 0.00651914968737401\n",
      "Validation Loss: 0.0038195212876549764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.00669092066353187\n",
      "Training Loss: 0.007345423696096987\n",
      "Training Loss: 0.006516241597710177\n",
      "Validation Loss: 0.0038177373157186286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.0066882480453932655\n",
      "Training Loss: 0.00734265357546974\n",
      "Training Loss: 0.006513356466311962\n",
      "Validation Loss: 0.003815959584428353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.00668558006582316\n",
      "Training Loss: 0.007339890337898396\n",
      "Training Loss: 0.006510490921791643\n",
      "Validation Loss: 0.003814187026425694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006682916717254557\n",
      "Training Loss: 0.007337131178937853\n",
      "Training Loss: 0.006507644929224625\n",
      "Validation Loss: 0.0038124223362675375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0066802575264591725\n",
      "Training Loss: 0.007334376254002564\n",
      "Training Loss: 0.00650481624645181\n",
      "Validation Loss: 0.0038106634812638835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006677600587136112\n",
      "Training Loss: 0.007331624872749671\n",
      "Training Loss: 0.006502003831556067\n",
      "Validation Loss: 0.0038089153129251653\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006674946843413636\n",
      "Training Loss: 0.0073288763809250665\n",
      "Training Loss: 0.006499207692686468\n",
      "Validation Loss: 0.0038071704853137726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006672293284791522\n",
      "Training Loss: 0.0073261312482645734\n",
      "Training Loss: 0.006496424897341058\n",
      "Validation Loss: 0.0038054344320167482\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006669641229673289\n",
      "Training Loss: 0.007323387683136388\n",
      "Training Loss: 0.006493655191734433\n",
      "Validation Loss: 0.0038037054277328627\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.0066669881693087515\n",
      "Training Loss: 0.007320644308929331\n",
      "Training Loss: 0.006490896677132696\n",
      "Validation Loss: 0.0038019820977148885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006664335281820968\n",
      "Training Loss: 0.007317901748465374\n",
      "Training Loss: 0.006488147836644202\n",
      "Validation Loss: 0.0038002670998946667\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006661681268014945\n",
      "Training Loss: 0.007315157453995198\n",
      "Training Loss: 0.006485409766901285\n",
      "Validation Loss: 0.00379855751080878\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006659024630789645\n",
      "Training Loss: 0.0073124137526610865\n",
      "Training Loss: 0.006482679441105575\n",
      "Validation Loss: 0.003796857885043189\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006656365859089419\n",
      "Training Loss: 0.0073096686019562184\n",
      "Training Loss: 0.00647995694540441\n",
      "Validation Loss: 0.0037951610558613967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006653703711926937\n",
      "Training Loss: 0.007306921810377389\n",
      "Training Loss: 0.0064772403438109905\n",
      "Validation Loss: 0.0037934742213927963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006651037331321277\n",
      "Training Loss: 0.0073041721503250305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [10:18<24:04, 206.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.006474529906408861\n",
      "Validation Loss: 0.003791792006912024\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.41186952494084833\n",
      "Training Loss: 0.31144462786614896\n",
      "Training Loss: 0.23839793153107167\n",
      "Validation Loss: 0.15363935230488188\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.13989964321255685\n",
      "Training Loss: 0.08957063114270568\n",
      "Training Loss: 0.06693267118185758\n",
      "Validation Loss: 0.04958665601155731\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.05237032137811184\n",
      "Training Loss: 0.04906103877350688\n",
      "Training Loss: 0.04787019154988229\n",
      "Validation Loss: 0.04447051219307305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.045586007218807935\n",
      "Training Loss: 0.04339593729935586\n",
      "Training Loss: 0.04175447864457965\n",
      "Validation Loss: 0.038479238057906706\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.039068337865173816\n",
      "Training Loss: 0.036751996036618946\n",
      "Training Loss: 0.03493520325049758\n",
      "Validation Loss: 0.0318687515689081\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.0323258488252759\n",
      "Training Loss: 0.030204552267678084\n",
      "Training Loss: 0.02840193629730493\n",
      "Validation Loss: 0.02560708966901463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.02622365768067539\n",
      "Training Loss: 0.024554572901688516\n",
      "Training Loss: 0.02290058876387775\n",
      "Validation Loss: 0.020468948241532517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.02140125649049878\n",
      "Training Loss: 0.020339755048044025\n",
      "Training Loss: 0.018902689768001437\n",
      "Validation Loss: 0.016823831600335876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.01806165087968111\n",
      "Training Loss: 0.01752143424237147\n",
      "Training Loss: 0.016233615675009787\n",
      "Validation Loss: 0.01435118153942435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.0157978175021708\n",
      "Training Loss: 0.015575166770722716\n",
      "Training Loss: 0.014340615007095039\n",
      "Validation Loss: 0.012500589598346962\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.01407985575031489\n",
      "Training Loss: 0.014026605052640662\n",
      "Training Loss: 0.012812369321472943\n",
      "Validation Loss: 0.01094318399456947\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.012650442260783165\n",
      "Training Loss: 0.01272418454522267\n",
      "Training Loss: 0.011570110754109918\n",
      "Validation Loss: 0.009654152939577451\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.011538661438971759\n",
      "Training Loss: 0.011732609779573976\n",
      "Training Loss: 0.010669001714559272\n",
      "Validation Loss: 0.008678986390089889\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.010764308040961623\n",
      "Training Loss: 0.011038025245070457\n",
      "Training Loss: 0.010043936313595623\n",
      "Validation Loss: 0.007970779922299004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.010225946591235697\n",
      "Training Loss: 0.010541039415402338\n",
      "Training Loss: 0.00959136700606905\n",
      "Validation Loss: 0.007456851380176089\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.009833041955716907\n",
      "Training Loss: 0.010171496277907864\n",
      "Training Loss: 0.009251278139417991\n",
      "Validation Loss: 0.007074157137088896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.009533797731855885\n",
      "Training Loss: 0.009887094176374376\n",
      "Training Loss: 0.00898618261795491\n",
      "Validation Loss: 0.006775688850896412\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.009294972186908126\n",
      "Training Loss: 0.009658742913743482\n",
      "Training Loss: 0.00876985166221857\n",
      "Validation Loss: 0.006532048301645711\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.009094904157100245\n",
      "Training Loss: 0.009467088775709271\n",
      "Training Loss: 0.008585145997349173\n",
      "Validation Loss: 0.006325228375204829\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.008920041889650747\n",
      "Training Loss: 0.009299827401991933\n",
      "Training Loss: 0.008421494126087054\n",
      "Validation Loss: 0.00614367606735715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.00876218524412252\n",
      "Training Loss: 0.009149430691031739\n",
      "Training Loss: 0.008272702560061589\n",
      "Validation Loss: 0.005980108294301153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.008616668645991012\n",
      "Training Loss: 0.009011599995428696\n",
      "Training Loss: 0.008135468336986378\n",
      "Validation Loss: 0.00583028562597177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.00848117821617052\n",
      "Training Loss: 0.00888422602089122\n",
      "Training Loss: 0.008008371911710128\n",
      "Validation Loss: 0.005692139945484781\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008354925354942679\n",
      "Training Loss: 0.008766602086834609\n",
      "Training Loss: 0.007891142190201208\n",
      "Validation Loss: 0.005565003251259247\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.008238026857143269\n",
      "Training Loss: 0.00865882241167128\n",
      "Training Loss: 0.007784081181744114\n",
      "Validation Loss: 0.005448944782942868\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008130985490279272\n",
      "Training Loss: 0.008561272635124623\n",
      "Training Loss: 0.007687603117665276\n",
      "Validation Loss: 0.00534422062285077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008034294897224754\n",
      "Training Loss: 0.008474248226266355\n",
      "Training Loss: 0.007601892090169713\n",
      "Validation Loss: 0.005250867955867996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007948141033994034\n",
      "Training Loss: 0.008397706407122315\n",
      "Training Loss: 0.0075267407135106625\n",
      "Validation Loss: 0.005168520916106828\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.007872281948802993\n",
      "Training Loss: 0.008331186133436858\n",
      "Training Loss: 0.0074615121260285374\n",
      "Validation Loss: 0.0050963826236848754\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.00780605792417191\n",
      "Training Loss: 0.008273850410478189\n",
      "Training Loss: 0.00740524465800263\n",
      "Validation Loss: 0.005033348469858926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007748505364870653\n",
      "Training Loss: 0.00822462618467398\n",
      "Training Loss: 0.007356793495127931\n",
      "Validation Loss: 0.0049781566145672895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007698503411374986\n",
      "Training Loss: 0.008182339147897438\n",
      "Training Loss: 0.00731497973902151\n",
      "Validation Loss: 0.004929552846221944\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007654911059653387\n",
      "Training Loss: 0.008145848350832239\n",
      "Training Loss: 0.007278695884160698\n",
      "Validation Loss: 0.004886380381086904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.0076166645949706435\n",
      "Training Loss: 0.008114116583019495\n",
      "Training Loss: 0.007246966543607414\n",
      "Validation Loss: 0.004847643394651038\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.0075828272057697176\n",
      "Training Loss: 0.008086250530323014\n",
      "Training Loss: 0.007218963612103835\n",
      "Validation Loss: 0.004812512156982603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.0075526046880986545\n",
      "Training Loss: 0.008061504040379077\n",
      "Training Loss: 0.007194008621154353\n",
      "Validation Loss: 0.004780316107979651\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007525344843743369\n",
      "Training Loss: 0.008039273971226066\n",
      "Training Loss: 0.0071715528890490534\n",
      "Validation Loss: 0.004750522599147444\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007500518569722772\n",
      "Training Loss: 0.008019073624163866\n",
      "Training Loss: 0.007151155883911997\n",
      "Validation Loss: 0.0047227166739574975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007477706457721069\n",
      "Training Loss: 0.008000523342052474\n",
      "Training Loss: 0.007132469650823623\n",
      "Validation Loss: 0.0046965719378563794\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007456572556402534\n",
      "Training Loss: 0.007983321541687474\n",
      "Training Loss: 0.007115215793019161\n",
      "Validation Loss: 0.004671836279730281\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007436854189727456\n",
      "Training Loss: 0.007967235484393313\n",
      "Training Loss: 0.007099173282040283\n",
      "Validation Loss: 0.004648316178073207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007418342692544684\n",
      "Training Loss: 0.00795208333292976\n",
      "Training Loss: 0.007084164962871\n",
      "Validation Loss: 0.004625863542131493\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0074008744314778595\n",
      "Training Loss: 0.007937725506490096\n",
      "Training Loss: 0.007070051140617579\n",
      "Validation Loss: 0.004604363026557846\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0073843211680650715\n",
      "Training Loss: 0.00792405208107084\n",
      "Training Loss: 0.007056717162486166\n",
      "Validation Loss: 0.0045837261635558995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007368579233298078\n",
      "Training Loss: 0.007910979114240036\n",
      "Training Loss: 0.0070440698030870405\n",
      "Validation Loss: 0.004563885084301066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007353566795354709\n",
      "Training Loss: 0.007898437663679942\n",
      "Training Loss: 0.007032031819690019\n",
      "Validation Loss: 0.0045447851756189985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007339215124957264\n",
      "Training Loss: 0.007886375484522432\n",
      "Training Loss: 0.007020540699595585\n",
      "Validation Loss: 0.0045263831676350215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007325469030765816\n",
      "Training Loss: 0.007874748746398836\n",
      "Training Loss: 0.007009540691506118\n",
      "Validation Loss: 0.004508641993187451\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.0073122812132351105\n",
      "Training Loss: 0.00786352072376758\n",
      "Training Loss: 0.00699898406630382\n",
      "Validation Loss: 0.004491530643943488\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0072996091842651364\n",
      "Training Loss: 0.007852659247582779\n",
      "Training Loss: 0.006988828206667677\n",
      "Validation Loss: 0.0044750189774910385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007287415923783556\n",
      "Training Loss: 0.007842137879924849\n",
      "Training Loss: 0.006979037404526025\n",
      "Validation Loss: 0.004459083556417334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007275669377995655\n",
      "Training Loss: 0.007831932774279267\n",
      "Training Loss: 0.006969577249838039\n",
      "Validation Loss: 0.004443698112716835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007264337941305712\n",
      "Training Loss: 0.007822020592866465\n",
      "Training Loss: 0.006960417319787666\n",
      "Validation Loss: 0.00442884168901554\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007253394847502932\n",
      "Training Loss: 0.0078123834438156336\n",
      "Training Loss: 0.006951530770165846\n",
      "Validation Loss: 0.004414491172293934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007242815099889412\n",
      "Training Loss: 0.007803001099964604\n",
      "Training Loss: 0.006942891195649281\n",
      "Validation Loss: 0.004400624485498064\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007232572035863995\n",
      "Training Loss: 0.007793857732322067\n",
      "Training Loss: 0.006934475378366187\n",
      "Validation Loss: 0.004387222785042243\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007222643451532349\n",
      "Training Loss: 0.007784934225492179\n",
      "Training Loss: 0.006926261802436784\n",
      "Validation Loss: 0.004374263583148798\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007213007535319775\n",
      "Training Loss: 0.007776218007784337\n",
      "Training Loss: 0.0069182305992580954\n",
      "Validation Loss: 0.004361728284759133\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007203644354594871\n",
      "Training Loss: 0.0077676916331984106\n",
      "Training Loss: 0.006910364835057407\n",
      "Validation Loss: 0.004349597167286477\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007194533786969259\n",
      "Training Loss: 0.007759345136582852\n",
      "Training Loss: 0.0069026447250507775\n",
      "Validation Loss: 0.004337846981675437\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007185657991794869\n",
      "Training Loss: 0.007751161934575066\n",
      "Training Loss: 0.006895058205118403\n",
      "Validation Loss: 0.004326466010527664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007176999162184074\n",
      "Training Loss: 0.0077431323344353585\n",
      "Training Loss: 0.006887588842073455\n",
      "Validation Loss: 0.00431543010747416\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007168541854480281\n",
      "Training Loss: 0.007735243922797963\n",
      "Training Loss: 0.006880225189961493\n",
      "Validation Loss: 0.004304722090136636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.00716027025016956\n",
      "Training Loss: 0.007727486240910366\n",
      "Training Loss: 0.0068729552498552945\n",
      "Validation Loss: 0.00429432843602524\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007152170529589057\n",
      "Training Loss: 0.007719850291032344\n",
      "Training Loss: 0.006865768468705937\n",
      "Validation Loss: 0.004284229385928156\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007144229813711718\n",
      "Training Loss: 0.007712325637694448\n",
      "Training Loss: 0.0068586564564611764\n",
      "Validation Loss: 0.004274411935338311\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0071364366996567695\n",
      "Training Loss: 0.007704904483398422\n",
      "Training Loss: 0.00685161028872244\n",
      "Validation Loss: 0.004264859262895718\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007128778764745221\n",
      "Training Loss: 0.007697580207604915\n",
      "Training Loss: 0.006844621181953698\n",
      "Validation Loss: 0.004255557503404744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007121245639864355\n",
      "Training Loss: 0.0076903434935957195\n",
      "Training Loss: 0.006837683642515913\n",
      "Validation Loss: 0.004246493299188239\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.00711382738663815\n",
      "Training Loss: 0.00768318900023587\n",
      "Training Loss: 0.006830790721578523\n",
      "Validation Loss: 0.004237650671262252\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007106515546329319\n",
      "Training Loss: 0.007676109090680256\n",
      "Training Loss: 0.006823936718283221\n",
      "Validation Loss: 0.004229022812600551\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007099302087444812\n",
      "Training Loss: 0.007669099412159994\n",
      "Training Loss: 0.006817116549937055\n",
      "Validation Loss: 0.004220590761894088\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007092177126323805\n",
      "Training Loss: 0.007662151361582801\n",
      "Training Loss: 0.006810326193226501\n",
      "Validation Loss: 0.0042123488501091015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007085135455708951\n",
      "Training Loss: 0.007655262673506513\n",
      "Training Loss: 0.00680356016731821\n",
      "Validation Loss: 0.004204280936065015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.00707816930138506\n",
      "Training Loss: 0.007648425797233358\n",
      "Training Loss: 0.006796815760899335\n",
      "Validation Loss: 0.004196379386056005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.0070712724525947126\n",
      "Training Loss: 0.007641638789791614\n",
      "Training Loss: 0.006790089018177241\n",
      "Validation Loss: 0.004188635284499673\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007064439427340403\n",
      "Training Loss: 0.00763489383039996\n",
      "Training Loss: 0.006783376754028723\n",
      "Validation Loss: 0.004181036500657877\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0070576634886674585\n",
      "Training Loss: 0.007628188426606357\n",
      "Training Loss: 0.006776674940483645\n",
      "Validation Loss: 0.004173573616661885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007050939943874255\n",
      "Training Loss: 0.007621518565574661\n",
      "Training Loss: 0.0067699801479466255\n",
      "Validation Loss: 0.004166239800632753\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007044263236457482\n",
      "Training Loss: 0.0076148778933566065\n",
      "Training Loss: 0.006763290764065459\n",
      "Validation Loss: 0.0041590251598842025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007037628713296726\n",
      "Training Loss: 0.007608263980364427\n",
      "Training Loss: 0.006756602122914046\n",
      "Validation Loss: 0.004151922504135062\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007031032493105158\n",
      "Training Loss: 0.007601672881282866\n",
      "Training Loss: 0.0067499122489243744\n",
      "Validation Loss: 0.0041449226483472445\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.0070244686328805985\n",
      "Training Loss: 0.007595098551828414\n",
      "Training Loss: 0.006743218506453558\n",
      "Validation Loss: 0.004138017694591388\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007017933675087989\n",
      "Training Loss: 0.007588538341224194\n",
      "Training Loss: 0.006736516586970538\n",
      "Validation Loss: 0.0041312019921462525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.0070114227489102635\n",
      "Training Loss: 0.0075819881132338195\n",
      "Training Loss: 0.006729805639479309\n",
      "Validation Loss: 0.004124466600576729\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.0070049315667711195\n",
      "Training Loss: 0.007575443146051839\n",
      "Training Loss: 0.0067230809410102664\n",
      "Validation Loss: 0.00411780637275595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006998456725850702\n",
      "Training Loss: 0.007568899049656466\n",
      "Training Loss: 0.006716338862897828\n",
      "Validation Loss: 0.004111210773751307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006991991924587637\n",
      "Training Loss: 0.007562350956723094\n",
      "Training Loss: 0.006709576668217778\n",
      "Validation Loss: 0.0041046733091574875\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006985534834675491\n",
      "Training Loss: 0.00755579489399679\n",
      "Training Loss: 0.006702790652634576\n",
      "Validation Loss: 0.004098187616680947\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.0069790793373249475\n",
      "Training Loss: 0.007549225600669161\n",
      "Training Loss: 0.006695977622875944\n",
      "Validation Loss: 0.004091748525650146\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00697262306814082\n",
      "Training Loss: 0.007542637616861612\n",
      "Training Loss: 0.006689132561441511\n",
      "Validation Loss: 0.004085345594003127\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006966158243594691\n",
      "Training Loss: 0.007536025148583576\n",
      "Training Loss: 0.00668225004337728\n",
      "Validation Loss: 0.0040789675870073145\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006959680787986144\n",
      "Training Loss: 0.007529381403001026\n",
      "Training Loss: 0.0066753258556127544\n",
      "Validation Loss: 0.004072612734888209\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006953185932943597\n",
      "Training Loss: 0.007522701249690726\n",
      "Training Loss: 0.006668354545254261\n",
      "Validation Loss: 0.00406626600829696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006946667050942779\n",
      "Training Loss: 0.00751597702736035\n",
      "Training Loss: 0.006661330114584416\n",
      "Validation Loss: 0.004059922867036016\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006940118478378281\n",
      "Training Loss: 0.0075092014321126044\n",
      "Training Loss: 0.006654247432015837\n",
      "Validation Loss: 0.004053573687874785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006933533878764137\n",
      "Training Loss: 0.007502365545951762\n",
      "Training Loss: 0.0066470983112230896\n",
      "Validation Loss: 0.004047204646166791\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006926904919091612\n",
      "Training Loss: 0.007495461366488598\n",
      "Training Loss: 0.0066398756206035615\n",
      "Validation Loss: 0.004040804964992521\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006920224773930386\n",
      "Training Loss: 0.007488479291787371\n",
      "Training Loss: 0.0066325724055059255\n",
      "Validation Loss: 0.0040343702925118\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006913486524717882\n",
      "Training Loss: 0.00748140960757155\n",
      "Training Loss: 0.006625179331749677\n",
      "Validation Loss: 0.004027879672658661\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0069066791457589715\n",
      "Training Loss: 0.007474240719457157\n",
      "Training Loss: 0.006617685938253999\n",
      "Validation Loss: 0.004021325034842816\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.0068997946451418105\n",
      "Training Loss: 0.0074669608060503375\n",
      "Training Loss: 0.006610085085267201\n",
      "Validation Loss: 0.004014688634229929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006892823771340773\n",
      "Training Loss: 0.00745955973456148\n",
      "Training Loss: 0.006602366701699793\n",
      "Validation Loss: 0.0040079607233258615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006885756187839434\n",
      "Training Loss: 0.007452023716177791\n",
      "Training Loss: 0.006594519199570641\n",
      "Validation Loss: 0.00400112599547785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006878581343917176\n",
      "Training Loss: 0.007444341196096502\n",
      "Training Loss: 0.006586534195812419\n",
      "Validation Loss: 0.0039941706168831565\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006871290853014216\n",
      "Training Loss: 0.007436500453623012\n",
      "Training Loss: 0.006578402000013739\n",
      "Validation Loss: 0.00398708028676972\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006863874258706346\n",
      "Training Loss: 0.007428489962476306\n",
      "Training Loss: 0.006570114376954734\n",
      "Validation Loss: 0.003979843431623296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.00685632204869762\n",
      "Training Loss: 0.007420301362872124\n",
      "Training Loss: 0.006561664432520047\n",
      "Validation Loss: 0.003972452596939179\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006848629047162831\n",
      "Training Loss: 0.007411925834603607\n",
      "Training Loss: 0.006553047985071316\n",
      "Validation Loss: 0.003964899143130843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006840788186527788\n",
      "Training Loss: 0.007403359586023725\n",
      "Training Loss: 0.0065442625188734385\n",
      "Validation Loss: 0.0039571836653552695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.0068327967054210605\n",
      "Training Loss: 0.007394601231208071\n",
      "Training Loss: 0.006535308206221089\n",
      "Validation Loss: 0.0039493037568237846\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006824653723742813\n",
      "Training Loss: 0.007385653350502253\n",
      "Training Loss: 0.00652618998545222\n",
      "Validation Loss: 0.003941274143420578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006816363353282213\n",
      "Training Loss: 0.0073765232169535015\n",
      "Training Loss: 0.006516914553940296\n",
      "Validation Loss: 0.003933100932371918\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0068079289933666584\n",
      "Training Loss: 0.007367221767199225\n",
      "Training Loss: 0.006507494557881728\n",
      "Validation Loss: 0.00392480804114027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006799359794240445\n",
      "Training Loss: 0.007357763815089129\n",
      "Training Loss: 0.006497942736605183\n",
      "Validation Loss: 0.003916421109873257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.0067906658351421356\n",
      "Training Loss: 0.0073481704539153725\n",
      "Training Loss: 0.006488275479059666\n",
      "Validation Loss: 0.003907965148244513\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006781859019538388\n",
      "Training Loss: 0.007338459648308344\n",
      "Training Loss: 0.006478509722510353\n",
      "Validation Loss: 0.0038994710926447858\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006772950653685257\n",
      "Training Loss: 0.007328653684235178\n",
      "Training Loss: 0.00646866288385354\n",
      "Validation Loss: 0.00389096981667903\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006763953169574961\n",
      "Training Loss: 0.0073187750187935306\n",
      "Training Loss: 0.006458751321770251\n",
      "Validation Loss: 0.0038824863574896634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.00675487769767642\n",
      "Training Loss: 0.0073088423302397135\n",
      "Training Loss: 0.0064487904694397\n",
      "Validation Loss: 0.003874050208702265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0067457341635599735\n",
      "Training Loss: 0.007298875157721341\n",
      "Training Loss: 0.006438794652931392\n",
      "Validation Loss: 0.003865683847630208\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.0067365316231735054\n",
      "Training Loss: 0.007288889230112545\n",
      "Training Loss: 0.0064287755533587185\n",
      "Validation Loss: 0.003857400393887852\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006727275365265086\n",
      "Training Loss: 0.0072788980737095695\n",
      "Training Loss: 0.006418742961250246\n",
      "Validation Loss: 0.0038492165027144416\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006717972040642053\n",
      "Training Loss: 0.0072689124266617\n",
      "Training Loss: 0.006408704775385559\n",
      "Validation Loss: 0.0038411391171720927\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006708626198815182\n",
      "Training Loss: 0.0072589428536593914\n",
      "Training Loss: 0.006398669725749642\n",
      "Validation Loss: 0.0038331701666158573\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006699242747854442\n",
      "Training Loss: 0.007248996295966208\n",
      "Training Loss: 0.006388645210536197\n",
      "Validation Loss: 0.0038253098930326407\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006689826643560081\n",
      "Training Loss: 0.007239080989966169\n",
      "Training Loss: 0.006378636454464868\n",
      "Validation Loss: 0.003817553524702285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.00668038236675784\n",
      "Training Loss: 0.00722920251195319\n",
      "Training Loss: 0.0063686533039435745\n",
      "Validation Loss: 0.0038099019824830668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006670916428556666\n",
      "Training Loss: 0.007219369684462435\n",
      "Training Loss: 0.006358701343415305\n",
      "Validation Loss: 0.003802347007427323\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006661435111891478\n",
      "Training Loss: 0.0072095877956599\n",
      "Training Loss: 0.006348789664916694\n",
      "Validation Loss: 0.0037948799848012377\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006651945958146825\n",
      "Training Loss: 0.007199864154681563\n",
      "Training Loss: 0.006338926393073052\n",
      "Validation Loss: 0.003787491071623949\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006642456171102822\n",
      "Training Loss: 0.0071902051486540585\n",
      "Training Loss: 0.006329120290465653\n",
      "Validation Loss: 0.0037801762522208723\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.0066329743957612665\n",
      "Training Loss: 0.007180617867270485\n",
      "Training Loss: 0.006319379978813231\n",
      "Validation Loss: 0.0037729243860381207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006623508101329207\n",
      "Training Loss: 0.00717110865865834\n",
      "Training Loss: 0.0063097122637555\n",
      "Validation Loss: 0.0037657270794001856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006614066767506302\n",
      "Training Loss: 0.007161682044388726\n",
      "Training Loss: 0.006300127592403441\n",
      "Validation Loss: 0.0037585787757645163\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006604657362913713\n",
      "Training Loss: 0.007152343700872734\n",
      "Training Loss: 0.0062906318169552835\n",
      "Validation Loss: 0.003751470151441067\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00659528709598817\n",
      "Training Loss: 0.007143096585641615\n",
      "Training Loss: 0.006281230322783813\n",
      "Validation Loss: 0.003744389432785886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006585961721139028\n",
      "Training Loss: 0.00713394274818711\n",
      "Training Loss: 0.006271927619818598\n",
      "Validation Loss: 0.0037373296309555514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006576687375199981\n",
      "Training Loss: 0.007124882856151089\n",
      "Training Loss: 0.00626272709807381\n",
      "Validation Loss: 0.0037302845066120283\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006567467279965058\n",
      "Training Loss: 0.007115914968308061\n",
      "Training Loss: 0.006253630117280409\n",
      "Validation Loss: 0.0037232444783819193\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006558302763733081\n",
      "Training Loss: 0.007107037726091221\n",
      "Training Loss: 0.006244635242037475\n",
      "Validation Loss: 0.003716200561356846\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006549195857369341\n",
      "Training Loss: 0.007098244872759096\n",
      "Training Loss: 0.0062357411859557035\n",
      "Validation Loss: 0.003709146416787937\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.0065401461417786775\n",
      "Training Loss: 0.0070895325497258455\n",
      "Training Loss: 0.006226944007212296\n",
      "Validation Loss: 0.00370207736844176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.0065311514225322755\n",
      "Training Loss: 0.007080892190570012\n",
      "Training Loss: 0.006218236971180886\n",
      "Validation Loss: 0.0036949805399988976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.00652220960590057\n",
      "Training Loss: 0.007072318205609917\n",
      "Training Loss: 0.006209614819381386\n",
      "Validation Loss: 0.003687857488023766\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006513317232020199\n",
      "Training Loss: 0.0070638021890772505\n",
      "Training Loss: 0.006201070026727393\n",
      "Validation Loss: 0.003680700334207563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.00650447117339354\n",
      "Training Loss: 0.007055337153142318\n",
      "Training Loss: 0.006192596957553178\n",
      "Validation Loss: 0.0036735094173319556\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006495667613344267\n",
      "Training Loss: 0.007046915955725126\n",
      "Training Loss: 0.006184187065809965\n",
      "Validation Loss: 0.0036662784535856393\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.0064869014878058805\n",
      "Training Loss: 0.0070385314169107\n",
      "Training Loss: 0.006175832068547607\n",
      "Validation Loss: 0.003659008299733062\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006478170047048479\n",
      "Training Loss: 0.007030176529078744\n",
      "Training Loss: 0.006167525881901384\n",
      "Validation Loss: 0.0036517002324186515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006469469195581041\n",
      "Training Loss: 0.007021846725838259\n",
      "Training Loss: 0.006159261028515175\n",
      "Validation Loss: 0.0036443564308325897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0064607950206846\n",
      "Training Loss: 0.0070135364838643\n",
      "Training Loss: 0.006151031389599666\n",
      "Validation Loss: 0.003636974333837796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.0064521451032487675\n",
      "Training Loss: 0.007005242684390396\n",
      "Training Loss: 0.006142831371398642\n",
      "Validation Loss: 0.0036295567864154496\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.0064435161434812475\n",
      "Training Loss: 0.006996960150427185\n",
      "Training Loss: 0.0061346551321912555\n",
      "Validation Loss: 0.003622106137800585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.0064349060115637255\n",
      "Training Loss: 0.006988688031560742\n",
      "Training Loss: 0.006126498038647696\n",
      "Validation Loss: 0.0036146267162887064\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006426312392577529\n",
      "Training Loss: 0.00698042334872298\n",
      "Training Loss: 0.006118357266532257\n",
      "Validation Loss: 0.0036071222170852542\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006417734156711958\n",
      "Training Loss: 0.006972165041952394\n",
      "Training Loss: 0.0061102275212761015\n",
      "Validation Loss: 0.003599590349816874\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006409169146209024\n",
      "Training Loss: 0.006963912076316774\n",
      "Training Loss: 0.006102107607293874\n",
      "Validation Loss: 0.0035920361517353004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.0064006169018102806\n",
      "Training Loss: 0.006955663480330259\n",
      "Training Loss: 0.0060939939680974935\n",
      "Validation Loss: 0.003584459582291376\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006392076285555959\n",
      "Training Loss: 0.006947418648633174\n",
      "Training Loss: 0.006085883941268549\n",
      "Validation Loss: 0.003576868043668317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006383545816061087\n",
      "Training Loss: 0.0069391771598020565\n",
      "Training Loss: 0.006077776891179383\n",
      "Validation Loss: 0.003569260472431779\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006375025223824196\n",
      "Training Loss: 0.006930939381709323\n",
      "Training Loss: 0.00606967078987509\n",
      "Validation Loss: 0.0035616370490909126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006366513526299969\n",
      "Training Loss: 0.006922705444740131\n",
      "Training Loss: 0.006061563816620037\n",
      "Validation Loss: 0.0035539988815533313\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006358010737458244\n",
      "Training Loss: 0.00691447394376155\n",
      "Training Loss: 0.006053455116925761\n",
      "Validation Loss: 0.0035463520443443667\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006349515703623183\n",
      "Training Loss: 0.006906246916041709\n",
      "Training Loss: 0.0060453445324674246\n",
      "Validation Loss: 0.00353869393708582\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006341027399757877\n",
      "Training Loss: 0.006898023433750495\n",
      "Training Loss: 0.006037229633657262\n",
      "Validation Loss: 0.003531025700386237\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.0063325468287803235\n",
      "Training Loss: 0.006889805173268542\n",
      "Training Loss: 0.0060291107383091\n",
      "Validation Loss: 0.0035233488934261078\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006324071606504731\n",
      "Training Loss: 0.0068815892795100805\n",
      "Training Loss: 0.006020985653158277\n",
      "Validation Loss: 0.0035156602657326823\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006315601493697614\n",
      "Training Loss: 0.006873377694864758\n",
      "Training Loss: 0.00601285565760918\n",
      "Validation Loss: 0.0035079644359857515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006307136656832881\n",
      "Training Loss: 0.006865170182427391\n",
      "Training Loss: 0.006004719303455204\n",
      "Validation Loss: 0.0035002626298162877\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006298675409634597\n",
      "Training Loss: 0.006856966797495261\n",
      "Training Loss: 0.005996576171601192\n",
      "Validation Loss: 0.0034925514711799583\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006290218390058726\n",
      "Training Loss: 0.006848766730981879\n",
      "Training Loss: 0.0059884256229270245\n",
      "Validation Loss: 0.003484832326975766\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006281763283768668\n",
      "Training Loss: 0.006840570657514036\n",
      "Training Loss: 0.005980268332641572\n",
      "Validation Loss: 0.003477105982026106\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.00627331031661015\n",
      "Training Loss: 0.006832377886166796\n",
      "Training Loss: 0.005972101332154125\n",
      "Validation Loss: 0.003469372925536937\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006264857990900054\n",
      "Training Loss: 0.0068241879652487115\n",
      "Training Loss: 0.005963926843833178\n",
      "Validation Loss: 0.0034616310829610635\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.0062564060353906825\n",
      "Training Loss: 0.006816000653780066\n",
      "Training Loss: 0.005955742497462779\n",
      "Validation Loss: 0.003453880193999058\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006247953145648352\n",
      "Training Loss: 0.006807815909269266\n",
      "Training Loss: 0.005947549296543002\n",
      "Validation Loss: 0.003446122809323702\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006239499414805323\n",
      "Training Loss: 0.006799632799229584\n",
      "Training Loss: 0.0059393460780847816\n",
      "Validation Loss: 0.003438354464603609\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.0062310429255012425\n",
      "Training Loss: 0.006791450275341049\n",
      "Training Loss: 0.00593113134149462\n",
      "Validation Loss: 0.0034305747595793578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006222581978654489\n",
      "Training Loss: 0.006783268479630351\n",
      "Training Loss: 0.005922907335916534\n",
      "Validation Loss: 0.0034227879179034677\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006214116882183589\n",
      "Training Loss: 0.006775086062261835\n",
      "Training Loss: 0.005914670538622886\n",
      "Validation Loss: 0.003414989719847531\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006205646414891817\n",
      "Training Loss: 0.006766903094248846\n",
      "Training Loss: 0.00590642241644673\n",
      "Validation Loss: 0.0034071799077281958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.0061971698200795795\n",
      "Training Loss: 0.006758718180353753\n",
      "Training Loss: 0.005898162544472143\n",
      "Validation Loss: 0.0033993573827941098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006188684463850223\n",
      "Training Loss: 0.00675053057319019\n",
      "Training Loss: 0.005889888823730871\n",
      "Validation Loss: 0.003391523629667635\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006180190903251059\n",
      "Training Loss: 0.006742339647607878\n",
      "Training Loss: 0.005881602723384276\n",
      "Validation Loss: 0.00338367836581271\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006171688120230101\n",
      "Training Loss: 0.006734144825604744\n",
      "Training Loss: 0.005873302520485595\n",
      "Validation Loss: 0.0033758156291952128\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.0061631748301442715\n",
      "Training Loss: 0.006725944829522632\n",
      "Training Loss: 0.0058649872313253585\n",
      "Validation Loss: 0.0033679347344035848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006154649686068297\n",
      "Training Loss: 0.006717737579601817\n",
      "Training Loss: 0.005856658616103232\n",
      "Validation Loss: 0.003360043587215412\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0061461124254856255\n",
      "Training Loss: 0.006709523854078725\n",
      "Training Loss: 0.005848314168397337\n",
      "Validation Loss: 0.003352133448633334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006137561474461108\n",
      "Training Loss: 0.00670130303944461\n",
      "Training Loss: 0.0058399546379223465\n",
      "Validation Loss: 0.0033442061865346486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006128995256731286\n",
      "Training Loss: 0.006693072636262514\n",
      "Training Loss: 0.005831578868674114\n",
      "Validation Loss: 0.003336259434679837\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.0061204149719560515\n",
      "Training Loss: 0.006684830936137587\n",
      "Training Loss: 0.005823186176130548\n",
      "Validation Loss: 0.0033282933016359974\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006111817333148793\n",
      "Training Loss: 0.006676578077604063\n",
      "Training Loss: 0.005814775933977216\n",
      "Validation Loss: 0.0033203043642694528\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006103203463717364\n",
      "Training Loss: 0.006668313105474226\n",
      "Training Loss: 0.005806348235346377\n",
      "Validation Loss: 0.0033122925140131056\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006094570828136057\n",
      "Training Loss: 0.006660033978405409\n",
      "Training Loss: 0.005797901531914249\n",
      "Validation Loss: 0.0033042567816112985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006085919338511303\n",
      "Training Loss: 0.006651740699890069\n",
      "Training Loss: 0.0057894372404552995\n",
      "Validation Loss: 0.003296197910029232\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006077248630463146\n",
      "Training Loss: 0.006643430917174556\n",
      "Training Loss: 0.005780953115317971\n",
      "Validation Loss: 0.0032881126527182676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006068556716782041\n",
      "Training Loss: 0.006635104887536727\n",
      "Training Loss: 0.005772449201904237\n",
      "Validation Loss: 0.003280002457675723\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.00605984371737577\n",
      "Training Loss: 0.006626760240178556\n",
      "Training Loss: 0.005763924922794103\n",
      "Validation Loss: 0.0032718646120321886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.006051109088584781\n",
      "Training Loss: 0.0066183960041962565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [13:45<20:38, 206.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.005755379046313465\n",
      "Validation Loss: 0.003263691748921456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.11914786418899893\n",
      "Training Loss: 0.08989881873130798\n",
      "Training Loss: 0.07932112570852042\n",
      "Validation Loss: 0.0698681671297952\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.07382284911349415\n",
      "Training Loss: 0.07025158692151308\n",
      "Training Loss: 0.06782931484282016\n",
      "Validation Loss: 0.06117651251594672\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06449850901961326\n",
      "Training Loss: 0.060938708689063786\n",
      "Training Loss: 0.058406323231756686\n",
      "Validation Loss: 0.052328040886126206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.05448445506393909\n",
      "Training Loss: 0.05071018993854523\n",
      "Training Loss: 0.047898928401991725\n",
      "Validation Loss: 0.04225831995770503\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.04346110021695495\n",
      "Training Loss: 0.03959335486404598\n",
      "Training Loss: 0.036548361014574766\n",
      "Validation Loss: 0.03144411150407925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.03212070506997407\n",
      "Training Loss: 0.028620852222666145\n",
      "Training Loss: 0.025827167481184007\n",
      "Validation Loss: 0.021651395659349607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.022555996710434557\n",
      "Training Loss: 0.02021134105976671\n",
      "Training Loss: 0.018461728296242656\n",
      "Validation Loss: 0.015746417821625645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.01731281531509012\n",
      "Training Loss: 0.016338058435358106\n",
      "Training Loss: 0.015425753216259181\n",
      "Validation Loss: 0.01351642698635546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.015212281153071671\n",
      "Training Loss: 0.014740304232109338\n",
      "Training Loss: 0.013920978105161338\n",
      "Validation Loss: 0.012058791374850475\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.013820243519730865\n",
      "Training Loss: 0.013592399475164711\n",
      "Training Loss: 0.012754527002107352\n",
      "Validation Loss: 0.010818017149616159\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.012676196086686105\n",
      "Training Loss: 0.012638796232640742\n",
      "Training Loss: 0.01179061059257947\n",
      "Validation Loss: 0.009779902946810876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.011734778652898967\n",
      "Training Loss: 0.011845752522349358\n",
      "Training Loss: 0.011001507024047896\n",
      "Validation Loss: 0.008932688032810608\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.010974206058308482\n",
      "Training Loss: 0.01119673233362846\n",
      "Training Loss: 0.010365667502628639\n",
      "Validation Loss: 0.008254512087598946\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.010369850334245711\n",
      "Training Loss: 0.010674609066918493\n",
      "Training Loss: 0.009859948202501983\n",
      "Validation Loss: 0.007718061426591673\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.009894819604232908\n",
      "Training Loss: 0.010259878188371659\n",
      "Training Loss: 0.009460334690520539\n",
      "Validation Loss: 0.007294903059353989\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.009522320129908621\n",
      "Training Loss: 0.009931594226509332\n",
      "Training Loss: 0.009143488883273676\n",
      "Validation Loss: 0.006958394442172198\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.00922758201835677\n",
      "Training Loss: 0.009669305197894574\n",
      "Training Loss: 0.00888832917320542\n",
      "Validation Loss: 0.00668542981775624\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.008989335694350302\n",
      "Training Loss: 0.009455039635067806\n",
      "Training Loss: 0.008677363246679306\n",
      "Validation Loss: 0.0064574651775902575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.00879074578289874\n",
      "Training Loss: 0.009274568224791438\n",
      "Training Loss: 0.008497295166598633\n",
      "Validation Loss: 0.006260721502155902\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.008619525992544367\n",
      "Training Loss: 0.009117643494391814\n",
      "Training Loss: 0.008338821134530008\n",
      "Validation Loss: 0.006085703387500697\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.00846737229381688\n",
      "Training Loss: 0.008977443032199517\n",
      "Training Loss: 0.008195877104299143\n",
      "Validation Loss: 0.005926256109908056\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.00832901862449944\n",
      "Training Loss: 0.00884971370222047\n",
      "Training Loss: 0.008064758316613733\n",
      "Validation Loss: 0.005778638682631629\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008201344675617292\n",
      "Training Loss: 0.00873197186505422\n",
      "Training Loss: 0.007943343550432474\n",
      "Validation Loss: 0.005640698803505034\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008082620459608733\n",
      "Training Loss: 0.00862284671398811\n",
      "Training Loss: 0.007830500231357292\n",
      "Validation Loss: 0.005511284795369995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.007971966448239982\n",
      "Training Loss: 0.008521624818677083\n",
      "Training Loss: 0.007725671200314537\n",
      "Validation Loss: 0.0053898165148965425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007868974637240171\n",
      "Training Loss: 0.008427938049426303\n",
      "Training Loss: 0.007628583108307794\n",
      "Validation Loss: 0.005275990369237876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.0077734475594479595\n",
      "Training Loss: 0.008341556095983834\n",
      "Training Loss: 0.007539071764331311\n",
      "Validation Loss: 0.0051696225739178365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007685259049758315\n",
      "Training Loss: 0.008262286999961361\n",
      "Training Loss: 0.007456981898285448\n",
      "Validation Loss: 0.005070551565398326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.007604266430716961\n",
      "Training Loss: 0.008189903509337454\n",
      "Training Loss: 0.007382109927712008\n",
      "Validation Loss: 0.0049785860536850236\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.0075302673096302895\n",
      "Training Loss: 0.008124119035201147\n",
      "Training Loss: 0.007314179569948465\n",
      "Validation Loss: 0.004893483421399018\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007462984198937193\n",
      "Training Loss: 0.008064589389832691\n",
      "Training Loss: 0.007252844404429197\n",
      "Validation Loss: 0.004814960028494845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007402075277641415\n",
      "Training Loss: 0.008010916240746156\n",
      "Training Loss: 0.007197696258081123\n",
      "Validation Loss: 0.004742683936956893\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007347138074692339\n",
      "Training Loss: 0.007962661562487483\n",
      "Training Loss: 0.007148281568661332\n",
      "Validation Loss: 0.004676295462681839\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007297737358603626\n",
      "Training Loss: 0.007919367878930644\n",
      "Training Loss: 0.0071041208785027265\n",
      "Validation Loss: 0.004615419405627619\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007253412731224671\n",
      "Training Loss: 0.007880572217982263\n",
      "Training Loss: 0.007064723888179287\n",
      "Validation Loss: 0.004559660592022237\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007213700799038634\n",
      "Training Loss: 0.007845819023204968\n",
      "Training Loss: 0.007029607163276524\n",
      "Validation Loss: 0.0045086336505254\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007178146295482293\n",
      "Training Loss: 0.007814670208608732\n",
      "Training Loss: 0.006998306290479377\n",
      "Validation Loss: 0.004461965411168973\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007146316469879821\n",
      "Training Loss: 0.007786719192517921\n",
      "Training Loss: 0.006970383563311771\n",
      "Validation Loss: 0.004419292011978418\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007117801673593931\n",
      "Training Loss: 0.007761588213033974\n",
      "Training Loss: 0.0069454342708922924\n",
      "Validation Loss: 0.004380270888442906\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007092227002140135\n",
      "Training Loss: 0.007738936390960589\n",
      "Training Loss: 0.00692308972007595\n",
      "Validation Loss: 0.0043445841770284295\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007069251234643161\n",
      "Training Loss: 0.007718455811263993\n",
      "Training Loss: 0.0069030198245309295\n",
      "Validation Loss: 0.00431193602585307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0070485663309227675\n",
      "Training Loss: 0.007699872498051263\n",
      "Training Loss: 0.006884930203668773\n",
      "Validation Loss: 0.00428205549068163\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0070298962877131995\n",
      "Training Loss: 0.007682947426219471\n",
      "Training Loss: 0.006868559202412143\n",
      "Validation Loss: 0.004254690850195423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007012997682904825\n",
      "Training Loss: 0.0076674698886927215\n",
      "Training Loss: 0.006853679765481502\n",
      "Validation Loss: 0.004229612185441879\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006997652992140502\n",
      "Training Loss: 0.00765325527114328\n",
      "Training Loss: 0.0068400924932211634\n",
      "Validation Loss: 0.004206610336020756\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006983674298389815\n",
      "Training Loss: 0.007640141905867494\n",
      "Training Loss: 0.006827624352881685\n",
      "Validation Loss: 0.004185492944579278\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006970892491517588\n",
      "Training Loss: 0.007627992543857545\n",
      "Training Loss: 0.006816126188496128\n",
      "Validation Loss: 0.004166088213495324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006959163726423867\n",
      "Training Loss: 0.007616684552049264\n",
      "Training Loss: 0.006805468325037509\n",
      "Validation Loss: 0.004148236481117063\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006948359109228477\n",
      "Training Loss: 0.00760611557692755\n",
      "Training Loss: 0.0067955404811073095\n",
      "Validation Loss: 0.004131793162230862\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006938366950489581\n",
      "Training Loss: 0.007596194248180837\n",
      "Training Loss: 0.006786246608244255\n",
      "Validation Loss: 0.004116626924657253\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006929090279736556\n",
      "Training Loss: 0.007586844484903849\n",
      "Training Loss: 0.006777504788478836\n",
      "Validation Loss: 0.004102618030659603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006920444554416463\n",
      "Training Loss: 0.007577998339547776\n",
      "Training Loss: 0.006769245231989771\n",
      "Validation Loss: 0.004089657170174832\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006912355726817623\n",
      "Training Loss: 0.0075695986195933075\n",
      "Training Loss: 0.006761407302692532\n",
      "Validation Loss: 0.004077646044198047\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006904759221943096\n",
      "Training Loss: 0.007561594631406479\n",
      "Training Loss: 0.006753940389025956\n",
      "Validation Loss: 0.004066493341259742\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006897598758805543\n",
      "Training Loss: 0.007553944418905303\n",
      "Training Loss: 0.006746800568653271\n",
      "Validation Loss: 0.004056120955835233\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.006890825757291168\n",
      "Training Loss: 0.007546610401477665\n",
      "Training Loss: 0.006739950256887823\n",
      "Validation Loss: 0.004046454269627339\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006884397861431353\n",
      "Training Loss: 0.007539559865253977\n",
      "Training Loss: 0.00673335867933929\n",
      "Validation Loss: 0.004037428014189675\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006878278062795289\n",
      "Training Loss: 0.007532766919466667\n",
      "Training Loss: 0.006726996555225923\n",
      "Validation Loss: 0.004028977190531539\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006872433495009318\n",
      "Training Loss: 0.007526205476024188\n",
      "Training Loss: 0.006720841805217787\n",
      "Validation Loss: 0.004021052914682064\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006866836189874448\n",
      "Training Loss: 0.007519855948630721\n",
      "Training Loss: 0.006714873732998967\n",
      "Validation Loss: 0.004013601243182012\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.00686146107269451\n",
      "Training Loss: 0.007513699380797334\n",
      "Training Loss: 0.006709075577091426\n",
      "Validation Loss: 0.004006579787773865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006856286935508251\n",
      "Training Loss: 0.007507720668800175\n",
      "Training Loss: 0.006703432253561914\n",
      "Validation Loss: 0.003999946458432614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006851293772342615\n",
      "Training Loss: 0.007501904728123918\n",
      "Training Loss: 0.00669793066335842\n",
      "Validation Loss: 0.00399366366739772\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006846465145936236\n",
      "Training Loss: 0.007496239794418216\n",
      "Training Loss: 0.006692559432704002\n",
      "Validation Loss: 0.003987701259176718\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006841787668527104\n",
      "Training Loss: 0.007490714383893646\n",
      "Training Loss: 0.006687309521948919\n",
      "Validation Loss: 0.00398202890300014\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006837245995411649\n",
      "Training Loss: 0.007485319651314057\n",
      "Training Loss: 0.006682171588763595\n",
      "Validation Loss: 0.003976616490464867\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0068328301020665096\n",
      "Training Loss: 0.007480045695556328\n",
      "Training Loss: 0.006677137998631224\n",
      "Validation Loss: 0.0039714441551000216\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.006828529324848205\n",
      "Training Loss: 0.007474885240080767\n",
      "Training Loss: 0.006672202110057697\n",
      "Validation Loss: 0.003966488428099939\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006824333977419883\n",
      "Training Loss: 0.007469831585185602\n",
      "Training Loss: 0.006667357310652733\n",
      "Validation Loss: 0.003961725908676895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006820235509658232\n",
      "Training Loss: 0.007464877491584048\n",
      "Training Loss: 0.006662599371047691\n",
      "Validation Loss: 0.003957141392205036\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0068162274884525685\n",
      "Training Loss: 0.007460016466211528\n",
      "Training Loss: 0.006657921053702011\n",
      "Validation Loss: 0.003952718138422692\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.00681230231653899\n",
      "Training Loss: 0.007455244611483067\n",
      "Training Loss: 0.0066533185739535835\n",
      "Validation Loss: 0.003948441913267702\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006808454120764509\n",
      "Training Loss: 0.007450555188115686\n",
      "Training Loss: 0.006648787831654773\n",
      "Validation Loss: 0.0039443007926718236\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.0068046773370588195\n",
      "Training Loss: 0.00744594402029179\n",
      "Training Loss: 0.00664432390127331\n",
      "Validation Loss: 0.003940280432697762\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006800967130693607\n",
      "Training Loss: 0.007441406433936208\n",
      "Training Loss: 0.0066399238060694185\n",
      "Validation Loss: 0.003936369024384557\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006797318122116849\n",
      "Training Loss: 0.007436938082100823\n",
      "Training Loss: 0.006635582857998088\n",
      "Validation Loss: 0.003932559109303389\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.0067937263788189735\n",
      "Training Loss: 0.0074325341440271584\n",
      "Training Loss: 0.00663129796856083\n",
      "Validation Loss: 0.003928839569137002\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00679018723545596\n",
      "Training Loss: 0.0074281912355218084\n",
      "Training Loss: 0.006627065259963274\n",
      "Validation Loss: 0.003925205606004495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006786697474890389\n",
      "Training Loss: 0.007423905008472502\n",
      "Training Loss: 0.0066228814842179415\n",
      "Validation Loss: 0.003921647904064046\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006783253246103413\n",
      "Training Loss: 0.00741967266483698\n",
      "Training Loss: 0.0066187431348953395\n",
      "Validation Loss: 0.0039181580369392136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006779850276070647\n",
      "Training Loss: 0.007415489249979146\n",
      "Training Loss: 0.006614647471578792\n",
      "Validation Loss: 0.003914733676323562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006776486786548048\n",
      "Training Loss: 0.007411351808113978\n",
      "Training Loss: 0.006610590299824253\n",
      "Validation Loss: 0.003911366785635774\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006773158343276009\n",
      "Training Loss: 0.007407256794977002\n",
      "Training Loss: 0.006606568685965613\n",
      "Validation Loss: 0.003908053469540698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006769862860091962\n",
      "Training Loss: 0.007403200439875945\n",
      "Training Loss: 0.006602579986210912\n",
      "Validation Loss: 0.003904789900721124\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.0067665970098460095\n",
      "Training Loss: 0.007399179860949516\n",
      "Training Loss: 0.006598621031735093\n",
      "Validation Loss: 0.0039015692172132514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006763358148746192\n",
      "Training Loss: 0.007395191182731651\n",
      "Training Loss: 0.0065946883405558765\n",
      "Validation Loss: 0.0038983908330163593\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006760143314022571\n",
      "Training Loss: 0.0073912306380225345\n",
      "Training Loss: 0.006590777962701395\n",
      "Validation Loss: 0.0038952484407744716\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006756950393901206\n",
      "Training Loss: 0.007387296633678489\n",
      "Training Loss: 0.006586888617603108\n",
      "Validation Loss: 0.00389214135507603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006753776681725867\n",
      "Training Loss: 0.007383384107379242\n",
      "Training Loss: 0.006583016574149951\n",
      "Validation Loss: 0.0038890627531983544\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006750619553495199\n",
      "Training Loss: 0.007379490763996728\n",
      "Training Loss: 0.0065791580628138035\n",
      "Validation Loss: 0.0038860150249256345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006747476574964821\n",
      "Training Loss: 0.007375614267657511\n",
      "Training Loss: 0.006575311732012779\n",
      "Validation Loss: 0.0038829934613235045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006744346169289202\n",
      "Training Loss: 0.007371750015299767\n",
      "Training Loss: 0.006571473818039522\n",
      "Validation Loss: 0.0038799965005953995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006741225698497146\n",
      "Training Loss: 0.007367895799688995\n",
      "Training Loss: 0.006567641750443727\n",
      "Validation Loss: 0.0038770210950143554\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006738112830207683\n",
      "Training Loss: 0.0073640498053282495\n",
      "Training Loss: 0.006563813514076173\n",
      "Validation Loss: 0.0038740641092149058\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006735006739618256\n",
      "Training Loss: 0.007360208223690279\n",
      "Training Loss: 0.006559985830681399\n",
      "Validation Loss: 0.0038711271481588483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006731904193293304\n",
      "Training Loss: 0.007356368651962839\n",
      "Training Loss: 0.0065561575058382\n",
      "Validation Loss: 0.003868207513365183\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.00672880404337775\n",
      "Training Loss: 0.007352529217023403\n",
      "Training Loss: 0.006552325368393213\n",
      "Validation Loss: 0.003865306270884329\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006725704613490962\n",
      "Training Loss: 0.007348687619669363\n",
      "Training Loss: 0.00654848906211555\n",
      "Validation Loss: 0.0038624220067279393\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006722605086979456\n",
      "Training Loss: 0.007344842128222808\n",
      "Training Loss: 0.006544644370442256\n",
      "Validation Loss: 0.003859549004772908\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.0067195028607966376\n",
      "Training Loss: 0.007340990821830928\n",
      "Training Loss: 0.006540791736915708\n",
      "Validation Loss: 0.0038566928032492654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006716398083372041\n",
      "Training Loss: 0.007337131521780975\n",
      "Training Loss: 0.006536929435096681\n",
      "Validation Loss: 0.0038538505336311595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.0067132881045108665\n",
      "Training Loss: 0.007333263775799423\n",
      "Training Loss: 0.006533056505722925\n",
      "Validation Loss: 0.0038510220507264473\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006710172505700029\n",
      "Training Loss: 0.007329386041383259\n",
      "Training Loss: 0.0065291717229411005\n",
      "Validation Loss: 0.0038482059523191176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006707050707773305\n",
      "Training Loss: 0.007325496224802918\n",
      "Training Loss: 0.006525274323066697\n",
      "Validation Loss: 0.0038454049054972746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006703922086162492\n",
      "Training Loss: 0.007321595176472328\n",
      "Training Loss: 0.006521363166393712\n",
      "Validation Loss: 0.0038426141110719757\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0067007842351449656\n",
      "Training Loss: 0.007317680912092328\n",
      "Training Loss: 0.00651743829715997\n",
      "Validation Loss: 0.00383983794442807\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006697637370089069\n",
      "Training Loss: 0.007313753991038539\n",
      "Training Loss: 0.006513499580323696\n",
      "Validation Loss: 0.0038370746645678797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.00669448213535361\n",
      "Training Loss: 0.00730981242202688\n",
      "Training Loss: 0.0065095485642086715\n",
      "Validation Loss: 0.0038343228104137134\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006691315083880909\n",
      "Training Loss: 0.007305857024039142\n",
      "Training Loss: 0.006505581646924838\n",
      "Validation Loss: 0.0038315853512102967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006688138459576293\n",
      "Training Loss: 0.007301887463545427\n",
      "Training Loss: 0.006501601639902219\n",
      "Validation Loss: 0.0038288593451816883\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006684950195485726\n",
      "Training Loss: 0.007297903052531183\n",
      "Training Loss: 0.006497608203208074\n",
      "Validation Loss: 0.0038261468040892916\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0066817502968478945\n",
      "Training Loss: 0.007293903436511755\n",
      "Training Loss: 0.006493601150577888\n",
      "Validation Loss: 0.0038234472949727535\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006678538011619821\n",
      "Training Loss: 0.0072898891824297605\n",
      "Training Loss: 0.006489580813795328\n",
      "Validation Loss: 0.0038207572181133574\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006675312114530243\n",
      "Training Loss: 0.007285859681433067\n",
      "Training Loss: 0.006485548997297883\n",
      "Validation Loss: 0.0038180788442638987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.0066720741998869924\n",
      "Training Loss: 0.007281814943416976\n",
      "Training Loss: 0.006481504325056449\n",
      "Validation Loss: 0.003815409210719838\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.00666882046381943\n",
      "Training Loss: 0.007277755124378018\n",
      "Training Loss: 0.006477447290671989\n",
      "Validation Loss: 0.0038127518478739126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006665553060593084\n",
      "Training Loss: 0.007273679100326263\n",
      "Training Loss: 0.006473378239898011\n",
      "Validation Loss: 0.0038101013456837514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006662270009401255\n",
      "Training Loss: 0.007269587020855397\n",
      "Training Loss: 0.006469297951553017\n",
      "Validation Loss: 0.003807461399354794\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006658970810822212\n",
      "Training Loss: 0.00726547694823239\n",
      "Training Loss: 0.006465204722480848\n",
      "Validation Loss: 0.003804825480472757\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.0066556543041951955\n",
      "Training Loss: 0.007261351030319929\n",
      "Training Loss: 0.0064611009159125385\n",
      "Validation Loss: 0.0038021968264300167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0066523195966146885\n",
      "Training Loss: 0.007257206236827188\n",
      "Training Loss: 0.006456984783289954\n",
      "Validation Loss: 0.003799574168430369\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006648966838256456\n",
      "Training Loss: 0.0072530423203716055\n",
      "Training Loss: 0.006452856978867203\n",
      "Validation Loss: 0.003796954677189083\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006645593675784767\n",
      "Training Loss: 0.007248858509701677\n",
      "Training Loss: 0.006448716376908124\n",
      "Validation Loss: 0.0037943348903313615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006642199975322001\n",
      "Training Loss: 0.007244654136011377\n",
      "Training Loss: 0.006444563191616908\n",
      "Validation Loss: 0.003791718072718365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006638784079113975\n",
      "Training Loss: 0.0072404277691384775\n",
      "Training Loss: 0.006440396015532315\n",
      "Validation Loss: 0.0037890962976439113\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006635344566311687\n",
      "Training Loss: 0.007236177244922146\n",
      "Training Loss: 0.006436215434223414\n",
      "Validation Loss: 0.0037864765945005785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006631880911299959\n",
      "Training Loss: 0.007231902829371393\n",
      "Training Loss: 0.006432020142674446\n",
      "Validation Loss: 0.003783850620626315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006628391403355635\n",
      "Training Loss: 0.007227602144703269\n",
      "Training Loss: 0.00642780939117074\n",
      "Validation Loss: 0.0037812208521357748\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006624875651323236\n",
      "Training Loss: 0.007223273892886937\n",
      "Training Loss: 0.006423581601120531\n",
      "Validation Loss: 0.00377858051339562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006621330620255321\n",
      "Training Loss: 0.007218915624544024\n",
      "Training Loss: 0.006419337665429339\n",
      "Validation Loss: 0.0037759323081189996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.00661775546730496\n",
      "Training Loss: 0.007214526787865907\n",
      "Training Loss: 0.006415074961259961\n",
      "Validation Loss: 0.003773270674531212\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006614149459637702\n",
      "Training Loss: 0.007210104201803915\n",
      "Training Loss: 0.006410791222006083\n",
      "Validation Loss: 0.003770598294108771\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006610509819001891\n",
      "Training Loss: 0.007205647402442992\n",
      "Training Loss: 0.006406486693304032\n",
      "Validation Loss: 0.003767913877126876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006606835077400319\n",
      "Training Loss: 0.007201152150519192\n",
      "Training Loss: 0.006402159046847373\n",
      "Validation Loss: 0.003765212105105767\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.0066031237307470295\n",
      "Training Loss: 0.007196617642184719\n",
      "Training Loss: 0.006397807362955064\n",
      "Validation Loss: 0.003762493467251404\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006599374145735055\n",
      "Training Loss: 0.007192042048554868\n",
      "Training Loss: 0.0063934293796774\n",
      "Validation Loss: 0.0037597513409708154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.0065955836704233665\n",
      "Training Loss: 0.007187421160633676\n",
      "Training Loss: 0.006389022454386577\n",
      "Validation Loss: 0.003756989082687775\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006591749611543491\n",
      "Training Loss: 0.007182752542430535\n",
      "Training Loss: 0.006384586025960743\n",
      "Validation Loss: 0.0037542010508373044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.0065878711274126546\n",
      "Training Loss: 0.007178034783573821\n",
      "Training Loss: 0.006380117650842294\n",
      "Validation Loss: 0.003751390295762443\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006583945251186378\n",
      "Training Loss: 0.007173263535369187\n",
      "Training Loss: 0.006375613835407421\n",
      "Validation Loss: 0.0037485507730227173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006579968997393735\n",
      "Training Loss: 0.007168436950305477\n",
      "Training Loss: 0.006371072771726176\n",
      "Validation Loss: 0.0037456782328048614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006575940284528769\n",
      "Training Loss: 0.007163550519617274\n",
      "Training Loss: 0.006366492415545508\n",
      "Validation Loss: 0.003742774805901677\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.0065718563162954526\n",
      "Training Loss: 0.007158600789844058\n",
      "Training Loss: 0.006361869295360521\n",
      "Validation Loss: 0.003739837069179486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006567713319673203\n",
      "Training Loss: 0.007153585184714757\n",
      "Training Loss: 0.006357199844205752\n",
      "Validation Loss: 0.0037368602731547664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.0065635091089643535\n",
      "Training Loss: 0.007148499493487179\n",
      "Training Loss: 0.006352482441579923\n",
      "Validation Loss: 0.0037338444649168616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006559240173082799\n",
      "Training Loss: 0.00714333871321287\n",
      "Training Loss: 0.006347712469287217\n",
      "Validation Loss: 0.003730788932893467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.0065549028763780375\n",
      "Training Loss: 0.007138099928270094\n",
      "Training Loss: 0.006342886803904549\n",
      "Validation Loss: 0.0037276849838685285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006550492561655119\n",
      "Training Loss: 0.007132776783546433\n",
      "Training Loss: 0.006338001523399726\n",
      "Validation Loss: 0.0037245356721091975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006546006463468075\n",
      "Training Loss: 0.007127365616615862\n",
      "Training Loss: 0.006333051235415042\n",
      "Validation Loss: 0.0037213375051558186\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006541439004358836\n",
      "Training Loss: 0.007121860879706219\n",
      "Training Loss: 0.0063280339119955896\n",
      "Validation Loss: 0.003718082071020362\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006536786240176297\n",
      "Training Loss: 0.0071162573253968735\n",
      "Training Loss: 0.006322942567057907\n",
      "Validation Loss: 0.0037147684998580077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006532042297185399\n",
      "Training Loss: 0.007110549156786874\n",
      "Training Loss: 0.006317773114424199\n",
      "Validation Loss: 0.0037113950310505173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006527203391306103\n",
      "Training Loss: 0.007104728456470184\n",
      "Training Loss: 0.00631251965998672\n",
      "Validation Loss: 0.003707957460565932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006522262499202043\n",
      "Training Loss: 0.007098789300653152\n",
      "Training Loss: 0.0063071767706424\n",
      "Validation Loss: 0.003704449730883405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006517214962514117\n",
      "Training Loss: 0.007092724222457036\n",
      "Training Loss: 0.006301737730391324\n",
      "Validation Loss: 0.0037008693423436095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006512051752069965\n",
      "Training Loss: 0.007086525647900998\n",
      "Training Loss: 0.006296195217873901\n",
      "Validation Loss: 0.003697209782228711\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006506766464444809\n",
      "Training Loss: 0.007080183840007521\n",
      "Training Loss: 0.006290542238857597\n",
      "Validation Loss: 0.0036934697542737207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006501352494815364\n",
      "Training Loss: 0.007073691043769941\n",
      "Training Loss: 0.006284771170467138\n",
      "Validation Loss: 0.0036896387810996744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006495800275588408\n",
      "Training Loss: 0.007067035412183032\n",
      "Training Loss: 0.00627887275069952\n",
      "Validation Loss: 0.0036857133820192532\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006490101240342483\n",
      "Training Loss: 0.0070602084579877555\n",
      "Training Loss: 0.006272838418371975\n",
      "Validation Loss: 0.003681686755238373\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006484245774336159\n",
      "Training Loss: 0.007053197180503048\n",
      "Training Loss: 0.006266658023232594\n",
      "Validation Loss: 0.0036775504913850784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006478222514269874\n",
      "Training Loss: 0.0070459879463305695\n",
      "Training Loss: 0.006260319019202143\n",
      "Validation Loss: 0.003673297961326211\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006472020030487329\n",
      "Training Loss: 0.007038568194839172\n",
      "Training Loss: 0.006253810235066339\n",
      "Validation Loss: 0.003668918597428233\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006465625608689152\n",
      "Training Loss: 0.007030921511468478\n",
      "Training Loss: 0.006247119015315547\n",
      "Validation Loss: 0.003664405625365842\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006459025453077629\n",
      "Training Loss: 0.007023033314617351\n",
      "Training Loss: 0.006240230666007846\n",
      "Validation Loss: 0.0036597479228976737\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006452205989626236\n",
      "Training Loss: 0.00701488325605169\n",
      "Training Loss: 0.006233129693428054\n",
      "Validation Loss: 0.003654932527373848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006445148814818822\n",
      "Training Loss: 0.007006453279755079\n",
      "Training Loss: 0.006225799204548821\n",
      "Validation Loss: 0.0036499499345951703\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006437838046113029\n",
      "Training Loss: 0.006997721993247979\n",
      "Training Loss: 0.006218221392482519\n",
      "Validation Loss: 0.003644783927514898\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006430252962745726\n",
      "Training Loss: 0.006988666646066122\n",
      "Training Loss: 0.006210375278024003\n",
      "Validation Loss: 0.003639421678210996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006422374434769154\n",
      "Training Loss: 0.0069792624562978745\n",
      "Training Loss: 0.006202241732971743\n",
      "Validation Loss: 0.0036338428545513012\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006414178856648505\n",
      "Training Loss: 0.0069694813899695875\n",
      "Training Loss: 0.006193796588340774\n",
      "Validation Loss: 0.0036280290995400104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006405643103062175\n",
      "Training Loss: 0.006959296151180751\n",
      "Training Loss: 0.006185016232775524\n",
      "Validation Loss: 0.0036219657814716187\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006396741184289567\n",
      "Training Loss: 0.00694867507845629\n",
      "Training Loss: 0.006175873454194516\n",
      "Validation Loss: 0.003615628368106116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006387446051812731\n",
      "Training Loss: 0.0069375855685211716\n",
      "Training Loss: 0.00616634166566655\n",
      "Validation Loss: 0.003608991454742598\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006377729748492129\n",
      "Training Loss: 0.006925993990735151\n",
      "Training Loss: 0.006156392351258546\n",
      "Validation Loss: 0.003602030004238647\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.0063675610325299205\n",
      "Training Loss: 0.006913864840753376\n",
      "Training Loss: 0.0061459971615113315\n",
      "Validation Loss: 0.0035947196857920023\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006356910894974135\n",
      "Training Loss: 0.006901163351722061\n",
      "Training Loss: 0.00613512716605328\n",
      "Validation Loss: 0.003587028644769714\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006345748720923439\n",
      "Training Loss: 0.006887853625230491\n",
      "Training Loss: 0.00612375343684107\n",
      "Validation Loss: 0.0035789300873352403\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.0063340455025900156\n",
      "Training Loss: 0.006873900657519698\n",
      "Training Loss: 0.006111849989974871\n",
      "Validation Loss: 0.003570395460341837\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006321772070368752\n",
      "Training Loss: 0.006859274741727859\n",
      "Training Loss: 0.006099394493503496\n",
      "Validation Loss: 0.003561392786201048\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006308906796621158\n",
      "Training Loss: 0.006843949636095204\n",
      "Training Loss: 0.00608636925346218\n",
      "Validation Loss: 0.0035519005110738484\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006295430324971676\n",
      "Training Loss: 0.0068279077467741445\n",
      "Training Loss: 0.006072762816911563\n",
      "Validation Loss: 0.003541893595201748\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006281332582002506\n",
      "Training Loss: 0.006811140139470808\n",
      "Training Loss: 0.006058574584312737\n",
      "Validation Loss: 0.0035313556292565183\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006266612337203697\n",
      "Training Loss: 0.006793654321809299\n",
      "Training Loss: 0.006043816073797643\n",
      "Validation Loss: 0.0035202816675490374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0062512823462020604\n",
      "Training Loss: 0.006775465915561654\n",
      "Training Loss: 0.006028512134216726\n",
      "Validation Loss: 0.003508672326164885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006235367809422314\n",
      "Training Loss: 0.006756620314554311\n",
      "Training Loss: 0.006012702131411061\n",
      "Validation Loss: 0.0034965461700777996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006218911780160851\n",
      "Training Loss: 0.006737175898160785\n",
      "Training Loss: 0.005996445304481313\n",
      "Validation Loss: 0.003483932802788495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006201976219890639\n",
      "Training Loss: 0.00671721558959689\n",
      "Training Loss: 0.005979816087055952\n",
      "Validation Loss: 0.003470882207019192\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006184636225807481\n",
      "Training Loss: 0.006696842755191028\n",
      "Training Loss: 0.0059629044216126205\n",
      "Validation Loss: 0.003457460783976601\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006166984777664765\n",
      "Training Loss: 0.0066761774313636125\n",
      "Training Loss: 0.005945809644181281\n",
      "Validation Loss: 0.003443742259876447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006149125048541464\n",
      "Training Loss: 0.0066553547553485255\n",
      "Training Loss: 0.0059286419802810995\n",
      "Validation Loss: 0.003429816477262321\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.0061311664793174715\n",
      "Training Loss: 0.006634517257334664\n",
      "Training Loss: 0.005911508860299364\n",
      "Validation Loss: 0.003415780684785143\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006113218406680971\n",
      "Training Loss: 0.00661380166944582\n",
      "Training Loss: 0.005894511196529493\n",
      "Validation Loss: 0.003401722095833484\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006095381993800402\n",
      "Training Loss: 0.0065933396125910804\n",
      "Training Loss: 0.005877739102579653\n",
      "Validation Loss: 0.003387727437206031\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006077747776289471\n",
      "Training Loss: 0.006573242833837867\n",
      "Training Loss: 0.005861264128470794\n",
      "Validation Loss: 0.003373869591434434\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006060388401965611\n",
      "Training Loss: 0.006553603786742314\n",
      "Training Loss: 0.00584513864829205\n",
      "Validation Loss: 0.0033602045647527896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006043358371243812\n",
      "Training Loss: 0.006534488067845814\n",
      "Training Loss: 0.005829393626190722\n",
      "Validation Loss: 0.0033467703935784405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006026689804857596\n",
      "Training Loss: 0.006515938789234497\n",
      "Training Loss: 0.005814038298558444\n",
      "Validation Loss: 0.003333589335474489\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.0060103997867554425\n",
      "Training Loss: 0.006497971567441709\n",
      "Training Loss: 0.0057990670797880735\n",
      "Validation Loss: 0.003320665347253841\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005994484932743944\n",
      "Training Loss: 0.006480583566008136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [17:12<17:13, 206.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.005784459189744667\n",
      "Validation Loss: 0.003307985930619866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.22237679440528155\n",
      "Training Loss: 0.15236600082367657\n",
      "Training Loss: 0.1123185182735324\n",
      "Validation Loss: 0.0781582903242513\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.07532366848550737\n",
      "Training Loss: 0.0603836006578058\n",
      "Training Loss: 0.054111654330044986\n",
      "Validation Loss: 0.04384766554648287\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.05055562512949109\n",
      "Training Loss: 0.048224672144278884\n",
      "Training Loss: 0.0449829291831702\n",
      "Validation Loss: 0.0367142776383108\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.042162460759282115\n",
      "Training Loss: 0.039630705313757064\n",
      "Training Loss: 0.03604646421968937\n",
      "Validation Loss: 0.028920735379032204\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.03252816268242895\n",
      "Training Loss: 0.02959971683099866\n",
      "Training Loss: 0.026269281343556942\n",
      "Validation Loss: 0.02121936512085494\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.02342967445962131\n",
      "Training Loss: 0.02135326536372304\n",
      "Training Loss: 0.019320483906194567\n",
      "Validation Loss: 0.016460492537262734\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.018235016032122076\n",
      "Training Loss: 0.017268294955138118\n",
      "Training Loss: 0.015986177790910005\n",
      "Validation Loss: 0.014000959171170599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.01571705376263708\n",
      "Training Loss: 0.01524598136311397\n",
      "Training Loss: 0.014120229801628739\n",
      "Validation Loss: 0.012375544992097642\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.014097342938184739\n",
      "Training Loss: 0.01383533188374713\n",
      "Training Loss: 0.012693657574709504\n",
      "Validation Loss: 0.010973646781627046\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.012682154795620591\n",
      "Training Loss: 0.01247970035066828\n",
      "Training Loss: 0.01128811963251792\n",
      "Validation Loss: 0.00961883973560474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.011315889216493815\n",
      "Training Loss: 0.011260264945449308\n",
      "Training Loss: 0.010224986881949007\n",
      "Validation Loss: 0.008651624684839437\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.010456167014781385\n",
      "Training Loss: 0.010546745847677811\n",
      "Training Loss: 0.009607078594854102\n",
      "Validation Loss: 0.007920627838021584\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.009895089264027775\n",
      "Training Loss: 0.010074295158265158\n",
      "Training Loss: 0.009172639183234424\n",
      "Validation Loss: 0.007355194949555431\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.009473306476138532\n",
      "Training Loss: 0.009717701721237973\n",
      "Training Loss: 0.008830225956626237\n",
      "Validation Loss: 0.006894923058558214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.00912752777338028\n",
      "Training Loss: 0.009424256576457991\n",
      "Training Loss: 0.008541167872026563\n",
      "Validation Loss: 0.006503949219130733\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.008830569013953208\n",
      "Training Loss: 0.009171841300558299\n",
      "Training Loss: 0.008289786087116226\n",
      "Validation Loss: 0.006166011291252596\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.008571164002642035\n",
      "Training Loss: 0.00895146742113866\n",
      "Training Loss: 0.008069885376607999\n",
      "Validation Loss: 0.005873764059349393\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.008344658518908546\n",
      "Training Loss: 0.008759620804339648\n",
      "Training Loss: 0.007878943290561437\n",
      "Validation Loss: 0.005623235853828406\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.008148843015078455\n",
      "Training Loss: 0.008594497659942135\n",
      "Training Loss: 0.007715148320421576\n",
      "Validation Loss: 0.005410680416552873\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.0079816326755099\n",
      "Training Loss: 0.008453980033518747\n",
      "Training Loss: 0.007575944403652102\n",
      "Validation Loss: 0.005231216309194485\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.007839996602851897\n",
      "Training Loss: 0.008334977049380542\n",
      "Training Loss: 0.007457835805835203\n",
      "Validation Loss: 0.005079076743130101\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0077200512471608815\n",
      "Training Loss: 0.008233814825071023\n",
      "Training Loss: 0.007357017856556922\n",
      "Validation Loss: 0.004948666473599464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.0076177674299106\n",
      "Training Loss: 0.008146964350016788\n",
      "Training Loss: 0.007270074336556718\n",
      "Validation Loss: 0.0048353929056922035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.007529622501460835\n",
      "Training Loss: 0.008071518528740853\n",
      "Training Loss: 0.007194326443132013\n",
      "Validation Loss: 0.004735902909273177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.007452876222087071\n",
      "Training Loss: 0.008005297714844347\n",
      "Training Loss: 0.00712782034301199\n",
      "Validation Loss: 0.004647871877035398\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007385510395979508\n",
      "Training Loss: 0.007946724985376932\n",
      "Training Loss: 0.007069133755285293\n",
      "Validation Loss: 0.004569655438633949\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.0073260309954639525\n",
      "Training Loss: 0.007894629406509921\n",
      "Training Loss: 0.007017178180394695\n",
      "Validation Loss: 0.004499988643494383\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.0072732780035585165\n",
      "Training Loss: 0.007848094000364654\n",
      "Training Loss: 0.00697105796309188\n",
      "Validation Loss: 0.004437815672665667\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.007226300917100161\n",
      "Training Loss: 0.007806356105138548\n",
      "Training Loss: 0.0069299973244778814\n",
      "Validation Loss: 0.004382208821604426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.007184286746196449\n",
      "Training Loss: 0.007768760986509733\n",
      "Training Loss: 0.006893306830897927\n",
      "Validation Loss: 0.004332342661205637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007146530740428716\n",
      "Training Loss: 0.007734736548736691\n",
      "Training Loss: 0.006860374365933239\n",
      "Validation Loss: 0.004287485687292359\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007112420060439035\n",
      "Training Loss: 0.007703787714708596\n",
      "Training Loss: 0.006830660084960982\n",
      "Validation Loss: 0.004246992864802982\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007081428212113678\n",
      "Training Loss: 0.00767548437230289\n",
      "Training Loss: 0.006803697330178693\n",
      "Validation Loss: 0.004210314675485402\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007053106335224584\n",
      "Training Loss: 0.007649460117099807\n",
      "Training Loss: 0.006779085627058521\n",
      "Validation Loss: 0.004176968385549157\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007027076139347627\n",
      "Training Loss: 0.007625403928104788\n",
      "Training Loss: 0.0067564850137569014\n",
      "Validation Loss: 0.004146550867047286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007003019630210474\n",
      "Training Loss: 0.007603052038466558\n",
      "Training Loss: 0.0067356105695944276\n",
      "Validation Loss: 0.004118710684359827\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0069806728709954765\n",
      "Training Loss: 0.007582184243365191\n",
      "Training Loss: 0.006716220532543957\n",
      "Validation Loss: 0.004093147661233467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.0069598119670990855\n",
      "Training Loss: 0.0075626114726765085\n",
      "Training Loss: 0.00669811335275881\n",
      "Validation Loss: 0.004069600313158936\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006940250255865976\n",
      "Training Loss: 0.007544175870716572\n",
      "Training Loss: 0.006681116763502359\n",
      "Validation Loss: 0.004047841580553252\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006921830165665597\n",
      "Training Loss: 0.0075267421681201084\n",
      "Training Loss: 0.0066650893713813275\n",
      "Validation Loss: 0.004027676896788598\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006904418008634821\n",
      "Training Loss: 0.007510194659698754\n",
      "Training Loss: 0.006649906567763537\n",
      "Validation Loss: 0.004008927568270082\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006887898228596896\n",
      "Training Loss: 0.007494434818509035\n",
      "Training Loss: 0.0066354660678189245\n",
      "Validation Loss: 0.003991441916297661\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006872173080919311\n",
      "Training Loss: 0.0074793750571552665\n",
      "Training Loss: 0.006621677199145779\n",
      "Validation Loss: 0.003975082012259642\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0068571557279210535\n",
      "Training Loss: 0.007464940777863376\n",
      "Training Loss: 0.006608462968142703\n",
      "Validation Loss: 0.003959725440866994\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006842771887313574\n",
      "Training Loss: 0.007451066049397923\n",
      "Training Loss: 0.006595757366158068\n",
      "Validation Loss: 0.003945269595747918\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006828956104582176\n",
      "Training Loss: 0.007437693719402887\n",
      "Training Loss: 0.006583501821151003\n",
      "Validation Loss: 0.003931607539619036\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006815648673800752\n",
      "Training Loss: 0.007424770328216254\n",
      "Training Loss: 0.006571644656360149\n",
      "Validation Loss: 0.003918659628703772\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006802799499128014\n",
      "Training Loss: 0.007412251267232932\n",
      "Training Loss: 0.006560141500085592\n",
      "Validation Loss: 0.003906346164287978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.0067903609666973355\n",
      "Training Loss: 0.007400094336480834\n",
      "Training Loss: 0.0065489523496944455\n",
      "Validation Loss: 0.0038945980322955365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006778292130911722\n",
      "Training Loss: 0.007388263350003399\n",
      "Training Loss: 0.006538043561158701\n",
      "Validation Loss: 0.0038833539433295976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006766556159127504\n",
      "Training Loss: 0.00737672460032627\n",
      "Training Loss: 0.006527382814092562\n",
      "Validation Loss: 0.003872560052033723\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006755119684385136\n",
      "Training Loss: 0.007365447851479984\n",
      "Training Loss: 0.006516943229362368\n",
      "Validation Loss: 0.0038621614862421757\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.0067439514643047\n",
      "Training Loss: 0.00735440653457772\n",
      "Training Loss: 0.006506699399324134\n",
      "Validation Loss: 0.003852118193852098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006733025042922236\n",
      "Training Loss: 0.007343574102269486\n",
      "Training Loss: 0.006496630030451343\n",
      "Validation Loss: 0.003842393375849456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0067223143379669635\n",
      "Training Loss: 0.0073329302028287205\n",
      "Training Loss: 0.00648671378265135\n",
      "Validation Loss: 0.0038329444242264617\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.00671179799886886\n",
      "Training Loss: 0.007322453191736713\n",
      "Training Loss: 0.006476934201782569\n",
      "Validation Loss: 0.0038237478610211877\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006701455242000521\n",
      "Training Loss: 0.007312124928575941\n",
      "Training Loss: 0.006467276179464534\n",
      "Validation Loss: 0.003814776490829634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006691267607384361\n",
      "Training Loss: 0.007301929419045336\n",
      "Training Loss: 0.00645772332791239\n",
      "Validation Loss: 0.0038059980548353174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006681217680452391\n",
      "Training Loss: 0.007291850237525068\n",
      "Training Loss: 0.006448265051003546\n",
      "Validation Loss: 0.0037973961711787944\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.0066712909087073054\n",
      "Training Loss: 0.007281873239553534\n",
      "Training Loss: 0.006438888906268403\n",
      "Validation Loss: 0.003788948477653975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006661471701227129\n",
      "Training Loss: 0.007271986083942465\n",
      "Training Loss: 0.006429583225399256\n",
      "Validation Loss: 0.0037806367376044896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006651747495634481\n",
      "Training Loss: 0.007262176959193312\n",
      "Training Loss: 0.006420339901233092\n",
      "Validation Loss: 0.0037724461885211007\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006642106737708673\n",
      "Training Loss: 0.007252435404807329\n",
      "Training Loss: 0.006411150507628918\n",
      "Validation Loss: 0.0037643634321774993\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006632539039710537\n",
      "Training Loss: 0.007242751740850508\n",
      "Training Loss: 0.006402006552089006\n",
      "Validation Loss: 0.0037563702240690925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0066230335022555665\n",
      "Training Loss: 0.007233117354335263\n",
      "Training Loss: 0.006392902847146616\n",
      "Validation Loss: 0.0037484681887782357\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006613582767895423\n",
      "Training Loss: 0.007223523764405399\n",
      "Training Loss: 0.00638383105979301\n",
      "Validation Loss: 0.003740627695335431\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0066041758155915885\n",
      "Training Loss: 0.007213963627582416\n",
      "Training Loss: 0.006374786522937939\n",
      "Validation Loss: 0.003732858635118922\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0065948071877937765\n",
      "Training Loss: 0.007204430325655267\n",
      "Training Loss: 0.006365763081703335\n",
      "Validation Loss: 0.0037251381253237638\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006585468470584601\n",
      "Training Loss: 0.007194917211891152\n",
      "Training Loss: 0.00635675685480237\n",
      "Validation Loss: 0.0037174649010778644\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006576153694768436\n",
      "Training Loss: 0.007185418904409744\n",
      "Training Loss: 0.006347763618687168\n",
      "Validation Loss: 0.0037098300978122803\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006566857057623565\n",
      "Training Loss: 0.007175930651719682\n",
      "Training Loss: 0.006338779096258804\n",
      "Validation Loss: 0.003702227767881299\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006557572793681175\n",
      "Training Loss: 0.007166446467163041\n",
      "Training Loss: 0.006329799104714766\n",
      "Validation Loss: 0.0036946553828487738\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006548295264365151\n",
      "Training Loss: 0.007156963771558366\n",
      "Training Loss: 0.00632082094438374\n",
      "Validation Loss: 0.003687101132445707\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006539019652991556\n",
      "Training Loss: 0.007147477076505311\n",
      "Training Loss: 0.006311841368442401\n",
      "Validation Loss: 0.0036795634561835717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006529742907732725\n",
      "Training Loss: 0.007137984387227334\n",
      "Training Loss: 0.006302858025301248\n",
      "Validation Loss: 0.003672038912616168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006520460274768994\n",
      "Training Loss: 0.007128481798572466\n",
      "Training Loss: 0.006293868141947314\n",
      "Validation Loss: 0.0036645210949766836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006511167719727382\n",
      "Training Loss: 0.007118965461268089\n",
      "Training Loss: 0.006284869343508035\n",
      "Validation Loss: 0.003657005612183823\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0065018619928741824\n",
      "Training Loss: 0.0071094347961479796\n",
      "Training Loss: 0.0062758590711746364\n",
      "Validation Loss: 0.0036494884694915977\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.0064925400383071975\n",
      "Training Loss: 0.007099884783965536\n",
      "Training Loss: 0.0062668357230722905\n",
      "Validation Loss: 0.0036419706073788443\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.0064831991319078955\n",
      "Training Loss: 0.007090314888046123\n",
      "Training Loss: 0.00625779767637141\n",
      "Validation Loss: 0.0036344455916099668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0064738365571247415\n",
      "Training Loss: 0.007080723139224574\n",
      "Training Loss: 0.0062487425841391084\n",
      "Validation Loss: 0.003626911180208992\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006464450166677124\n",
      "Training Loss: 0.0070711082348134365\n",
      "Training Loss: 0.006239669464994222\n",
      "Validation Loss: 0.0036193635667873063\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006455038272542879\n",
      "Training Loss: 0.007061467823805287\n",
      "Training Loss: 0.006230578306131065\n",
      "Validation Loss: 0.0036118094079468526\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.0064455976616591215\n",
      "Training Loss: 0.007051801825291477\n",
      "Training Loss: 0.006221464705886319\n",
      "Validation Loss: 0.003604230774420031\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.00643612775427755\n",
      "Training Loss: 0.007042108636815101\n",
      "Training Loss: 0.006212329999543726\n",
      "Validation Loss: 0.003596635216198276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006426626066677273\n",
      "Training Loss: 0.007032387003418989\n",
      "Training Loss: 0.006203171466477215\n",
      "Validation Loss: 0.0035890251701551205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006417092134361155\n",
      "Training Loss: 0.0070226374245248736\n",
      "Training Loss: 0.006193990606116131\n",
      "Validation Loss: 0.0035813916069089196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.0064075249718735\n",
      "Training Loss: 0.0070128589821979405\n",
      "Training Loss: 0.006184785184450448\n",
      "Validation Loss: 0.00357373563698336\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006397923118202016\n",
      "Training Loss: 0.00700305141042918\n",
      "Training Loss: 0.00617555407457985\n",
      "Validation Loss: 0.0035660566704536086\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006388286811416038\n",
      "Training Loss: 0.0069932153634727\n",
      "Training Loss: 0.006166297945892438\n",
      "Validation Loss: 0.0035583543240647305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006378613960696384\n",
      "Training Loss: 0.006983349663787521\n",
      "Training Loss: 0.006157016142969951\n",
      "Validation Loss: 0.0035506314297175305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006368906400166452\n",
      "Training Loss: 0.006973456797422842\n",
      "Training Loss: 0.0061477075587026775\n",
      "Validation Loss: 0.0035428762922502972\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006359160978463479\n",
      "Training Loss: 0.006963535542599857\n",
      "Training Loss: 0.006138372713467106\n",
      "Validation Loss: 0.00353509945444386\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006349380487808958\n",
      "Training Loss: 0.006953587883035652\n",
      "Training Loss: 0.006129012455930933\n",
      "Validation Loss: 0.0035272977050665893\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006339564927038736\n",
      "Training Loss: 0.006943614110350609\n",
      "Training Loss: 0.006119625855935737\n",
      "Validation Loss: 0.00351946946270136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006329713306040503\n",
      "Training Loss: 0.006933615553425625\n",
      "Training Loss: 0.006110214296495542\n",
      "Validation Loss: 0.003511620264270165\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.0063198273698799315\n",
      "Training Loss: 0.006923593519022688\n",
      "Training Loss: 0.006100776542443782\n",
      "Validation Loss: 0.0035037442379934566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006309907644172199\n",
      "Training Loss: 0.00691354900598526\n",
      "Training Loss: 0.006091313661308959\n",
      "Validation Loss: 0.0034958436389275816\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.00629995423485525\n",
      "Training Loss: 0.0069034841406391935\n",
      "Training Loss: 0.0060818273678887634\n",
      "Validation Loss: 0.0034879237384629553\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.00628996932588052\n",
      "Training Loss: 0.006893401019624434\n",
      "Training Loss: 0.006072316697100177\n",
      "Validation Loss: 0.003479978038270152\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006279953482444398\n",
      "Training Loss: 0.00688330037984997\n",
      "Training Loss: 0.0060627825825940816\n",
      "Validation Loss: 0.003472009183200641\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006269907286041416\n",
      "Training Loss: 0.006873183760326355\n",
      "Training Loss: 0.006053226679796353\n",
      "Validation Loss: 0.0034640198207219666\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006259832751238719\n",
      "Training Loss: 0.006863053872366436\n",
      "Training Loss: 0.0060436494275927545\n",
      "Validation Loss: 0.0034560122333592578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006249731591087766\n",
      "Training Loss: 0.0068529135815333575\n",
      "Training Loss: 0.0060340514278505\n",
      "Validation Loss: 0.0034479863138534546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006239605654845945\n",
      "Training Loss: 0.006842764043831266\n",
      "Training Loss: 0.006024434034479782\n",
      "Validation Loss: 0.0034399410746363776\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006229455913999118\n",
      "Training Loss: 0.006832606934476643\n",
      "Training Loss: 0.006014798833057285\n",
      "Validation Loss: 0.0034318804175833637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.0062192847253754735\n",
      "Training Loss: 0.006822445929283276\n",
      "Training Loss: 0.006005146091338247\n",
      "Validation Loss: 0.003423802222365911\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006209093576180749\n",
      "Training Loss: 0.006812282094615512\n",
      "Training Loss: 0.00599547742633149\n",
      "Validation Loss: 0.003415709850639942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006198884498444386\n",
      "Training Loss: 0.006802118642372079\n",
      "Training Loss: 0.005985795099986717\n",
      "Validation Loss: 0.003407609536511342\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006188660420011729\n",
      "Training Loss: 0.006791958031244576\n",
      "Training Loss: 0.005976100075058639\n",
      "Validation Loss: 0.00339949453050752\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006178422425873578\n",
      "Training Loss: 0.006781802350305952\n",
      "Training Loss: 0.005966392079135403\n",
      "Validation Loss: 0.0033913679209045984\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0061681725061498585\n",
      "Training Loss: 0.006771653237519786\n",
      "Training Loss: 0.005956674782792107\n",
      "Validation Loss: 0.003383232317237037\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006157913248753175\n",
      "Training Loss: 0.006761513416422531\n",
      "Training Loss: 0.0059469481464475395\n",
      "Validation Loss: 0.003375089240752244\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006147646665922366\n",
      "Training Loss: 0.00675138488120865\n",
      "Training Loss: 0.005937214121222496\n",
      "Validation Loss: 0.0033669375469175616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006137374946847558\n",
      "Training Loss: 0.006741270530037582\n",
      "Training Loss: 0.005927474529016763\n",
      "Validation Loss: 0.0033587829518560946\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006127101467573084\n",
      "Training Loss: 0.006731172620784491\n",
      "Training Loss: 0.0059177318867295985\n",
      "Validation Loss: 0.003350625458183918\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.0061168272909708325\n",
      "Training Loss: 0.006721093147643842\n",
      "Training Loss: 0.005907985515659675\n",
      "Validation Loss: 0.0033424651234546738\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006106554840807803\n",
      "Training Loss: 0.006711033168248832\n",
      "Training Loss: 0.005898238350637257\n",
      "Validation Loss: 0.003334296769148597\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006096286523388699\n",
      "Training Loss: 0.00670099608541932\n",
      "Training Loss: 0.005888491038931533\n",
      "Validation Loss: 0.0033261318340520847\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006086025143158622\n",
      "Training Loss: 0.006690983603475615\n",
      "Training Loss: 0.005878747063688934\n",
      "Validation Loss: 0.0033179624594768947\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006075772409094498\n",
      "Training Loss: 0.006680997028015554\n",
      "Training Loss: 0.005869007346918806\n",
      "Validation Loss: 0.0033097989331365802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006065531388157979\n",
      "Training Loss: 0.006671038618660532\n",
      "Training Loss: 0.005859272319357842\n",
      "Validation Loss: 0.003301633863312224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006055303401662968\n",
      "Training Loss: 0.006661110403947532\n",
      "Training Loss: 0.005849545479286462\n",
      "Validation Loss: 0.0032934704625434924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006045091858250089\n",
      "Training Loss: 0.006651213732548058\n",
      "Training Loss: 0.005839827525196597\n",
      "Validation Loss: 0.003285315150677572\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006034899362712167\n",
      "Training Loss: 0.0066413510794518515\n",
      "Training Loss: 0.0058301218808628616\n",
      "Validation Loss: 0.0032771655784794287\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00602472742262762\n",
      "Training Loss: 0.00663152395922225\n",
      "Training Loss: 0.005820428618462757\n",
      "Validation Loss: 0.003269016044214368\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006014579182956368\n",
      "Training Loss: 0.006621733320644125\n",
      "Training Loss: 0.005810750138480216\n",
      "Validation Loss: 0.003260875320888721\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006004457372473553\n",
      "Training Loss: 0.006611981526948512\n",
      "Training Loss: 0.005801089544547722\n",
      "Validation Loss: 0.0032527418166210646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00599436360411346\n",
      "Training Loss: 0.0066022690414683895\n",
      "Training Loss: 0.005791446614312008\n",
      "Validation Loss: 0.0032446132004888876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.005984300461714156\n",
      "Training Loss: 0.00659259707084857\n",
      "Training Loss: 0.005781825192971155\n",
      "Validation Loss: 0.0032364975273860303\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.005974270979641006\n",
      "Training Loss: 0.0065829692530678585\n",
      "Training Loss: 0.005772227094275877\n",
      "Validation Loss: 0.0032283912433750846\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.005964277567109093\n",
      "Training Loss: 0.006573385818046518\n",
      "Training Loss: 0.005762654194259084\n",
      "Validation Loss: 0.003220300150909618\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.005954322274774313\n",
      "Training Loss: 0.00656384753936436\n",
      "Training Loss: 0.0057531082356581465\n",
      "Validation Loss: 0.0032122195305909667\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.005944408631767146\n",
      "Training Loss: 0.0065543563739629464\n",
      "Training Loss: 0.005743591957143508\n",
      "Validation Loss: 0.003204156346409843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005934537893044762\n",
      "Training Loss: 0.006544914565747604\n",
      "Training Loss: 0.005734106863383204\n",
      "Validation Loss: 0.003196104924100336\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.005924713925924152\n",
      "Training Loss: 0.006535521415644325\n",
      "Training Loss: 0.0057246564456727355\n",
      "Validation Loss: 0.0031880724840284732\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.0059149375569541\n",
      "Training Loss: 0.00652618017513305\n",
      "Training Loss: 0.005715240688296035\n",
      "Validation Loss: 0.0031800570850668663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005905212332145311\n",
      "Training Loss: 0.006516890815692023\n",
      "Training Loss: 0.005705863598850555\n",
      "Validation Loss: 0.0031720589914390547\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.005895539639750496\n",
      "Training Loss: 0.0065076544065959755\n",
      "Training Loss: 0.005696525785606354\n",
      "Validation Loss: 0.0031640826766326856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005885923214373178\n",
      "Training Loss: 0.0064984727621776985\n",
      "Training Loss: 0.005687230281764642\n",
      "Validation Loss: 0.003156129778310489\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.005876363466959447\n",
      "Training Loss: 0.006489347445312888\n",
      "Training Loss: 0.005677979643223807\n",
      "Validation Loss: 0.0031481971663391488\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.0058668639842653645\n",
      "Training Loss: 0.006480278488597832\n",
      "Training Loss: 0.00566877523669973\n",
      "Validation Loss: 0.003140293325956785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005857427428127266\n",
      "Training Loss: 0.006471267970046029\n",
      "Training Loss: 0.005659619159996509\n",
      "Validation Loss: 0.003132414221868254\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.0058480548748048025\n",
      "Training Loss: 0.006462315224343911\n",
      "Training Loss: 0.0056505132105667145\n",
      "Validation Loss: 0.003124562323648022\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.00583874783187639\n",
      "Training Loss: 0.006453422382473945\n",
      "Training Loss: 0.005641459563048556\n",
      "Validation Loss: 0.0031167393814500294\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005829508414026349\n",
      "Training Loss: 0.006444590175524354\n",
      "Training Loss: 0.005632460733759217\n",
      "Validation Loss: 0.0031089464992577775\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005820339386700652\n",
      "Training Loss: 0.006435819271719084\n",
      "Training Loss: 0.005623516611522063\n",
      "Validation Loss: 0.003101186867374299\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005811241543851793\n",
      "Training Loss: 0.006427110120421275\n",
      "Training Loss: 0.005614631014759652\n",
      "Validation Loss: 0.00309346166564926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.0058022165385773405\n",
      "Training Loss: 0.006418463962618261\n",
      "Training Loss: 0.005605803877697327\n",
      "Validation Loss: 0.0030857694552416043\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.0057932667573913936\n",
      "Training Loss: 0.006409880646388046\n",
      "Training Loss: 0.0055970365210669115\n",
      "Validation Loss: 0.0030781135886509933\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005784392652567476\n",
      "Training Loss: 0.006401360829477199\n",
      "Training Loss: 0.005588332632323727\n",
      "Validation Loss: 0.0030704994850760596\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0057755960110807796\n",
      "Training Loss: 0.006392906259279698\n",
      "Training Loss: 0.005579691301099956\n",
      "Validation Loss: 0.0030629246945628958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005766878254362382\n",
      "Training Loss: 0.006384516100515611\n",
      "Training Loss: 0.00557111544359941\n",
      "Validation Loss: 0.0030553924492717293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005758239745046012\n",
      "Training Loss: 0.006376191262970679\n",
      "Training Loss: 0.005562604101723992\n",
      "Validation Loss: 0.0030478999957839928\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005749681896995753\n",
      "Training Loss: 0.0063679301150841635\n",
      "Training Loss: 0.005554159783641808\n",
      "Validation Loss: 0.003040451718640796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.0057412047858815644\n",
      "Training Loss: 0.006359735831501893\n",
      "Training Loss: 0.005545783044071868\n",
      "Validation Loss: 0.0030330485465486397\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005732810363406315\n",
      "Training Loss: 0.006351606121752411\n",
      "Training Loss: 0.005537475624587387\n",
      "Validation Loss: 0.0030256936057167275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005724498728523031\n",
      "Training Loss: 0.006343542850227095\n",
      "Training Loss: 0.0055292360915336755\n",
      "Validation Loss: 0.003018384023695096\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005716270059929229\n",
      "Training Loss: 0.006335543829482049\n",
      "Training Loss: 0.005521066897781566\n",
      "Validation Loss: 0.003011126850804921\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.0057081248715985565\n",
      "Training Loss: 0.006327611080487259\n",
      "Training Loss: 0.005512969036935829\n",
      "Validation Loss: 0.003003919306158852\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005700064615230076\n",
      "Training Loss: 0.006319743521744386\n",
      "Training Loss: 0.005504940915270709\n",
      "Validation Loss: 0.00299676225436956\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005692088290234096\n",
      "Training Loss: 0.006311941620078869\n",
      "Training Loss: 0.005496983546181582\n",
      "Validation Loss: 0.0029896584043823432\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005684196923975832\n",
      "Training Loss: 0.006304203239851631\n",
      "Training Loss: 0.005489099076366983\n",
      "Validation Loss: 0.002982607520750483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005676390388398431\n",
      "Training Loss: 0.006296530853724107\n",
      "Training Loss: 0.005481285756104626\n",
      "Validation Loss: 0.002975615008284202\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005668668579310179\n",
      "Training Loss: 0.00628892146749422\n",
      "Training Loss: 0.0054735431191511455\n",
      "Validation Loss: 0.002968673022683668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005661030795308762\n",
      "Training Loss: 0.006281376401893794\n",
      "Training Loss: 0.005465873481007293\n",
      "Validation Loss: 0.002961787367710488\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005653477019513957\n",
      "Training Loss: 0.006273893569596112\n",
      "Training Loss: 0.005458274841075763\n",
      "Validation Loss: 0.002954959716344399\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005646008042385802\n",
      "Training Loss: 0.006266473994473927\n",
      "Training Loss: 0.0054507472494151445\n",
      "Validation Loss: 0.002948187655256538\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005638622334809043\n",
      "Training Loss: 0.006259117511799559\n",
      "Training Loss: 0.005443291387637146\n",
      "Validation Loss: 0.002941476703056375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005631320885149762\n",
      "Training Loss: 0.006251822030171752\n",
      "Training Loss: 0.005435908010113053\n",
      "Validation Loss: 0.002934826098466187\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005624103044392541\n",
      "Training Loss: 0.006244587958790362\n",
      "Training Loss: 0.00542859535315074\n",
      "Validation Loss: 0.0029282331456210505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005616967272362672\n",
      "Training Loss: 0.006237414192873984\n",
      "Training Loss: 0.005421351763070561\n",
      "Validation Loss: 0.0029216991486341764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005609913538792171\n",
      "Training Loss: 0.006230299946037121\n",
      "Training Loss: 0.005414178940700367\n",
      "Validation Loss: 0.0029152244645997546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005602941366378218\n",
      "Training Loss: 0.006223245108849369\n",
      "Training Loss: 0.005407075894181617\n",
      "Validation Loss: 0.0029088085703028554\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.0055960490473080425\n",
      "Training Loss: 0.006216247561387718\n",
      "Training Loss: 0.005400041633401997\n",
      "Validation Loss: 0.002902455214578449\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005589238016982563\n",
      "Training Loss: 0.006209308042889461\n",
      "Training Loss: 0.005393075610045344\n",
      "Validation Loss: 0.002896163010906972\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005582506075734272\n",
      "Training Loss: 0.006202425854280591\n",
      "Training Loss: 0.005386177762411535\n",
      "Validation Loss: 0.002889931636203206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005575853027403354\n",
      "Training Loss: 0.006195599243510514\n",
      "Training Loss: 0.005379348163842224\n",
      "Validation Loss: 0.0028837617811108573\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005569277777685784\n",
      "Training Loss: 0.0061888288921909404\n",
      "Training Loss: 0.0053725845296867195\n",
      "Validation Loss: 0.0028776510440733996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005562779699685052\n",
      "Training Loss: 0.006182111864327453\n",
      "Training Loss: 0.005365886942017823\n",
      "Validation Loss: 0.0028716010771419726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005556357861496508\n",
      "Training Loss: 0.00617544898414053\n",
      "Training Loss: 0.0053592548816232015\n",
      "Validation Loss: 0.0028656117495128447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005550011460436508\n",
      "Training Loss: 0.0061688389594201\n",
      "Training Loss: 0.005352687592385337\n",
      "Validation Loss: 0.002859686242332787\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005543739286367782\n",
      "Training Loss: 0.006162281593424268\n",
      "Training Loss: 0.005346184498048388\n",
      "Validation Loss: 0.0028538194045508174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005537540846853517\n",
      "Training Loss: 0.006155775838997215\n",
      "Training Loss: 0.005339744788361713\n",
      "Validation Loss: 0.002848014962765273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0055314155016094445\n",
      "Training Loss: 0.006149321474367753\n",
      "Training Loss: 0.005333367848070338\n",
      "Validation Loss: 0.002842270311365804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005525361316394992\n",
      "Training Loss: 0.006142916298704222\n",
      "Training Loss: 0.005327052632346749\n",
      "Validation Loss: 0.0028365854699729702\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.0055193790228804575\n",
      "Training Loss: 0.006136561198509298\n",
      "Training Loss: 0.005320798622560688\n",
      "Validation Loss: 0.002830960950029365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005513466407428495\n",
      "Training Loss: 0.006130253908922896\n",
      "Training Loss: 0.005314604933955707\n",
      "Validation Loss: 0.002825397123017589\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005507622572476976\n",
      "Training Loss: 0.006123994570225477\n",
      "Training Loss: 0.005308470736490562\n",
      "Validation Loss: 0.002819889349329254\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.0055018458323320375\n",
      "Training Loss: 0.006117782997898757\n",
      "Training Loss: 0.005302395683829672\n",
      "Validation Loss: 0.0028144427970198268\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005496138078160584\n",
      "Training Loss: 0.006111617887509056\n",
      "Training Loss: 0.0052963793341768905\n",
      "Validation Loss: 0.0028090548840236295\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005490495809353888\n",
      "Training Loss: 0.006105498900869861\n",
      "Training Loss: 0.005290420496603474\n",
      "Validation Loss: 0.002803726246046802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005484918716247194\n",
      "Training Loss: 0.006099424649728462\n",
      "Training Loss: 0.005284517867257818\n",
      "Validation Loss: 0.0027984517058776168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005479405327350833\n",
      "Training Loss: 0.0060933952656341715\n",
      "Training Loss: 0.005278671075357125\n",
      "Validation Loss: 0.0027932385950652735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005473956087953411\n",
      "Training Loss: 0.00608740963798482\n",
      "Training Loss: 0.005272879609256051\n",
      "Validation Loss: 0.002788081387152079\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005468569172080606\n",
      "Training Loss: 0.006081468327902257\n",
      "Training Loss: 0.005267144445097074\n",
      "Validation Loss: 0.0027829831076283634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0054632434633094815\n",
      "Training Loss: 0.006075569557142444\n",
      "Training Loss: 0.005261461662594229\n",
      "Validation Loss: 0.002777937480531047\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005457978381309658\n",
      "Training Loss: 0.006069713336182758\n",
      "Training Loss: 0.005255832092370838\n",
      "Validation Loss: 0.0027729512749532803\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.0054527734377188605\n",
      "Training Loss: 0.006063899005530402\n",
      "Training Loss: 0.00525025584094692\n",
      "Validation Loss: 0.0027680175190561273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005447626662789844\n",
      "Training Loss: 0.006058126352145337\n",
      "Training Loss: 0.005244731943239458\n",
      "Validation Loss: 0.002763140126487261\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [20:38<13:45, 206.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Loss: 0.31447374530136585\n",
      "Training Loss: 0.23592617467045784\n",
      "Training Loss: 0.1919391915947199\n",
      "Validation Loss: 0.1471681372168359\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.1366914350911975\n",
      "Training Loss: 0.1005255812779069\n",
      "Training Loss: 0.0819757517799735\n",
      "Validation Loss: 0.06184990055273088\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.0640979839488864\n",
      "Training Loss: 0.057967515401542186\n",
      "Training Loss: 0.055599354384467004\n",
      "Validation Loss: 0.04887065931819798\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.052146725030615924\n",
      "Training Loss: 0.04905484838411212\n",
      "Training Loss: 0.04633278530091047\n",
      "Validation Loss: 0.040267937303928844\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.042329170498996975\n",
      "Training Loss: 0.038974963994696736\n",
      "Training Loss: 0.0362275729700923\n",
      "Validation Loss: 0.03128044215146075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.03248952427878976\n",
      "Training Loss: 0.029403702546842395\n",
      "Training Loss: 0.02717597212642431\n",
      "Validation Loss: 0.023488759084112857\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.02461137989535928\n",
      "Training Loss: 0.022263846858404577\n",
      "Training Loss: 0.020788527033291756\n",
      "Validation Loss: 0.01809454331530279\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.019586502723395825\n",
      "Training Loss: 0.01801740852650255\n",
      "Training Loss: 0.017052493707742544\n",
      "Validation Loss: 0.014760708488691389\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.016548042953945696\n",
      "Training Loss: 0.015478923642076552\n",
      "Training Loss: 0.014671466683503241\n",
      "Validation Loss: 0.012458423550209303\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.014422095101326703\n",
      "Training Loss: 0.013728356033097952\n",
      "Training Loss: 0.012976092747412621\n",
      "Validation Loss: 0.010823617929990372\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.012880802082363516\n",
      "Training Loss: 0.012485078372992574\n",
      "Training Loss: 0.011749459912534803\n",
      "Validation Loss: 0.00965983895166369\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.011754150476772338\n",
      "Training Loss: 0.011577405268326402\n",
      "Training Loss: 0.010834139846265316\n",
      "Validation Loss: 0.008796867250014892\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.010901785134337842\n",
      "Training Loss: 0.010882530800299718\n",
      "Training Loss: 0.010121012427844108\n",
      "Validation Loss: 0.008121894049887241\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.010230924587231129\n",
      "Training Loss: 0.010327560883015395\n",
      "Training Loss: 0.009545981811825185\n",
      "Validation Loss: 0.007570970322141487\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.009687316326890141\n",
      "Training Loss: 0.009872289696941152\n",
      "Training Loss: 0.009072887927759439\n",
      "Validation Loss: 0.007109185241246491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.009239515003282578\n",
      "Training Loss: 0.009493839103961364\n",
      "Training Loss: 0.008680056512821466\n",
      "Validation Loss: 0.006716453551388021\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.00886779378168285\n",
      "Training Loss: 0.009177512660389766\n",
      "Training Loss: 0.008352642321260645\n",
      "Validation Loss: 0.006379591505137387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.008558051438303665\n",
      "Training Loss: 0.008912360047688708\n",
      "Training Loss: 0.008079127977835015\n",
      "Validation Loss: 0.0060887306149044395\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.0082991748128552\n",
      "Training Loss: 0.008689512939890847\n",
      "Training Loss: 0.007850104038370774\n",
      "Validation Loss: 0.005835999826262339\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.008082094105193392\n",
      "Training Loss: 0.008501694080187007\n",
      "Training Loss: 0.00765787435695529\n",
      "Validation Loss: 0.005615134093010526\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.007899463317589834\n",
      "Training Loss: 0.00834304428542964\n",
      "Training Loss: 0.007496268275426701\n",
      "Validation Loss: 0.0054212671212768284\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.007745421559084207\n",
      "Training Loss: 0.008208905039355159\n",
      "Training Loss: 0.007360364128835499\n",
      "Validation Loss: 0.005250656359902258\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.007615305648650974\n",
      "Training Loss: 0.0080955538561102\n",
      "Training Loss: 0.0072461915004532786\n",
      "Validation Loss: 0.005100392992869856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.007505356961628422\n",
      "Training Loss: 0.007999931857921184\n",
      "Training Loss: 0.0071504532149992885\n",
      "Validation Loss: 0.004968122618398472\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.007412460101768374\n",
      "Training Loss: 0.007919434817740694\n",
      "Training Loss: 0.007070318732876331\n",
      "Validation Loss: 0.004851825247529183\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007333959738025442\n",
      "Training Loss: 0.007851771058049052\n",
      "Training Loss: 0.007003304391400888\n",
      "Validation Loss: 0.004749707720671477\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.007267557637533173\n",
      "Training Loss: 0.007794908828800544\n",
      "Training Loss: 0.006947218929417432\n",
      "Validation Loss: 0.004660122512494412\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007211250922409818\n",
      "Training Loss: 0.00774704477051273\n",
      "Training Loss: 0.006900145505787805\n",
      "Validation Loss: 0.004581569115830104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.007163306619040668\n",
      "Training Loss: 0.007706601721001789\n",
      "Training Loss: 0.006860426689963788\n",
      "Validation Loss: 0.004512670599028803\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.007122247004881501\n",
      "Training Loss: 0.007672227558214217\n",
      "Training Loss: 0.006826668025460094\n",
      "Validation Loss: 0.0044521918426152695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007086831082124263\n",
      "Training Loss: 0.007642786494689063\n",
      "Training Loss: 0.006797711397521198\n",
      "Validation Loss: 0.004399026568790668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007056030415114946\n",
      "Training Loss: 0.007617337682750076\n",
      "Training Loss: 0.006772612782660872\n",
      "Validation Loss: 0.00435220502745988\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007029002702329308\n",
      "Training Loss: 0.007595111897680909\n",
      "Training Loss: 0.006750611740862951\n",
      "Validation Loss: 0.0043108782160692334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007005060867522843\n",
      "Training Loss: 0.00757548967259936\n",
      "Training Loss: 0.006731099943863228\n",
      "Validation Loss: 0.004274295090980242\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006983651152695529\n",
      "Training Loss: 0.00755796953686513\n",
      "Training Loss: 0.00671359415515326\n",
      "Validation Loss: 0.004241821515698279\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.0069643255649134515\n",
      "Training Loss: 0.007542151246452704\n",
      "Training Loss: 0.006697710052831099\n",
      "Validation Loss: 0.0042128930865064935\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006946718580438755\n",
      "Training Loss: 0.007527711865259334\n",
      "Training Loss: 0.006683141365647316\n",
      "Validation Loss: 0.004187029161166107\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006930534769780934\n",
      "Training Loss: 0.007514391064178199\n",
      "Training Loss: 0.006669644332723692\n",
      "Validation Loss: 0.0041638095706198995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006915533231804148\n",
      "Training Loss: 0.007501979365479201\n",
      "Training Loss: 0.006657023614970967\n",
      "Validation Loss: 0.0041428776981120696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006901514254277572\n",
      "Training Loss: 0.007490304872626439\n",
      "Training Loss: 0.0066451180668082085\n",
      "Validation Loss: 0.0041239191202467745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006888314551324584\n",
      "Training Loss: 0.007479226941941306\n",
      "Training Loss: 0.00663379882927984\n",
      "Validation Loss: 0.004106668241353434\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006875795924570411\n",
      "Training Loss: 0.007468630477087572\n",
      "Training Loss: 0.006622959248488769\n",
      "Validation Loss: 0.004090892478007447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0068638428533449765\n",
      "Training Loss: 0.007458419818431139\n",
      "Training Loss: 0.006612510005943477\n",
      "Validation Loss: 0.004076388944463616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006852358517935499\n",
      "Training Loss: 0.007448513666167856\n",
      "Training Loss: 0.006602376428199932\n",
      "Validation Loss: 0.004062983985686821\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006841260836226866\n",
      "Training Loss: 0.007438846090808511\n",
      "Training Loss: 0.0065924965182784945\n",
      "Validation Loss: 0.004050530413469153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006830478981137276\n",
      "Training Loss: 0.007429361228714697\n",
      "Training Loss: 0.006582815219881013\n",
      "Validation Loss: 0.0040388962206838845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006819952384103089\n",
      "Training Loss: 0.007420009069610387\n",
      "Training Loss: 0.006573288474464789\n",
      "Validation Loss: 0.004027972178366245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006809630499919877\n",
      "Training Loss: 0.00741075167839881\n",
      "Training Loss: 0.006563875876599923\n",
      "Validation Loss: 0.0040176523132563645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006799467150121927\n",
      "Training Loss: 0.007401554574607871\n",
      "Training Loss: 0.006554543605307117\n",
      "Validation Loss: 0.004007855820040522\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006789424666203558\n",
      "Training Loss: 0.007392387964064256\n",
      "Training Loss: 0.006545262848958373\n",
      "Validation Loss: 0.003998506199463867\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006779469361645169\n",
      "Training Loss: 0.007383228624821641\n",
      "Training Loss: 0.006536008832044899\n",
      "Validation Loss: 0.0039895406814313\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006769573396304622\n",
      "Training Loss: 0.007374056128319353\n",
      "Training Loss: 0.00652675909223035\n",
      "Validation Loss: 0.003980899759400846\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006759711368940771\n",
      "Training Loss: 0.007364853956969455\n",
      "Training Loss: 0.006517495457082987\n",
      "Validation Loss: 0.00397253440944164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.0067498635721858595\n",
      "Training Loss: 0.007355607878416777\n",
      "Training Loss: 0.006508202995173633\n",
      "Validation Loss: 0.003964400648798668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006740011686924845\n",
      "Training Loss: 0.007346307360567152\n",
      "Training Loss: 0.006498867612099275\n",
      "Validation Loss: 0.003956459454794446\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0067301405960461125\n",
      "Training Loss: 0.007336943867267109\n",
      "Training Loss: 0.006489479463780299\n",
      "Validation Loss: 0.003948677764169537\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006720239450223744\n",
      "Training Loss: 0.007327510474715382\n",
      "Training Loss: 0.006480028657242656\n",
      "Validation Loss: 0.003941024690238612\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006710296784876846\n",
      "Training Loss: 0.0073180031753145155\n",
      "Training Loss: 0.006470508783822879\n",
      "Validation Loss: 0.003933470638657219\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006700305446283892\n",
      "Training Loss: 0.007308419415494427\n",
      "Training Loss: 0.006460915999487043\n",
      "Validation Loss: 0.003925999629133371\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.00669025941111613\n",
      "Training Loss: 0.00729875739722047\n",
      "Training Loss: 0.006451245696516708\n",
      "Validation Loss: 0.003918588307968686\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006680155543144792\n",
      "Training Loss: 0.007289018039591611\n",
      "Training Loss: 0.0064414966921322045\n",
      "Validation Loss: 0.003911215581753281\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006669990273658186\n",
      "Training Loss: 0.007279202351346612\n",
      "Training Loss: 0.006431669504381716\n",
      "Validation Loss: 0.0039038703910315806\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006659763031057082\n",
      "Training Loss: 0.0072693124180659655\n",
      "Training Loss: 0.0064217642683070154\n",
      "Validation Loss: 0.0038965376135840844\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.0066494741744827475\n",
      "Training Loss: 0.007259352507535368\n",
      "Training Loss: 0.006411784766241908\n",
      "Validation Loss: 0.0038892085967438942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0066391258605290205\n",
      "Training Loss: 0.007249327140743844\n",
      "Training Loss: 0.006401733125094324\n",
      "Validation Loss: 0.0038818720272961963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006628720129956491\n",
      "Training Loss: 0.007239241905044764\n",
      "Training Loss: 0.006391615332104266\n",
      "Validation Loss: 0.0038745142166303954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.006618260582908988\n",
      "Training Loss: 0.007229101760894991\n",
      "Training Loss: 0.006381436528172344\n",
      "Validation Loss: 0.0038671368324940795\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0066077518952079115\n",
      "Training Loss: 0.007218914644327014\n",
      "Training Loss: 0.006371202665613964\n",
      "Validation Loss: 0.0038597244596673868\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006597198939998634\n",
      "Training Loss: 0.007208685429068282\n",
      "Training Loss: 0.006360920957522467\n",
      "Validation Loss: 0.003852280539596516\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006586608276702464\n",
      "Training Loss: 0.007198423010413535\n",
      "Training Loss: 0.006350599264260381\n",
      "Validation Loss: 0.003844797702799185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006575986127136275\n",
      "Training Loss: 0.007188135332544334\n",
      "Training Loss: 0.006340247159823775\n",
      "Validation Loss: 0.003837272935557399\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006565339643857442\n",
      "Training Loss: 0.007177828918211162\n",
      "Training Loss: 0.006329871142515913\n",
      "Validation Loss: 0.0038297085635615197\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006554675956722349\n",
      "Training Loss: 0.007167512481100857\n",
      "Training Loss: 0.00631948075373657\n",
      "Validation Loss: 0.0038221002231990354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006544002256123349\n",
      "Training Loss: 0.00715719336993061\n",
      "Training Loss: 0.006309085884131491\n",
      "Validation Loss: 0.0038144486299663614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006533327781362459\n",
      "Training Loss: 0.00714688015752472\n",
      "Training Loss: 0.00629869444295764\n",
      "Validation Loss: 0.0038067556438925727\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00652265842887573\n",
      "Training Loss: 0.007136579425423406\n",
      "Training Loss: 0.006288315824931488\n",
      "Validation Loss: 0.0037990206999055456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.0065120022732298825\n",
      "Training Loss: 0.007126297967042774\n",
      "Training Loss: 0.006277958530699834\n",
      "Validation Loss: 0.0037912462531913367\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.006501367877353914\n",
      "Training Loss: 0.007116044866852462\n",
      "Training Loss: 0.006267631965456531\n",
      "Validation Loss: 0.0037834325091118054\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006490762825706042\n",
      "Training Loss: 0.00710582559753675\n",
      "Training Loss: 0.0062573437776882205\n",
      "Validation Loss: 0.003775589101361843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006480194912292063\n",
      "Training Loss: 0.007095646537491121\n",
      "Training Loss: 0.006247101714834571\n",
      "Validation Loss: 0.0037677117552754754\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006469670025981031\n",
      "Training Loss: 0.007085512928315438\n",
      "Training Loss: 0.006236912200693041\n",
      "Validation Loss: 0.0037598075264060264\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006459195031784475\n",
      "Training Loss: 0.007075430555851199\n",
      "Training Loss: 0.006226782423909754\n",
      "Validation Loss: 0.0037518765586376022\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006448778077028692\n",
      "Training Loss: 0.00706540385086555\n",
      "Training Loss: 0.006216719840886072\n",
      "Validation Loss: 0.0037439285628392957\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.00643842330435291\n",
      "Training Loss: 0.007055439038085751\n",
      "Training Loss: 0.006206729705445469\n",
      "Validation Loss: 0.0037359656057100784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006428137560142204\n",
      "Training Loss: 0.007045538142556325\n",
      "Training Loss: 0.006196815891889855\n",
      "Validation Loss: 0.003727990338641606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006417925350833684\n",
      "Training Loss: 0.007035703747533262\n",
      "Training Loss: 0.006186982773942873\n",
      "Validation Loss: 0.003720009205025736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006407791816163808\n",
      "Training Loss: 0.007025939800078049\n",
      "Training Loss: 0.006177235330687836\n",
      "Validation Loss: 0.0037120246103431068\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006397741130786017\n",
      "Training Loss: 0.00701624883513432\n",
      "Training Loss: 0.006167575919535011\n",
      "Validation Loss: 0.0037040427128334392\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006387777188210748\n",
      "Training Loss: 0.007006632273551076\n",
      "Training Loss: 0.006158006638288498\n",
      "Validation Loss: 0.0036960680369978374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006377902660751715\n",
      "Training Loss: 0.006997090588556603\n",
      "Training Loss: 0.006148530426435173\n",
      "Validation Loss: 0.0036881059065482086\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0063681217737030235\n",
      "Training Loss: 0.006987625081092119\n",
      "Training Loss: 0.006139148354995996\n",
      "Validation Loss: 0.003680159714533372\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0063584356423234565\n",
      "Training Loss: 0.0069782365311402824\n",
      "Training Loss: 0.006129860951332376\n",
      "Validation Loss: 0.003672233326203619\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006348846899345517\n",
      "Training Loss: 0.006968924541724846\n",
      "Training Loss: 0.006120668284129351\n",
      "Validation Loss: 0.0036643308841131546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006339357342803851\n",
      "Training Loss: 0.006959689015056938\n",
      "Training Loss: 0.006111569879576564\n",
      "Validation Loss: 0.00365645451382263\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006329967170022428\n",
      "Training Loss: 0.006950529243331402\n",
      "Training Loss: 0.006102566937915981\n",
      "Validation Loss: 0.0036486104965272746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006320678554475307\n",
      "Training Loss: 0.006941444288822822\n",
      "Training Loss: 0.0060936581913847475\n",
      "Validation Loss: 0.003640800892385874\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006311492240638472\n",
      "Training Loss: 0.006932433297624812\n",
      "Training Loss: 0.006084842614363879\n",
      "Validation Loss: 0.0036330287844424\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.00630240706668701\n",
      "Training Loss: 0.006923494971124456\n",
      "Training Loss: 0.006076118723722174\n",
      "Validation Loss: 0.0036252998247261296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006293424251489341\n",
      "Training Loss: 0.0069146277423715215\n",
      "Training Loss: 0.006067483922233805\n",
      "Validation Loss: 0.0036176115331982965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006284542751964182\n",
      "Training Loss: 0.00690582946233917\n",
      "Training Loss: 0.006058937291381881\n",
      "Validation Loss: 0.0036099702237550628\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006275761914439499\n",
      "Training Loss: 0.006897098768386059\n",
      "Training Loss: 0.0060504775773733855\n",
      "Validation Loss: 0.00360237865766322\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006267081025871448\n",
      "Training Loss: 0.006888434087741189\n",
      "Training Loss: 0.006042100408812985\n",
      "Validation Loss: 0.0035948368375388422\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006258498205570504\n",
      "Training Loss: 0.006879832107224502\n",
      "Training Loss: 0.006033804126782343\n",
      "Validation Loss: 0.003587344679667541\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006250011829542927\n",
      "Training Loss: 0.006871291156858206\n",
      "Training Loss: 0.0060255873075220736\n",
      "Validation Loss: 0.0035799062559695055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006241621333756484\n",
      "Training Loss: 0.006862809129524976\n",
      "Training Loss: 0.006017446314217523\n",
      "Validation Loss: 0.003572521413404369\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006233323332853615\n",
      "Training Loss: 0.0068543831811985\n",
      "Training Loss: 0.006009378804592415\n",
      "Validation Loss: 0.0035651881506750254\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006225116748828441\n",
      "Training Loss: 0.006846010932931677\n",
      "Training Loss: 0.0060013816447462886\n",
      "Validation Loss: 0.003557909157106213\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006216998986783438\n",
      "Training Loss: 0.00683769105526153\n",
      "Training Loss: 0.005993453069822863\n",
      "Validation Loss: 0.0035506849846896832\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006208967124694027\n",
      "Training Loss: 0.006829420028370805\n",
      "Training Loss: 0.005985588916810229\n",
      "Validation Loss: 0.003543511455554222\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006201017699204385\n",
      "Training Loss: 0.006821195288794115\n",
      "Training Loss: 0.005977786445291713\n",
      "Validation Loss: 0.0035363932825583084\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006193148648017086\n",
      "Training Loss: 0.006813014614162966\n",
      "Training Loss: 0.00597004291950725\n",
      "Validation Loss: 0.0035293255344012314\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006185355863999575\n",
      "Training Loss: 0.006804873677319847\n",
      "Training Loss: 0.005962353935465217\n",
      "Validation Loss: 0.0035223079311630027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.0061776369961444285\n",
      "Training Loss: 0.006796771800145507\n",
      "Training Loss: 0.005954718615394086\n",
      "Validation Loss: 0.0035153380307379398\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0061699860624503345\n",
      "Training Loss: 0.006788704658974893\n",
      "Training Loss: 0.005947131388820708\n",
      "Validation Loss: 0.0035084179207536107\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006162401384208351\n",
      "Training Loss: 0.006780669831787236\n",
      "Training Loss: 0.00593958955607377\n",
      "Validation Loss: 0.0035015421820113832\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.0061548785027116535\n",
      "Training Loss: 0.006772663995507173\n",
      "Training Loss: 0.005932089746929705\n",
      "Validation Loss: 0.0034947090146518994\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006147412527352572\n",
      "Training Loss: 0.006764684632653371\n",
      "Training Loss: 0.0059246286435518415\n",
      "Validation Loss: 0.003487917725415377\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006139999550068751\n",
      "Training Loss: 0.006756728135515005\n",
      "Training Loss: 0.0059172027709428225\n",
      "Validation Loss: 0.003481163584438854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006132635427638889\n",
      "Training Loss: 0.006748791785212233\n",
      "Training Loss: 0.005909807835705578\n",
      "Validation Loss: 0.003474446483155231\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.0061253153940197085\n",
      "Training Loss: 0.006740872101509012\n",
      "Training Loss: 0.005902439929777756\n",
      "Validation Loss: 0.0034677624804480525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006118035501567647\n",
      "Training Loss: 0.0067329659155802805\n",
      "Training Loss: 0.00589509595069103\n",
      "Validation Loss: 0.0034611076705178684\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006110790495295077\n",
      "Training Loss: 0.006725069943931885\n",
      "Training Loss: 0.005887771833222359\n",
      "Validation Loss: 0.003454478559596987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006103575152810663\n",
      "Training Loss: 0.0067171804449753835\n",
      "Training Loss: 0.005880464399233461\n",
      "Validation Loss: 0.0034478715244220214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006096385744749569\n",
      "Training Loss: 0.006709295036271215\n",
      "Training Loss: 0.0058731688640546054\n",
      "Validation Loss: 0.003441285530335448\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006089216863620095\n",
      "Training Loss: 0.006701408593216911\n",
      "Training Loss: 0.0058658805640880015\n",
      "Validation Loss: 0.0034347169880828495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0060820645140483975\n",
      "Training Loss: 0.006693519706605002\n",
      "Training Loss: 0.005858596633188427\n",
      "Validation Loss: 0.0034281581174582243\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006074923163396306\n",
      "Training Loss: 0.006685623344965279\n",
      "Training Loss: 0.005851314143510536\n",
      "Validation Loss: 0.003421609531931077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.0060677892563398925\n",
      "Training Loss: 0.006677717356360517\n",
      "Training Loss: 0.00584402761189267\n",
      "Validation Loss: 0.003415067406800272\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006060658670612611\n",
      "Training Loss: 0.006669798548100517\n",
      "Training Loss: 0.005836733748437837\n",
      "Validation Loss: 0.003408529285571716\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006053525026654825\n",
      "Training Loss: 0.006661861772881821\n",
      "Training Loss: 0.005829429002478719\n",
      "Validation Loss: 0.003401987063646149\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.0060463854519184675\n",
      "Training Loss: 0.006653906324645504\n",
      "Training Loss: 0.005822109602158889\n",
      "Validation Loss: 0.003395440412706204\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006039235049393028\n",
      "Training Loss: 0.006645927375648171\n",
      "Training Loss: 0.005814770933939144\n",
      "Validation Loss: 0.0033888849665232924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006032071159570478\n",
      "Training Loss: 0.0066379212879110125\n",
      "Training Loss: 0.005807411239948124\n",
      "Validation Loss: 0.003382318531518823\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.00602488801931031\n",
      "Training Loss: 0.006629887226154096\n",
      "Training Loss: 0.005800025495700538\n",
      "Validation Loss: 0.0033757367872455147\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006017682958627119\n",
      "Training Loss: 0.006621819887659513\n",
      "Training Loss: 0.005792611088836565\n",
      "Validation Loss: 0.003369139843578503\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006010452334303409\n",
      "Training Loss: 0.006613718615844846\n",
      "Training Loss: 0.005785165319684893\n",
      "Validation Loss: 0.003362521581519186\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006003192228963599\n",
      "Training Loss: 0.006605578891467303\n",
      "Training Loss: 0.005777684889035299\n",
      "Validation Loss: 0.0033558783882684756\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005995900089619682\n",
      "Training Loss: 0.006597398931044154\n",
      "Training Loss: 0.005770166645525023\n",
      "Validation Loss: 0.0033492067936033514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.005988571981433779\n",
      "Training Loss: 0.006589176249108277\n",
      "Training Loss: 0.005762606881326064\n",
      "Validation Loss: 0.003342506355407198\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005981204777490348\n",
      "Training Loss: 0.006580908055184409\n",
      "Training Loss: 0.005755003832746297\n",
      "Validation Loss: 0.0033357722300178046\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.005973797439946793\n",
      "Training Loss: 0.006572592061711475\n",
      "Training Loss: 0.005747355538187548\n",
      "Validation Loss: 0.0033290012245160644\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.005966345619526692\n",
      "Training Loss: 0.0065642273024423044\n",
      "Training Loss: 0.005739660538965836\n",
      "Validation Loss: 0.00332219398245634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005958848698064685\n",
      "Training Loss: 0.006555811335565522\n",
      "Training Loss: 0.005731914170319215\n",
      "Validation Loss: 0.003315346714454504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.0059513028978835794\n",
      "Training Loss: 0.006547342303092591\n",
      "Training Loss: 0.0057241172215435655\n",
      "Validation Loss: 0.003308456443417608\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005943708255654201\n",
      "Training Loss: 0.006538818189874292\n",
      "Training Loss: 0.005716266827657818\n",
      "Validation Loss: 0.003301522408067929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005936061164247803\n",
      "Training Loss: 0.006530238568084315\n",
      "Training Loss: 0.00570836297236383\n",
      "Validation Loss: 0.0032945398458415704\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005928361543919891\n",
      "Training Loss: 0.0065216027910355474\n",
      "Training Loss: 0.005700402073562145\n",
      "Validation Loss: 0.0032875101170973496\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005920608385349624\n",
      "Training Loss: 0.006512908519944176\n",
      "Training Loss: 0.005692386034643278\n",
      "Validation Loss: 0.0032804328176517334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.005912799897487275\n",
      "Training Loss: 0.00650415602256544\n",
      "Training Loss: 0.005684312839293853\n",
      "Validation Loss: 0.0032733022209173175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005904936780571007\n",
      "Training Loss: 0.006495344974682666\n",
      "Training Loss: 0.005676182144088671\n",
      "Validation Loss: 0.0032661212568977074\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005897018373943865\n",
      "Training Loss: 0.006486475658020936\n",
      "Training Loss: 0.005667993596289307\n",
      "Validation Loss: 0.003258886000172894\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005889044009381905\n",
      "Training Loss: 0.006477547110407613\n",
      "Training Loss: 0.005659748589387163\n",
      "Validation Loss: 0.003251597136154436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.00588101384753827\n",
      "Training Loss: 0.006468560043140314\n",
      "Training Loss: 0.0056514471443369985\n",
      "Validation Loss: 0.0032442545719716825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005872929909965023\n",
      "Training Loss: 0.006459514751331881\n",
      "Training Loss: 0.005643089134246111\n",
      "Validation Loss: 0.003236857215413468\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005864790889900178\n",
      "Training Loss: 0.006450413097045384\n",
      "Training Loss: 0.0056346766673959795\n",
      "Validation Loss: 0.0032294011789928663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.00585659863660112\n",
      "Training Loss: 0.006441255856771022\n",
      "Training Loss: 0.005626210023183376\n",
      "Validation Loss: 0.003221891289367602\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005848355219932273\n",
      "Training Loss: 0.006432044102111832\n",
      "Training Loss: 0.005617691685911268\n",
      "Validation Loss: 0.003214329134494987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005840061384951696\n",
      "Training Loss: 0.00642277926497627\n",
      "Training Loss: 0.00560912280343473\n",
      "Validation Loss: 0.0032067065666105303\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005831718716071918\n",
      "Training Loss: 0.006413464577053674\n",
      "Training Loss: 0.005600506423506886\n",
      "Validation Loss: 0.0031990333685253777\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00582332945545204\n",
      "Training Loss: 0.006404102356755175\n",
      "Training Loss: 0.005591844196897\n",
      "Validation Loss: 0.0031913069947988966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005814896498341113\n",
      "Training Loss: 0.006394694150658325\n",
      "Training Loss: 0.005583140024682507\n",
      "Validation Loss: 0.0031835308594085026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005806422575842589\n",
      "Training Loss: 0.006385243179393001\n",
      "Training Loss: 0.0055743964423891155\n",
      "Validation Loss: 0.003175699934258722\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0057979099324438725\n",
      "Training Loss: 0.0063757546962006015\n",
      "Training Loss: 0.005565616752719506\n",
      "Validation Loss: 0.0031678210760812077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.00578936263860669\n",
      "Training Loss: 0.006366230309940875\n",
      "Training Loss: 0.005556805001106113\n",
      "Validation Loss: 0.0031598949336624715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005780782834626734\n",
      "Training Loss: 0.0063566740066744384\n",
      "Training Loss: 0.005547963933786378\n",
      "Validation Loss: 0.003151922005364734\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005772176282480359\n",
      "Training Loss: 0.006347091860952787\n",
      "Training Loss: 0.005539098902372644\n",
      "Validation Loss: 0.0031439070576760038\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005763542946078814\n",
      "Training Loss: 0.006337486208649352\n",
      "Training Loss: 0.005530213480815291\n",
      "Validation Loss: 0.0031358506622085913\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005754891100805253\n",
      "Training Loss: 0.006327863231417723\n",
      "Training Loss: 0.005521312725031749\n",
      "Validation Loss: 0.003127754953679409\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.00574622250162065\n",
      "Training Loss: 0.006318227325100452\n",
      "Training Loss: 0.005512400933075696\n",
      "Validation Loss: 0.003119626405565173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.00573754241457209\n",
      "Training Loss: 0.006308584194630385\n",
      "Training Loss: 0.005503483804641291\n",
      "Validation Loss: 0.003111465265084937\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005728855003253557\n",
      "Training Loss: 0.006298939128755592\n",
      "Training Loss: 0.005494565431727097\n",
      "Validation Loss: 0.00310327318036573\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005720164522062987\n",
      "Training Loss: 0.00628929692611564\n",
      "Training Loss: 0.005485652132192626\n",
      "Validation Loss: 0.003095058284783631\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.0057114772126078605\n",
      "Training Loss: 0.006279665078618563\n",
      "Training Loss: 0.005476748807122931\n",
      "Validation Loss: 0.0030868182134071594\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005702796730911359\n",
      "Training Loss: 0.006270048336009495\n",
      "Training Loss: 0.005467860390199348\n",
      "Validation Loss: 0.0030785637222272293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005694128564209677\n",
      "Training Loss: 0.006260452992282808\n",
      "Training Loss: 0.005458992918720469\n",
      "Validation Loss: 0.0030702927733216893\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.00568547764676623\n",
      "Training Loss: 0.006250884865876287\n",
      "Training Loss: 0.005450151476543397\n",
      "Validation Loss: 0.003062013230610932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.0056768490478862075\n",
      "Training Loss: 0.006241350168711506\n",
      "Training Loss: 0.005441341294208542\n",
      "Validation Loss: 0.003053726638887036\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.00566824670182541\n",
      "Training Loss: 0.006231855038786307\n",
      "Training Loss: 0.005432567691896111\n",
      "Validation Loss: 0.003045438187134065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.00565967676055152\n",
      "Training Loss: 0.006222404959844425\n",
      "Training Loss: 0.0054238354973495\n",
      "Validation Loss: 0.003037149489470077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005651142995338887\n",
      "Training Loss: 0.0062130050983978434\n",
      "Training Loss: 0.005415150596527383\n",
      "Validation Loss: 0.0030288688270793704\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.0056426508742151785\n",
      "Training Loss: 0.006203662158222869\n",
      "Training Loss: 0.005406517194351181\n",
      "Validation Loss: 0.0030205997395109426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005634204683010466\n",
      "Training Loss: 0.006194381885579787\n",
      "Training Loss: 0.005397938563255593\n",
      "Validation Loss: 0.0030123427565199104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005625809332123026\n",
      "Training Loss: 0.006185168517986312\n",
      "Training Loss: 0.005389422752195969\n",
      "Validation Loss: 0.0030041091900130505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.0056174681265838446\n",
      "Training Loss: 0.006176028214395046\n",
      "Training Loss: 0.005380970985861495\n",
      "Validation Loss: 0.0029958951250346525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005609185916837305\n",
      "Training Loss: 0.0061669637641171\n",
      "Training Loss: 0.005372587669407949\n",
      "Validation Loss: 0.0029877071645571275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005600966023630463\n",
      "Training Loss: 0.006157980443676933\n",
      "Training Loss: 0.005364276475738734\n",
      "Validation Loss: 0.0029795497428270036\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005592813248513267\n",
      "Training Loss: 0.006149081794428639\n",
      "Training Loss: 0.0053560400387505066\n",
      "Validation Loss: 0.0029714272443853904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005584729452384636\n",
      "Training Loss: 0.006140272473567165\n",
      "Training Loss: 0.005347882218193263\n",
      "Validation Loss: 0.0029633420943334867\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005576718623633497\n",
      "Training Loss: 0.006131554743042215\n",
      "Training Loss: 0.005339805605472065\n",
      "Validation Loss: 0.002955300772688195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005568782738992013\n",
      "Training Loss: 0.006122931779245846\n",
      "Training Loss: 0.005331811851356179\n",
      "Validation Loss: 0.002947302450153851\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005560926033649593\n",
      "Training Loss: 0.006114405798143707\n",
      "Training Loss: 0.005323904051911086\n",
      "Validation Loss: 0.0029393487670092604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005553149549523368\n",
      "Training Loss: 0.006105979677522555\n",
      "Training Loss: 0.0053160828119143844\n",
      "Validation Loss: 0.0029314466309995294\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.00554545536171645\n",
      "Training Loss: 0.006097654335317202\n",
      "Training Loss: 0.005308350728591904\n",
      "Validation Loss: 0.002923599068923027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005537847335799598\n",
      "Training Loss: 0.006089432573644444\n",
      "Training Loss: 0.00530070889624767\n",
      "Validation Loss: 0.0029158091271986788\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.0055303249851567675\n",
      "Training Loss: 0.006081314349430613\n",
      "Training Loss: 0.005293157905107364\n",
      "Validation Loss: 0.0029080763153124896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005522891090949997\n",
      "Training Loss: 0.006073301399010233\n",
      "Training Loss: 0.0052856976195471365\n",
      "Validation Loss: 0.002900404918394732\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005515546137467027\n",
      "Training Loss: 0.006065393523313105\n",
      "Training Loss: 0.005278330659493804\n",
      "Validation Loss: 0.0028927976846318234\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005508291556034237\n",
      "Training Loss: 0.0060575917910318825\n",
      "Training Loss: 0.0052710561850108206\n",
      "Validation Loss: 0.0028852534485625083\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.00550112894445192\n",
      "Training Loss: 0.006049895614851266\n",
      "Training Loss: 0.005263874327647499\n",
      "Validation Loss: 0.0028777790813068493\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.00549405871599447\n",
      "Training Loss: 0.006042305823066272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [24:03<10:18, 206.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0052567849273327735\n",
      "Validation Loss: 0.002870370567190262\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.4146188522875309\n",
      "Training Loss: 0.2680022932589054\n",
      "Training Loss: 0.16845780543982983\n",
      "Validation Loss: 0.10223428377609574\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.09087620289996266\n",
      "Training Loss: 0.07311459580436348\n",
      "Training Loss: 0.06996360469609499\n",
      "Validation Loss: 0.06364876241161582\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.0681205572374165\n",
      "Training Loss: 0.06601518442854286\n",
      "Training Loss: 0.06485149621963501\n",
      "Validation Loss: 0.05907474077317152\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.06325562715530396\n",
      "Training Loss: 0.06099578097462654\n",
      "Training Loss: 0.0593636635132134\n",
      "Validation Loss: 0.05344494047124734\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.05712620792910457\n",
      "Training Loss: 0.05439937150105834\n",
      "Training Loss: 0.05194082629866898\n",
      "Validation Loss: 0.045497939744022456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.04849717212840915\n",
      "Training Loss: 0.04504471836611629\n",
      "Training Loss: 0.04161889278329909\n",
      "Validation Loss: 0.03509664131600535\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.03737201194278896\n",
      "Training Loss: 0.03384193797595799\n",
      "Training Loss: 0.030435347156599166\n",
      "Validation Loss: 0.02532263687194398\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.027128841495141387\n",
      "Training Loss: 0.024671019148081542\n",
      "Training Loss: 0.022302927887067198\n",
      "Validation Loss: 0.019075854796539532\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.020722766327671706\n",
      "Training Loss: 0.019496333315037192\n",
      "Training Loss: 0.017969494122080504\n",
      "Validation Loss: 0.01580711549378178\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.017452409772668032\n",
      "Training Loss: 0.016970222042873503\n",
      "Training Loss: 0.015810489393770694\n",
      "Validation Loss: 0.014063034656593639\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.015756848792079836\n",
      "Training Loss: 0.015655103449244052\n",
      "Training Loss: 0.014630287035834045\n",
      "Validation Loss: 0.013008159255671703\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.014761351302731783\n",
      "Training Loss: 0.014852545561734588\n",
      "Training Loss: 0.013879281668923795\n",
      "Validation Loss: 0.01227249979470553\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.01408309199847281\n",
      "Training Loss: 0.014276811149902641\n",
      "Training Loss: 0.013325283934827894\n",
      "Validation Loss: 0.011691105018338461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.013551993558648974\n",
      "Training Loss: 0.013798742836806923\n",
      "Training Loss: 0.012854089165339247\n",
      "Validation Loss: 0.011171781002763616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.013076652390882373\n",
      "Training Loss: 0.01334318638429977\n",
      "Training Loss: 0.012397083479445428\n",
      "Validation Loss: 0.010657867270239284\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.012604896610137075\n",
      "Training Loss: 0.012869695946574212\n",
      "Training Loss: 0.011921715852804483\n",
      "Validation Loss: 0.010121867165304301\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.01211532240267843\n",
      "Training Loss: 0.01236988858319819\n",
      "Training Loss: 0.011425060889450833\n",
      "Validation Loss: 0.00955300596071763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.011607520037796349\n",
      "Training Loss: 0.011856706467224285\n",
      "Training Loss: 0.0109238009294495\n",
      "Validation Loss: 0.008964444178397233\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.011100444586481899\n",
      "Training Loss: 0.011360441003926098\n",
      "Training Loss: 0.010449027265422047\n",
      "Validation Loss: 0.008395437993569656\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.010625580219784752\n",
      "Training Loss: 0.010915710823610425\n",
      "Training Loss: 0.010029206203762442\n",
      "Validation Loss: 0.007881378655562574\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.010205648788250983\n",
      "Training Loss: 0.010538757807807997\n",
      "Training Loss: 0.009671681514009833\n",
      "Validation Loss: 0.007430076855496409\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.009843002785928548\n",
      "Training Loss: 0.010223481861175969\n",
      "Training Loss: 0.009367706142365933\n",
      "Validation Loss: 0.007035064207536451\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.009530771888094022\n",
      "Training Loss: 0.009956911558983848\n",
      "Training Loss: 0.00910699754487723\n",
      "Validation Loss: 0.006691096183121874\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.009261881243437528\n",
      "Training Loss: 0.009728025433141739\n",
      "Training Loss: 0.008881077877013012\n",
      "Validation Loss: 0.006392988983248727\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.009029034200357272\n",
      "Training Loss: 0.00952834328287281\n",
      "Training Loss: 0.00868260841583833\n",
      "Validation Loss: 0.006134223752770196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008825332265114411\n",
      "Training Loss: 0.009351710268529132\n",
      "Training Loss: 0.008505838757846504\n",
      "Validation Loss: 0.005908617226595289\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008645334019092843\n",
      "Training Loss: 0.009193878310034051\n",
      "Training Loss: 0.008346697146771475\n",
      "Validation Loss: 0.00571119034495414\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008485107229789719\n",
      "Training Loss: 0.009051984592806548\n",
      "Training Loss: 0.008202518481994048\n",
      "Validation Loss: 0.005538169200417031\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.00834197599091567\n",
      "Training Loss: 0.00892421516124159\n",
      "Training Loss: 0.008071769046364353\n",
      "Validation Loss: 0.005386699247435572\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008214232784230262\n",
      "Training Loss: 0.008809522085357457\n",
      "Training Loss: 0.007953729931032286\n",
      "Validation Loss: 0.005254562727039617\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008100826587760821\n",
      "Training Loss: 0.00870732142124325\n",
      "Training Loss: 0.007848127306206152\n",
      "Validation Loss: 0.005139932970338491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008001026863930748\n",
      "Training Loss: 0.008617184094619005\n",
      "Training Loss: 0.007754775563953444\n",
      "Validation Loss: 0.005041140152580953\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007914142657537013\n",
      "Training Loss: 0.008538605000358074\n",
      "Training Loss: 0.007673320481553674\n",
      "Validation Loss: 0.004956553857564256\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007839333828305825\n",
      "Training Loss: 0.008470865957206116\n",
      "Training Loss: 0.007603105733869598\n",
      "Validation Loss: 0.004884513832754299\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007775535538094118\n",
      "Training Loss: 0.00841299511375837\n",
      "Training Loss: 0.007543148645199835\n",
      "Validation Loss: 0.00482333999315507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007721476083388552\n",
      "Training Loss: 0.008363826068816707\n",
      "Training Loss: 0.0074922356067691\n",
      "Validation Loss: 0.004771387556389895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007675768662011251\n",
      "Training Loss: 0.008322085485560819\n",
      "Training Loss: 0.007449033296434209\n",
      "Validation Loss: 0.004727104601575836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007637018153909594\n",
      "Training Loss: 0.0082865050714463\n",
      "Training Loss: 0.007412221903214231\n",
      "Validation Loss: 0.004689093985270416\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007603915634099394\n",
      "Training Loss: 0.008255906882695854\n",
      "Training Loss: 0.007380579365417361\n",
      "Validation Loss: 0.004656143500888197\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007575299186864868\n",
      "Training Loss: 0.008229251430602744\n",
      "Training Loss: 0.007353043378097937\n",
      "Validation Loss: 0.004627238430626942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007550191904883831\n",
      "Training Loss: 0.008205669001908973\n",
      "Training Loss: 0.007328724919352681\n",
      "Validation Loss: 0.004601556100108231\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0075277954945340755\n",
      "Training Loss: 0.008184456046437845\n",
      "Training Loss: 0.007306907750898972\n",
      "Validation Loss: 0.00457844237294676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007507480761269107\n",
      "Training Loss: 0.008165058278245851\n",
      "Training Loss: 0.007287028037244454\n",
      "Validation Loss: 0.004557385946508874\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007488760262494907\n",
      "Training Loss: 0.008147047489183023\n",
      "Training Loss: 0.007268649544566869\n",
      "Validation Loss: 0.004537988802041398\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007471262386534363\n",
      "Training Loss: 0.008130099114496261\n",
      "Training Loss: 0.007251439644023776\n",
      "Validation Loss: 0.004519940741109044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007454707724973559\n",
      "Training Loss: 0.008113965460797772\n",
      "Training Loss: 0.007235141879646107\n",
      "Validation Loss: 0.004503007593637939\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007438882823335007\n",
      "Training Loss: 0.008098462122725324\n",
      "Training Loss: 0.007219565400155261\n",
      "Validation Loss: 0.004486996544462242\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007423628077376634\n",
      "Training Loss: 0.008083449736004696\n",
      "Training Loss: 0.007204562204424292\n",
      "Validation Loss: 0.004471759905703784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.0074088223685976114\n",
      "Training Loss: 0.008068821568740532\n",
      "Training Loss: 0.00719001805759035\n",
      "Validation Loss: 0.004457170640777671\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007394370014080778\n",
      "Training Loss: 0.008054495909018442\n",
      "Training Loss: 0.007175845589954406\n",
      "Validation Loss: 0.0044431294670349425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007380199161125347\n",
      "Training Loss: 0.00804040971561335\n",
      "Training Loss: 0.007161975691560656\n",
      "Validation Loss: 0.004429553690355983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.00736625102465041\n",
      "Training Loss: 0.008026513095246627\n",
      "Training Loss: 0.007148353200173005\n",
      "Validation Loss: 0.004416365210448255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007352479611290619\n",
      "Training Loss: 0.008012767907930539\n",
      "Training Loss: 0.007134934994392097\n",
      "Validation Loss: 0.004403505159091999\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007338847502833232\n",
      "Training Loss: 0.007999144024215638\n",
      "Training Loss: 0.007121687309117988\n",
      "Validation Loss: 0.00439091639467672\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007325325349811464\n",
      "Training Loss: 0.00798561615869403\n",
      "Training Loss: 0.007108582267537713\n",
      "Validation Loss: 0.004378550381169476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007311887997202575\n",
      "Training Loss: 0.00797216643113643\n",
      "Training Loss: 0.00709559811395593\n",
      "Validation Loss: 0.004366366212319038\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007298517927993089\n",
      "Training Loss: 0.007958780713379384\n",
      "Training Loss: 0.007082717851735651\n",
      "Validation Loss: 0.004354327363799211\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.00728520012460649\n",
      "Training Loss: 0.007945448881946504\n",
      "Training Loss: 0.0070699271955527365\n",
      "Validation Loss: 0.0043423999182021856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007271924738306552\n",
      "Training Loss: 0.007932164665544406\n",
      "Training Loss: 0.007057220936985686\n",
      "Validation Loss: 0.004330561417835147\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.00725868503912352\n",
      "Training Loss: 0.007918924299301579\n",
      "Training Loss: 0.0070445909956470135\n",
      "Validation Loss: 0.004318785945246561\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007245476859388873\n",
      "Training Loss: 0.007905728569021449\n",
      "Training Loss: 0.007032034761505201\n",
      "Validation Loss: 0.0043070536457378876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007232300883624703\n",
      "Training Loss: 0.007892578759929166\n",
      "Training Loss: 0.007019552456913516\n",
      "Validation Loss: 0.004295353037357497\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007219160095555708\n",
      "Training Loss: 0.007879480242263526\n",
      "Training Loss: 0.007007146225078031\n",
      "Validation Loss: 0.0042836719278894955\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007206059308955446\n",
      "Training Loss: 0.007866441175574437\n",
      "Training Loss: 0.006994820665568113\n",
      "Validation Loss: 0.0042720017627697815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007193007131572813\n",
      "Training Loss: 0.007853469119872898\n",
      "Training Loss: 0.0069825804547872395\n",
      "Validation Loss: 0.004260339452414198\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007180012307362631\n",
      "Training Loss: 0.007840574521105736\n",
      "Training Loss: 0.0069704344845376905\n",
      "Validation Loss: 0.004248681841836719\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007167087848065421\n",
      "Training Loss: 0.007827770260628314\n",
      "Training Loss: 0.006958391109947115\n",
      "Validation Loss: 0.004237032647171382\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007154245136771351\n",
      "Training Loss: 0.007815069392090663\n",
      "Training Loss: 0.0069464583473745735\n",
      "Validation Loss: 0.004225392511972551\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.00714150006766431\n",
      "Training Loss: 0.0078024846804328266\n",
      "Training Loss: 0.0069346493994817135\n",
      "Validation Loss: 0.004213771275296974\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007128867782885209\n",
      "Training Loss: 0.007790031481999904\n",
      "Training Loss: 0.006922973138280213\n",
      "Validation Loss: 0.004202175786028083\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007116363542154431\n",
      "Training Loss: 0.007777723317267373\n",
      "Training Loss: 0.006911441006232053\n",
      "Validation Loss: 0.004190614089903453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007104002712294459\n",
      "Training Loss: 0.007765574747463688\n",
      "Training Loss: 0.006900063036009669\n",
      "Validation Loss: 0.004179100706137382\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007091802948853001\n",
      "Training Loss: 0.007753599431598559\n",
      "Training Loss: 0.006888850687537342\n",
      "Validation Loss: 0.004167647840026138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007079777901526541\n",
      "Training Loss: 0.007741810439620167\n",
      "Training Loss: 0.006877814991166815\n",
      "Validation Loss: 0.004156265575229452\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007067942804424092\n",
      "Training Loss: 0.007730219450313598\n",
      "Training Loss: 0.0068669633124955\n",
      "Validation Loss: 0.004144972390390598\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007056312274653464\n",
      "Training Loss: 0.007718839031876996\n",
      "Training Loss: 0.006856305567780509\n",
      "Validation Loss: 0.004133781533311592\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007044897740706801\n",
      "Training Loss: 0.007707679069135338\n",
      "Training Loss: 0.006845848563825712\n",
      "Validation Loss: 0.004122702852205447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007033710679970682\n",
      "Training Loss: 0.007696746297879145\n",
      "Training Loss: 0.006835598904872313\n",
      "Validation Loss: 0.004111755746573712\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007022760492982343\n",
      "Training Loss: 0.007686050310730934\n",
      "Training Loss: 0.006825561665464192\n",
      "Validation Loss: 0.004100949517869799\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007012055213563144\n",
      "Training Loss: 0.007675593609455973\n",
      "Training Loss: 0.006815740627935156\n",
      "Validation Loss: 0.004090296608144731\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007001601916272193\n",
      "Training Loss: 0.007665381801780313\n",
      "Training Loss: 0.006806138791143894\n",
      "Validation Loss: 0.0040798086733191035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006991404659347608\n",
      "Training Loss: 0.007655416203197092\n",
      "Training Loss: 0.006796758022392169\n",
      "Validation Loss: 0.004069497208424917\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.0069814657047390935\n",
      "Training Loss: 0.007645698970882222\n",
      "Training Loss: 0.006787598232040182\n",
      "Validation Loss: 0.004059367773928836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006971787741640582\n",
      "Training Loss: 0.007636228317860514\n",
      "Training Loss: 0.006778658318798989\n",
      "Validation Loss: 0.004049433213449345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006962370104156434\n",
      "Training Loss: 0.007627002827357501\n",
      "Training Loss: 0.006769938099896535\n",
      "Validation Loss: 0.004039697219575807\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006953211528016254\n",
      "Training Loss: 0.007618018806679175\n",
      "Training Loss: 0.006761433068895713\n",
      "Validation Loss: 0.00403016816243906\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006944309016689658\n",
      "Training Loss: 0.007609272378031165\n",
      "Training Loss: 0.006753140244400129\n",
      "Validation Loss: 0.004020845089787931\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006935658047441393\n",
      "Training Loss: 0.007600757470354438\n",
      "Training Loss: 0.00674505450995639\n",
      "Validation Loss: 0.004011734785103982\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006927253501489758\n",
      "Training Loss: 0.0075924688868690285\n",
      "Training Loss: 0.006737171163549647\n",
      "Validation Loss: 0.004002840162694371\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006919088762952014\n",
      "Training Loss: 0.007584397782338783\n",
      "Training Loss: 0.006729483528761193\n",
      "Validation Loss: 0.003994158347493059\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0069111572706606235\n",
      "Training Loss: 0.007576537055429071\n",
      "Training Loss: 0.006721984868636355\n",
      "Validation Loss: 0.00398569300331259\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006903450451791287\n",
      "Training Loss: 0.007568877850426361\n",
      "Training Loss: 0.006714668308850378\n",
      "Validation Loss: 0.003977439610884012\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006895960120018572\n",
      "Training Loss: 0.00756141192978248\n",
      "Training Loss: 0.006707527498947457\n",
      "Validation Loss: 0.003969395537128191\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006888677445240319\n",
      "Training Loss: 0.0075541294435970486\n",
      "Training Loss: 0.006700552337570116\n",
      "Validation Loss: 0.0039615612006170696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0068815932935103775\n",
      "Training Loss: 0.007547021352220327\n",
      "Training Loss: 0.006693737027235329\n",
      "Validation Loss: 0.003953930123949821\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006874697533203289\n",
      "Training Loss: 0.0075400782085489485\n",
      "Training Loss: 0.006687072014901787\n",
      "Validation Loss: 0.003946500734865582\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006867980093229562\n",
      "Training Loss: 0.007533289261045866\n",
      "Training Loss: 0.006680549186421558\n",
      "Validation Loss: 0.003939263837861965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.00686143237631768\n",
      "Training Loss: 0.007526646059704945\n",
      "Training Loss: 0.006674161062110215\n",
      "Validation Loss: 0.003932216848257218\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006855042946990579\n",
      "Training Loss: 0.007520139045082033\n",
      "Training Loss: 0.0066678986418992284\n",
      "Validation Loss: 0.003925355478304993\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006848803766770288\n",
      "Training Loss: 0.007513757619890384\n",
      "Training Loss: 0.006661753966473043\n",
      "Validation Loss: 0.003918672730005608\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006842703060247004\n",
      "Training Loss: 0.007507493759621866\n",
      "Training Loss: 0.006655718454858288\n",
      "Validation Loss: 0.0039121623103914975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006836734368698671\n",
      "Training Loss: 0.00750133840017952\n",
      "Training Loss: 0.006649785897461697\n",
      "Validation Loss: 0.0039058160533856474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006830886504612863\n",
      "Training Loss: 0.00749528331507463\n",
      "Training Loss: 0.006643946752883494\n",
      "Validation Loss: 0.0038996318805167513\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.00682515068212524\n",
      "Training Loss: 0.00748931897978764\n",
      "Training Loss: 0.006638194731203839\n",
      "Validation Loss: 0.0038935946499447475\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0068195183505304156\n",
      "Training Loss: 0.007483438666677103\n",
      "Training Loss: 0.00663252197089605\n",
      "Validation Loss: 0.00388770659318131\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006813982529565692\n",
      "Training Loss: 0.00747763404797297\n",
      "Training Loss: 0.006626922420691699\n",
      "Validation Loss: 0.003881954607616566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006808533380972221\n",
      "Training Loss: 0.007471897040377371\n",
      "Training Loss: 0.006621388625353575\n",
      "Validation Loss: 0.003876338185732033\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006803164938464761\n",
      "Training Loss: 0.00746622261533048\n",
      "Training Loss: 0.0066159146488644184\n",
      "Validation Loss: 0.0038708479201232785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.0067978688760194925\n",
      "Training Loss: 0.007460604190710001\n",
      "Training Loss: 0.006610494295600801\n",
      "Validation Loss: 0.0038654735924027275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006792638568440452\n",
      "Training Loss: 0.007455032297875732\n",
      "Training Loss: 0.006605121266329661\n",
      "Validation Loss: 0.0038602109802258985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006787467841058969\n",
      "Training Loss: 0.0074495046318043025\n",
      "Training Loss: 0.006599790162872523\n",
      "Validation Loss: 0.0038550552857118878\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006782349845161661\n",
      "Training Loss: 0.007444014097563923\n",
      "Training Loss: 0.006594495525350794\n",
      "Validation Loss: 0.0038499965298439513\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006777279165107757\n",
      "Training Loss: 0.007438555136905052\n",
      "Training Loss: 0.006589232319965959\n",
      "Validation Loss: 0.0038450337551387674\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.00677225059363991\n",
      "Training Loss: 0.00743312391161453\n",
      "Training Loss: 0.006583997651468962\n",
      "Validation Loss: 0.00384015472098306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.0067672580247744914\n",
      "Training Loss: 0.007427714599762112\n",
      "Training Loss: 0.006578784339362756\n",
      "Validation Loss: 0.003835354488227893\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006762297054519877\n",
      "Training Loss: 0.007422323361388408\n",
      "Training Loss: 0.006573589728213846\n",
      "Validation Loss: 0.003830634256343493\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006757362697971985\n",
      "Training Loss: 0.007416945623699576\n",
      "Training Loss: 0.006568408597959205\n",
      "Validation Loss: 0.00382597883901653\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006752450994681567\n",
      "Training Loss: 0.007411578352912329\n",
      "Training Loss: 0.006563237619120628\n",
      "Validation Loss: 0.0038213875102862884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.00674755688989535\n",
      "Training Loss: 0.007406217142706737\n",
      "Training Loss: 0.006558073656633496\n",
      "Validation Loss: 0.0038168533650237355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006742677099537104\n",
      "Training Loss: 0.0074008588923607024\n",
      "Training Loss: 0.006552913158666343\n",
      "Validation Loss: 0.003812374146864488\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0067378081625793125\n",
      "Training Loss: 0.0073955001111607995\n",
      "Training Loss: 0.006547751908656209\n",
      "Validation Loss: 0.0038079389690138985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.0067329454165883365\n",
      "Training Loss: 0.007390136220492422\n",
      "Training Loss: 0.006542587137082592\n",
      "Validation Loss: 0.003803546486809599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006728087146766484\n",
      "Training Loss: 0.0073847658111481\n",
      "Training Loss: 0.006537416878854856\n",
      "Validation Loss: 0.0037991894890418215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006723227686015889\n",
      "Training Loss: 0.007379385834792629\n",
      "Training Loss: 0.006532236508792266\n",
      "Validation Loss: 0.0037948626467665092\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006718366477871314\n",
      "Training Loss: 0.007373993339715525\n",
      "Training Loss: 0.006527045166585595\n",
      "Validation Loss: 0.0037905613112190133\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006713498112512753\n",
      "Training Loss: 0.007368584367795848\n",
      "Training Loss: 0.006521838086191565\n",
      "Validation Loss: 0.0037862841560494867\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0067086210765410215\n",
      "Training Loss: 0.007363157230429351\n",
      "Training Loss: 0.006516614168649539\n",
      "Validation Loss: 0.0037820231603730597\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006703732297755778\n",
      "Training Loss: 0.007357708839699626\n",
      "Training Loss: 0.006511370874941349\n",
      "Validation Loss: 0.0037777718206281575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00669882905553095\n",
      "Training Loss: 0.007352236946462654\n",
      "Training Loss: 0.006506105413427577\n",
      "Validation Loss: 0.0037735294330907003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.0066939083940815184\n",
      "Training Loss: 0.007346738969208673\n",
      "Training Loss: 0.006500814706087112\n",
      "Validation Loss: 0.003769289635467144\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006688968674279749\n",
      "Training Loss: 0.0073412124637980015\n",
      "Training Loss: 0.006495497756404802\n",
      "Validation Loss: 0.0037650456076508826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006684005602728575\n",
      "Training Loss: 0.007335654174676165\n",
      "Training Loss: 0.0064901507494505494\n",
      "Validation Loss: 0.003760796753176896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006679018250433728\n",
      "Training Loss: 0.007330060919048265\n",
      "Training Loss: 0.006484772170661017\n",
      "Validation Loss: 0.003756533516832533\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006674002236686647\n",
      "Training Loss: 0.007324431270826608\n",
      "Training Loss: 0.006479359434451908\n",
      "Validation Loss: 0.0037522534421236997\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006668956539360806\n",
      "Training Loss: 0.007318761948263273\n",
      "Training Loss: 0.006473910639761016\n",
      "Validation Loss: 0.003747952992117472\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006663877324899659\n",
      "Training Loss: 0.007313050800003111\n",
      "Training Loss: 0.006468422337202355\n",
      "Validation Loss: 0.0037436253100821978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006658763405866921\n",
      "Training Loss: 0.007307294213096611\n",
      "Training Loss: 0.00646289355121553\n",
      "Validation Loss: 0.0037392668787055135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006653610339853912\n",
      "Training Loss: 0.007301488696248271\n",
      "Training Loss: 0.006457320499466732\n",
      "Validation Loss: 0.0037348715214351757\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006648416336392984\n",
      "Training Loss: 0.0072956311708549035\n",
      "Training Loss: 0.006451701365876943\n",
      "Validation Loss: 0.0037304366915225148\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006643178276717663\n",
      "Training Loss: 0.007289720209664665\n",
      "Training Loss: 0.0064460332598537205\n",
      "Validation Loss: 0.0037259576865733505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006637894286541268\n",
      "Training Loss: 0.007283750764327124\n",
      "Training Loss: 0.006440315273357556\n",
      "Validation Loss: 0.003721424821879338\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006632560640573501\n",
      "Training Loss: 0.007277720872079954\n",
      "Training Loss: 0.0064345428859815005\n",
      "Validation Loss: 0.003716840946345768\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006627174410969019\n",
      "Training Loss: 0.007271626014262438\n",
      "Training Loss: 0.0064287136413622645\n",
      "Validation Loss: 0.0037121910894854686\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00662173198943492\n",
      "Training Loss: 0.00726546278630849\n",
      "Training Loss: 0.006422825222834945\n",
      "Validation Loss: 0.003707477765346176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.0066162311885273085\n",
      "Training Loss: 0.007259228035109117\n",
      "Training Loss: 0.006416874269489199\n",
      "Validation Loss: 0.003702691374241971\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006610667851637117\n",
      "Training Loss: 0.007252916509751231\n",
      "Training Loss: 0.0064108579268213365\n",
      "Validation Loss: 0.003697827301417174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006605038706329652\n",
      "Training Loss: 0.0072465247701620685\n",
      "Training Loss: 0.0064047735615167765\n",
      "Validation Loss: 0.003692880177378571\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006599339504609816\n",
      "Training Loss: 0.007240047642262652\n",
      "Training Loss: 0.006398616874357685\n",
      "Validation Loss: 0.003687844092413532\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.0065935675567016004\n",
      "Training Loss: 0.007233481969451532\n",
      "Training Loss: 0.006392385382205248\n",
      "Validation Loss: 0.003682711407584086\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006587718806695193\n",
      "Training Loss: 0.007226821830263361\n",
      "Training Loss: 0.006386075916234404\n",
      "Validation Loss: 0.00367747696268299\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006581789466436021\n",
      "Training Loss: 0.007220062121050432\n",
      "Training Loss: 0.0063796836184337735\n",
      "Validation Loss: 0.0036721318395117695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006575774408993311\n",
      "Training Loss: 0.007213198789395392\n",
      "Training Loss: 0.006373205747222528\n",
      "Validation Loss: 0.0036666715004889483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006569669537129812\n",
      "Training Loss: 0.007206225197878666\n",
      "Training Loss: 0.006366637407336384\n",
      "Validation Loss: 0.003661087865867976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006563469922402873\n",
      "Training Loss: 0.007199135320843197\n",
      "Training Loss: 0.006359974798979238\n",
      "Validation Loss: 0.0036553691096834085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006557171063032001\n",
      "Training Loss: 0.00719192367978394\n",
      "Training Loss: 0.006353213989641517\n",
      "Validation Loss: 0.003649511433394856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.0065507677337154745\n",
      "Training Loss: 0.007184582303161733\n",
      "Training Loss: 0.006346350092208013\n",
      "Validation Loss: 0.0036435077134310528\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006544254026957788\n",
      "Training Loss: 0.007177106061717496\n",
      "Training Loss: 0.006339377663098275\n",
      "Validation Loss: 0.0036373456503170426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006537624099873938\n",
      "Training Loss: 0.007169485715567134\n",
      "Training Loss: 0.006332292116712779\n",
      "Validation Loss: 0.003631013987083616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006530871902941726\n",
      "Training Loss: 0.007161715331603773\n",
      "Training Loss: 0.00632508761365898\n",
      "Validation Loss: 0.0036245028532811262\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006523990465211682\n",
      "Training Loss: 0.0071537846047431235\n",
      "Training Loss: 0.006317758891964331\n",
      "Validation Loss: 0.0036178019782005066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006516973554971628\n",
      "Training Loss: 0.007145686098374427\n",
      "Training Loss: 0.006310298496391625\n",
      "Validation Loss: 0.003610898159821047\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006509812686708756\n",
      "Training Loss: 0.007137408212875016\n",
      "Training Loss: 0.006302701667882502\n",
      "Validation Loss: 0.0036037807834198637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006502499653724954\n",
      "Training Loss: 0.007128942337003536\n",
      "Training Loss: 0.006294959837105125\n",
      "Validation Loss: 0.003596431761456842\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006495026085758582\n",
      "Training Loss: 0.007120277155772783\n",
      "Training Loss: 0.006287065213546157\n",
      "Validation Loss: 0.003588838063264161\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006487382522318512\n",
      "Training Loss: 0.007111399215064012\n",
      "Training Loss: 0.006279009259305895\n",
      "Validation Loss: 0.0035809832298081745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006479557586135343\n",
      "Training Loss: 0.00710229518765118\n",
      "Training Loss: 0.006270783765939996\n",
      "Validation Loss: 0.003572847849814996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006471540350466967\n",
      "Training Loss: 0.007092951447702944\n",
      "Training Loss: 0.006262376396916807\n",
      "Validation Loss: 0.003564409888087866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006463317602174357\n",
      "Training Loss: 0.007083350521279499\n",
      "Training Loss: 0.0062537775817327204\n",
      "Validation Loss: 0.0035556480183862567\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006454875499475748\n",
      "Training Loss: 0.007073475635843351\n",
      "Training Loss: 0.006244973856955767\n",
      "Validation Loss: 0.00354654137359074\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006446199028287083\n",
      "Training Loss: 0.007063307001953944\n",
      "Training Loss: 0.006235951295820996\n",
      "Validation Loss: 0.0035370613770099\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006437270554015413\n",
      "Training Loss: 0.007052821333054453\n",
      "Training Loss: 0.006226693572243675\n",
      "Validation Loss: 0.0035271784643841424\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006428070566034876\n",
      "Training Loss: 0.007041996353655123\n",
      "Training Loss: 0.006217183909611777\n",
      "Validation Loss: 0.0035168623311047473\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006418577878503129\n",
      "Training Loss: 0.007030802866793238\n",
      "Training Loss: 0.0062074003228917716\n",
      "Validation Loss: 0.0035060758446081638\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006408768091932871\n",
      "Training Loss: 0.007019211044535041\n",
      "Training Loss: 0.006197321924846619\n",
      "Validation Loss: 0.003494783540078428\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006398615300422534\n",
      "Training Loss: 0.007007187416311353\n",
      "Training Loss: 0.0061869228340219705\n",
      "Validation Loss: 0.0034829388657752216\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006388088291278109\n",
      "Training Loss: 0.006994692547013983\n",
      "Training Loss: 0.006176172941923142\n",
      "Validation Loss: 0.003470498957315439\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006377153506036848\n",
      "Training Loss: 0.006981682487530633\n",
      "Training Loss: 0.006165039874613285\n",
      "Validation Loss: 0.003457413824128636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006365773172583431\n",
      "Training Loss: 0.0069681111245881765\n",
      "Training Loss: 0.00615348547231406\n",
      "Validation Loss: 0.003443630518171978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006353905425639823\n",
      "Training Loss: 0.0069539228617213665\n",
      "Training Loss: 0.0061414683237671856\n",
      "Validation Loss: 0.0034290890018403362\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006341502791037783\n",
      "Training Loss: 0.006939060139120557\n",
      "Training Loss: 0.006128940915223211\n",
      "Validation Loss: 0.003413733080542238\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.00632851539470721\n",
      "Training Loss: 0.006923456492950208\n",
      "Training Loss: 0.0061158519645687194\n",
      "Validation Loss: 0.003397500736304046\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006314886409672909\n",
      "Training Loss: 0.006907042675884441\n",
      "Training Loss: 0.006102142551681027\n",
      "Validation Loss: 0.003380325112722061\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006300556176574901\n",
      "Training Loss: 0.00688974094693549\n",
      "Training Loss: 0.006087750212755054\n",
      "Validation Loss: 0.0033621473428845573\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006285462051746436\n",
      "Training Loss: 0.006871473009814508\n",
      "Training Loss: 0.006072609781986102\n",
      "Validation Loss: 0.0033429142419939464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006269537839689292\n",
      "Training Loss: 0.006852158927940764\n",
      "Training Loss: 0.006056652398547158\n",
      "Validation Loss: 0.0033225856441481227\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006252722587669268\n",
      "Training Loss: 0.006831721554044634\n",
      "Training Loss: 0.00603980866027996\n",
      "Validation Loss: 0.0033011313336849045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006234957176493481\n",
      "Training Loss: 0.006810091976076365\n",
      "Training Loss: 0.006022015784401447\n",
      "Validation Loss: 0.003278553169325329\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006216193331638351\n",
      "Training Loss: 0.0067872159392572936\n",
      "Training Loss: 0.006003217428224161\n",
      "Validation Loss: 0.003254876601551607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006196397547610104\n",
      "Training Loss: 0.006763063499238342\n",
      "Training Loss: 0.005983374000061303\n",
      "Validation Loss: 0.0032301748372886457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.00617555778240785\n",
      "Training Loss: 0.006737633201410063\n",
      "Training Loss: 0.005962464666226879\n",
      "Validation Loss: 0.0032045626293428324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006153689463390038\n",
      "Training Loss: 0.006710968879051506\n",
      "Training Loss: 0.005940499769058079\n",
      "Validation Loss: 0.0031782043064431695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006130841020494699\n",
      "Training Loss: 0.006683156522340142\n",
      "Training Loss: 0.005917519656941294\n",
      "Validation Loss: 0.003151305189900351\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006107097707572393\n",
      "Training Loss: 0.0066543364676181225\n",
      "Training Loss: 0.005893599824048579\n",
      "Validation Loss: 0.0031241102515604724\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006082571768783965\n",
      "Training Loss: 0.006624684371054172\n",
      "Training Loss: 0.005868848459213041\n",
      "Validation Loss: 0.0030968693893381888\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006057407018961385\n",
      "Training Loss: 0.006594418183667585\n",
      "Training Loss: 0.0058434018742991615\n",
      "Validation Loss: 0.0030698347169633828\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006031760681653395\n",
      "Training Loss: 0.006563771083019674\n",
      "Training Loss: 0.005817416621139273\n",
      "Validation Loss: 0.0030432309343197037\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006005802037543617\n",
      "Training Loss: 0.006532984379446134\n",
      "Training Loss: 0.0057910633581923325\n",
      "Validation Loss: 0.0030172439501359224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005979695834685117\n",
      "Training Loss: 0.006502285844180733\n",
      "Training Loss: 0.005764513445319608\n",
      "Validation Loss: 0.0029920154745596344\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.0059536033263429995\n",
      "Training Loss: 0.006471886360086501\n",
      "Training Loss: 0.00573793756775558\n",
      "Validation Loss: 0.0029676454481825736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005927669085212983\n",
      "Training Loss: 0.006441968240542337\n",
      "Training Loss: 0.005711495160940103\n",
      "Validation Loss: 0.002944195517495777\n",
      "Validation Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [27:29<06:51, 205.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.09502113219350576\n",
      "Training Loss: 0.07161808228120208\n",
      "Training Loss: 0.06373452425003051\n",
      "Validation Loss: 0.05549835175108374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.058104544635862114\n",
      "Training Loss: 0.054334569964557884\n",
      "Training Loss: 0.05198818470351398\n",
      "Validation Loss: 0.04590269770431385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.04751501164399088\n",
      "Training Loss: 0.043554477607831356\n",
      "Training Loss: 0.04038690485060215\n",
      "Validation Loss: 0.03455553199635463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.03555230215191841\n",
      "Training Loss: 0.03188635110855102\n",
      "Training Loss: 0.02872796565759927\n",
      "Validation Loss: 0.024160690668426202\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.02514688574708998\n",
      "Training Loss: 0.022763860365375876\n",
      "Training Loss: 0.020600519618019463\n",
      "Validation Loss: 0.01772975954070185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.019029447101056574\n",
      "Training Loss: 0.017995897359214724\n",
      "Training Loss: 0.01664875712711364\n",
      "Validation Loss: 0.014724809901391188\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.0161882238346152\n",
      "Training Loss: 0.015862486886326223\n",
      "Training Loss: 0.014790454937610775\n",
      "Validation Loss: 0.013143496190229158\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.014652995031792671\n",
      "Training Loss: 0.01460531380958855\n",
      "Training Loss: 0.013571708430536091\n",
      "Validation Loss: 0.01194864891343907\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.013463676313403995\n",
      "Training Loss: 0.01351495107403025\n",
      "Training Loss: 0.012465618592686951\n",
      "Validation Loss: 0.01078175257347273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.01233748485101387\n",
      "Training Loss: 0.012465201426530258\n",
      "Training Loss: 0.011455838191322983\n",
      "Validation Loss: 0.009692116483543695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.01140496762469411\n",
      "Training Loss: 0.011636817825492472\n",
      "Training Loss: 0.010691157566616311\n",
      "Validation Loss: 0.00883369470077954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.010722233788110316\n",
      "Training Loss: 0.01102991264895536\n",
      "Training Loss: 0.010118396653560922\n",
      "Validation Loss: 0.008172391442937797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.01019608520786278\n",
      "Training Loss: 0.010555135898757726\n",
      "Training Loss: 0.00966214275569655\n",
      "Validation Loss: 0.007640661755472087\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.009766125604510307\n",
      "Training Loss: 0.010164808315457775\n",
      "Training Loss: 0.00928560234606266\n",
      "Validation Loss: 0.0071989260954008006\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.00940546089434065\n",
      "Training Loss: 0.009837861337000504\n",
      "Training Loss: 0.008970968287903815\n",
      "Validation Loss: 0.006826568598953191\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.009100701442221179\n",
      "Training Loss: 0.00956291135167703\n",
      "Training Loss: 0.008707389185437932\n",
      "Validation Loss: 0.0065113927232457345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.008843060313956812\n",
      "Training Loss: 0.00933176084770821\n",
      "Training Loss: 0.00848656470188871\n",
      "Validation Loss: 0.0062445841593605075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.008625412352848798\n",
      "Training Loss: 0.009137508420972154\n",
      "Training Loss: 0.008301474222680554\n",
      "Validation Loss: 0.006018854620123512\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.008441560704959557\n",
      "Training Loss: 0.008974166888510809\n",
      "Training Loss: 0.008146094693802296\n",
      "Validation Loss: 0.005827907852179716\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.008286125817103312\n",
      "Training Loss: 0.008836630203295498\n",
      "Training Loss: 0.008015354191884398\n",
      "Validation Loss: 0.005666314579776666\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008154520172392949\n",
      "Training Loss: 0.008720603436231614\n",
      "Training Loss: 0.007905035486910492\n",
      "Validation Loss: 0.005529419254783666\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0080428785411641\n",
      "Training Loss: 0.008622515950119123\n",
      "Training Loss: 0.007811666784109547\n",
      "Validation Loss: 0.005413267879192246\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.007947967231739312\n",
      "Training Loss: 0.008539406979689374\n",
      "Training Loss: 0.007732391842873767\n",
      "Validation Loss: 0.005314510396754976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.007867083735764027\n",
      "Training Loss: 0.008468807569006458\n",
      "Training Loss: 0.007664847589330748\n",
      "Validation Loss: 0.005230311346188021\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0077979565062560145\n",
      "Training Loss: 0.008408652268117293\n",
      "Training Loss: 0.007607070938684046\n",
      "Validation Loss: 0.005158278222582983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007738675259752199\n",
      "Training Loss: 0.008357209386304021\n",
      "Training Loss: 0.00755742812412791\n",
      "Validation Loss: 0.005096391863660531\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.007687634025933221\n",
      "Training Loss: 0.008313025551615283\n",
      "Training Loss: 0.007514550271444023\n",
      "Validation Loss: 0.005042953552741013\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007643479491816834\n",
      "Training Loss: 0.008274875144707039\n",
      "Training Loss: 0.007477291226387024\n",
      "Validation Loss: 0.004996529788176498\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.007605067157419399\n",
      "Training Loss: 0.008241727774729953\n",
      "Training Loss: 0.0074446892226114865\n",
      "Validation Loss: 0.004955923263234704\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.007571438481099904\n",
      "Training Loss: 0.0082127224188298\n",
      "Training Loss: 0.007415938245831057\n",
      "Validation Loss: 0.004920123984744207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.00754178109113127\n",
      "Training Loss: 0.008187129058060237\n",
      "Training Loss: 0.007390358772827312\n",
      "Validation Loss: 0.004888291954680273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007515413197688758\n",
      "Training Loss: 0.008164340778021141\n",
      "Training Loss: 0.007367384085664525\n",
      "Validation Loss: 0.004859718148795406\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007491764241131023\n",
      "Training Loss: 0.008143847493920475\n",
      "Training Loss: 0.0073465385055169466\n",
      "Validation Loss: 0.0048338164303326206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007470354189863428\n",
      "Training Loss: 0.0081252265261719\n",
      "Training Loss: 0.007327425074763596\n",
      "Validation Loss: 0.004810091874593597\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007450779728824273\n",
      "Training Loss: 0.008108120498945937\n",
      "Training Loss: 0.007309712196001783\n",
      "Validation Loss: 0.004788135623119855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007432704721577465\n",
      "Training Loss: 0.008092236049124039\n",
      "Training Loss: 0.007293122574919835\n",
      "Validation Loss: 0.004767605713097735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0074158476060256365\n",
      "Training Loss: 0.008077328246436082\n",
      "Training Loss: 0.0072774279303848746\n",
      "Validation Loss: 0.004748215673889002\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007399973495630548\n",
      "Training Loss: 0.008063192502013407\n",
      "Training Loss: 0.007262438657926395\n",
      "Validation Loss: 0.004729727850808354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007384888253873214\n",
      "Training Loss: 0.008049662805278785\n",
      "Training Loss: 0.0072479989274870605\n",
      "Validation Loss: 0.0047119464411327005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.00737042895401828\n",
      "Training Loss: 0.008036600520135835\n",
      "Training Loss: 0.007233982244506478\n",
      "Validation Loss: 0.0046947088113494136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007356463541509583\n",
      "Training Loss: 0.008023893148638307\n",
      "Training Loss: 0.007220286946976557\n",
      "Validation Loss: 0.004677877350699868\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007342882084194571\n",
      "Training Loss: 0.008011448872857728\n",
      "Training Loss: 0.007206830458017066\n",
      "Validation Loss: 0.004661342856380042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0073295940598472956\n",
      "Training Loss: 0.007999192693969235\n",
      "Training Loss: 0.007193546131020412\n",
      "Validation Loss: 0.004645012999267391\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007316527472576126\n",
      "Training Loss: 0.007987065362976864\n",
      "Training Loss: 0.00718038501101546\n",
      "Validation Loss: 0.004628812917769792\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007303624462801963\n",
      "Training Loss: 0.00797501885856036\n",
      "Training Loss: 0.007167307618074119\n",
      "Validation Loss: 0.004612685489897313\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007290839740308002\n",
      "Training Loss: 0.007963016316643916\n",
      "Training Loss: 0.007154285253491253\n",
      "Validation Loss: 0.004596584740754092\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0072781377320643515\n",
      "Training Loss: 0.007951029385440051\n",
      "Training Loss: 0.007141299304785207\n",
      "Validation Loss: 0.004580479047229785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007265493972226977\n",
      "Training Loss: 0.007939040976925753\n",
      "Training Loss: 0.007128337214235217\n",
      "Validation Loss: 0.004564344786170326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007252893024124205\n",
      "Training Loss: 0.007927040187641978\n",
      "Training Loss: 0.007115396274020896\n",
      "Validation Loss: 0.0045481761211131734\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007240328108891845\n",
      "Training Loss: 0.007915024496614933\n",
      "Training Loss: 0.007102479316527024\n",
      "Validation Loss: 0.004531976764268252\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.0072278027329593895\n",
      "Training Loss: 0.007903003085521049\n",
      "Training Loss: 0.007089599029859528\n",
      "Validation Loss: 0.004515760329688031\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007215327224694193\n",
      "Training Loss: 0.007890988999861293\n",
      "Training Loss: 0.0070767742220778015\n",
      "Validation Loss: 0.0044995574908477535\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00720292137353681\n",
      "Training Loss: 0.007879008406307548\n",
      "Training Loss: 0.007064027754822746\n",
      "Validation Loss: 0.004483403965014588\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007190610207617283\n",
      "Training Loss: 0.007867089671781286\n",
      "Training Loss: 0.00705138934426941\n",
      "Validation Loss: 0.004467348327462593\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007178420620039106\n",
      "Training Loss: 0.007855264091049322\n",
      "Training Loss: 0.007038888883544132\n",
      "Validation Loss: 0.004451436785918274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007166383669245988\n",
      "Training Loss: 0.007843565634684637\n",
      "Training Loss: 0.007026555815245956\n",
      "Validation Loss: 0.004435723440805345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007154524868819862\n",
      "Training Loss: 0.007832023439696058\n",
      "Training Loss: 0.007014413886936382\n",
      "Validation Loss: 0.004420248239583681\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007142866732319817\n",
      "Training Loss: 0.007820661567384377\n",
      "Training Loss: 0.007002483208198101\n",
      "Validation Loss: 0.004405049410952109\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00713142397464253\n",
      "Training Loss: 0.007809497294947505\n",
      "Training Loss: 0.00699077574769035\n",
      "Validation Loss: 0.004390158209154445\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007120205882238224\n",
      "Training Loss: 0.0077985398971941325\n",
      "Training Loss: 0.006979297951329499\n",
      "Validation Loss: 0.004375593922509069\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.00710921548306942\n",
      "Training Loss: 0.007787792454473674\n",
      "Training Loss: 0.00696805001818575\n",
      "Validation Loss: 0.004361372828232438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007098449625773356\n",
      "Training Loss: 0.007777254786342383\n",
      "Training Loss: 0.0069570270238909875\n",
      "Validation Loss: 0.004347496624157\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.0070879031112417576\n",
      "Training Loss: 0.007766920097637921\n",
      "Training Loss: 0.006946223615668714\n",
      "Validation Loss: 0.00433397052281149\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007077567218802869\n",
      "Training Loss: 0.0077567800530232486\n",
      "Training Loss: 0.006935629030922427\n",
      "Validation Loss: 0.004320792151416202\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007067432538606227\n",
      "Training Loss: 0.00774682525137905\n",
      "Training Loss: 0.006925232860958204\n",
      "Validation Loss: 0.0043079521888971664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007057487042620778\n",
      "Training Loss: 0.007737044870737009\n",
      "Training Loss: 0.00691502518253401\n",
      "Validation Loss: 0.004295444569885312\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007047720450209453\n",
      "Training Loss: 0.007727427572244778\n",
      "Training Loss: 0.00690499440766871\n",
      "Validation Loss: 0.00428326098965167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0070381226856261496\n",
      "Training Loss: 0.0077179635426728056\n",
      "Training Loss: 0.006895130722550675\n",
      "Validation Loss: 0.004271389641851354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007028683895478025\n",
      "Training Loss: 0.00770864118123427\n",
      "Training Loss: 0.0068854248127900065\n",
      "Validation Loss: 0.0042598185016972465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0070193941064644605\n",
      "Training Loss: 0.007699452820816077\n",
      "Training Loss: 0.006875865736510604\n",
      "Validation Loss: 0.004248536664082177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007010245794663206\n",
      "Training Loss: 0.007690388330956921\n",
      "Training Loss: 0.006866447315551341\n",
      "Validation Loss: 0.004237533214742716\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007001231153262779\n",
      "Training Loss: 0.007681439568987116\n",
      "Training Loss: 0.006857160626677797\n",
      "Validation Loss: 0.004226793361084766\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006992342331213877\n",
      "Training Loss: 0.0076725980476476255\n",
      "Training Loss: 0.0068479979957919565\n",
      "Validation Loss: 0.004216311737539225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006983573930338025\n",
      "Training Loss: 0.007663856792496517\n",
      "Training Loss: 0.0068389545055106285\n",
      "Validation Loss: 0.004206070883115858\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006974919632775709\n",
      "Training Loss: 0.007655209430959075\n",
      "Training Loss: 0.006830021055648104\n",
      "Validation Loss: 0.004196063590529008\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.0069663723820121955\n",
      "Training Loss: 0.007646647991496138\n",
      "Training Loss: 0.006821191938361153\n",
      "Validation Loss: 0.004186277538067086\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006957927649491467\n",
      "Training Loss: 0.007638166676624678\n",
      "Training Loss: 0.006812460913788527\n",
      "Validation Loss: 0.004176705054091185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0069495798216667026\n",
      "Training Loss: 0.007629758780822158\n",
      "Training Loss: 0.006803820489440113\n",
      "Validation Loss: 0.004167331806436348\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.0069413217576220635\n",
      "Training Loss: 0.007621417483896949\n",
      "Training Loss: 0.006795264050597325\n",
      "Validation Loss: 0.004158152297421704\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006933148541138507\n",
      "Training Loss: 0.0076131353591335936\n",
      "Training Loss: 0.006786782809067518\n",
      "Validation Loss: 0.004149153822080724\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0069250530714634806\n",
      "Training Loss: 0.007604906049673446\n",
      "Training Loss: 0.00677837026421912\n",
      "Validation Loss: 0.0041403284981804955\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0069170280813705175\n",
      "Training Loss: 0.0075967209727969024\n",
      "Training Loss: 0.006770017630187795\n",
      "Validation Loss: 0.004131665055671435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006909064948558807\n",
      "Training Loss: 0.007588570568477735\n",
      "Training Loss: 0.006761715232860297\n",
      "Validation Loss: 0.004123152826201129\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006901154150837101\n",
      "Training Loss: 0.007580448919907212\n",
      "Training Loss: 0.0067534527601674195\n",
      "Validation Loss: 0.004114777240849864\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006893285032128915\n",
      "Training Loss: 0.007572343504289165\n",
      "Training Loss: 0.006745221203891561\n",
      "Validation Loss: 0.004106525781700451\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006885446301894262\n",
      "Training Loss: 0.007564245638204739\n",
      "Training Loss: 0.006737007373012602\n",
      "Validation Loss: 0.00409838341286385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006877622258034535\n",
      "Training Loss: 0.007556142181274481\n",
      "Training Loss: 0.0067288000730331985\n",
      "Validation Loss: 0.004090328606661786\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006869794801459648\n",
      "Training Loss: 0.00754801856179256\n",
      "Training Loss: 0.006720580228138715\n",
      "Validation Loss: 0.004082340684332205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006861942376708612\n",
      "Training Loss: 0.007539857176598161\n",
      "Training Loss: 0.006712328540161252\n",
      "Validation Loss: 0.004074394506705778\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006854038496967405\n",
      "Training Loss: 0.00753163565590512\n",
      "Training Loss: 0.006704018589807674\n",
      "Validation Loss: 0.0040664608077554225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0068460502044763415\n",
      "Training Loss: 0.007523328617098741\n",
      "Training Loss: 0.006695619069505483\n",
      "Validation Loss: 0.004058507815254539\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006837939855176956\n",
      "Training Loss: 0.007514902819530107\n",
      "Training Loss: 0.006687091510975733\n",
      "Validation Loss: 0.004050505775278121\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006829665126861073\n",
      "Training Loss: 0.007506321955588646\n",
      "Training Loss: 0.0066783925809431825\n",
      "Validation Loss: 0.004042418461732483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006821178420796059\n",
      "Training Loss: 0.007497545887599699\n",
      "Training Loss: 0.0066694724280387165\n",
      "Validation Loss: 0.004034211869571316\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006812429172568955\n",
      "Training Loss: 0.007488528401590883\n",
      "Training Loss: 0.006660277820192278\n",
      "Validation Loss: 0.004025844545783789\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006803362542996183\n",
      "Training Loss: 0.007479221314424649\n",
      "Training Loss: 0.006650749266846105\n",
      "Validation Loss: 0.0040172746057067525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.0067939224501606076\n",
      "Training Loss: 0.007469573278794997\n",
      "Training Loss: 0.006640827745432034\n",
      "Validation Loss: 0.004008450125490598\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006784053242299705\n",
      "Training Loss: 0.007459532265784219\n",
      "Training Loss: 0.006630451962118969\n",
      "Validation Loss: 0.00399931375816297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006773700565099716\n",
      "Training Loss: 0.007449046506662853\n",
      "Training Loss: 0.0066195612587034705\n",
      "Validation Loss: 0.003989802566199993\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006762812488595955\n",
      "Training Loss: 0.007438063176814467\n",
      "Training Loss: 0.00660809859749861\n",
      "Validation Loss: 0.00397985035114074\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006751341872732155\n",
      "Training Loss: 0.007426532995887101\n",
      "Training Loss: 0.006596007642801851\n",
      "Validation Loss: 0.003969383705930596\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006739247621735558\n",
      "Training Loss: 0.007414406506577507\n",
      "Training Loss: 0.006583238458260894\n",
      "Validation Loss: 0.003958334306547983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006726496323826723\n",
      "Training Loss: 0.00740163991576992\n",
      "Training Loss: 0.006569746563327499\n",
      "Validation Loss: 0.0039466336458461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006713060668553226\n",
      "Training Loss: 0.007388191069476306\n",
      "Training Loss: 0.006555493495543487\n",
      "Validation Loss: 0.003934221143384328\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006698926204699092\n",
      "Training Loss: 0.00737402310653124\n",
      "Training Loss: 0.0065404518943978475\n",
      "Validation Loss: 0.003921034550974459\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006684083798900247\n",
      "Training Loss: 0.007359104826464318\n",
      "Training Loss: 0.006524602447170764\n",
      "Validation Loss: 0.003907032656284531\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006668533640913665\n",
      "Training Loss: 0.007343411670881324\n",
      "Training Loss: 0.006507934104884043\n",
      "Validation Loss: 0.0038921655132173657\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006652282587601803\n",
      "Training Loss: 0.007326925090746954\n",
      "Training Loss: 0.006490445964736864\n",
      "Validation Loss: 0.003876397930336802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006635344729293138\n",
      "Training Loss: 0.007309634566190652\n",
      "Training Loss: 0.006472148269531317\n",
      "Validation Loss: 0.0038596999391997126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.00661774079722818\n",
      "Training Loss: 0.007291543015162461\n",
      "Training Loss: 0.006453061320935376\n",
      "Validation Loss: 0.0038420545026199536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.00659950069792103\n",
      "Training Loss: 0.00727266566187609\n",
      "Training Loss: 0.006433219860191457\n",
      "Validation Loss: 0.003823461986847975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006580671475385316\n",
      "Training Loss: 0.007253038269700482\n",
      "Training Loss: 0.006412679564673454\n",
      "Validation Loss: 0.0038039366534147203\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006561315594008192\n",
      "Training Loss: 0.0072327189613133665\n",
      "Training Loss: 0.0063915194617584345\n",
      "Validation Loss: 0.003783532188096073\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0065415242122253405\n",
      "Training Loss: 0.00721179974207189\n",
      "Training Loss: 0.006369848681497387\n",
      "Validation Loss: 0.003762329220928754\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006521413325099274\n",
      "Training Loss: 0.007190401856787503\n",
      "Training Loss: 0.006347808400169015\n",
      "Validation Loss: 0.003740452301918707\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.00650113238079939\n",
      "Training Loss: 0.007168686056975276\n",
      "Training Loss: 0.006325575214577839\n",
      "Validation Loss: 0.003718068129958564\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.00648085952270776\n",
      "Training Loss: 0.007146847628755495\n",
      "Training Loss: 0.0063033556705340745\n",
      "Validation Loss: 0.0036953840956740667\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006460798095213249\n",
      "Training Loss: 0.007125110570923425\n",
      "Training Loss: 0.0062813819979783145\n",
      "Validation Loss: 0.0036726432127794357\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006441163212875836\n",
      "Training Loss: 0.007103714406839572\n",
      "Training Loss: 0.006259893840760924\n",
      "Validation Loss: 0.003650104595620311\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006422164170071482\n",
      "Training Loss: 0.0070828983990941195\n",
      "Training Loss: 0.006239118663361296\n",
      "Validation Loss: 0.003628033212009357\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006403991236002184\n",
      "Training Loss: 0.007062882544123568\n",
      "Training Loss: 0.006219259225181304\n",
      "Validation Loss: 0.0036066747322845995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006386795111466199\n",
      "Training Loss: 0.007043846618034877\n",
      "Training Loss: 0.00620046827243641\n",
      "Validation Loss: 0.003586238195198808\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006370673212222755\n",
      "Training Loss: 0.007025917882565409\n",
      "Training Loss: 0.0061828407028224315\n",
      "Validation Loss: 0.0035668762460916064\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006355664810398593\n",
      "Training Loss: 0.007009163015172817\n",
      "Training Loss: 0.006166409364668652\n",
      "Validation Loss: 0.0035486822448189508\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006341756365145557\n",
      "Training Loss: 0.0069935891393106435\n",
      "Training Loss: 0.006151149710640311\n",
      "Validation Loss: 0.003531696372979394\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006328886279952713\n",
      "Training Loss: 0.0069791511900257315\n",
      "Training Loss: 0.00613699147826992\n",
      "Validation Loss: 0.003515892273620859\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006316959306132048\n",
      "Training Loss: 0.006965765557251871\n",
      "Training Loss: 0.006123834534664638\n",
      "Validation Loss: 0.0035012077770373794\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.0063058642589021476\n",
      "Training Loss: 0.006953324753558263\n",
      "Training Loss: 0.006111559843411669\n",
      "Validation Loss: 0.0034875575141730102\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006295481147826649\n",
      "Training Loss: 0.006941710537066683\n",
      "Training Loss: 0.006100045468192547\n",
      "Validation Loss: 0.003474828104352516\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.00628569568390958\n",
      "Training Loss: 0.006930802861461416\n",
      "Training Loss: 0.00608917458273936\n",
      "Validation Loss: 0.0034629110484959537\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006276402632356621\n",
      "Training Loss: 0.006920493005891331\n",
      "Training Loss: 0.006078842567512765\n",
      "Validation Loss: 0.0034516953661765776\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006267510075122118\n",
      "Training Loss: 0.00691067794628907\n",
      "Training Loss: 0.006068955726223066\n",
      "Validation Loss: 0.0034410711822591804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006258940657717176\n",
      "Training Loss: 0.006901273767580278\n",
      "Training Loss: 0.00605943747679703\n",
      "Validation Loss: 0.0034309550204178257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006250631631701253\n",
      "Training Loss: 0.006892207448254339\n",
      "Training Loss: 0.006050224141799845\n",
      "Validation Loss: 0.0034212608771984664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006242530855233781\n",
      "Training Loss: 0.006883418360375799\n",
      "Training Loss: 0.006041263648658059\n",
      "Validation Loss: 0.0034119219746796435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006234598078299314\n",
      "Training Loss: 0.0068748591165058315\n",
      "Training Loss: 0.006032513427198865\n",
      "Validation Loss: 0.0034028756199404597\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.0062268004415091125\n",
      "Training Loss: 0.006866488509112969\n",
      "Training Loss: 0.006023940169834532\n",
      "Validation Loss: 0.0033940757164375836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.0062191128858830776\n",
      "Training Loss: 0.006858274944243022\n",
      "Training Loss: 0.006015515283797868\n",
      "Validation Loss: 0.003385480703901123\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006211514715105295\n",
      "Training Loss: 0.006850193240097724\n",
      "Training Loss: 0.006007218513987027\n",
      "Validation Loss: 0.0033770580278981604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00620398900937289\n",
      "Training Loss: 0.006842221670085564\n",
      "Training Loss: 0.005999029746162705\n",
      "Validation Loss: 0.003368774965258964\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006196522432728671\n",
      "Training Loss: 0.006834343646187335\n",
      "Training Loss: 0.005990934535511769\n",
      "Validation Loss: 0.0033606132230815594\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.0061891045648371805\n",
      "Training Loss: 0.006826544994255528\n",
      "Training Loss: 0.005982921051909216\n",
      "Validation Loss: 0.0033525510277766526\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006181726137292572\n",
      "Training Loss: 0.0068188149441266435\n",
      "Training Loss: 0.005974976751604117\n",
      "Validation Loss: 0.0033445732191916596\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006174379690783098\n",
      "Training Loss: 0.006811141914222389\n",
      "Training Loss: 0.005967094546649605\n",
      "Validation Loss: 0.0033366671276770614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006167058584396728\n",
      "Training Loss: 0.006803519623936154\n",
      "Training Loss: 0.0059592648991383616\n",
      "Validation Loss: 0.003328822809533122\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006159756460692734\n",
      "Training Loss: 0.006795940630836412\n",
      "Training Loss: 0.005951482291566208\n",
      "Validation Loss: 0.003321028703017935\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006152470464003273\n",
      "Training Loss: 0.006788398710195907\n",
      "Training Loss: 0.005943740194779821\n",
      "Validation Loss: 0.0033132818820520065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006145195151912048\n",
      "Training Loss: 0.006780890819500201\n",
      "Training Loss: 0.00593603296612855\n",
      "Validation Loss: 0.0033055749025949267\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006137928457465022\n",
      "Training Loss: 0.006773410579189659\n",
      "Training Loss: 0.005928356613731011\n",
      "Validation Loss: 0.003297901801304536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006130665566888638\n",
      "Training Loss: 0.0067659566272050145\n",
      "Training Loss: 0.005920707525219769\n",
      "Validation Loss: 0.003290259633788818\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006123404990648851\n",
      "Training Loss: 0.006758523601456545\n",
      "Training Loss: 0.005913081709295511\n",
      "Validation Loss: 0.0032826463372729133\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.00611614424851723\n",
      "Training Loss: 0.006751111110206693\n",
      "Training Loss: 0.005905476832413115\n",
      "Validation Loss: 0.0032750599758615824\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006108881591353565\n",
      "Training Loss: 0.006743715282646008\n",
      "Training Loss: 0.005897889783373102\n",
      "Validation Loss: 0.003267500486769033\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006101615569205024\n",
      "Training Loss: 0.006736336188623682\n",
      "Training Loss: 0.005890320023754611\n",
      "Validation Loss: 0.003259960728111478\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006094344880548306\n",
      "Training Loss: 0.006728970278054476\n",
      "Training Loss: 0.005882763147237711\n",
      "Validation Loss: 0.0032524441020939958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006087068333290518\n",
      "Training Loss: 0.006721617334987968\n",
      "Training Loss: 0.00587522016488947\n",
      "Validation Loss: 0.003244950870324052\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006079785899491981\n",
      "Training Loss: 0.006714275839622133\n",
      "Training Loss: 0.005867688977741636\n",
      "Validation Loss: 0.0032374811374446316\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006072496332344599\n",
      "Training Loss: 0.006706946558551863\n",
      "Training Loss: 0.005860168678336777\n",
      "Validation Loss: 0.0032300311899377725\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006065200314624235\n",
      "Training Loss: 0.006699626548797823\n",
      "Training Loss: 0.005852659250376746\n",
      "Validation Loss: 0.0032226050212414243\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006057896689744666\n",
      "Training Loss: 0.006692317958804778\n",
      "Training Loss: 0.00584516012808308\n",
      "Validation Loss: 0.0032151996581867505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006050586761557497\n",
      "Training Loss: 0.0066850189818069335\n",
      "Training Loss: 0.005837671331246384\n",
      "Validation Loss: 0.003207820382628464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006043271529488265\n",
      "Training Loss: 0.006677731277304702\n",
      "Training Loss: 0.005830193742876872\n",
      "Validation Loss: 0.0032004669591198477\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006035950584919192\n",
      "Training Loss: 0.006670455082203261\n",
      "Training Loss: 0.005822728377534076\n",
      "Validation Loss: 0.0031931414661322084\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006028626008774154\n",
      "Training Loss: 0.006663190187537111\n",
      "Training Loss: 0.005815275537897833\n",
      "Validation Loss: 0.0031858409370368952\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006021297618863173\n",
      "Training Loss: 0.006655937706818804\n",
      "Training Loss: 0.005807836069725454\n",
      "Validation Loss: 0.0031785750486940313\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006013967948965728\n",
      "Training Loss: 0.00664869919011835\n",
      "Training Loss: 0.005800410473020747\n",
      "Validation Loss: 0.00317133959576362\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006006638183025643\n",
      "Training Loss: 0.006641473196796141\n",
      "Training Loss: 0.0057930014759767804\n",
      "Validation Loss: 0.0031641320550416627\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005999309718608856\n",
      "Training Loss: 0.006634263656451367\n",
      "Training Loss: 0.005785610043094493\n",
      "Validation Loss: 0.0031569640288192234\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.00599198485375382\n",
      "Training Loss: 0.006627071377006359\n",
      "Training Loss: 0.005778238413622603\n",
      "Validation Loss: 0.003149834048170387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005984665132709779\n",
      "Training Loss: 0.00661989621934481\n",
      "Training Loss: 0.005770886656246148\n",
      "Validation Loss: 0.003142744563049061\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005977352372137829\n",
      "Training Loss: 0.006612741212593392\n",
      "Training Loss: 0.005763559007900767\n",
      "Validation Loss: 0.003135693339327497\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005970048914314248\n",
      "Training Loss: 0.0066056069376645606\n",
      "Training Loss: 0.005756255774758756\n",
      "Validation Loss: 0.0031286889132572694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005962757462984882\n",
      "Training Loss: 0.006598495768848807\n",
      "Training Loss: 0.005748979690251872\n",
      "Validation Loss: 0.0031217317400353676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.00595548048033379\n",
      "Training Loss: 0.00659140884235967\n",
      "Training Loss: 0.0057417334290221335\n",
      "Validation Loss: 0.0031148215907552605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.0059482193877920505\n",
      "Training Loss: 0.006584348450414837\n",
      "Training Loss: 0.005734518390381709\n",
      "Validation Loss: 0.0031079652279699115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005940977375721559\n",
      "Training Loss: 0.006577316725160927\n",
      "Training Loss: 0.005727338170981966\n",
      "Validation Loss: 0.003101158118021957\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005933757142047398\n",
      "Training Loss: 0.006570313981501386\n",
      "Training Loss: 0.0057201933523174375\n",
      "Validation Loss: 0.0030944117951845185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005926560117513873\n",
      "Training Loss: 0.00656334453553427\n",
      "Training Loss: 0.005713087989715859\n",
      "Validation Loss: 0.0030877209108929786\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005919390480266884\n",
      "Training Loss: 0.006556408616597764\n",
      "Training Loss: 0.005706023956881836\n",
      "Validation Loss: 0.0030810920628875998\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005912248359527439\n",
      "Training Loss: 0.006549506896408275\n",
      "Training Loss: 0.00569900142843835\n",
      "Validation Loss: 0.003074527754751819\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005905136452056468\n",
      "Training Loss: 0.006542642256245017\n",
      "Training Loss: 0.005692024606978521\n",
      "Validation Loss: 0.0030680299079924654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005898058645543642\n",
      "Training Loss: 0.006535817249095998\n",
      "Training Loss: 0.005685095678782091\n",
      "Validation Loss: 0.00306160013018741\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005891017906251363\n",
      "Training Loss: 0.006529033129918389\n",
      "Training Loss: 0.005678216617670841\n",
      "Validation Loss: 0.0030552405730580514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005884014851762914\n",
      "Training Loss: 0.006522291595465504\n",
      "Training Loss: 0.005671388501650654\n",
      "Validation Loss: 0.0030489525420256377\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.00587705165205989\n",
      "Training Loss: 0.006515592054929584\n",
      "Training Loss: 0.00566461417474784\n",
      "Validation Loss: 0.0030427408624398573\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005870130116818473\n",
      "Training Loss: 0.006508938326733187\n",
      "Training Loss: 0.005657894210307859\n",
      "Validation Loss: 0.003036605508139964\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005863252044073306\n",
      "Training Loss: 0.006502330370713025\n",
      "Training Loss: 0.005651230759103783\n",
      "Validation Loss: 0.003030546377099046\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005856420478085056\n",
      "Training Loss: 0.006495769298635423\n",
      "Training Loss: 0.005644625262357294\n",
      "Validation Loss: 0.0030245630494371224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005849635315244086\n",
      "Training Loss: 0.006489256253698841\n",
      "Training Loss: 0.005638080130447634\n",
      "Validation Loss: 0.003018662088885485\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005842898961273022\n",
      "Training Loss: 0.00648279337619897\n",
      "Training Loss: 0.005631595138693228\n",
      "Validation Loss: 0.0030128413960442283\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005836212969152257\n",
      "Training Loss: 0.006476379942032508\n",
      "Training Loss: 0.005625171404099092\n",
      "Validation Loss: 0.003007102070972742\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005829578369739466\n",
      "Training Loss: 0.006470016723615118\n",
      "Training Loss: 0.005618811930762604\n",
      "Validation Loss: 0.003001448983493983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005822996782953851\n",
      "Training Loss: 0.006463705427595414\n",
      "Training Loss: 0.005612514964886941\n",
      "Validation Loss: 0.002995879530613677\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005816467774566263\n",
      "Training Loss: 0.006457447155262344\n",
      "Training Loss: 0.00560628229228314\n",
      "Validation Loss: 0.002990389845773494\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005809994282899424\n",
      "Training Loss: 0.006451239170855843\n",
      "Training Loss: 0.005600115638226271\n",
      "Validation Loss: 0.002984987042080402\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005803575363242999\n",
      "Training Loss: 0.0064450828806729985\n",
      "Training Loss: 0.005594013234949671\n",
      "Validation Loss: 0.0029796688069244114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0057972115045413376\n",
      "Training Loss: 0.006438978859805502\n",
      "Training Loss: 0.005587976394454017\n",
      "Validation Loss: 0.0029744337851788554\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005790903962333687\n",
      "Training Loss: 0.006432928391732275\n",
      "Training Loss: 0.005582006167969666\n",
      "Validation Loss: 0.002969282590313239\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.0057846540043829005\n",
      "Training Loss: 0.00642693011846859\n",
      "Training Loss: 0.005576101682381704\n",
      "Validation Loss: 0.0029642141955182628\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005778460804140195\n",
      "Training Loss: 0.006420983320567757\n",
      "Training Loss: 0.005570262696710415\n",
      "Validation Loss: 0.0029592314562394044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [30:55<03:25, 205.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Loss: 0.5026608310639858\n",
      "Training Loss: 0.39217217966914175\n",
      "Training Loss: 0.31789714492857457\n",
      "Validation Loss: 0.23670476842462346\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.21162731308490038\n",
      "Training Loss: 0.14247654359787704\n",
      "Training Loss: 0.10183185851201415\n",
      "Validation Loss: 0.06350153609273139\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06372854772955179\n",
      "Training Loss: 0.05436440235003829\n",
      "Training Loss: 0.05244145849719643\n",
      "Validation Loss: 0.04745057154070126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.04956642288714647\n",
      "Training Loss: 0.04701839502900839\n",
      "Training Loss: 0.045006966842338444\n",
      "Validation Loss: 0.0403326814471001\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.04147849620319903\n",
      "Training Loss: 0.038639313522726294\n",
      "Training Loss: 0.036402567923069\n",
      "Validation Loss: 0.032157081461856876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.032967927241697906\n",
      "Training Loss: 0.030446255365386605\n",
      "Training Loss: 0.028407198498025536\n",
      "Validation Loss: 0.024777073695669682\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.025744163570925592\n",
      "Training Loss: 0.023836610354483128\n",
      "Training Loss: 0.022052676952444016\n",
      "Validation Loss: 0.019025805678344176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.020314995707012715\n",
      "Training Loss: 0.01907238344894722\n",
      "Training Loss: 0.017543542152270674\n",
      "Validation Loss: 0.015042019652181797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.016602361435070635\n",
      "Training Loss: 0.01591407231753692\n",
      "Training Loss: 0.014583146737422795\n",
      "Validation Loss: 0.01243233634158969\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.014143208290915936\n",
      "Training Loss: 0.013821548593696206\n",
      "Training Loss: 0.01262692453339696\n",
      "Validation Loss: 0.010693837331314937\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.012481720701325685\n",
      "Training Loss: 0.01243346945848316\n",
      "Training Loss: 0.011411392171867192\n",
      "Validation Loss: 0.00963419318209622\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.01149503877852112\n",
      "Training Loss: 0.011646720643620939\n",
      "Training Loss: 0.010726931892568245\n",
      "Validation Loss: 0.008912173115512293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.0108694099530112\n",
      "Training Loss: 0.011127657222095877\n",
      "Training Loss: 0.010239407470216975\n",
      "Validation Loss: 0.008307357112029463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.010384312812238932\n",
      "Training Loss: 0.010719267934327946\n",
      "Training Loss: 0.00984938366455026\n",
      "Validation Loss: 0.007793448211395004\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.009990998477442191\n",
      "Training Loss: 0.010385889167664572\n",
      "Training Loss: 0.009531885380856692\n",
      "Validation Loss: 0.007359519082885445\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.009670234810328112\n",
      "Training Loss: 0.010111347946804017\n",
      "Training Loss: 0.00927180430968292\n",
      "Validation Loss: 0.006993795036118519\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.009407809963449835\n",
      "Training Loss: 0.009883823565905914\n",
      "Training Loss: 0.009056991045363248\n",
      "Validation Loss: 0.006685610797800375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.009191864702152087\n",
      "Training Loss: 0.009693549994844943\n",
      "Training Loss: 0.008877205844037235\n",
      "Validation Loss: 0.006425682225086716\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.009012342449277639\n",
      "Training Loss: 0.009532292785588653\n",
      "Training Loss: 0.008723894900176674\n",
      "Validation Loss: 0.006205952987911996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.008860864780144766\n",
      "Training Loss: 0.009393269731663167\n",
      "Training Loss: 0.008590188664384187\n",
      "Validation Loss: 0.0060194572852401255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008730687141651287\n",
      "Training Loss: 0.009271124063525349\n",
      "Training Loss: 0.008470846369164065\n",
      "Validation Loss: 0.005860208682820536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.00861658740323037\n",
      "Training Loss: 0.009161808710778131\n",
      "Training Loss: 0.008362063684035092\n",
      "Validation Loss: 0.00572310539297341\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008514637521002441\n",
      "Training Loss: 0.009062346890568734\n",
      "Training Loss: 0.008261164927389472\n",
      "Validation Loss: 0.005603843324116609\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008421921640401706\n",
      "Training Loss: 0.008970566084608436\n",
      "Training Loss: 0.00816629919456318\n",
      "Validation Loss: 0.005498842218514072\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.00833628369611688\n",
      "Training Loss: 0.008884859349345789\n",
      "Training Loss: 0.008076169138075785\n",
      "Validation Loss: 0.0054051500792207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008256110693328083\n",
      "Training Loss: 0.00880400655209087\n",
      "Training Loss: 0.007989845860283822\n",
      "Validation Loss: 0.005320373122067599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.00818018699879758\n",
      "Training Loss: 0.00872706872643903\n",
      "Training Loss: 0.007906667072093114\n",
      "Validation Loss: 0.0052425832869589665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008107616690685972\n",
      "Training Loss: 0.008653343068435787\n",
      "Training Loss: 0.007826178650138899\n",
      "Validation Loss: 0.005170284566375312\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008037762760650366\n",
      "Training Loss: 0.008582314621889964\n",
      "Training Loss: 0.00774809600552544\n",
      "Validation Loss: 0.005102344257547782\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.007970196723472328\n",
      "Training Loss: 0.008513634416740388\n",
      "Training Loss: 0.007672270342009142\n",
      "Validation Loss: 0.0050379432570398525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007904661732027307\n",
      "Training Loss: 0.008447094890289008\n",
      "Training Loss: 0.0075986653123982255\n",
      "Validation Loss: 0.004976513711840249\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007841027355752886\n",
      "Training Loss: 0.00838260279619135\n",
      "Training Loss: 0.007527334712212905\n",
      "Validation Loss: 0.004917690553964021\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007779274566564709\n",
      "Training Loss: 0.008320162075106054\n",
      "Training Loss: 0.0074584031163249164\n",
      "Validation Loss: 0.004861273533361179\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007719463397515938\n",
      "Training Loss: 0.00825985376839526\n",
      "Training Loss: 0.007392048127949238\n",
      "Validation Loss: 0.004807189619691854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007661715259309858\n",
      "Training Loss: 0.008201812418410554\n",
      "Training Loss: 0.007328470899956301\n",
      "Validation Loss: 0.004755445847580774\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007606183604802936\n",
      "Training Loss: 0.008146201042691245\n",
      "Training Loss: 0.007267880801809952\n",
      "Validation Loss: 0.004706100971448455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007553038764744997\n",
      "Training Loss: 0.008093193396925926\n",
      "Training Loss: 0.007210473787272349\n",
      "Validation Loss: 0.0046592466036129866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007502447769511491\n",
      "Training Loss: 0.008042953168042004\n",
      "Training Loss: 0.007156415960052982\n",
      "Validation Loss: 0.004614956080494972\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007454556550364941\n",
      "Training Loss: 0.007995613015955314\n",
      "Training Loss: 0.0071058219531551\n",
      "Validation Loss: 0.004573296956990039\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.00740947165293619\n",
      "Training Loss: 0.007951254734070972\n",
      "Training Loss: 0.007058745498070494\n",
      "Validation Loss: 0.004534271768074524\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007367250582901761\n",
      "Training Loss: 0.007909903400577605\n",
      "Training Loss: 0.007015174102270975\n",
      "Validation Loss: 0.004497855289556672\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007327895755879581\n",
      "Training Loss: 0.007871519819600508\n",
      "Training Loss: 0.006975025694118813\n",
      "Validation Loss: 0.004463963283832823\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007291350136511028\n",
      "Training Loss: 0.007836003900738433\n",
      "Training Loss: 0.006938160100253299\n",
      "Validation Loss: 0.0044324653003406655\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007257503812434151\n",
      "Training Loss: 0.007803198972251266\n",
      "Training Loss: 0.006904382799984887\n",
      "Validation Loss: 0.004403204916128784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.00722620333195664\n",
      "Training Loss: 0.007772911796346307\n",
      "Training Loss: 0.006873468247940764\n",
      "Validation Loss: 0.004375995387940594\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007197264671558515\n",
      "Training Loss: 0.007744920041877776\n",
      "Training Loss: 0.00684516572044231\n",
      "Validation Loss: 0.004350654166087173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007170479715568945\n",
      "Training Loss: 0.007718989048153162\n",
      "Training Loss: 0.006819216914009303\n",
      "Validation Loss: 0.004326985411220387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0071456384868361054\n",
      "Training Loss: 0.007694888965925201\n",
      "Training Loss: 0.006795369253959507\n",
      "Validation Loss: 0.004304809480110246\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007122530390042812\n",
      "Training Loss: 0.007672397253336385\n",
      "Training Loss: 0.006773382279789075\n",
      "Validation Loss: 0.0042839618085810304\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007100958182709291\n",
      "Training Loss: 0.00765131140127778\n",
      "Training Loss: 0.006753033624263481\n",
      "Validation Loss: 0.004264293774815925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007080739131197333\n",
      "Training Loss: 0.007631450945045799\n",
      "Training Loss: 0.0067341249471064655\n",
      "Validation Loss: 0.004245672466526373\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007061710135312751\n",
      "Training Loss: 0.007612659139558673\n",
      "Training Loss: 0.006716479158494621\n",
      "Validation Loss: 0.00422798567111447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007043727566488087\n",
      "Training Loss: 0.007594802565872669\n",
      "Training Loss: 0.006699940864928066\n",
      "Validation Loss: 0.004211136699769269\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007026668236358091\n",
      "Training Loss: 0.007577770270290785\n",
      "Training Loss: 0.006684380363440141\n",
      "Validation Loss: 0.004195040833529378\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007010426089400426\n",
      "Training Loss: 0.007561468082130886\n",
      "Training Loss: 0.006669683617074043\n",
      "Validation Loss: 0.004179630275512261\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.006994910738430917\n",
      "Training Loss: 0.007545821642270312\n",
      "Training Loss: 0.006655755222309381\n",
      "Validation Loss: 0.00416484899986326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006980047299293801\n",
      "Training Loss: 0.00753076868946664\n",
      "Training Loss: 0.006642513285623863\n",
      "Validation Loss: 0.00415064522792491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006965772128896788\n",
      "Training Loss: 0.007516256922390312\n",
      "Training Loss: 0.00662988840485923\n",
      "Validation Loss: 0.004136976857899866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006952029905514792\n",
      "Training Loss: 0.007502243696362712\n",
      "Training Loss: 0.006617821959080175\n",
      "Validation Loss: 0.004123808292860395\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006938775931484997\n",
      "Training Loss: 0.007488694482017309\n",
      "Training Loss: 0.006606262094574049\n",
      "Validation Loss: 0.004111111892669807\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.00692597063141875\n",
      "Training Loss: 0.007475577119621448\n",
      "Training Loss: 0.006595166045008228\n",
      "Validation Loss: 0.004098858395225128\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.0069135799771174785\n",
      "Training Loss: 0.007462865716661327\n",
      "Training Loss: 0.0065844948485028\n",
      "Validation Loss: 0.004087028590690219\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006901575303636491\n",
      "Training Loss: 0.007450537803815677\n",
      "Training Loss: 0.006574215107830242\n",
      "Validation Loss: 0.00407559557666144\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006889930255711079\n",
      "Training Loss: 0.007438571263337508\n",
      "Training Loss: 0.006564295195275917\n",
      "Validation Loss: 0.004064544165517423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0068786227062810215\n",
      "Training Loss: 0.0074269479827489705\n",
      "Training Loss: 0.006554710551863536\n",
      "Validation Loss: 0.004053856939433164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006867632060311735\n",
      "Training Loss: 0.007415649743634276\n",
      "Training Loss: 0.006545436626765877\n",
      "Validation Loss: 0.004043515269341094\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.006856940696015954\n",
      "Training Loss: 0.007404661414329894\n",
      "Training Loss: 0.00653645173762925\n",
      "Validation Loss: 0.004033505116076617\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.006846531749470159\n",
      "Training Loss: 0.0073939680174225945\n",
      "Training Loss: 0.006527737191645428\n",
      "Validation Loss: 0.004023810441794104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006836391324177384\n",
      "Training Loss: 0.007383555978303775\n",
      "Training Loss: 0.006519275939790532\n",
      "Validation Loss: 0.004014422092848363\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006826506330398842\n",
      "Training Loss: 0.0073734122089808805\n",
      "Training Loss: 0.006511051384732128\n",
      "Validation Loss: 0.004005320385893744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006816863855347037\n",
      "Training Loss: 0.007363525052787736\n",
      "Training Loss: 0.006503047496080399\n",
      "Validation Loss: 0.003996499713374239\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006807453196961433\n",
      "Training Loss: 0.007353882256429642\n",
      "Training Loss: 0.006495251420419663\n",
      "Validation Loss: 0.0039879434985328425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006798262146767229\n",
      "Training Loss: 0.007344474418787285\n",
      "Training Loss: 0.006487652050564066\n",
      "Validation Loss: 0.003979642366666054\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.00678928290028125\n",
      "Training Loss: 0.007335291255731136\n",
      "Training Loss: 0.006480236041825265\n",
      "Validation Loss: 0.003971583529092958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006780505174538121\n",
      "Training Loss: 0.007326323032611981\n",
      "Training Loss: 0.006472993794595823\n",
      "Validation Loss: 0.0039637598413136904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.0067719209229107945\n",
      "Training Loss: 0.0073175613943021744\n",
      "Training Loss: 0.006465914275031537\n",
      "Validation Loss: 0.003956155146451227\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006763522337423638\n",
      "Training Loss: 0.00730899702582974\n",
      "Training Loss: 0.006458990037208423\n",
      "Validation Loss: 0.0039487660161397434\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.006755300301592797\n",
      "Training Loss: 0.007300622815964743\n",
      "Training Loss: 0.006452210838906467\n",
      "Validation Loss: 0.003941581661687389\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006747249857289717\n",
      "Training Loss: 0.007292430740199052\n",
      "Training Loss: 0.006445569068891928\n",
      "Validation Loss: 0.003934590230967975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006739362555090338\n",
      "Training Loss: 0.007284415176836773\n",
      "Training Loss: 0.006439057065872476\n",
      "Validation Loss: 0.003927786969265827\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006731632283190265\n",
      "Training Loss: 0.00727656705363188\n",
      "Training Loss: 0.0064326674200128765\n",
      "Validation Loss: 0.003921161782456918\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006724053235957399\n",
      "Training Loss: 0.007268881529453211\n",
      "Training Loss: 0.006426393841393292\n",
      "Validation Loss: 0.003914705792891929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006716619129292667\n",
      "Training Loss: 0.007261351894121617\n",
      "Training Loss: 0.006420229525538161\n",
      "Validation Loss: 0.003908410999306551\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006709323849063367\n",
      "Training Loss: 0.007253971617901697\n",
      "Training Loss: 0.006414169709896669\n",
      "Validation Loss: 0.003902273308851997\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006702162692090496\n",
      "Training Loss: 0.007246735700173304\n",
      "Training Loss: 0.006408207991626114\n",
      "Validation Loss: 0.0038962834069944836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006695130452280864\n",
      "Training Loss: 0.007239639343461022\n",
      "Training Loss: 0.006402338903862983\n",
      "Validation Loss: 0.0038904327103930912\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006688221695367247\n",
      "Training Loss: 0.007232676044804975\n",
      "Training Loss: 0.006396558260312304\n",
      "Validation Loss: 0.0038847190634023103\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006681431414326653\n",
      "Training Loss: 0.007225840584142134\n",
      "Training Loss: 0.006390860779210925\n",
      "Validation Loss: 0.003879134872865476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006674755623098463\n",
      "Training Loss: 0.007219129935256205\n",
      "Training Loss: 0.006385242069372907\n",
      "Validation Loss: 0.0038736701623819183\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006668189928168431\n",
      "Training Loss: 0.007212537655141205\n",
      "Training Loss: 0.00637969852075912\n",
      "Validation Loss: 0.0038683244558260516\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006661729296902195\n",
      "Training Loss: 0.007206059666350484\n",
      "Training Loss: 0.006374225096078589\n",
      "Validation Loss: 0.003863091967748792\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0066553693683817985\n",
      "Training Loss: 0.0071996913431212305\n",
      "Training Loss: 0.006368818285409361\n",
      "Validation Loss: 0.0038579636517640076\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006649107139091939\n",
      "Training Loss: 0.0071934291394427415\n",
      "Training Loss: 0.006363475155085325\n",
      "Validation Loss: 0.0038529404078013777\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.0066429374285507945\n",
      "Training Loss: 0.007187268571578897\n",
      "Training Loss: 0.006358191476901993\n",
      "Validation Loss: 0.0038480094249433512\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0066368573403451595\n",
      "Training Loss: 0.007181206196546555\n",
      "Training Loss: 0.006352964718826115\n",
      "Validation Loss: 0.003843169694693152\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006630863185273483\n",
      "Training Loss: 0.007175236832699739\n",
      "Training Loss: 0.006347791033331305\n",
      "Validation Loss: 0.0038384190339364865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006624952224083245\n",
      "Training Loss: 0.007169357630191371\n",
      "Training Loss: 0.006342668607831001\n",
      "Validation Loss: 0.003833750923415332\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.0066191185149364175\n",
      "Training Loss: 0.0071635647845687345\n",
      "Training Loss: 0.006337592714698985\n",
      "Validation Loss: 0.0038291599766319888\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006613360652700066\n",
      "Training Loss: 0.007157855012919754\n",
      "Training Loss: 0.006332561684539542\n",
      "Validation Loss: 0.00382464479921867\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006607675056438893\n",
      "Training Loss: 0.007152224468300119\n",
      "Training Loss: 0.006327574003953487\n",
      "Validation Loss: 0.0038201993728956478\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0066020588425453755\n",
      "Training Loss: 0.007146671035443432\n",
      "Training Loss: 0.0063226261152885856\n",
      "Validation Loss: 0.0038158213863609715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006596509019145742\n",
      "Training Loss: 0.007141190283582546\n",
      "Training Loss: 0.0063177157554309814\n",
      "Validation Loss: 0.0038115079357717812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006591022639768198\n",
      "Training Loss: 0.007135779059608467\n",
      "Training Loss: 0.006312840831233188\n",
      "Validation Loss: 0.003807251842619244\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006585595713695511\n",
      "Training Loss: 0.00713043564115651\n",
      "Training Loss: 0.00630799843580462\n",
      "Validation Loss: 0.003803052541831236\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006580227988306433\n",
      "Training Loss: 0.007125155102694407\n",
      "Training Loss: 0.0063031883770599964\n",
      "Validation Loss: 0.0037989093597685353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006574915283126756\n",
      "Training Loss: 0.007119935534428805\n",
      "Training Loss: 0.006298406590940431\n",
      "Validation Loss: 0.0037948130028259554\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006569654389750212\n",
      "Training Loss: 0.007114774997462519\n",
      "Training Loss: 0.00629365231259726\n",
      "Validation Loss: 0.003790763413449854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006564444054383785\n",
      "Training Loss: 0.00710967012331821\n",
      "Training Loss: 0.006288924135733395\n",
      "Validation Loss: 0.003786761118779273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.0065592815412674095\n",
      "Training Loss: 0.007104618223384023\n",
      "Training Loss: 0.006284219181397929\n",
      "Validation Loss: 0.0037828000037397227\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006554164554690942\n",
      "Training Loss: 0.007099617131752893\n",
      "Training Loss: 0.0062795365520287305\n",
      "Validation Loss: 0.003778878649110707\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.0065490911097731445\n",
      "Training Loss: 0.007094664183678105\n",
      "Training Loss: 0.00627487437450327\n",
      "Validation Loss: 0.0037749933413742634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.00654405904118903\n",
      "Training Loss: 0.007089757876237854\n",
      "Training Loss: 0.006270231320522726\n",
      "Validation Loss: 0.0037711410589641734\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006539066189434379\n",
      "Training Loss: 0.0070848951471270995\n",
      "Training Loss: 0.006265605641528964\n",
      "Validation Loss: 0.0037673198319762274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0065341103915125135\n",
      "Training Loss: 0.007080073640681803\n",
      "Training Loss: 0.0062609974981751295\n",
      "Validation Loss: 0.0037635323902843205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006529190154979005\n",
      "Training Loss: 0.007075291835935787\n",
      "Training Loss: 0.006256403417792171\n",
      "Validation Loss: 0.00375976991248474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006524304156191647\n",
      "Training Loss: 0.0070705477939918635\n",
      "Training Loss: 0.006251823098864406\n",
      "Validation Loss: 0.003756033362600994\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006519450134364888\n",
      "Training Loss: 0.0070658393879421055\n",
      "Training Loss: 0.0062472556403372435\n",
      "Validation Loss: 0.003752320424098982\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006514626235002652\n",
      "Training Loss: 0.0070611648145131765\n",
      "Training Loss: 0.0062426990445237605\n",
      "Validation Loss: 0.0037486283932655546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006509830285795033\n",
      "Training Loss: 0.007056521258782596\n",
      "Training Loss: 0.006238153567537665\n",
      "Validation Loss: 0.003744956058858151\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006505062647629529\n",
      "Training Loss: 0.007051909026340581\n",
      "Training Loss: 0.006233616867102682\n",
      "Validation Loss: 0.0037413018760846907\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006500320123741403\n",
      "Training Loss: 0.007047325644525699\n",
      "Training Loss: 0.006229088178370148\n",
      "Validation Loss: 0.00373766151141752\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006495602250797674\n",
      "Training Loss: 0.007042768804822117\n",
      "Training Loss: 0.006224566794699058\n",
      "Validation Loss: 0.0037340404821580714\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006490907681873068\n",
      "Training Loss: 0.0070382375549525025\n",
      "Training Loss: 0.006220052144490182\n",
      "Validation Loss: 0.003730429290647336\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006486234784824773\n",
      "Training Loss: 0.007033730098628439\n",
      "Training Loss: 0.006215543143916875\n",
      "Validation Loss: 0.0037268322337879224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006481582127744332\n",
      "Training Loss: 0.007029245484154671\n",
      "Training Loss: 0.006211038706824184\n",
      "Validation Loss: 0.003723244106899319\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006476948909694329\n",
      "Training Loss: 0.007024782405351288\n",
      "Training Loss: 0.006206539064878598\n",
      "Validation Loss: 0.0037196655967011213\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0064723344135563824\n",
      "Training Loss: 0.007020339191658422\n",
      "Training Loss: 0.006202042846707627\n",
      "Validation Loss: 0.0037160941198196135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.0064677367976401\n",
      "Training Loss: 0.007015913522918709\n",
      "Training Loss: 0.006197549264179543\n",
      "Validation Loss: 0.003712530113139263\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.0064631562202703205\n",
      "Training Loss: 0.007011507949209772\n",
      "Training Loss: 0.006193057101918384\n",
      "Validation Loss: 0.003708972086805557\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006458590070251375\n",
      "Training Loss: 0.007007116863387636\n",
      "Training Loss: 0.006188566372729838\n",
      "Validation Loss: 0.0037054189590716297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006454038644442335\n",
      "Training Loss: 0.0070027412340277805\n",
      "Training Loss: 0.006184076201170683\n",
      "Validation Loss: 0.0037018666632494396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006449500742601231\n",
      "Training Loss: 0.006998381072771735\n",
      "Training Loss: 0.006179587201913818\n",
      "Validation Loss: 0.003698316653876492\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006444975796621293\n",
      "Training Loss: 0.006994033426162787\n",
      "Training Loss: 0.006175097881350666\n",
      "Validation Loss: 0.003694767498651917\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.0064404625224415216\n",
      "Training Loss: 0.006989697985118255\n",
      "Training Loss: 0.006170607460662722\n",
      "Validation Loss: 0.003691221703775227\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006435960737289861\n",
      "Training Loss: 0.006985374306677841\n",
      "Training Loss: 0.006166116093518212\n",
      "Validation Loss: 0.003687672137826932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006431468909140676\n",
      "Training Loss: 0.006981060405960307\n",
      "Training Loss: 0.00616162238875404\n",
      "Validation Loss: 0.0036841213357833662\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006426987492013723\n",
      "Training Loss: 0.006976756718358956\n",
      "Training Loss: 0.0061571273137815295\n",
      "Validation Loss: 0.003680566290466722\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006422515191370621\n",
      "Training Loss: 0.006972462174016983\n",
      "Training Loss: 0.006152630023425445\n",
      "Validation Loss: 0.0036770084747270253\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006418050966458395\n",
      "Training Loss: 0.006968175494694151\n",
      "Training Loss: 0.006148130730725825\n",
      "Validation Loss: 0.003673447301255518\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006413595562335104\n",
      "Training Loss: 0.006963896302622743\n",
      "Training Loss: 0.006143627600977197\n",
      "Validation Loss: 0.0036698801932186723\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006409147238591686\n",
      "Training Loss: 0.006959623662987724\n",
      "Training Loss: 0.006139121465384961\n",
      "Validation Loss: 0.0036663060270124273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006404705882305279\n",
      "Training Loss: 0.006955356451217085\n",
      "Training Loss: 0.006134611727902666\n",
      "Validation Loss: 0.0036627282480070932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.0064002715353854\n",
      "Training Loss: 0.006951094741816632\n",
      "Training Loss: 0.006130097664427012\n",
      "Validation Loss: 0.0036591408300747194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006395842615747824\n",
      "Training Loss: 0.006946837346768007\n",
      "Training Loss: 0.006125579633517191\n",
      "Validation Loss: 0.0036555452735341164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006391419464489445\n",
      "Training Loss: 0.006942584217176773\n",
      "Training Loss: 0.00612105751526542\n",
      "Validation Loss: 0.003651941719653315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006387002096744254\n",
      "Training Loss: 0.0069383343297522515\n",
      "Training Loss: 0.00611653107800521\n",
      "Validation Loss: 0.0036483295261859894\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006382588975830003\n",
      "Training Loss: 0.006934087992995046\n",
      "Training Loss: 0.006111999469576404\n",
      "Validation Loss: 0.003644706203937112\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006378180873580277\n",
      "Training Loss: 0.006929843500256538\n",
      "Training Loss: 0.0061074631894007326\n",
      "Validation Loss: 0.0036410737751323857\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006373776426771656\n",
      "Training Loss: 0.006925601155380719\n",
      "Training Loss: 0.00610292183351703\n",
      "Validation Loss: 0.0036374284896288026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006369375764625147\n",
      "Training Loss: 0.006921360017149709\n",
      "Training Loss: 0.006098374740686268\n",
      "Validation Loss: 0.003633773400385477\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006364978705532849\n",
      "Training Loss: 0.0069171190937049685\n",
      "Training Loss: 0.0060938220540992915\n",
      "Validation Loss: 0.0036301040129861637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006360584913054481\n",
      "Training Loss: 0.00691287919471506\n",
      "Training Loss: 0.006089264883194119\n",
      "Validation Loss: 0.003626422480460298\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006356194606050849\n",
      "Training Loss: 0.0069086393457837405\n",
      "Training Loss: 0.006084701634244993\n",
      "Validation Loss: 0.003622725390138502\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006351806377060711\n",
      "Training Loss: 0.006904398173792288\n",
      "Training Loss: 0.0060801330721005796\n",
      "Validation Loss: 0.0036190184202107988\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006347421288955957\n",
      "Training Loss: 0.006900156990159303\n",
      "Training Loss: 0.006075557892909273\n",
      "Validation Loss: 0.003615295055343278\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.00634303774277214\n",
      "Training Loss: 0.006895914051565342\n",
      "Training Loss: 0.006070977951167151\n",
      "Validation Loss: 0.0036115577794988167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006338656271109358\n",
      "Training Loss: 0.006891668254975229\n",
      "Training Loss: 0.006066391199128702\n",
      "Validation Loss: 0.0036078047234920805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006334276400157251\n",
      "Training Loss: 0.006887421839055605\n",
      "Training Loss: 0.006061796835856512\n",
      "Validation Loss: 0.0036040325426716317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006329898124095052\n",
      "Training Loss: 0.006883171593071893\n",
      "Training Loss: 0.006057197937043384\n",
      "Validation Loss: 0.0036002477092061487\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006325521158287301\n",
      "Training Loss: 0.0068789186357753355\n",
      "Training Loss: 0.006052591962506995\n",
      "Validation Loss: 0.0035964440897086197\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006321144842077047\n",
      "Training Loss: 0.006874661954352632\n",
      "Training Loss: 0.006047980017028749\n",
      "Validation Loss: 0.003592625019674221\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006316769832046702\n",
      "Training Loss: 0.00687040229095146\n",
      "Training Loss: 0.006043361021438614\n",
      "Validation Loss: 0.003588787366353561\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0063123951037414375\n",
      "Training Loss: 0.0068661371123744175\n",
      "Training Loss: 0.006038735186448321\n",
      "Validation Loss: 0.003584931911952961\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006308020884753205\n",
      "Training Loss: 0.006861868332489394\n",
      "Training Loss: 0.006034102158155292\n",
      "Validation Loss: 0.003581056541376067\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0063036460359580816\n",
      "Training Loss: 0.006857594323228113\n",
      "Training Loss: 0.006029462890001014\n",
      "Validation Loss: 0.0035771612951720363\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006299272474134341\n",
      "Training Loss: 0.006853314544423483\n",
      "Training Loss: 0.0060248163132928315\n",
      "Validation Loss: 0.0035732494735190375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006294898580526933\n",
      "Training Loss: 0.00684902977780439\n",
      "Training Loss: 0.006020163056673482\n",
      "Validation Loss: 0.003569314503529517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006290523579809815\n",
      "Training Loss: 0.0068447391537483785\n",
      "Training Loss: 0.00601550304912962\n",
      "Validation Loss: 0.003565358968577191\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006286147916107439\n",
      "Training Loss: 0.0068404416763223706\n",
      "Training Loss: 0.006010834643384442\n",
      "Validation Loss: 0.0035613847417715057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006281771556823515\n",
      "Training Loss: 0.006836137234349735\n",
      "Training Loss: 0.006006159246899187\n",
      "Validation Loss: 0.003557387209664821\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006277394060161896\n",
      "Training Loss: 0.00683182522887364\n",
      "Training Loss: 0.006001475974917412\n",
      "Validation Loss: 0.003553369133523927\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006273014997714199\n",
      "Training Loss: 0.006827505764667876\n",
      "Training Loss: 0.005996784284943715\n",
      "Validation Loss: 0.003549326054249587\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006268634312436916\n",
      "Training Loss: 0.006823178057093173\n",
      "Training Loss: 0.005992085826583207\n",
      "Validation Loss: 0.0035452626546154196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006264252209221013\n",
      "Training Loss: 0.006818842469947413\n",
      "Training Loss: 0.005987378689460456\n",
      "Validation Loss: 0.0035411753676036434\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006259867032058537\n",
      "Training Loss: 0.006814497956074774\n",
      "Training Loss: 0.005982663092436269\n",
      "Validation Loss: 0.003537062967583286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006255479787359946\n",
      "Training Loss: 0.006810144732589833\n",
      "Training Loss: 0.00597793826716952\n",
      "Validation Loss: 0.0035329256677644307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006251089406432584\n",
      "Training Loss: 0.006805780913564377\n",
      "Training Loss: 0.005973206018097699\n",
      "Validation Loss: 0.003528767557071752\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006246696710004471\n",
      "Training Loss: 0.00680140724638477\n",
      "Training Loss: 0.005968464103061706\n",
      "Validation Loss: 0.0035245819043451814\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006242300102603622\n",
      "Training Loss: 0.006797023753169924\n",
      "Training Loss: 0.005963713334640488\n",
      "Validation Loss: 0.0035203700764837227\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006237900439882651\n",
      "Training Loss: 0.006792629576520995\n",
      "Training Loss: 0.0059589536394923925\n",
      "Validation Loss: 0.003516132978649203\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006233496207860298\n",
      "Training Loss: 0.0067882244201609866\n",
      "Training Loss: 0.0059541843028273434\n",
      "Validation Loss: 0.0035118695147063456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006229088351828977\n",
      "Training Loss: 0.006783807166502811\n",
      "Training Loss: 0.005949405611027032\n",
      "Validation Loss: 0.003507581211134708\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006224675797275267\n",
      "Training Loss: 0.006779378909268417\n",
      "Training Loss: 0.005944617672357708\n",
      "Validation Loss: 0.0035032636206073875\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006220258494722657\n",
      "Training Loss: 0.006774937273003161\n",
      "Training Loss: 0.0059398177894763645\n",
      "Validation Loss: 0.0034989199425836796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.006215835701441392\n",
      "Training Loss: 0.006770482829306274\n",
      "Training Loss: 0.005935007666703313\n",
      "Validation Loss: 0.003494545460280994\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006211406442453153\n",
      "Training Loss: 0.006766015669563785\n",
      "Training Loss: 0.00593018664047122\n",
      "Validation Loss: 0.003490144364650916\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.0062069711403455585\n",
      "Training Loss: 0.006761533378739842\n",
      "Training Loss: 0.00592535340343602\n",
      "Validation Loss: 0.0034857121220360815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006202529579168186\n",
      "Training Loss: 0.0067570371623151\n",
      "Training Loss: 0.0059205090219620615\n",
      "Validation Loss: 0.003481250810907798\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006198080794420093\n",
      "Training Loss: 0.006752526757190935\n",
      "Training Loss: 0.005915653326082975\n",
      "Validation Loss: 0.0034767597537027316\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006193624180159531\n",
      "Training Loss: 0.006748001279775053\n",
      "Training Loss: 0.005910784612642601\n",
      "Validation Loss: 0.003472237850361493\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.0061891597812063995\n",
      "Training Loss: 0.006743459751596674\n",
      "Training Loss: 0.005905903186649084\n",
      "Validation Loss: 0.003467682432487942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006184686121996492\n",
      "Training Loss: 0.006738901495700702\n",
      "Training Loss: 0.005901008018990978\n",
      "Validation Loss: 0.0034630968761264107\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006180203694384545\n",
      "Training Loss: 0.006734326324658468\n",
      "Training Loss: 0.005896098269149661\n",
      "Validation Loss: 0.0034584788948276574\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.00617571120033972\n",
      "Training Loss: 0.00672973288455978\n",
      "Training Loss: 0.00589117401978001\n",
      "Validation Loss: 0.0034538276880728396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006171208349987864\n",
      "Training Loss: 0.006725122547941283\n",
      "Training Loss: 0.005886234786594287\n",
      "Validation Loss: 0.003449142344159943\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006166694755083881\n",
      "Training Loss: 0.006720492620952427\n",
      "Training Loss: 0.00588128010625951\n",
      "Validation Loss: 0.00344442337976371\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006162169222952798\n",
      "Training Loss: 0.0067158442200161515\n",
      "Training Loss: 0.005876309934537857\n",
      "Validation Loss: 0.003439669051270388\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006157631300156936\n",
      "Training Loss: 0.006711175466771238\n",
      "Training Loss: 0.0058713227591942994\n",
      "Validation Loss: 0.003434876887797472\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006153080307994969\n",
      "Training Loss: 0.00670648631406948\n",
      "Training Loss: 0.005866317400941626\n",
      "Validation Loss: 0.0034300529664863695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0061485156131675465\n",
      "Training Loss: 0.006701775269466453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [34:21<00:00, 205.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.005861294029746205\n",
      "Validation Loss: 0.003425190802088028\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [34:21<00:00, 206.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (22797, 12, 5)\n",
      "Shape of the data after splitting into sequences: (5692, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Loss: 0.26799303479492664\n",
      "Training Loss: 0.20990593083202838\n",
      "Training Loss: 0.14010058142244816\n",
      "Validation Loss: 0.08577977543633976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06969297718256712\n",
      "Training Loss: 0.06123011004179716\n",
      "Training Loss: 0.05785145822912455\n",
      "Validation Loss: 0.056861187090699594\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.055181442275643346\n",
      "Training Loss: 0.053571699876338245\n",
      "Training Loss: 0.05044915188103914\n",
      "Validation Loss: 0.048503293749991426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.04677301725372672\n",
      "Training Loss: 0.04457228615880013\n",
      "Training Loss: 0.04130879335105419\n",
      "Validation Loss: 0.03888046861825029\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.03729436447843909\n",
      "Training Loss: 0.035007167169824244\n",
      "Training Loss: 0.03216040939092636\n",
      "Validation Loss: 0.02990021515805065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.028760296809487045\n",
      "Training Loss: 0.02689102465752512\n",
      "Training Loss: 0.024870889903977515\n",
      "Validation Loss: 0.023122593153561098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.0226371373841539\n",
      "Training Loss: 0.021327084861695766\n",
      "Training Loss: 0.020100782797671853\n",
      "Validation Loss: 0.018763601617741116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.018866066925693304\n",
      "Training Loss: 0.01794628269970417\n",
      "Training Loss: 0.01723057927098125\n",
      "Validation Loss: 0.016066124987150175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.01658194191288203\n",
      "Training Loss: 0.015881513489875942\n",
      "Training Loss: 0.015447460473515093\n",
      "Validation Loss: 0.014293217769918147\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.015093599618412555\n",
      "Training Loss: 0.014521399890072643\n",
      "Training Loss: 0.014243257322814316\n",
      "Validation Loss: 0.01301996557962861\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.014029268801677972\n",
      "Training Loss: 0.013542853652033955\n",
      "Training Loss: 0.013356916888151318\n",
      "Validation Loss: 0.01203294148605861\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.013206259340513497\n",
      "Training Loss: 0.01278548701433465\n",
      "Training Loss: 0.012657502572983504\n",
      "Validation Loss: 0.011223526755243206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.012532937421929092\n",
      "Training Loss: 0.012168130311183632\n",
      "Training Loss: 0.012077426076866687\n",
      "Validation Loss: 0.010534223129847245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.011961692851036787\n",
      "Training Loss: 0.011648347715381533\n",
      "Training Loss: 0.011580808123108\n",
      "Validation Loss: 0.009933402999879772\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.011466740868054331\n",
      "Training Loss: 0.011201956805307419\n",
      "Training Loss: 0.011146941829938441\n",
      "Validation Loss: 0.009401370140755276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.011031900986563415\n",
      "Training Loss: 0.010812316157389432\n",
      "Training Loss: 0.010761866718530655\n",
      "Validation Loss: 0.008923941567864562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.010645011753076687\n",
      "Training Loss: 0.010466571449069307\n",
      "Training Loss: 0.01041551960282959\n",
      "Validation Loss: 0.00849098109462288\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.010296852150931955\n",
      "Training Loss: 0.010155421750387177\n",
      "Training Loss: 0.01010112709715031\n",
      "Validation Loss: 0.00809599021763614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.00998116701375693\n",
      "Training Loss: 0.00987291322206147\n",
      "Training Loss: 0.00981451710802503\n",
      "Validation Loss: 0.0077351376288727425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.009694193858886138\n",
      "Training Loss: 0.009615610777400434\n",
      "Training Loss: 0.009553332390496508\n",
      "Validation Loss: 0.007406293544028834\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.009433887554332615\n",
      "Training Loss: 0.009381673504831269\n",
      "Training Loss: 0.009316337244817986\n",
      "Validation Loss: 0.007108229555168681\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.009199169250205159\n",
      "Training Loss: 0.009170127084944397\n",
      "Training Loss: 0.009102825861191377\n",
      "Validation Loss: 0.006839955833824247\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.00898926926893182\n",
      "Training Loss: 0.008980285283178091\n",
      "Training Loss: 0.008912099654553458\n",
      "Validation Loss: 0.006600254526612967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008803229954792187\n",
      "Training Loss: 0.008811295186169445\n",
      "Training Loss: 0.00874307366553694\n",
      "Validation Loss: 0.006387414274674453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.008639619783498347\n",
      "Training Loss: 0.008661911415401846\n",
      "Training Loss: 0.008594147557159886\n",
      "Validation Loss: 0.006199237473277647\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008496508180396631\n",
      "Training Loss: 0.008530471150297671\n",
      "Training Loss: 0.008463304127799347\n",
      "Validation Loss: 0.006033238480024542\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008371653526555747\n",
      "Training Loss: 0.008415056798839941\n",
      "Training Loss: 0.008348346268758178\n",
      "Validation Loss: 0.00588687356127154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008262728460831568\n",
      "Training Loss: 0.008313684744061902\n",
      "Training Loss: 0.008247115963604302\n",
      "Validation Loss: 0.0057577377216618385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008167522992007435\n",
      "Training Loss: 0.008224470949498937\n",
      "Training Loss: 0.008157647608313709\n",
      "Validation Loss: 0.005643662681353142\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008084050037432462\n",
      "Training Loss: 0.008145715738646687\n",
      "Training Loss: 0.008078231965191663\n",
      "Validation Loss: 0.005542763788776284\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.00801059351535514\n",
      "Training Loss: 0.00807594510144554\n",
      "Training Loss: 0.008007422535447404\n",
      "Validation Loss: 0.005453424457607142\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007945697790710256\n",
      "Training Loss: 0.008013901265803725\n",
      "Training Loss: 0.007944013057276607\n",
      "Validation Loss: 0.005374252776552536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007888139955466613\n",
      "Training Loss: 0.00795852040988393\n",
      "Training Loss: 0.007886997057357802\n",
      "Validation Loss: 0.005304048257733412\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007836889919126407\n",
      "Training Loss: 0.00790890229633078\n",
      "Training Loss: 0.007835533638717607\n",
      "Validation Loss: 0.005241772976803353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007791083789197728\n",
      "Training Loss: 0.00786428808234632\n",
      "Training Loss: 0.007788913893746212\n",
      "Validation Loss: 0.0051865126843449105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007749985852278769\n",
      "Training Loss: 0.00782402625423856\n",
      "Training Loss: 0.007746533515164629\n",
      "Validation Loss: 0.005137461405038164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007712971476139501\n",
      "Training Loss: 0.007787563607562334\n",
      "Training Loss: 0.007707876701606438\n",
      "Validation Loss: 0.005093899456093486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007679503642721102\n",
      "Training Loss: 0.007754421242279932\n",
      "Training Loss: 0.007672498183092102\n",
      "Validation Loss: 0.005055195592432754\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.00764912344631739\n",
      "Training Loss: 0.007724186825798824\n",
      "Training Loss: 0.007640010605100543\n",
      "Validation Loss: 0.00502077637637934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007621432907180861\n",
      "Training Loss: 0.007696503152837977\n",
      "Training Loss: 0.007610077409772202\n",
      "Validation Loss: 0.0049901304156990366\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.00759608561405912\n",
      "Training Loss: 0.007671056648250669\n",
      "Training Loss: 0.007582403958076611\n",
      "Validation Loss: 0.004962811803084202\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007572783314390108\n",
      "Training Loss: 0.007647575424052775\n",
      "Training Loss: 0.00755673210718669\n",
      "Validation Loss: 0.0049384193366728306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007551263321656734\n",
      "Training Loss: 0.007625820469111204\n",
      "Training Loss: 0.007532832605065778\n",
      "Validation Loss: 0.004916591485412896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007531296060187742\n",
      "Training Loss: 0.007605579510563985\n",
      "Training Loss: 0.007510507036931813\n",
      "Validation Loss: 0.004897015385089044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007512681196676568\n",
      "Training Loss: 0.007586668386356905\n",
      "Training Loss: 0.007489577056840062\n",
      "Validation Loss: 0.004879404777619108\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007495242970762774\n",
      "Training Loss: 0.007568923617945984\n",
      "Training Loss: 0.00746988880331628\n",
      "Validation Loss: 0.004863516642094663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.00747882850584574\n",
      "Training Loss: 0.007552201861981303\n",
      "Training Loss: 0.007451304112328217\n",
      "Validation Loss: 0.00484912679138269\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007463301535462961\n",
      "Training Loss: 0.007536375774070621\n",
      "Training Loss: 0.007433701839763671\n",
      "Validation Loss: 0.004836036542461913\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.00744854241493158\n",
      "Training Loss: 0.007521334184566512\n",
      "Training Loss: 0.007416976948734373\n",
      "Validation Loss: 0.004824079087218583\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007434450149303302\n",
      "Training Loss: 0.007506978577002883\n",
      "Training Loss: 0.007401034479262308\n",
      "Validation Loss: 0.004813096526581166\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007420932062668726\n",
      "Training Loss: 0.007493224197532982\n",
      "Training Loss: 0.007385793210705743\n",
      "Validation Loss: 0.00480295888807499\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007407912316266447\n",
      "Training Loss: 0.00747999508981593\n",
      "Training Loss: 0.007371181013295427\n",
      "Validation Loss: 0.00479354260926229\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007395320363575593\n",
      "Training Loss: 0.0074672261090017854\n",
      "Training Loss: 0.007357133675832302\n",
      "Validation Loss: 0.004784740491906244\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007383098945720121\n",
      "Training Loss: 0.007454859216231853\n",
      "Training Loss: 0.007343595990678295\n",
      "Validation Loss: 0.004776460104203375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0073711971542797985\n",
      "Training Loss: 0.0074428467196412385\n",
      "Training Loss: 0.007330519555252977\n",
      "Validation Loss: 0.004768617776238223\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007359571837587282\n",
      "Training Loss: 0.0074311454326380045\n",
      "Training Loss: 0.0073178617178928105\n",
      "Validation Loss: 0.004761136776579314\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007348186309682206\n",
      "Training Loss: 0.0074197192757856105\n",
      "Training Loss: 0.007305583857232705\n",
      "Validation Loss: 0.0047539566850205986\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007337009811308235\n",
      "Training Loss: 0.007408536520088092\n",
      "Training Loss: 0.0072936549392761665\n",
      "Validation Loss: 0.004747015144152755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.0073260164342354986\n",
      "Training Loss: 0.007397571335313841\n",
      "Training Loss: 0.007282043659361079\n",
      "Validation Loss: 0.004740261751373581\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007315184709150344\n",
      "Training Loss: 0.007386800268432125\n",
      "Training Loss: 0.0072707254736451435\n",
      "Validation Loss: 0.004733652034722102\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007304496911820025\n",
      "Training Loss: 0.0073762049397919326\n",
      "Training Loss: 0.0072596776013961065\n",
      "Validation Loss: 0.004727146401216559\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007293939753435552\n",
      "Training Loss: 0.007365768972085789\n",
      "Training Loss: 0.007248878659447655\n",
      "Validation Loss: 0.0047207113068938\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007283501958008856\n",
      "Training Loss: 0.0073554796888493\n",
      "Training Loss: 0.007238311563269235\n",
      "Validation Loss: 0.004714316656431078\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007273173694266007\n",
      "Training Loss: 0.007345326077193022\n",
      "Training Loss: 0.0072279598447494205\n",
      "Validation Loss: 0.004707935233495795\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0072629491158295425\n",
      "Training Loss: 0.007335298915859312\n",
      "Training Loss: 0.0072178087156498805\n",
      "Validation Loss: 0.004701546610469062\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007252822427544743\n",
      "Training Loss: 0.007325390001060441\n",
      "Training Loss: 0.007207845562370494\n",
      "Validation Loss: 0.004695134209939854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0072427902615163475\n",
      "Training Loss: 0.007315592340892181\n",
      "Training Loss: 0.007198057152563706\n",
      "Validation Loss: 0.004688681655280878\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007232850098516792\n",
      "Training Loss: 0.007305901848012581\n",
      "Training Loss: 0.007188433015835471\n",
      "Validation Loss: 0.004682176608430075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0072229987900936975\n",
      "Training Loss: 0.007296311740064993\n",
      "Training Loss: 0.0071789622225333005\n",
      "Validation Loss: 0.004675606926876968\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007213235353119672\n",
      "Training Loss: 0.007286818332504481\n",
      "Training Loss: 0.007169634882593527\n",
      "Validation Loss: 0.004668970542614547\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0072035576729103925\n",
      "Training Loss: 0.0072774161293637\n",
      "Training Loss: 0.007160441022715531\n",
      "Validation Loss: 0.004662255688539047\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007193964474718087\n",
      "Training Loss: 0.007268101735971868\n",
      "Training Loss: 0.0071513736102497205\n",
      "Validation Loss: 0.004655459335890044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.0071844547305954624\n",
      "Training Loss: 0.0072588707378599794\n",
      "Training Loss: 0.007142420848831535\n",
      "Validation Loss: 0.004648577494499682\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007175024705356918\n",
      "Training Loss: 0.007249715931247919\n",
      "Training Loss: 0.007133574797771871\n",
      "Validation Loss: 0.004641609751682184\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007165672826813534\n",
      "Training Loss: 0.007240634091431275\n",
      "Training Loss: 0.007124826094950549\n",
      "Validation Loss: 0.004634548628734153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00715639398957137\n",
      "Training Loss: 0.007231616441858932\n",
      "Training Loss: 0.007116165062179789\n",
      "Validation Loss: 0.004627396897386676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007147184954956174\n",
      "Training Loss: 0.007222659551771358\n",
      "Training Loss: 0.007107582302414812\n",
      "Validation Loss: 0.004620149574671569\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007138041008729488\n",
      "Training Loss: 0.007213754070689902\n",
      "Training Loss: 0.0070990673609776424\n",
      "Validation Loss: 0.004612805247908521\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007128955110674724\n",
      "Training Loss: 0.0072048927505966275\n",
      "Training Loss: 0.007090609739534557\n",
      "Validation Loss: 0.004605361057727943\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007119920178083703\n",
      "Training Loss: 0.007196066968608647\n",
      "Training Loss: 0.0070821986469673\n",
      "Validation Loss: 0.004597815861559233\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007110929045593366\n",
      "Training Loss: 0.007187267092522234\n",
      "Training Loss: 0.007073823181563057\n",
      "Validation Loss: 0.0045901664802176735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007101972285308875\n",
      "Training Loss: 0.007178483275929466\n",
      "Training Loss: 0.00706547018897254\n",
      "Validation Loss: 0.004582409061533347\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007093039152096026\n",
      "Training Loss: 0.007169704273110256\n",
      "Training Loss: 0.007057127354783006\n",
      "Validation Loss: 0.004574539367465324\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.00708412010979373\n",
      "Training Loss: 0.007160919186426327\n",
      "Training Loss: 0.007048781843623147\n",
      "Validation Loss: 0.004566552539962982\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.007075201616971754\n",
      "Training Loss: 0.007152114968048409\n",
      "Training Loss: 0.007040419771219604\n",
      "Validation Loss: 0.004558444062373444\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007066271833027713\n",
      "Training Loss: 0.007143280064919964\n",
      "Training Loss: 0.007032026072847657\n",
      "Validation Loss: 0.004550210124383984\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.0070573153987061234\n",
      "Training Loss: 0.0071343993861228226\n",
      "Training Loss: 0.007023586140130647\n",
      "Validation Loss: 0.004541840542924036\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007048317858134396\n",
      "Training Loss: 0.0071254584717098625\n",
      "Training Loss: 0.0070150827488396315\n",
      "Validation Loss: 0.004533330518150639\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007039261338650249\n",
      "Training Loss: 0.007116442900151014\n",
      "Training Loss: 0.0070064988639205695\n",
      "Validation Loss: 0.00452466716785821\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007030128545011394\n",
      "Training Loss: 0.007107335320906714\n",
      "Training Loss: 0.006997816533548758\n",
      "Validation Loss: 0.004515842317467492\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007020899237832054\n",
      "Training Loss: 0.007098118525464088\n",
      "Training Loss: 0.0069890169869177045\n",
      "Validation Loss: 0.004506845541266996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007011554776108824\n",
      "Training Loss: 0.007088774769799784\n",
      "Training Loss: 0.006980078326305375\n",
      "Validation Loss: 0.0044976654567159295\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.00700207102112472\n",
      "Training Loss: 0.00707928492105566\n",
      "Training Loss: 0.006970980811747723\n",
      "Validation Loss: 0.004488285150237675\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006992423977935687\n",
      "Training Loss: 0.007069628259632737\n",
      "Training Loss: 0.006961699168314226\n",
      "Validation Loss: 0.0044786937664304804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006982587543316186\n",
      "Training Loss: 0.007059782209107652\n",
      "Training Loss: 0.006952209259616211\n",
      "Validation Loss: 0.004468872112462695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006972534073283896\n",
      "Training Loss: 0.007049724141834304\n",
      "Training Loss: 0.006942484376486391\n",
      "Validation Loss: 0.004458799225728164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.0069622326368698854\n",
      "Training Loss: 0.007039429304422811\n",
      "Training Loss: 0.0069324955402407795\n",
      "Validation Loss: 0.004448461670721515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006951650715200231\n",
      "Training Loss: 0.007028871577931568\n",
      "Training Loss: 0.006922212721547112\n",
      "Validation Loss: 0.004437829583642606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006940751731744968\n",
      "Training Loss: 0.00701802347204648\n",
      "Training Loss: 0.006911602722830139\n",
      "Validation Loss: 0.004426884985518422\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006929498339304701\n",
      "Training Loss: 0.007006854005157948\n",
      "Training Loss: 0.00690063041751273\n",
      "Validation Loss: 0.004415600961518966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006917849379824474\n",
      "Training Loss: 0.006995333895320073\n",
      "Training Loss: 0.006889258663868532\n",
      "Validation Loss: 0.004403947589636435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006905761054949835\n",
      "Training Loss: 0.006983429603278637\n",
      "Training Loss: 0.006877449730527587\n",
      "Validation Loss: 0.00439189940434619\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006893188648391515\n",
      "Training Loss: 0.006971108423313126\n",
      "Training Loss: 0.0068651625566417355\n",
      "Validation Loss: 0.00437942635479875\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006880084593431093\n",
      "Training Loss: 0.006958335733506829\n",
      "Training Loss: 0.006852354884613305\n",
      "Validation Loss: 0.004366494149487633\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006866398940328509\n",
      "Training Loss: 0.0069450759747996926\n",
      "Training Loss: 0.006838984312489629\n",
      "Validation Loss: 0.004353072680009717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006852080785902217\n",
      "Training Loss: 0.0069312921562232075\n",
      "Training Loss: 0.006825006382423453\n",
      "Validation Loss: 0.004339128660739221\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006837078940006905\n",
      "Training Loss: 0.006916948389261961\n",
      "Training Loss: 0.006810379773378372\n",
      "Validation Loss: 0.00432463853523198\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006821344422642142\n",
      "Training Loss: 0.006902009699260816\n",
      "Training Loss: 0.006795063606114126\n",
      "Validation Loss: 0.004309565745152826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006804828137392178\n",
      "Training Loss: 0.006886442048707977\n",
      "Training Loss: 0.006779020627145655\n",
      "Validation Loss: 0.004293891834749139\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006787485037348233\n",
      "Training Loss: 0.006870211031055078\n",
      "Training Loss: 0.006762213691836223\n",
      "Validation Loss: 0.004277590374472771\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006769272748497315\n",
      "Training Loss: 0.00685328430030495\n",
      "Training Loss: 0.006744612653856166\n",
      "Validation Loss: 0.004260641070124641\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006750151930027641\n",
      "Training Loss: 0.006835627090185881\n",
      "Training Loss: 0.006726187368039973\n",
      "Validation Loss: 0.0042430259839349085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006730086180032231\n",
      "Training Loss: 0.00681720765423961\n",
      "Training Loss: 0.0067069141095271335\n",
      "Validation Loss: 0.004224727630714646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0067090424912748855\n",
      "Training Loss: 0.006797990710474551\n",
      "Training Loss: 0.006686767772189342\n",
      "Validation Loss: 0.004205725390563479\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006686988567817025\n",
      "Training Loss: 0.006777939063031226\n",
      "Training Loss: 0.006665726876235567\n",
      "Validation Loss: 0.0041860033472832505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006663893464719877\n",
      "Training Loss: 0.006757011690642685\n",
      "Training Loss: 0.006643764435430057\n",
      "Validation Loss: 0.004165537082433199\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006639723709085956\n",
      "Training Loss: 0.006735162580152974\n",
      "Training Loss: 0.00662085312011186\n",
      "Validation Loss: 0.004144297089961389\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006614445439190604\n",
      "Training Loss: 0.006712343263207004\n",
      "Training Loss: 0.006596961112809368\n",
      "Validation Loss: 0.00412223974682307\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006588022531941533\n",
      "Training Loss: 0.006688499565934762\n",
      "Training Loss: 0.00657205082825385\n",
      "Validation Loss: 0.004099332509049623\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006560417892760597\n",
      "Training Loss: 0.0066635761759243906\n",
      "Training Loss: 0.006546084047295153\n",
      "Validation Loss: 0.004075526189859538\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006531597850844264\n",
      "Training Loss: 0.00663752400665544\n",
      "Training Loss: 0.006519029205664992\n",
      "Validation Loss: 0.004050782462451189\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006501537652220577\n",
      "Training Loss: 0.006610303465276956\n",
      "Training Loss: 0.0064908629067940635\n",
      "Validation Loss: 0.004025076704329989\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006470231424318626\n",
      "Training Loss: 0.006581899417797104\n",
      "Training Loss: 0.006461594716529362\n",
      "Validation Loss: 0.003998413209128455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006437706355936826\n",
      "Training Loss: 0.006552339041372761\n",
      "Training Loss: 0.00643127934250515\n",
      "Validation Loss: 0.003970839764682178\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006404039201443084\n",
      "Training Loss: 0.006521699295844883\n",
      "Training Loss: 0.006400033596437424\n",
      "Validation Loss: 0.0039424637209603125\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006369369975291192\n",
      "Training Loss: 0.006490133079933002\n",
      "Training Loss: 0.006368055725470185\n",
      "Validation Loss: 0.003913455793184092\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006333913968992419\n",
      "Training Loss: 0.0064578702324070035\n",
      "Training Loss: 0.006335625746287405\n",
      "Validation Loss: 0.0038840495935494727\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006297961642849259\n",
      "Training Loss: 0.006425208031432703\n",
      "Training Loss: 0.006303087327978574\n",
      "Validation Loss: 0.0038545021108414433\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006261848181602545\n",
      "Training Loss: 0.006392483899835497\n",
      "Training Loss: 0.006270815708558075\n",
      "Validation Loss: 0.003825064201766018\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006225922635640018\n",
      "Training Loss: 0.006360028842464089\n",
      "Training Loss: 0.006239151548361406\n",
      "Validation Loss: 0.0037959222210926956\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006190485163824633\n",
      "Training Loss: 0.006328108917223289\n",
      "Training Loss: 0.006208353124093264\n",
      "Validation Loss: 0.0037671571082090226\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006155742069822736\n",
      "Training Loss: 0.006296886409400031\n",
      "Training Loss: 0.006178553804056719\n",
      "Validation Loss: 0.0037387487049518006\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.0061217831604881215\n",
      "Training Loss: 0.006266410454409197\n",
      "Training Loss: 0.006149761255364865\n",
      "Validation Loss: 0.0037105921610112017\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.0060885868151672184\n",
      "Training Loss: 0.006236631966894493\n",
      "Training Loss: 0.006121884537860751\n",
      "Validation Loss: 0.003682534405935472\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006056050533079543\n",
      "Training Loss: 0.006207438142737374\n",
      "Training Loss: 0.006094766376772896\n",
      "Validation Loss: 0.0036544096289893215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006024025230435655\n",
      "Training Loss: 0.006178692934336141\n",
      "Training Loss: 0.006068234913400374\n",
      "Validation Loss: 0.0036260829558282088\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.0059923609002726155\n",
      "Training Loss: 0.006150275386171415\n",
      "Training Loss: 0.006042129169800319\n",
      "Validation Loss: 0.003597450612657023\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005960925580584444\n",
      "Training Loss: 0.006122092299629003\n",
      "Training Loss: 0.00601631285098847\n",
      "Validation Loss: 0.003568441388288806\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.005929625939461403\n",
      "Training Loss: 0.006094091269187629\n",
      "Training Loss: 0.00599068961921148\n",
      "Validation Loss: 0.003539044090175185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005898413222166709\n",
      "Training Loss: 0.006066260279039852\n",
      "Training Loss: 0.005965199842466973\n",
      "Validation Loss: 0.0035092820383325805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.0058672844729153436\n",
      "Training Loss: 0.006038617320009507\n",
      "Training Loss: 0.005939810275449418\n",
      "Validation Loss: 0.0034792128037347387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.005836273579043336\n",
      "Training Loss: 0.006011205588001758\n",
      "Training Loss: 0.005914510528091341\n",
      "Validation Loss: 0.003448924228282164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005805439294781536\n",
      "Training Loss: 0.005984071813290939\n",
      "Training Loss: 0.005889307035831734\n",
      "Validation Loss: 0.0034185269389045165\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005774857030482963\n",
      "Training Loss: 0.005957264115568251\n",
      "Training Loss: 0.005864209330175072\n",
      "Validation Loss: 0.0033881327309357944\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005744597043958493\n",
      "Training Loss: 0.0059308149421121925\n",
      "Training Loss: 0.0058392289961921055\n",
      "Validation Loss: 0.003357856688442839\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005714718764647841\n",
      "Training Loss: 0.00590474498574622\n",
      "Training Loss: 0.005814372422755696\n",
      "Validation Loss: 0.0033277976205650967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005685260434984229\n",
      "Training Loss: 0.005879053079406731\n",
      "Training Loss: 0.005789642727468163\n",
      "Validation Loss: 0.003298039634756514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.0056562439032131805\n",
      "Training Loss: 0.00585373064968735\n",
      "Training Loss: 0.005765040663536638\n",
      "Validation Loss: 0.003268643801971705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.005627668735687621\n",
      "Training Loss: 0.005828752830857411\n",
      "Training Loss: 0.00574056054756511\n",
      "Validation Loss: 0.003239646309717255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.00559951986419037\n",
      "Training Loss: 0.005804093506885693\n",
      "Training Loss: 0.0057162048347527165\n",
      "Validation Loss: 0.003211078477965856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005571777271106839\n",
      "Training Loss: 0.0057797304569976405\n",
      "Training Loss: 0.005691972746280954\n",
      "Validation Loss: 0.0031829438237028634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005544415658223442\n",
      "Training Loss: 0.005755641863215715\n",
      "Training Loss: 0.0056678767508128655\n",
      "Validation Loss: 0.0031552556689614115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005517416925868019\n",
      "Training Loss: 0.005731820163782686\n",
      "Training Loss: 0.005643933836254291\n",
      "Validation Loss: 0.003128012115452334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005490766740986146\n",
      "Training Loss: 0.005708260320243425\n",
      "Training Loss: 0.005620171476039104\n",
      "Validation Loss: 0.003101220451559599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005464466704288498\n",
      "Training Loss: 0.005684979659272358\n",
      "Training Loss: 0.005596635424881242\n",
      "Validation Loss: 0.003074891008161992\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005438526375801302\n",
      "Training Loss: 0.005662000137963332\n",
      "Training Loss: 0.005573376974207349\n",
      "Validation Loss: 0.0030490460477277555\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.00541296957351733\n",
      "Training Loss: 0.00563935604004655\n",
      "Training Loss: 0.0055504623037995774\n",
      "Validation Loss: 0.0030237142568234395\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005387831772095524\n",
      "Training Loss: 0.005617092726752162\n",
      "Training Loss: 0.0055279636051272975\n",
      "Validation Loss: 0.002998937237903141\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005363158124382607\n",
      "Training Loss: 0.005595263544237241\n",
      "Training Loss: 0.005505965034244582\n",
      "Validation Loss: 0.0029747639455717434\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005339002895634621\n",
      "Training Loss: 0.005573921746108681\n",
      "Training Loss: 0.005484546169172972\n",
      "Validation Loss: 0.002951251168269664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005315418471000157\n",
      "Training Loss: 0.005553127502207644\n",
      "Training Loss: 0.00546378651924897\n",
      "Validation Loss: 0.002928454063314777\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005292459487100132\n",
      "Training Loss: 0.005532933417125605\n",
      "Training Loss: 0.005443758041365072\n",
      "Validation Loss: 0.0029064358811788887\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005270176956546493\n",
      "Training Loss: 0.00551339280907996\n",
      "Training Loss: 0.0054245242441538725\n",
      "Validation Loss: 0.002885251575126051\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005248618056066334\n",
      "Training Loss: 0.005494552869349718\n",
      "Training Loss: 0.005406131192576141\n",
      "Validation Loss: 0.0028649508571147583\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005227821438456886\n",
      "Training Loss: 0.005476452207076364\n",
      "Training Loss: 0.005388615509145893\n",
      "Validation Loss: 0.0028455743688456923\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005207819229690358\n",
      "Training Loss: 0.00545912014960777\n",
      "Training Loss: 0.005371991035062819\n",
      "Validation Loss: 0.002827158383501882\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005188636361272075\n",
      "Training Loss: 0.0054425786121282725\n",
      "Training Loss: 0.005356258168467321\n",
      "Validation Loss: 0.0028097183738568315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005170287735527381\n",
      "Training Loss: 0.005426835454418324\n",
      "Training Loss: 0.005341398401651531\n",
      "Validation Loss: 0.0027932627103683864\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005152783008525148\n",
      "Training Loss: 0.005411892150295899\n",
      "Training Loss: 0.005327380373491905\n",
      "Validation Loss: 0.0027777956618163526\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005136130289174617\n",
      "Training Loss: 0.005397736675804481\n",
      "Training Loss: 0.005314169600023888\n",
      "Validation Loss: 0.002763304962407735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005120334195671603\n",
      "Training Loss: 0.005384358871961013\n",
      "Training Loss: 0.005301729398779571\n",
      "Validation Loss: 0.002749774175018512\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005105391112738289\n",
      "Training Loss: 0.0053717371961101885\n",
      "Training Loss: 0.005290027178707532\n",
      "Validation Loss: 0.0027371822925692612\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005091299386695027\n",
      "Training Loss: 0.005359846053179354\n",
      "Training Loss: 0.005279037366271951\n",
      "Validation Loss: 0.0027254872073764714\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005078041182132438\n",
      "Training Loss: 0.005348661583266221\n",
      "Training Loss: 0.005268733282573521\n",
      "Validation Loss: 0.0027146456319069637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005065588622819632\n",
      "Training Loss: 0.005338139983359724\n",
      "Training Loss: 0.005259080581599847\n",
      "Validation Loss: 0.0027045948287665717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005053896749741398\n",
      "Training Loss: 0.0053282402676995844\n",
      "Training Loss: 0.005250038851518184\n",
      "Validation Loss: 0.002695273656366665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005042911993805319\n",
      "Training Loss: 0.005318919981364161\n",
      "Training Loss: 0.005241562351584434\n",
      "Validation Loss: 0.002686613214822235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005032576993689872\n",
      "Training Loss: 0.005310130731668323\n",
      "Training Loss: 0.005233603243832477\n",
      "Validation Loss: 0.0026785543713629704\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.0050228401069762185\n",
      "Training Loss: 0.005301833570702002\n",
      "Training Loss: 0.00522611296386458\n",
      "Validation Loss: 0.0026710410105336584\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005013646584120579\n",
      "Training Loss: 0.00529398572514765\n",
      "Training Loss: 0.005219051309977658\n",
      "Validation Loss: 0.002664025096952643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005004951002192683\n",
      "Training Loss: 0.005286551087046973\n",
      "Training Loss: 0.0052123766555450856\n",
      "Validation Loss: 0.0026574545363984544\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.004996709877159447\n",
      "Training Loss: 0.005279497402370908\n",
      "Training Loss: 0.005206050137639977\n",
      "Validation Loss: 0.0026512938372403633\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.004988883196492679\n",
      "Training Loss: 0.005272792224423029\n",
      "Training Loss: 0.0052000402321573345\n",
      "Validation Loss: 0.0026455070526506543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.004981437624082901\n",
      "Training Loss: 0.00526640580908861\n",
      "Training Loss: 0.005194318486610427\n",
      "Validation Loss: 0.002640056876096812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.004974338987958618\n",
      "Training Loss: 0.005260309341247193\n",
      "Training Loss: 0.005188856270397082\n",
      "Validation Loss: 0.002634913108921139\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.004967560141230933\n",
      "Training Loss: 0.005254483348107897\n",
      "Training Loss: 0.00518362871138379\n",
      "Validation Loss: 0.0026300474782238892\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.004961073616286739\n",
      "Training Loss: 0.005248900421429425\n",
      "Training Loss: 0.005178616205230355\n",
      "Validation Loss: 0.0026254368490759242\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.004954856564290821\n",
      "Training Loss: 0.0052435446478193625\n",
      "Training Loss: 0.0051737988967215645\n",
      "Validation Loss: 0.0026210572235704807\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.004948888287181034\n",
      "Training Loss: 0.005238391033490188\n",
      "Training Loss: 0.005169158949865959\n",
      "Validation Loss: 0.0026168830073412426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.004943147099111229\n",
      "Training Loss: 0.00523342817905359\n",
      "Training Loss: 0.005164682175964117\n",
      "Validation Loss: 0.002612900163839686\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.004937617809628136\n",
      "Training Loss: 0.005228637041291222\n",
      "Training Loss: 0.005160354056861252\n",
      "Validation Loss: 0.002609089988786099\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.004932282305089757\n",
      "Training Loss: 0.0052240050921682265\n",
      "Training Loss: 0.005156160689657554\n",
      "Validation Loss: 0.002605433489805548\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.004927123701781966\n",
      "Training Loss: 0.005219518959056586\n",
      "Training Loss: 0.005152090655756183\n",
      "Validation Loss: 0.002601922709125963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.004922132623614743\n",
      "Training Loss: 0.0052151655027410015\n",
      "Training Loss: 0.005148134871851653\n",
      "Validation Loss: 0.002598539145213416\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.004917295562336221\n",
      "Training Loss: 0.005210936823277734\n",
      "Training Loss: 0.005144283309346065\n",
      "Validation Loss: 0.002595271520824119\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.004912601459072903\n",
      "Training Loss: 0.0052068196708569305\n",
      "Training Loss: 0.005140528866904788\n",
      "Validation Loss: 0.0025921105132490564\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0049080368381692096\n",
      "Training Loss: 0.005202808096073568\n",
      "Training Loss: 0.005136864486266859\n",
      "Validation Loss: 0.0025890477052680477\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.004903596790973097\n",
      "Training Loss: 0.005198894259519875\n",
      "Training Loss: 0.005133280089939945\n",
      "Validation Loss: 0.0025860723425252257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.004899271161411889\n",
      "Training Loss: 0.005195069556939416\n",
      "Training Loss: 0.005129772727377713\n",
      "Validation Loss: 0.002583176608516552\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.00489505167293828\n",
      "Training Loss: 0.005191327829379588\n",
      "Training Loss: 0.005126334942178801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [03:27<31:11, 207.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0025803515484185075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.08288745453581214\n",
      "Training Loss: 0.074304433260113\n",
      "Training Loss: 0.06987729379907251\n",
      "Validation Loss: 0.06862353297013245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06605898316949606\n",
      "Training Loss: 0.064022755054757\n",
      "Training Loss: 0.060569595564156774\n",
      "Validation Loss: 0.057837266391247845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.05513124322518706\n",
      "Training Loss: 0.05214610199443996\n",
      "Training Loss: 0.04824955854564905\n",
      "Validation Loss: 0.04456624420087659\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.04220123744569719\n",
      "Training Loss: 0.03918480495922268\n",
      "Training Loss: 0.035645666876807806\n",
      "Validation Loss: 0.0322591732413079\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.03064420958980918\n",
      "Training Loss: 0.02837406518869102\n",
      "Training Loss: 0.02577728279400617\n",
      "Validation Loss: 0.023397835515690652\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.022731100618839265\n",
      "Training Loss: 0.021457028645090758\n",
      "Training Loss: 0.019877180829644202\n",
      "Validation Loss: 0.01839281954499108\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.018487193151377142\n",
      "Training Loss: 0.017880580606870353\n",
      "Training Loss: 0.016938934614881874\n",
      "Validation Loss: 0.015853538855470802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.01639207946835086\n",
      "Training Loss: 0.016051504865754396\n",
      "Training Loss: 0.015382567986380309\n",
      "Validation Loss: 0.014317878296996435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.015082722138613463\n",
      "Training Loss: 0.014772115016821772\n",
      "Training Loss: 0.01414920591050759\n",
      "Validation Loss: 0.012844219529122281\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.013723621591925621\n",
      "Training Loss: 0.013334838391747325\n",
      "Training Loss: 0.012670573911163956\n",
      "Validation Loss: 0.011048328565705694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.012126515114214271\n",
      "Training Loss: 0.0118643130781129\n",
      "Training Loss: 0.011352410967228934\n",
      "Validation Loss: 0.009646010636142717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.010983065343461931\n",
      "Training Loss: 0.010889421796891839\n",
      "Training Loss: 0.010498466666322202\n",
      "Validation Loss: 0.008760393956895875\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.010264643188565969\n",
      "Training Loss: 0.010264882014598697\n",
      "Training Loss: 0.009927465177606792\n",
      "Validation Loss: 0.008155013002936593\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.009778687605867163\n",
      "Training Loss: 0.009832132677547633\n",
      "Training Loss: 0.009514977307990193\n",
      "Validation Loss: 0.007698133096015186\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.009417575536062942\n",
      "Training Loss: 0.00950173017103225\n",
      "Training Loss: 0.00919223666889593\n",
      "Validation Loss: 0.007329366230039617\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.009127841531299054\n",
      "Training Loss: 0.00923171553760767\n",
      "Training Loss: 0.008925737674580887\n",
      "Validation Loss: 0.007021507373854016\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.008884706879034638\n",
      "Training Loss: 0.009003239631420001\n",
      "Training Loss: 0.008699668688932434\n",
      "Validation Loss: 0.006761301446214235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.00867657262366265\n",
      "Training Loss: 0.008807408007560297\n",
      "Training Loss: 0.008506243402371183\n",
      "Validation Loss: 0.006541085352315411\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.008497620520647614\n",
      "Training Loss: 0.00863939025788568\n",
      "Training Loss: 0.008340975594474003\n",
      "Validation Loss: 0.006355054961684882\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.00834423042484559\n",
      "Training Loss: 0.008495682063512503\n",
      "Training Loss: 0.008200334628345445\n",
      "Validation Loss: 0.006197695666139297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.00821324811782688\n",
      "Training Loss: 0.008372940863482654\n",
      "Training Loss: 0.008080811658874153\n",
      "Validation Loss: 0.006063537927229334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.008101400805171579\n",
      "Training Loss: 0.008267768432851881\n",
      "Training Loss: 0.007978870653314516\n",
      "Validation Loss: 0.005947542132213293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008005409534089267\n",
      "Training Loss: 0.008176944185979664\n",
      "Training Loss: 0.00789122867048718\n",
      "Validation Loss: 0.005845498748835218\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.007922275624005125\n",
      "Training Loss: 0.008097675909521058\n",
      "Training Loss: 0.0078150969336275\n",
      "Validation Loss: 0.0057541761767077315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.007849485002225265\n",
      "Training Loss: 0.00802771965507418\n",
      "Training Loss: 0.007748255645856262\n",
      "Validation Loss: 0.005671241234339187\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007785047682700679\n",
      "Training Loss: 0.007965352842584253\n",
      "Training Loss: 0.007689007453154773\n",
      "Validation Loss: 0.005595075039799963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.007727443393087015\n",
      "Training Loss: 0.007909281959291548\n",
      "Training Loss: 0.007636071685701609\n",
      "Validation Loss: 0.005524568803859561\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007675526859238744\n",
      "Training Loss: 0.007858540074666963\n",
      "Training Loss: 0.007588478394318372\n",
      "Validation Loss: 0.0054589550421583685\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.007628430356271565\n",
      "Training Loss: 0.0078123911027796564\n",
      "Training Loss: 0.007545479732798412\n",
      "Validation Loss: 0.005397688407775308\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.0075854884553700685\n",
      "Training Loss: 0.007770262788981199\n",
      "Training Loss: 0.007506484391633421\n",
      "Validation Loss: 0.005340363490892302\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007546172427246347\n",
      "Training Loss: 0.007731690250802785\n",
      "Training Loss: 0.007471007885178551\n",
      "Validation Loss: 0.005286661916532752\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007510060243075714\n",
      "Training Loss: 0.007696291413158179\n",
      "Training Loss: 0.007438645804068073\n",
      "Validation Loss: 0.005236321874486094\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007476798604475334\n",
      "Training Loss: 0.007663738896371797\n",
      "Training Loss: 0.007409051542636007\n",
      "Validation Loss: 0.005189116120082065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007446088998112828\n",
      "Training Loss: 0.007633745268685743\n",
      "Training Loss: 0.0073819215851835904\n",
      "Validation Loss: 0.005144842321529273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007417673432501033\n",
      "Training Loss: 0.007606056814547628\n",
      "Training Loss: 0.00735698749194853\n",
      "Validation Loss: 0.005103315983433276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007391323812771589\n",
      "Training Loss: 0.00758044307352975\n",
      "Training Loss: 0.007334008397301659\n",
      "Validation Loss: 0.00506436233369508\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007366835211869329\n",
      "Training Loss: 0.007556694551603869\n",
      "Training Loss: 0.007312766602262854\n",
      "Validation Loss: 0.005027824517710951\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.0073440262977965175\n",
      "Training Loss: 0.007534620325313881\n",
      "Training Loss: 0.0072930675314273686\n",
      "Validation Loss: 0.004993554840408517\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007322732204338536\n",
      "Training Loss: 0.007514043207047507\n",
      "Training Loss: 0.007274730675853789\n",
      "Validation Loss: 0.004961409883997455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007302798521704972\n",
      "Training Loss: 0.00749480017577298\n",
      "Training Loss: 0.007257593584945425\n",
      "Validation Loss: 0.004931252689775749\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007284087139414624\n",
      "Training Loss: 0.007476741204736754\n",
      "Training Loss: 0.007241506860591471\n",
      "Validation Loss: 0.0049029604291145725\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007266470229951665\n",
      "Training Loss: 0.007459727114764974\n",
      "Training Loss: 0.00722633367870003\n",
      "Validation Loss: 0.004876411410723551\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.00724982843734324\n",
      "Training Loss: 0.007443627025932074\n",
      "Training Loss: 0.007211949933553115\n",
      "Validation Loss: 0.004851489535825892\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007234052438288927\n",
      "Training Loss: 0.007428323909407481\n",
      "Training Loss: 0.007198242400772869\n",
      "Validation Loss: 0.004828083563041402\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007219041308853775\n",
      "Training Loss: 0.007413706205552444\n",
      "Training Loss: 0.007185106840915978\n",
      "Validation Loss: 0.004806087454362364\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007204701269511133\n",
      "Training Loss: 0.0073996743082534525\n",
      "Training Loss: 0.007172450314974412\n",
      "Validation Loss: 0.004785401400000778\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007190946099581197\n",
      "Training Loss: 0.007386136575369164\n",
      "Training Loss: 0.00716018854174763\n",
      "Validation Loss: 0.004765927037511942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007177696902072057\n",
      "Training Loss: 0.007373008211143315\n",
      "Training Loss: 0.007148243833798915\n",
      "Validation Loss: 0.004747572191467697\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.00716488063102588\n",
      "Training Loss: 0.007360212827334181\n",
      "Training Loss: 0.0071365488681476566\n",
      "Validation Loss: 0.004730247183483136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007152430575806648\n",
      "Training Loss: 0.00734768197289668\n",
      "Training Loss: 0.0071250429976498705\n",
      "Validation Loss: 0.004713871003536696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007140285670757294\n",
      "Training Loss: 0.007335354309761896\n",
      "Training Loss: 0.0071136711718281734\n",
      "Validation Loss: 0.004698360975821283\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007128390819998458\n",
      "Training Loss: 0.0073231753299478446\n",
      "Training Loss: 0.007102387607446871\n",
      "Validation Loss: 0.004683639604357605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007116695237345994\n",
      "Training Loss: 0.00731109595973976\n",
      "Training Loss: 0.007091149548650719\n",
      "Validation Loss: 0.004669637724056087\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007105154396267608\n",
      "Training Loss: 0.0072990729927551005\n",
      "Training Loss: 0.0070799220789922405\n",
      "Validation Loss: 0.004656287200560563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0070937274163588885\n",
      "Training Loss: 0.007287069583544508\n",
      "Training Loss: 0.00706867303350009\n",
      "Validation Loss: 0.0046435220453501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007082376804901287\n",
      "Training Loss: 0.007275052970508114\n",
      "Training Loss: 0.007057377622113563\n",
      "Validation Loss: 0.0046312803876052576\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0070710693078581245\n",
      "Training Loss: 0.007262994437478482\n",
      "Training Loss: 0.007046010824851691\n",
      "Validation Loss: 0.004619503425816179\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007059775969246403\n",
      "Training Loss: 0.007250870782881975\n",
      "Training Loss: 0.007034556362195871\n",
      "Validation Loss: 0.00460814039113021\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007048471159068867\n",
      "Training Loss: 0.007238661395385862\n",
      "Training Loss: 0.007022998356842436\n",
      "Validation Loss: 0.004597139616073057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.0070371316687669605\n",
      "Training Loss: 0.0072263507533352825\n",
      "Training Loss: 0.007011324554332532\n",
      "Validation Loss: 0.004586454136872643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007025737324729562\n",
      "Training Loss: 0.007213924002135172\n",
      "Training Loss: 0.0069995247310725974\n",
      "Validation Loss: 0.004576037397210518\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007014269469073043\n",
      "Training Loss: 0.007201369977556169\n",
      "Training Loss: 0.006987590890494175\n",
      "Validation Loss: 0.004565847637995115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007002713194815442\n",
      "Training Loss: 0.007188680992694572\n",
      "Training Loss: 0.006975518781109713\n",
      "Validation Loss: 0.004555846417971542\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006991055099060759\n",
      "Training Loss: 0.007175851302454248\n",
      "Training Loss: 0.006963304957607761\n",
      "Validation Loss: 0.004545997519548354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006979284058324993\n",
      "Training Loss: 0.007162875059293583\n",
      "Training Loss: 0.0069509454671060665\n",
      "Validation Loss: 0.004536266269272172\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006967389358906075\n",
      "Training Loss: 0.007149751126999036\n",
      "Training Loss: 0.006938441532547586\n",
      "Validation Loss: 0.004526623684960009\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.006955364824971184\n",
      "Training Loss: 0.007136479201726616\n",
      "Training Loss: 0.006925792680121958\n",
      "Validation Loss: 0.004517038910665413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.006943202539114282\n",
      "Training Loss: 0.007123059271834791\n",
      "Training Loss: 0.006913001048960723\n",
      "Validation Loss: 0.004507487495900707\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006930898145074024\n",
      "Training Loss: 0.007109493572497741\n",
      "Training Loss: 0.0069000687176594515\n",
      "Validation Loss: 0.004497942598431968\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006918447224888951\n",
      "Training Loss: 0.007095785912824794\n",
      "Training Loss: 0.006886998976115138\n",
      "Validation Loss: 0.004488384638248493\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006905847555026412\n",
      "Training Loss: 0.007081939436029643\n",
      "Training Loss: 0.006873796308063902\n",
      "Validation Loss: 0.00447879495031169\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.0068930993194226175\n",
      "Training Loss: 0.007067962118890136\n",
      "Training Loss: 0.0068604655429953705\n",
      "Validation Loss: 0.004469155778014886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006880201675230637\n",
      "Training Loss: 0.007053860004525632\n",
      "Training Loss: 0.006847013026126661\n",
      "Validation Loss: 0.004459452908224521\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006867156178923323\n",
      "Training Loss: 0.0070396398613229395\n",
      "Training Loss: 0.006833444623043761\n",
      "Validation Loss: 0.004449673630087898\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006853966291528195\n",
      "Training Loss: 0.007025311341276392\n",
      "Training Loss: 0.0068197680765297265\n",
      "Validation Loss: 0.00443980888086842\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.0068406354484613985\n",
      "Training Loss: 0.0070108856167644265\n",
      "Training Loss: 0.00680599229352083\n",
      "Validation Loss: 0.004429851404883051\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006827169647440314\n",
      "Training Loss: 0.006996372671565041\n",
      "Training Loss: 0.006792124991770834\n",
      "Validation Loss: 0.004419797944464836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0068135765381157395\n",
      "Training Loss: 0.00698178370250389\n",
      "Training Loss: 0.006778177706873976\n",
      "Validation Loss: 0.004409643678188199\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006799863280029968\n",
      "Training Loss: 0.006967132708523422\n",
      "Training Loss: 0.006764160896418616\n",
      "Validation Loss: 0.00439939254520754\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.0067860407650005075\n",
      "Training Loss: 0.006952433893457055\n",
      "Training Loss: 0.006750085695530288\n",
      "Validation Loss: 0.004389044493201367\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006772120732348412\n",
      "Training Loss: 0.0069377030583564194\n",
      "Training Loss: 0.006735965933767147\n",
      "Validation Loss: 0.004378605658826785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006758117059944197\n",
      "Training Loss: 0.006922955032205209\n",
      "Training Loss: 0.006721815807977692\n",
      "Validation Loss: 0.00436807926966589\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006744041801430285\n",
      "Training Loss: 0.006908206961816177\n",
      "Training Loss: 0.006707648947485723\n",
      "Validation Loss: 0.004357476756656856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006729912172304466\n",
      "Training Loss: 0.00689347539562732\n",
      "Training Loss: 0.006693479438545182\n",
      "Validation Loss: 0.004346806442841176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006715744071407244\n",
      "Training Loss: 0.006878778014797717\n",
      "Training Loss: 0.006679323455900885\n",
      "Validation Loss: 0.004336076069183731\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006701553701423109\n",
      "Training Loss: 0.006864131285110489\n",
      "Training Loss: 0.006665195031673648\n",
      "Validation Loss: 0.004325297884800042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.0066873590112663805\n",
      "Training Loss: 0.006849551793420687\n",
      "Training Loss: 0.006651110035600141\n",
      "Validation Loss: 0.004314484361825992\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006673176432959735\n",
      "Training Loss: 0.0068350558134261516\n",
      "Training Loss: 0.006637080523069017\n",
      "Validation Loss: 0.004303646264754738\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006659023406682536\n",
      "Training Loss: 0.006820657727075741\n",
      "Training Loss: 0.006623119941796176\n",
      "Validation Loss: 0.004292791517022369\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006644914685748518\n",
      "Training Loss: 0.006806370418053121\n",
      "Training Loss: 0.0066092386684613305\n",
      "Validation Loss: 0.004281930952448021\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006630865393090062\n",
      "Training Loss: 0.006792204312514514\n",
      "Training Loss: 0.0065954471920849755\n",
      "Validation Loss: 0.004271072505586101\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0066168878285679964\n",
      "Training Loss: 0.006778168460587039\n",
      "Training Loss: 0.006581752069760114\n",
      "Validation Loss: 0.0042602228866680785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006602993351407349\n",
      "Training Loss: 0.0067642702185548844\n",
      "Training Loss: 0.006568160155438818\n",
      "Validation Loss: 0.0042493883729650735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006589191278908402\n",
      "Training Loss: 0.006750514102168381\n",
      "Training Loss: 0.006554673683131114\n",
      "Validation Loss: 0.004238569403977625\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006575486856745556\n",
      "Training Loss: 0.006736899698153138\n",
      "Training Loss: 0.006541294789640233\n",
      "Validation Loss: 0.004227771507471465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006561885617556982\n",
      "Training Loss: 0.006723427904071286\n",
      "Training Loss: 0.0065280222927685825\n",
      "Validation Loss: 0.004216993742967757\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.0065483904018765315\n",
      "Training Loss: 0.0067100961960386485\n",
      "Training Loss: 0.006514854846755042\n",
      "Validation Loss: 0.004206236943686276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006535000202711671\n",
      "Training Loss: 0.006696899243397638\n",
      "Training Loss: 0.006501789416652173\n",
      "Validation Loss: 0.0041954986923740485\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006521714850678108\n",
      "Training Loss: 0.006683831218397245\n",
      "Training Loss: 0.006488819512887858\n",
      "Validation Loss: 0.0041847749243050895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.0065085311292205\n",
      "Training Loss: 0.006670885412022471\n",
      "Training Loss: 0.006475940677337349\n",
      "Validation Loss: 0.004174061809546116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006495444196043536\n",
      "Training Loss: 0.006658051761332899\n",
      "Training Loss: 0.006463144710869529\n",
      "Validation Loss: 0.004163355824244575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006482447756570764\n",
      "Training Loss: 0.0066453218320384625\n",
      "Training Loss: 0.006450425458606333\n",
      "Validation Loss: 0.004152650961893077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006469536349759437\n",
      "Training Loss: 0.006632685923250392\n",
      "Training Loss: 0.006437776491511613\n",
      "Validation Loss: 0.0041419440622734475\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006456703736330382\n",
      "Training Loss: 0.006620134470285848\n",
      "Training Loss: 0.0064251900621457025\n",
      "Validation Loss: 0.004131227081610162\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.00644394138304051\n",
      "Training Loss: 0.0066076582239475105\n",
      "Training Loss: 0.006412659006891772\n",
      "Validation Loss: 0.004120497667398094\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006431242661783472\n",
      "Training Loss: 0.006595247890800238\n",
      "Training Loss: 0.00640017753758002\n",
      "Validation Loss: 0.004109746179402168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006418599877506494\n",
      "Training Loss: 0.00658289399347268\n",
      "Training Loss: 0.006387739017955027\n",
      "Validation Loss: 0.004098970644448078\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.00640600572922267\n",
      "Training Loss: 0.006570589478360489\n",
      "Training Loss: 0.006375336497440003\n",
      "Validation Loss: 0.0040881683280838\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006393454288481735\n",
      "Training Loss: 0.006558326460653916\n",
      "Training Loss: 0.006362966602900997\n",
      "Validation Loss: 0.004077330095630684\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006380938061629422\n",
      "Training Loss: 0.0065460988541599366\n",
      "Training Loss: 0.006350624444312416\n",
      "Validation Loss: 0.004066455331003147\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006368452548631467\n",
      "Training Loss: 0.006533900633221492\n",
      "Training Loss: 0.006338305072276853\n",
      "Validation Loss: 0.004055540318067154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.00635599022556562\n",
      "Training Loss: 0.0065217257651966065\n",
      "Training Loss: 0.006326005648588762\n",
      "Validation Loss: 0.0040445802949128275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006343547534779646\n",
      "Training Loss: 0.006509571132482961\n",
      "Training Loss: 0.006313722376944497\n",
      "Validation Loss: 0.004033578518553104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006331119925598614\n",
      "Training Loss: 0.006497431081952527\n",
      "Training Loss: 0.006301451091421768\n",
      "Validation Loss: 0.004022523490685886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006318700778065249\n",
      "Training Loss: 0.006485302690416575\n",
      "Training Loss: 0.006289190424140543\n",
      "Validation Loss: 0.004011422054308435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006306288546184078\n",
      "Training Loss: 0.006473182359477505\n",
      "Training Loss: 0.006276937975198962\n",
      "Validation Loss: 0.004000270107415703\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006293879409786314\n",
      "Training Loss: 0.006461068755015731\n",
      "Training Loss: 0.0062646912568015975\n",
      "Validation Loss: 0.003989065185665373\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006281469306559302\n",
      "Training Loss: 0.00644895919249393\n",
      "Training Loss: 0.0062524483347078785\n",
      "Validation Loss: 0.003977808032676661\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006269055420416407\n",
      "Training Loss: 0.006436850680038333\n",
      "Training Loss: 0.006240208121598698\n",
      "Validation Loss: 0.00396650112129413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006256636228063144\n",
      "Training Loss: 0.006424743065144867\n",
      "Training Loss: 0.00622796815179754\n",
      "Validation Loss: 0.003955142445642543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.00624420843727421\n",
      "Training Loss: 0.006412633608561009\n",
      "Training Loss: 0.006215727620874531\n",
      "Validation Loss: 0.003943731889960596\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006231770084705204\n",
      "Training Loss: 0.006400521263713017\n",
      "Training Loss: 0.006203485999722033\n",
      "Validation Loss: 0.003932274097780792\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006219320245436393\n",
      "Training Loss: 0.006388406166806817\n",
      "Training Loss: 0.006191241180058569\n",
      "Validation Loss: 0.003920766592988473\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006206855959026143\n",
      "Training Loss: 0.006376285436563194\n",
      "Training Loss: 0.006178993590874597\n",
      "Validation Loss: 0.003909213858881567\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006194375142222271\n",
      "Training Loss: 0.006364158090436831\n",
      "Training Loss: 0.0061667404032778\n",
      "Validation Loss: 0.0038976137914820417\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006181876828777604\n",
      "Training Loss: 0.0063520228362176565\n",
      "Training Loss: 0.006154481329140253\n",
      "Validation Loss: 0.0038859737596181588\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006169359759078361\n",
      "Training Loss: 0.00633987874025479\n",
      "Training Loss: 0.006142217376036569\n",
      "Validation Loss: 0.0038742911616036816\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006156821137410589\n",
      "Training Loss: 0.0063277243485208605\n",
      "Training Loss: 0.006129946289001964\n",
      "Validation Loss: 0.0038625721602564616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006144261896261014\n",
      "Training Loss: 0.006315559524809941\n",
      "Training Loss: 0.006117668536026031\n",
      "Validation Loss: 0.0038508168085520105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.0061316784721566365\n",
      "Training Loss: 0.0063033819606062025\n",
      "Training Loss: 0.006105382292880677\n",
      "Validation Loss: 0.0038390303968472844\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006119069835403934\n",
      "Training Loss: 0.006291188671020791\n",
      "Training Loss: 0.006093088219640777\n",
      "Validation Loss: 0.0038272118623619584\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006106433815439231\n",
      "Training Loss: 0.006278977615293116\n",
      "Training Loss: 0.006080782877397724\n",
      "Validation Loss: 0.003815364010836092\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006093767430866137\n",
      "Training Loss: 0.0062667465978302065\n",
      "Training Loss: 0.00606846650771331\n",
      "Validation Loss: 0.0038034885394481127\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.0060810697212582455\n",
      "Training Loss: 0.006254493083106354\n",
      "Training Loss: 0.006056137036066503\n",
      "Validation Loss: 0.0037915850740993457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006068338056793437\n",
      "Training Loss: 0.006242213590303436\n",
      "Training Loss: 0.006043793317512609\n",
      "Validation Loss: 0.003779661134042348\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006055568747688085\n",
      "Training Loss: 0.006229904720094055\n",
      "Training Loss: 0.006031434116885066\n",
      "Validation Loss: 0.0037677159115640717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.0060427586059086024\n",
      "Training Loss: 0.00621756084728986\n",
      "Training Loss: 0.006019055668730289\n",
      "Validation Loss: 0.0037557441974058747\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006029903793823906\n",
      "Training Loss: 0.0062051770847756415\n",
      "Training Loss: 0.00600665551086422\n",
      "Validation Loss: 0.0037437540052584215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006017000150750391\n",
      "Training Loss: 0.00619274906697683\n",
      "Training Loss: 0.005994230815558694\n",
      "Validation Loss: 0.0037317387537319172\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006004042839631438\n",
      "Training Loss: 0.006180270288605243\n",
      "Training Loss: 0.005981777418637649\n",
      "Validation Loss: 0.003719702589304678\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.005991026454139501\n",
      "Training Loss: 0.006167732235044241\n",
      "Training Loss: 0.005969289867789484\n",
      "Validation Loss: 0.0037076403040260998\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.005977944219484926\n",
      "Training Loss: 0.006155127824749798\n",
      "Training Loss: 0.005956764338770881\n",
      "Validation Loss: 0.0036955503347915796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005964789490099065\n",
      "Training Loss: 0.006142448887694627\n",
      "Training Loss: 0.0059441941708792\n",
      "Validation Loss: 0.0036834320962544154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005951555333449505\n",
      "Training Loss: 0.0061296853632666175\n",
      "Training Loss: 0.005931571725523099\n",
      "Validation Loss: 0.0036712780691620507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005938232676708139\n",
      "Training Loss: 0.006116825939388945\n",
      "Training Loss: 0.005918891399051063\n",
      "Validation Loss: 0.003659086907544079\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005924813438905403\n",
      "Training Loss: 0.006103860031580552\n",
      "Training Loss: 0.005906143298489042\n",
      "Validation Loss: 0.0036468511863266316\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005911285584443249\n",
      "Training Loss: 0.006090775381308049\n",
      "Training Loss: 0.005893318825983442\n",
      "Validation Loss: 0.003634561407196681\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005897640445036814\n",
      "Training Loss: 0.006077557224780321\n",
      "Training Loss: 0.005880407326621935\n",
      "Validation Loss: 0.0036222138553282266\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.005883863916387781\n",
      "Training Loss: 0.006064190519973636\n",
      "Training Loss: 0.00586739408376161\n",
      "Validation Loss: 0.003609795624971013\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005869942775461823\n",
      "Training Loss: 0.006050659869797528\n",
      "Training Loss: 0.005854270178824663\n",
      "Validation Loss: 0.003597300776327552\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005855864062323235\n",
      "Training Loss: 0.006036948915570974\n",
      "Training Loss: 0.005841019212384708\n",
      "Validation Loss: 0.003584717511555201\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005841613787342794\n",
      "Training Loss: 0.0060230393637903034\n",
      "Training Loss: 0.0058276274189120155\n",
      "Validation Loss: 0.0035720329621827668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005827174837468192\n",
      "Training Loss: 0.006008914079284295\n",
      "Training Loss: 0.005814078276744112\n",
      "Validation Loss: 0.0035592339915914064\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005812531795818358\n",
      "Training Loss: 0.0059945525549119335\n",
      "Training Loss: 0.005800354334642179\n",
      "Validation Loss: 0.0035463082636811175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005797666902653873\n",
      "Training Loss: 0.0059799362008925525\n",
      "Training Loss: 0.0057864383276319134\n",
      "Validation Loss: 0.0035332402690533508\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.0057825627917191015\n",
      "Training Loss: 0.005965044725453481\n",
      "Training Loss: 0.005772310168831609\n",
      "Validation Loss: 0.0035200186140489963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005767202990246006\n",
      "Training Loss: 0.005949860021355562\n",
      "Training Loss: 0.005757951390696689\n",
      "Validation Loss: 0.0035066238468450107\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005751568836858496\n",
      "Training Loss: 0.0059343625063775106\n",
      "Training Loss: 0.005743340569897555\n",
      "Validation Loss: 0.003493039289965514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.0057356437080306934\n",
      "Training Loss: 0.005918534394586459\n",
      "Training Loss: 0.005728460163227283\n",
      "Validation Loss: 0.0034792559611182034\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005719410242745653\n",
      "Training Loss: 0.005902358955936506\n",
      "Training Loss: 0.00571328928344883\n",
      "Validation Loss: 0.0034652568813245952\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005702854588162154\n",
      "Training Loss: 0.005885822998243384\n",
      "Training Loss: 0.005697807453689165\n",
      "Validation Loss: 0.0034510294901562875\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005685962694115005\n",
      "Training Loss: 0.0058689144952222706\n",
      "Training Loss: 0.005681999620865099\n",
      "Validation Loss: 0.0034365629969593764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0056687240675091745\n",
      "Training Loss: 0.005851626780349761\n",
      "Training Loss: 0.005665849821525626\n",
      "Validation Loss: 0.00342184678635696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005651130214100703\n",
      "Training Loss: 0.005833954151603394\n",
      "Training Loss: 0.005649342593969777\n",
      "Validation Loss: 0.0034068770800832282\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005633175428956747\n",
      "Training Loss: 0.005815898771979846\n",
      "Training Loss: 0.005632470090058632\n",
      "Validation Loss: 0.003391651637470245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005614859672496095\n",
      "Training Loss: 0.005797465849318542\n",
      "Training Loss: 0.005615224401699379\n",
      "Validation Loss: 0.00337616676363958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005596184788737446\n",
      "Training Loss: 0.005778666258556768\n",
      "Training Loss: 0.005597603031783365\n",
      "Validation Loss: 0.0033604349676791706\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005577161624096334\n",
      "Training Loss: 0.005759519698331133\n",
      "Training Loss: 0.005579609122360125\n",
      "Validation Loss: 0.0033444586867895604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005557800634414889\n",
      "Training Loss: 0.005740045946440659\n",
      "Training Loss: 0.005561251353356056\n",
      "Validation Loss: 0.003328255479736777\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.0055381216370733455\n",
      "Training Loss: 0.005720276175998152\n",
      "Training Loss: 0.005542541739996523\n",
      "Validation Loss: 0.0033118452819906634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005518146281829104\n",
      "Training Loss: 0.005700244074105285\n",
      "Training Loss: 0.005523500358103775\n",
      "Validation Loss: 0.0032952495142974463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.0054979033395648\n",
      "Training Loss: 0.005679988117772154\n",
      "Training Loss: 0.005504152417415753\n",
      "Validation Loss: 0.0032784947837713393\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005477425101562403\n",
      "Training Loss: 0.005659550532000139\n",
      "Training Loss: 0.005484527661465109\n",
      "Validation Loss: 0.0032616055163743204\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005456746878335253\n",
      "Training Loss: 0.005638977232738398\n",
      "Training Loss: 0.005464661784353666\n",
      "Validation Loss: 0.003244616049721188\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005435907377395779\n",
      "Training Loss: 0.005618316020118073\n",
      "Training Loss: 0.005444599061156623\n",
      "Validation Loss: 0.0032275599012873395\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.0054149504180531945\n",
      "Training Loss: 0.0055976184998871754\n",
      "Training Loss: 0.0054243824648438025\n",
      "Validation Loss: 0.0032104713890670137\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005393920640926808\n",
      "Training Loss: 0.005576933899428696\n",
      "Training Loss: 0.005404065960319713\n",
      "Validation Loss: 0.003193383107061257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.00537286619306542\n",
      "Training Loss: 0.005556315390858799\n",
      "Training Loss: 0.0053837045369436965\n",
      "Validation Loss: 0.0031763342133294165\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.00535183534026146\n",
      "Training Loss: 0.005535813830210828\n",
      "Training Loss: 0.005363355017034337\n",
      "Validation Loss: 0.0031593523139992123\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005330879737157374\n",
      "Training Loss: 0.005515481032198295\n",
      "Training Loss: 0.005343082248000428\n",
      "Validation Loss: 0.003142484022288719\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005310054447036237\n",
      "Training Loss: 0.005495370106655173\n",
      "Training Loss: 0.005322951340349391\n",
      "Validation Loss: 0.0031257551519875155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005289412892889232\n",
      "Training Loss: 0.005475530350231565\n",
      "Training Loss: 0.00530302672937978\n",
      "Validation Loss: 0.003109210039020694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005269012416247279\n",
      "Training Loss: 0.005456013484508731\n",
      "Training Loss: 0.005283375979634002\n",
      "Validation Loss: 0.0030928762584500895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005248908853973262\n",
      "Training Loss: 0.0054368669440737\n",
      "Training Loss: 0.005264064118382521\n",
      "Validation Loss: 0.00307679731788616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005229160591843538\n",
      "Training Loss: 0.005418138019740582\n",
      "Training Loss: 0.005245151048875414\n",
      "Validation Loss: 0.0030610005899719644\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005209819540032186\n",
      "Training Loss: 0.005399868478998542\n",
      "Training Loss: 0.005226693362928927\n",
      "Validation Loss: 0.0030455247170731344\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005190939778694883\n",
      "Training Loss: 0.00538209862424992\n",
      "Training Loss: 0.005208740291418507\n",
      "Validation Loss: 0.0030303962065660385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005172566547989845\n",
      "Training Loss: 0.005364860716508701\n",
      "Training Loss: 0.005191331168171018\n",
      "Validation Loss: 0.003015645486014906\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005154739451245405\n",
      "Training Loss: 0.005348180257715285\n",
      "Training Loss: 0.005174495977698825\n",
      "Validation Loss: 0.00300128582382334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005137489775079302\n",
      "Training Loss: 0.005332076768390834\n",
      "Training Loss: 0.005158254339476116\n",
      "Validation Loss: 0.0029873365070016825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005120840975432656\n",
      "Training Loss: 0.005316562770167366\n",
      "Training Loss: 0.005142617368837818\n",
      "Validation Loss: 0.0029738081640072085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005104806335293688\n",
      "Training Loss: 0.005301640909165144\n",
      "Training Loss: 0.005127584816073068\n",
      "Validation Loss: 0.002960702111379484\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005089390946668573\n",
      "Training Loss: 0.005287307716207579\n",
      "Training Loss: 0.0051131494727451356\n",
      "Validation Loss: 0.002948012022795993\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005074592471937649\n",
      "Training Loss: 0.00527355277677998\n",
      "Training Loss: 0.005099298523855395\n",
      "Validation Loss: 0.002935736527433619\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005060399121721276\n",
      "Training Loss: 0.005260359976673499\n",
      "Training Loss: 0.00508600979403127\n",
      "Validation Loss: 0.0029238652839492798\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005046796848764643\n",
      "Training Loss: 0.005247710070107132\n",
      "Training Loss: 0.005073262639343739\n",
      "Validation Loss: 0.0029123815546973787\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005033765728585422\n",
      "Training Loss: 0.005235581010347232\n",
      "Training Loss: 0.005061031235381961\n",
      "Validation Loss: 0.002901276294599821\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005021282740635798\n",
      "Training Loss: 0.005223946443293243\n",
      "Training Loss: 0.005049289086018689\n",
      "Validation Loss: 0.002890529510306111\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005009323204285465\n",
      "Training Loss: 0.005212781724985689\n",
      "Training Loss: 0.0050380104850046336\n",
      "Validation Loss: 0.002880127614578546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0049978619179455565\n",
      "Training Loss: 0.005202060325536877\n",
      "Training Loss: 0.0050271690037334334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [06:57<27:50, 208.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0028700574770114604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.2554911184310913\n",
      "Training Loss: 0.21135311804711818\n",
      "Training Loss: 0.1583061885088682\n",
      "Validation Loss: 0.12118830736935808\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.1035859952121973\n",
      "Training Loss: 0.08395234972238541\n",
      "Training Loss: 0.07011580398306251\n",
      "Validation Loss: 0.06700582603474012\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06493029341101647\n",
      "Training Loss: 0.06414879374206066\n",
      "Training Loss: 0.06156036078929901\n",
      "Validation Loss: 0.06064479915278681\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.058590105082839725\n",
      "Training Loss: 0.05704073888249695\n",
      "Training Loss: 0.053565200716257096\n",
      "Validation Loss: 0.051378072501066026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.049005850311368705\n",
      "Training Loss: 0.04630974785424769\n",
      "Training Loss: 0.04179541428573429\n",
      "Validation Loss: 0.037791670708174116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.03539014725014567\n",
      "Training Loss: 0.031578606166876855\n",
      "Training Loss: 0.02681864163838327\n",
      "Validation Loss: 0.022422237499627504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.021837512459605933\n",
      "Training Loss: 0.020048905885778366\n",
      "Training Loss: 0.01766930359182879\n",
      "Validation Loss: 0.015347195553771231\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.01565306812990457\n",
      "Training Loss: 0.015194208628963679\n",
      "Training Loss: 0.014052664565388112\n",
      "Validation Loss: 0.01260039864861396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.013238507818896324\n",
      "Training Loss: 0.013196551632136106\n",
      "Training Loss: 0.012504645835142582\n",
      "Validation Loss: 0.011212436034961531\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.012090626616263762\n",
      "Training Loss: 0.012168887970037758\n",
      "Training Loss: 0.01167016465915367\n",
      "Validation Loss: 0.01037722104752248\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.011432552279438823\n",
      "Training Loss: 0.011560054839355871\n",
      "Training Loss: 0.011166286597726867\n",
      "Validation Loss: 0.009844616557774919\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.011021415272261947\n",
      "Training Loss: 0.01117392681655474\n",
      "Training Loss: 0.010838065141579136\n",
      "Validation Loss: 0.009486748826470269\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.010743596646934748\n",
      "Training Loss: 0.010912168783834204\n",
      "Training Loss: 0.010606353557668625\n",
      "Validation Loss: 0.009230087441915457\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.010540577450301498\n",
      "Training Loss: 0.010721823073690757\n",
      "Training Loss: 0.01043043052079156\n",
      "Validation Loss: 0.009033872539261252\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.010382395470514894\n",
      "Training Loss: 0.010574373301351444\n",
      "Training Loss: 0.010288907260401174\n",
      "Validation Loss: 0.008875164217734186\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.01025263977353461\n",
      "Training Loss: 0.010453919054707512\n",
      "Training Loss: 0.010169874435523524\n",
      "Validation Loss: 0.008740891459178222\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.01014176110853441\n",
      "Training Loss: 0.010351334885926917\n",
      "Training Loss: 0.010066394547466188\n",
      "Validation Loss: 0.008623605350744022\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.010044100125087425\n",
      "Training Loss: 0.010261319122510031\n",
      "Training Loss: 0.009974286443321035\n",
      "Validation Loss: 0.008518950647433823\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.009956231601536273\n",
      "Training Loss: 0.010180691068526357\n",
      "Training Loss: 0.009890929659595713\n",
      "Validation Loss: 0.008424313729785885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.00987596723716706\n",
      "Training Loss: 0.010107410518685356\n",
      "Training Loss: 0.00981459069531411\n",
      "Validation Loss: 0.008338065764322626\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.009801795119419693\n",
      "Training Loss: 0.010040042146574706\n",
      "Training Loss: 0.009744036959018558\n",
      "Validation Loss: 0.00825908220650398\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.009732614804524928\n",
      "Training Loss: 0.009977487140567974\n",
      "Training Loss: 0.009678331511095166\n",
      "Validation Loss: 0.008186427583352904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.009667596445651724\n",
      "Training Loss: 0.00991881228168495\n",
      "Training Loss: 0.009616709796246141\n",
      "Validation Loss: 0.008119243139589435\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.009606053781462833\n",
      "Training Loss: 0.009863150558667258\n",
      "Training Loss: 0.009558490525232628\n",
      "Validation Loss: 0.00805677899965242\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.009547312706708909\n",
      "Training Loss: 0.00980961648398079\n",
      "Training Loss: 0.009502979855751619\n",
      "Validation Loss: 0.007998464986148259\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.009490583421429619\n",
      "Training Loss: 0.009757254725554958\n",
      "Training Loss: 0.009449396801646798\n",
      "Validation Loss: 0.007943841648350857\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.009434939950006082\n",
      "Training Loss: 0.009705078318947926\n",
      "Training Loss: 0.009396893020020797\n",
      "Validation Loss: 0.007892217814106118\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.009379444472724572\n",
      "Training Loss: 0.009652241430012509\n",
      "Training Loss: 0.009344722186215222\n",
      "Validation Loss: 0.007842250522801632\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.00932339689345099\n",
      "Training Loss: 0.009598291331203655\n",
      "Training Loss: 0.009292455345857889\n",
      "Validation Loss: 0.0077921728436113075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.00926653362228535\n",
      "Training Loss: 0.009543307014973834\n",
      "Training Loss: 0.009239957438549027\n",
      "Validation Loss: 0.007740793184617932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.009208974037319422\n",
      "Training Loss: 0.009487568663898856\n",
      "Training Loss: 0.009187097221147269\n",
      "Validation Loss: 0.007688042444825759\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.009150854137260467\n",
      "Training Loss: 0.009430859255371616\n",
      "Training Loss: 0.009133451230591163\n",
      "Validation Loss: 0.007634347902474862\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.009091941198566928\n",
      "Training Loss: 0.009372011333471163\n",
      "Training Loss: 0.009078102465718985\n",
      "Validation Loss: 0.007579720358029426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.00903138664085418\n",
      "Training Loss: 0.00930836761253886\n",
      "Training Loss: 0.009019314886536449\n",
      "Validation Loss: 0.007522988526851692\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008967254225863143\n",
      "Training Loss: 0.00923475940595381\n",
      "Training Loss: 0.008953207763843238\n",
      "Validation Loss: 0.0074574335745085825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008895670672645793\n",
      "Training Loss: 0.009146331469528377\n",
      "Training Loss: 0.00887480245437473\n",
      "Validation Loss: 0.007371061877870744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008812592838658019\n",
      "Training Loss: 0.009045544529799372\n",
      "Training Loss: 0.008786077803233639\n",
      "Validation Loss: 0.007262602748003987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008718514147913083\n",
      "Training Loss: 0.008939105082536116\n",
      "Training Loss: 0.008691837680526077\n",
      "Validation Loss: 0.007141247586598306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008618337959051132\n",
      "Training Loss: 0.008831446296535433\n",
      "Training Loss: 0.008595557232620194\n",
      "Validation Loss: 0.007016135019961786\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008517382922582329\n",
      "Training Loss: 0.008726208853768186\n",
      "Training Loss: 0.008500815350562334\n",
      "Validation Loss: 0.006893667228928024\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.008420094818575308\n",
      "Training Loss: 0.008627011855132878\n",
      "Training Loss: 0.008410995951853692\n",
      "Validation Loss: 0.006777925482229068\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008329573350492865\n",
      "Training Loss: 0.008536321318242699\n",
      "Training Loss: 0.008328386364737525\n",
      "Validation Loss: 0.006671200451451574\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.008247484950115904\n",
      "Training Loss: 0.008455226032529026\n",
      "Training Loss: 0.008254045799840242\n",
      "Validation Loss: 0.0065745030244644944\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.008174345173174516\n",
      "Training Loss: 0.008383797282585875\n",
      "Training Loss: 0.00818810354685411\n",
      "Validation Loss: 0.006487980524857602\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.008109877505339681\n",
      "Training Loss: 0.008321433084784075\n",
      "Training Loss: 0.008130056468071417\n",
      "Validation Loss: 0.006411170880598006\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.008053305365610868\n",
      "Training Loss: 0.008267125734128057\n",
      "Training Loss: 0.008079024277394637\n",
      "Validation Loss: 0.006343209110148084\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.00800358344335109\n",
      "Training Loss: 0.008219669103855267\n",
      "Training Loss: 0.008033947924850508\n",
      "Validation Loss: 0.006283020933572036\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.00795957910711877\n",
      "Training Loss: 0.008177819713018834\n",
      "Training Loss: 0.007993737094802783\n",
      "Validation Loss: 0.006229452905899144\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007920196866616607\n",
      "Training Loss: 0.008140413276851177\n",
      "Training Loss: 0.007957377416314557\n",
      "Validation Loss: 0.006181396015532566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007884459884371609\n",
      "Training Loss: 0.008106433280045167\n",
      "Training Loss: 0.007923976136371493\n",
      "Validation Loss: 0.00613784541511971\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007851539003895595\n",
      "Training Loss: 0.008075024604331703\n",
      "Training Loss: 0.007892779667163268\n",
      "Validation Loss: 0.00609792286420262\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007820755769498646\n",
      "Training Loss: 0.008045490854419768\n",
      "Training Loss: 0.007863162697758526\n",
      "Validation Loss: 0.006060875698484564\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007791562718339264\n",
      "Training Loss: 0.008017263615038246\n",
      "Training Loss: 0.007834604143863545\n",
      "Validation Loss: 0.006026058066688645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.0077635168877895925\n",
      "Training Loss: 0.007989878000225872\n",
      "Training Loss: 0.007806669749552384\n",
      "Validation Loss: 0.005992907166135696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007736255381023511\n",
      "Training Loss: 0.007962943555321544\n",
      "Training Loss: 0.007778990335064009\n",
      "Validation Loss: 0.0059609108511620105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0077094679733272645\n",
      "Training Loss: 0.007936127195134758\n",
      "Training Loss: 0.007751250271685422\n",
      "Validation Loss: 0.00592959870053246\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007682879565982148\n",
      "Training Loss: 0.00790913354139775\n",
      "Training Loss: 0.007723175457213074\n",
      "Validation Loss: 0.005898520726516968\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007656233722809702\n",
      "Training Loss: 0.007881701410515234\n",
      "Training Loss: 0.007694535178598016\n",
      "Validation Loss: 0.005867253928157511\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007629288623575121\n",
      "Training Loss: 0.007853605167474598\n",
      "Training Loss: 0.0076651423028670255\n",
      "Validation Loss: 0.0058353964534975334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.0076018124143593015\n",
      "Training Loss: 0.007824655363801867\n",
      "Training Loss: 0.0076348617789335545\n",
      "Validation Loss: 0.005802601808028066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007573599863098935\n",
      "Training Loss: 0.007794712092727423\n",
      "Training Loss: 0.007603617614367977\n",
      "Validation Loss: 0.005768584214370656\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007544477782212198\n",
      "Training Loss: 0.007763695066096261\n",
      "Training Loss: 0.007571404870832339\n",
      "Validation Loss: 0.005733148372088549\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007514331844868138\n",
      "Training Loss: 0.007731589679606259\n",
      "Training Loss: 0.007538291214732453\n",
      "Validation Loss: 0.0056962106655498316\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007483118234667927\n",
      "Training Loss: 0.007698443948756903\n",
      "Training Loss: 0.00750440732575953\n",
      "Validation Loss: 0.005657811421545201\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0074508720310404895\n",
      "Training Loss: 0.0076643630280159416\n",
      "Training Loss: 0.007469927945639938\n",
      "Validation Loss: 0.005618098669350566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007417699132347479\n",
      "Training Loss: 0.007629496494773775\n",
      "Training Loss: 0.007435044080484659\n",
      "Validation Loss: 0.005577294877991917\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007383754408219829\n",
      "Training Loss: 0.007594017912633717\n",
      "Training Loss: 0.007399944480275735\n",
      "Validation Loss: 0.005535647593532804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007349219379248098\n",
      "Training Loss: 0.007558114478597417\n",
      "Training Loss: 0.00736480186926201\n",
      "Validation Loss: 0.005493415546409911\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007314280843129382\n",
      "Training Loss: 0.007521971783135086\n",
      "Training Loss: 0.007329767133342102\n",
      "Validation Loss: 0.005450836669920494\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007279110638191924\n",
      "Training Loss: 0.007485755628440529\n",
      "Training Loss: 0.0072949601826258\n",
      "Validation Loss: 0.005408130051160043\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0072438589599914845\n",
      "Training Loss: 0.00744961146148853\n",
      "Training Loss: 0.007260480171535164\n",
      "Validation Loss: 0.00536548285980531\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007208651163382456\n",
      "Training Loss: 0.007413659606827423\n",
      "Training Loss: 0.007226402490632608\n",
      "Validation Loss: 0.005323048390636451\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007173585657728836\n",
      "Training Loss: 0.007377995250280946\n",
      "Training Loss: 0.00719278585864231\n",
      "Validation Loss: 0.005280953718825463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007138741151429713\n",
      "Training Loss: 0.0073426946543622765\n",
      "Training Loss: 0.0071596759930253025\n",
      "Validation Loss: 0.005239305465110693\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007104180090827868\n",
      "Training Loss: 0.007307815294479951\n",
      "Training Loss: 0.007127108213026077\n",
      "Validation Loss: 0.005198180296175768\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007069949272554368\n",
      "Training Loss: 0.007273399799596518\n",
      "Training Loss: 0.007095110251102596\n",
      "Validation Loss: 0.005157645589023242\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00703609126037918\n",
      "Training Loss: 0.007239480392308905\n",
      "Training Loss: 0.007063705571927131\n",
      "Validation Loss: 0.0051177515173487865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00700263928854838\n",
      "Training Loss: 0.007206079246243462\n",
      "Training Loss: 0.007032907172106207\n",
      "Validation Loss: 0.005078533203916603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00696962604066357\n",
      "Training Loss: 0.007173212950583548\n",
      "Training Loss: 0.00700273115071468\n",
      "Validation Loss: 0.005040024819893741\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006937086791731417\n",
      "Training Loss: 0.007140896824421361\n",
      "Training Loss: 0.006973188130650669\n",
      "Validation Loss: 0.0050022503749593\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006905055134557188\n",
      "Training Loss: 0.007109145196154714\n",
      "Training Loss: 0.006944291632389649\n",
      "Validation Loss: 0.004965240578559552\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006873575105564669\n",
      "Training Loss: 0.007077975943684578\n",
      "Training Loss: 0.006916056339396164\n",
      "Validation Loss: 0.0049290196673180695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.0068426901299972085\n",
      "Training Loss: 0.007047407738864422\n",
      "Training Loss: 0.006888497835025192\n",
      "Validation Loss: 0.004893621357204916\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006812453146558255\n",
      "Training Loss: 0.007017470834543928\n",
      "Training Loss: 0.006861641885479912\n",
      "Validation Loss: 0.004859081263460344\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006782920748228207\n",
      "Training Loss: 0.006988197726896033\n",
      "Training Loss: 0.00683551246416755\n",
      "Validation Loss: 0.004825439852181027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006754154460504651\n",
      "Training Loss: 0.006959626238094643\n",
      "Training Loss: 0.00681013978086412\n",
      "Validation Loss: 0.004792744828142184\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006726216344395653\n",
      "Training Loss: 0.006931802922626957\n",
      "Training Loss: 0.006785559003474191\n",
      "Validation Loss: 0.004761042899394596\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006699167757760733\n",
      "Training Loss: 0.006904775680741295\n",
      "Training Loss: 0.0067618024395778775\n",
      "Validation Loss: 0.004730377553601153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006673062921036035\n",
      "Training Loss: 0.006878590395208448\n",
      "Training Loss: 0.0067389010556507855\n",
      "Validation Loss: 0.00470079388796467\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006647948499303311\n",
      "Training Loss: 0.006853291988372803\n",
      "Training Loss: 0.00671687902067788\n",
      "Validation Loss: 0.004672317825822851\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006623853330966085\n",
      "Training Loss: 0.006828915152000264\n",
      "Training Loss: 0.006695750931976363\n",
      "Validation Loss: 0.004644970101529334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0066007930564228445\n",
      "Training Loss: 0.006805483245989308\n",
      "Training Loss: 0.006675520633580163\n",
      "Validation Loss: 0.0046187546254617025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.00657876295153983\n",
      "Training Loss: 0.006783006520709023\n",
      "Training Loss: 0.006656177948461846\n",
      "Validation Loss: 0.004593660681524178\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006557742560980842\n",
      "Training Loss: 0.006761483154259622\n",
      "Training Loss: 0.006637700693681836\n",
      "Validation Loss: 0.00456965786765926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006537693477002904\n",
      "Training Loss: 0.006740892846137285\n",
      "Training Loss: 0.006620054334634915\n",
      "Validation Loss: 0.0045467064862422055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006518565011210739\n",
      "Training Loss: 0.006721206683432683\n",
      "Training Loss: 0.006603196238866076\n",
      "Validation Loss: 0.00452475325241057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006500297477468848\n",
      "Training Loss: 0.006702384643722326\n",
      "Training Loss: 0.006587076636496931\n",
      "Validation Loss: 0.004503739220116371\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.0064828273537568745\n",
      "Training Loss: 0.006684378306381404\n",
      "Training Loss: 0.006571639900794252\n",
      "Validation Loss: 0.0044836021071624306\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006466086742002517\n",
      "Training Loss: 0.006667134690796956\n",
      "Training Loss: 0.006556831736816093\n",
      "Validation Loss: 0.004464275468522788\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006450009086402133\n",
      "Training Loss: 0.006650600513676181\n",
      "Training Loss: 0.006542596264043823\n",
      "Validation Loss: 0.00444569309004007\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006434531188569963\n",
      "Training Loss: 0.006634720792062581\n",
      "Training Loss: 0.006528880078112706\n",
      "Validation Loss: 0.004427795450129871\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006419594772160054\n",
      "Training Loss: 0.006619442120427266\n",
      "Training Loss: 0.0065156324033159765\n",
      "Validation Loss: 0.004410520764957234\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006405146007891744\n",
      "Training Loss: 0.006604714478598907\n",
      "Training Loss: 0.00650280729169026\n",
      "Validation Loss: 0.004393815433114684\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006391134975710883\n",
      "Training Loss: 0.006590488936053589\n",
      "Training Loss: 0.006490363666089251\n",
      "Validation Loss: 0.004377632023206797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006377520617097616\n",
      "Training Loss: 0.006576723933685571\n",
      "Training Loss: 0.006478262185119092\n",
      "Validation Loss: 0.004361921156824598\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006364264881704003\n",
      "Training Loss: 0.006563379630679264\n",
      "Training Loss: 0.006466469747247175\n",
      "Validation Loss: 0.004346643877346487\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.00635133532457985\n",
      "Training Loss: 0.006550419632112607\n",
      "Training Loss: 0.006454956694506108\n",
      "Validation Loss: 0.004331764167310733\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006338703088695183\n",
      "Training Loss: 0.006537812025053427\n",
      "Training Loss: 0.0064436993200797585\n",
      "Validation Loss: 0.004317249032129858\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006326344260014593\n",
      "Training Loss: 0.006525528028141707\n",
      "Training Loss: 0.006432672132505104\n",
      "Validation Loss: 0.004303070644617834\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006314237349433824\n",
      "Training Loss: 0.006513542213942855\n",
      "Training Loss: 0.006421857649693266\n",
      "Validation Loss: 0.00428920165763833\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006302364942384884\n",
      "Training Loss: 0.006501832050271332\n",
      "Training Loss: 0.006411237353459001\n",
      "Validation Loss: 0.004275620425789712\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006290711137698963\n",
      "Training Loss: 0.006490378368180245\n",
      "Training Loss: 0.006400799216935411\n",
      "Validation Loss: 0.004262306138198153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.00627926250686869\n",
      "Training Loss: 0.006479162935866043\n",
      "Training Loss: 0.006390529525233433\n",
      "Validation Loss: 0.004249240677892702\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006268007372273132\n",
      "Training Loss: 0.006468169837025925\n",
      "Training Loss: 0.0063804181222803895\n",
      "Validation Loss: 0.004236407940317825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006256935634883121\n",
      "Training Loss: 0.006457386423135176\n",
      "Training Loss: 0.006370456056902185\n",
      "Validation Loss: 0.0042237886855525245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.0062460389558691536\n",
      "Training Loss: 0.006446800207486376\n",
      "Training Loss: 0.0063606359669938686\n",
      "Validation Loss: 0.004211374208608412\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006235309018520639\n",
      "Training Loss: 0.006436401706887409\n",
      "Training Loss: 0.006350952351931482\n",
      "Validation Loss: 0.00419915106613189\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.0062247407250106334\n",
      "Training Loss: 0.006426181448623538\n",
      "Training Loss: 0.006341399061493575\n",
      "Validation Loss: 0.004187105367420597\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006214327465277165\n",
      "Training Loss: 0.006416131465812214\n",
      "Training Loss: 0.00633197310846299\n",
      "Validation Loss: 0.004175229649467582\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006204064060002565\n",
      "Training Loss: 0.006406245171092451\n",
      "Training Loss: 0.006322669876972213\n",
      "Validation Loss: 0.0041635154047980905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006193946680286899\n",
      "Training Loss: 0.006396517117391341\n",
      "Training Loss: 0.006313486668514088\n",
      "Validation Loss: 0.0041519516177757985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006183971448335796\n",
      "Training Loss: 0.0063869416119996456\n",
      "Training Loss: 0.006304422754328698\n",
      "Validation Loss: 0.004140531505573164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006174135971814394\n",
      "Training Loss: 0.006377514799241908\n",
      "Training Loss: 0.006295475086662918\n",
      "Validation Loss: 0.004129247551553705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006164435424143449\n",
      "Training Loss: 0.006368233277462423\n",
      "Training Loss: 0.006286643074126914\n",
      "Validation Loss: 0.004118094726967929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006154868665616959\n",
      "Training Loss: 0.006359093106584624\n",
      "Training Loss: 0.00627792592276819\n",
      "Validation Loss: 0.004107069689126455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00614543350879103\n",
      "Training Loss: 0.006350091706844978\n",
      "Training Loss: 0.006269322162261233\n",
      "Validation Loss: 0.004096161627100802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006136126490309835\n",
      "Training Loss: 0.006341227099183016\n",
      "Training Loss: 0.006260832119151019\n",
      "Validation Loss: 0.004085368538939844\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006126946826698259\n",
      "Training Loss: 0.006332496686372906\n",
      "Training Loss: 0.00625245381554123\n",
      "Validation Loss: 0.0040746903383131194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006117892044130713\n",
      "Training Loss: 0.006323898678529076\n",
      "Training Loss: 0.006244188830023632\n",
      "Validation Loss: 0.004064118150841403\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.0061089620541315524\n",
      "Training Loss: 0.00631543060473632\n",
      "Training Loss: 0.006236035087495111\n",
      "Validation Loss: 0.004053649856196193\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006100152794970199\n",
      "Training Loss: 0.006307091243215836\n",
      "Training Loss: 0.006227993824286387\n",
      "Validation Loss: 0.004043284617233603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006091463534394279\n",
      "Training Loss: 0.0062988788308575745\n",
      "Training Loss: 0.006220063920482062\n",
      "Validation Loss: 0.004033020059211871\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006082893053535372\n",
      "Training Loss: 0.006290791138890199\n",
      "Training Loss: 0.00621224427188281\n",
      "Validation Loss: 0.004022852955202917\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.0060744389100000264\n",
      "Training Loss: 0.006282826202223078\n",
      "Training Loss: 0.0062045341363409535\n",
      "Validation Loss: 0.0040127815354322445\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006066099539166317\n",
      "Training Loss: 0.006274982176255435\n",
      "Training Loss: 0.006196933405008167\n",
      "Validation Loss: 0.0040028057940136854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006057873822283\n",
      "Training Loss: 0.006267257877043448\n",
      "Training Loss: 0.006189441346214153\n",
      "Validation Loss: 0.003992924222126185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006049759013112634\n",
      "Training Loss: 0.006259649762068875\n",
      "Training Loss: 0.0061820571834687145\n",
      "Validation Loss: 0.0039831365228452705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006041754101170227\n",
      "Training Loss: 0.006252155986730941\n",
      "Training Loss: 0.006174778662389144\n",
      "Validation Loss: 0.003973439863616119\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.0060338564263656734\n",
      "Training Loss: 0.006244774688966572\n",
      "Training Loss: 0.0061676056758733465\n",
      "Validation Loss: 0.0039638340063759365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00602606465225108\n",
      "Training Loss: 0.0062375032767886295\n",
      "Training Loss: 0.006160535929375328\n",
      "Validation Loss: 0.003954318888338931\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006018376109423116\n",
      "Training Loss: 0.006230339342728257\n",
      "Training Loss: 0.006153568194131367\n",
      "Validation Loss: 0.0039448951125103104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.0060107890074141326\n",
      "Training Loss: 0.006223279848345555\n",
      "Training Loss: 0.006146701191901229\n",
      "Validation Loss: 0.00393556027537149\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006003301240270958\n",
      "Training Loss: 0.00621632315160241\n",
      "Training Loss: 0.006139932984369807\n",
      "Validation Loss: 0.003926314425319852\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.0059959103516303005\n",
      "Training Loss: 0.006209464934072457\n",
      "Training Loss: 0.00613326218968723\n",
      "Validation Loss: 0.0039171560901593875\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005988614234374836\n",
      "Training Loss: 0.006202703532180749\n",
      "Training Loss: 0.006126685946364887\n",
      "Validation Loss: 0.003908087887926802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0059814112051390115\n",
      "Training Loss: 0.006196036198525689\n",
      "Training Loss: 0.006120202650199644\n",
      "Validation Loss: 0.003899104576008499\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005974297873908654\n",
      "Training Loss: 0.006189459324232303\n",
      "Training Loss: 0.006113811194663868\n",
      "Validation Loss: 0.0038902078456967398\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.0059672730509191755\n",
      "Training Loss: 0.006182971216621809\n",
      "Training Loss: 0.0061075086204800755\n",
      "Validation Loss: 0.0038813986139256884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.0059603347501251845\n",
      "Training Loss: 0.006176568001974374\n",
      "Training Loss: 0.006101293383981101\n",
      "Validation Loss: 0.003872676264609765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.0059534805931616575\n",
      "Training Loss: 0.006170247384579853\n",
      "Training Loss: 0.006095163456047885\n",
      "Validation Loss: 0.003864038294165531\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005946707999100909\n",
      "Training Loss: 0.00616400734113995\n",
      "Training Loss: 0.006089115992654115\n",
      "Validation Loss: 0.0038554845966419643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005940015178639442\n",
      "Training Loss: 0.006157843800028786\n",
      "Training Loss: 0.00608315042743925\n",
      "Validation Loss: 0.003847016678244043\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005933400004869327\n",
      "Training Loss: 0.006151755186729133\n",
      "Training Loss: 0.006077262534527108\n",
      "Validation Loss: 0.003838626917692299\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.0059268604847602544\n",
      "Training Loss: 0.006145738051272929\n",
      "Training Loss: 0.006071452075848356\n",
      "Validation Loss: 0.003830321696900836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005920394472777844\n",
      "Training Loss: 0.00613979130750522\n",
      "Training Loss: 0.006065716881421395\n",
      "Validation Loss: 0.0038220997568837295\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.0059140012064017354\n",
      "Training Loss: 0.006133911412907764\n",
      "Training Loss: 0.006060054364497774\n",
      "Validation Loss: 0.003813957802040942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.0059076776553411035\n",
      "Training Loss: 0.0061280961893498895\n",
      "Training Loss: 0.006054462420288473\n",
      "Validation Loss: 0.0038058973163408176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005901422310853377\n",
      "Training Loss: 0.006122343350434676\n",
      "Training Loss: 0.006048939517349936\n",
      "Validation Loss: 0.0037979158459053365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005895233461633325\n",
      "Training Loss: 0.00611665079719387\n",
      "Training Loss: 0.006043483582907357\n",
      "Validation Loss: 0.0037900129054526514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005889109644340351\n",
      "Training Loss: 0.006111016680952161\n",
      "Training Loss: 0.006038093167589977\n",
      "Validation Loss: 0.003782186750060973\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.0058830486331135036\n",
      "Training Loss: 0.0061054389865603294\n",
      "Training Loss: 0.006032766224816442\n",
      "Validation Loss: 0.0037744393391702115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005877049803966656\n",
      "Training Loss: 0.006099915750673972\n",
      "Training Loss: 0.006027501006028615\n",
      "Validation Loss: 0.0037667667879095154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005871111209271476\n",
      "Training Loss: 0.006094444643822499\n",
      "Training Loss: 0.00602229583018925\n",
      "Validation Loss: 0.003759171699927178\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005865231691859663\n",
      "Training Loss: 0.006089023970416747\n",
      "Training Loss: 0.006017148313694634\n",
      "Validation Loss: 0.003751649017696886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0058594090305268765\n",
      "Training Loss: 0.006083653363166377\n",
      "Training Loss: 0.006012058598571457\n",
      "Validation Loss: 0.003744201750358504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005853644612943754\n",
      "Training Loss: 0.0060783298319438475\n",
      "Training Loss: 0.006007022981066257\n",
      "Validation Loss: 0.0037368285584818113\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005847934724297375\n",
      "Training Loss: 0.006073053134023212\n",
      "Training Loss: 0.0060020421294029805\n",
      "Validation Loss: 0.0037295288743786096\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005842278561322019\n",
      "Training Loss: 0.006067820739117451\n",
      "Training Loss: 0.0059971135295927526\n",
      "Validation Loss: 0.0037223013991678363\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.0058366755431052296\n",
      "Training Loss: 0.006062631765962579\n",
      "Training Loss: 0.005992235545418226\n",
      "Validation Loss: 0.0037151449575779563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.00583112446591258\n",
      "Training Loss: 0.006057485836208798\n",
      "Training Loss: 0.005987407422508113\n",
      "Validation Loss: 0.0037080610381554436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005825624505523592\n",
      "Training Loss: 0.006052380058099516\n",
      "Training Loss: 0.005982628021738492\n",
      "Validation Loss: 0.0037010464715258626\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005820174486143514\n",
      "Training Loss: 0.006047315252362751\n",
      "Training Loss: 0.0059778950206236916\n",
      "Validation Loss: 0.0036941012831959415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005814773673191667\n",
      "Training Loss: 0.006042289388715289\n",
      "Training Loss: 0.0059732084930874405\n",
      "Validation Loss: 0.003687227132411025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005809421457815916\n",
      "Training Loss: 0.006037301571341231\n",
      "Training Loss: 0.005968567220261321\n",
      "Validation Loss: 0.0036804219622824214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005804116122890264\n",
      "Training Loss: 0.006032350606983527\n",
      "Training Loss: 0.005963969510048628\n",
      "Validation Loss: 0.0036736844621567326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005798857865156606\n",
      "Training Loss: 0.006027436321601271\n",
      "Training Loss: 0.0059594141284469515\n",
      "Validation Loss: 0.0036670156091378396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005793646309757605\n",
      "Training Loss: 0.006022558000404387\n",
      "Training Loss: 0.0059549005306325856\n",
      "Validation Loss: 0.0036604120232572967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005788478869944811\n",
      "Training Loss: 0.006017714338377118\n",
      "Training Loss: 0.005950427728821523\n",
      "Validation Loss: 0.0036538751086931717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005783356578322127\n",
      "Training Loss: 0.006012904916424304\n",
      "Training Loss: 0.005945994512876496\n",
      "Validation Loss: 0.0036474070884548963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005778278408106416\n",
      "Training Loss: 0.006008128695539199\n",
      "Training Loss: 0.005941600747755729\n",
      "Validation Loss: 0.0036410058101670546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005773244003066793\n",
      "Training Loss: 0.0060033861425472426\n",
      "Training Loss: 0.005937245352542959\n",
      "Validation Loss: 0.0036346678016445696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005768251640256494\n",
      "Training Loss: 0.005998675563023426\n",
      "Training Loss: 0.005932926724199206\n",
      "Validation Loss: 0.0036283973473636958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005763302447739989\n",
      "Training Loss: 0.005993997095501981\n",
      "Training Loss: 0.005928644961095415\n",
      "Validation Loss: 0.003622189349902982\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005758394530275836\n",
      "Training Loss: 0.005989350235904567\n",
      "Training Loss: 0.005924398634233512\n",
      "Validation Loss: 0.003616048692165759\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005753528011264279\n",
      "Training Loss: 0.005984734437661245\n",
      "Training Loss: 0.005920187752344645\n",
      "Validation Loss: 0.0036099688522779286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005748702004784718\n",
      "Training Loss: 0.00598014933813829\n",
      "Training Loss: 0.005916010278742761\n",
      "Validation Loss: 0.0036039530107322454\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005743915947386995\n",
      "Training Loss: 0.005975594719056971\n",
      "Training Loss: 0.005911867211107164\n",
      "Validation Loss: 0.0035980010328008636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005739169892622158\n",
      "Training Loss: 0.005971069647348486\n",
      "Training Loss: 0.005907756617525592\n",
      "Validation Loss: 0.0035921113383746966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005734462803229689\n",
      "Training Loss: 0.005966573578189127\n",
      "Training Loss: 0.005903678926988505\n",
      "Validation Loss: 0.0035862824304050273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005729794214712456\n",
      "Training Loss: 0.005962107126833871\n",
      "Training Loss: 0.005899631718639284\n",
      "Validation Loss: 0.0035805169301986526\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.0057251645170617845\n",
      "Training Loss: 0.005957669045892544\n",
      "Training Loss: 0.005895617055357434\n",
      "Validation Loss: 0.0035748128665433255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005720571981510147\n",
      "Training Loss: 0.005953260753885843\n",
      "Training Loss: 0.005891631997074\n",
      "Validation Loss: 0.0035691672165647902\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.0057160171947907654\n",
      "Training Loss: 0.005948879648349248\n",
      "Training Loss: 0.005887676978600212\n",
      "Validation Loss: 0.003563584487759665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005711499247699976\n",
      "Training Loss: 0.005944527204264887\n",
      "Training Loss: 0.005883752203080803\n",
      "Validation Loss: 0.0035580597481732206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.00570701741729863\n",
      "Training Loss: 0.00594020195887424\n",
      "Training Loss: 0.005879856178653426\n",
      "Validation Loss: 0.0035525969127611664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005702572155278176\n",
      "Training Loss: 0.00593590468459297\n",
      "Training Loss: 0.00587598801124841\n",
      "Validation Loss: 0.0035471900515362956\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005698162351036444\n",
      "Training Loss: 0.005931635571178049\n",
      "Training Loss: 0.005872147973277606\n",
      "Validation Loss: 0.0035418429571528283\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005693787977797911\n",
      "Training Loss: 0.005927392278681509\n",
      "Training Loss: 0.0058683358784765004\n",
      "Validation Loss: 0.0035365517538961735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005689447183394805\n",
      "Training Loss: 0.005923176033538766\n",
      "Training Loss: 0.005864549604011699\n",
      "Validation Loss: 0.003531317643852632\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005685141563881189\n",
      "Training Loss: 0.005918986991164274\n",
      "Training Loss: 0.005860790794249624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [10:26<24:21, 208.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0035261419010505584\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.4598026625812054\n",
      "Training Loss: 0.39679266028106214\n",
      "Training Loss: 0.31312607012689114\n",
      "Validation Loss: 0.23513045136848193\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.18701400879770516\n",
      "Training Loss: 0.11838552378118038\n",
      "Training Loss: 0.07364368261769413\n",
      "Validation Loss: 0.060134507506416086\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.05599725347943604\n",
      "Training Loss: 0.05291077789850533\n",
      "Training Loss: 0.04807221178896725\n",
      "Validation Loss: 0.046002660469895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.04331773854792118\n",
      "Training Loss: 0.04100051614455879\n",
      "Training Loss: 0.03728569848462939\n",
      "Validation Loss: 0.035729412052236245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.03372340216301382\n",
      "Training Loss: 0.03198512649163604\n",
      "Training Loss: 0.029103023000061512\n",
      "Validation Loss: 0.02782989653831955\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.026483091549016534\n",
      "Training Loss: 0.02516653239261359\n",
      "Training Loss: 0.02293290283996612\n",
      "Validation Loss: 0.021820476760020417\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.021153281200677155\n",
      "Training Loss: 0.020212754681706427\n",
      "Training Loss: 0.018501612616237253\n",
      "Validation Loss: 0.017447069920390176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.01738857481861487\n",
      "Training Loss: 0.016788673053961246\n",
      "Training Loss: 0.015503838853910566\n",
      "Validation Loss: 0.014467150443809087\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.014882077742367984\n",
      "Training Loss: 0.01455624294700101\n",
      "Training Loss: 0.013592817562166602\n",
      "Validation Loss: 0.01251300432708826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.013233816027641296\n",
      "Training Loss: 0.0130609825020656\n",
      "Training Loss: 0.012287418921478093\n",
      "Validation Loss: 0.011105654939088258\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.012031480087898672\n",
      "Training Loss: 0.011937098689377307\n",
      "Training Loss: 0.011279121388215572\n",
      "Validation Loss: 0.009985142070904708\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.011081371060572564\n",
      "Training Loss: 0.01103809331310913\n",
      "Training Loss: 0.010462182207265868\n",
      "Validation Loss: 0.009063453871359149\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.010317124972352758\n",
      "Training Loss: 0.010313601533416658\n",
      "Training Loss: 0.009801109065301716\n",
      "Validation Loss: 0.00830708824234146\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.0097066627710592\n",
      "Training Loss: 0.009735361916245893\n",
      "Training Loss: 0.009272944635013118\n",
      "Validation Loss: 0.007691440042997762\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.009222659702645615\n",
      "Training Loss: 0.009276949052000417\n",
      "Training Loss: 0.008854357022792102\n",
      "Validation Loss: 0.007191474045877963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.008838907672325149\n",
      "Training Loss: 0.00891296383109875\n",
      "Training Loss: 0.008522672960534692\n",
      "Validation Loss: 0.0067834453055572305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.008532484001480043\n",
      "Training Loss: 0.008621660529170185\n",
      "Training Loss: 0.00825839304830879\n",
      "Validation Loss: 0.0064474567116939285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.00828519977396354\n",
      "Training Loss: 0.008386147456476464\n",
      "Training Loss: 0.008046186754945665\n",
      "Validation Loss: 0.006168255705407329\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.008083554602926597\n",
      "Training Loss: 0.008194024440599605\n",
      "Training Loss: 0.00787455343757756\n",
      "Validation Loss: 0.0059346318522237995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.007917780152056367\n",
      "Training Loss: 0.008036268946016207\n",
      "Training Loss: 0.007734912957530469\n",
      "Validation Loss: 0.005738246080653972\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.0077806762850377706\n",
      "Training Loss: 0.007906109254108742\n",
      "Training Loss: 0.007620701981941238\n",
      "Validation Loss: 0.005572662377478868\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.007666722425492481\n",
      "Training Loss: 0.0077982559811789545\n",
      "Training Loss: 0.007526760172331706\n",
      "Validation Loss: 0.005432712959303531\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.007571527833351866\n",
      "Training Loss: 0.007708442508010194\n",
      "Training Loss: 0.007448953057173639\n",
      "Validation Loss: 0.005314140104468954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.007491518812021241\n",
      "Training Loss: 0.00763317117234692\n",
      "Training Loss: 0.007383932501543313\n",
      "Validation Loss: 0.005213380456389336\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.007423759892117232\n",
      "Training Loss: 0.007569559225812554\n",
      "Training Loss: 0.00732898895512335\n",
      "Validation Loss: 0.005127439360406375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007365835566306486\n",
      "Training Loss: 0.00751523798215203\n",
      "Training Loss: 0.0072819374385289844\n",
      "Validation Loss: 0.005053796105641412\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.007315770572749898\n",
      "Training Loss: 0.0074682792846579105\n",
      "Training Loss: 0.007241033437894657\n",
      "Validation Loss: 0.004990343684644512\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007271964015671983\n",
      "Training Loss: 0.007427128003910184\n",
      "Training Loss: 0.007204897013725713\n",
      "Validation Loss: 0.004935324570069906\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.0072331294172909115\n",
      "Training Loss: 0.0073905483691487465\n",
      "Training Loss: 0.007172451796941459\n",
      "Validation Loss: 0.004887281219851686\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.007198246722109616\n",
      "Training Loss: 0.0073575748945586385\n",
      "Training Loss: 0.007142874015262351\n",
      "Validation Loss: 0.004845015818431052\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007166520981118083\n",
      "Training Loss: 0.0073274667735677215\n",
      "Training Loss: 0.0071155430399812754\n",
      "Validation Loss: 0.004807553399260041\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007137341205962002\n",
      "Training Loss: 0.007299667516490444\n",
      "Training Loss: 0.007090004533529282\n",
      "Validation Loss: 0.004774100014421922\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.0071102464513387535\n",
      "Training Loss: 0.007273769971216097\n",
      "Training Loss: 0.007065930010285228\n",
      "Validation Loss: 0.00474401135023767\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007084893020801246\n",
      "Training Loss: 0.007249478976009413\n",
      "Training Loss: 0.007043089504586533\n",
      "Validation Loss: 0.004716762378183978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.00706102977739647\n",
      "Training Loss: 0.007226586965844035\n",
      "Training Loss: 0.007021323348162696\n",
      "Validation Loss: 0.004691935923955079\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.00703847152646631\n",
      "Training Loss: 0.0072049437055829915\n",
      "Training Loss: 0.007000516634434462\n",
      "Validation Loss: 0.004669182685993905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007017079223878682\n",
      "Training Loss: 0.0071844382479321215\n",
      "Training Loss: 0.006980587493162602\n",
      "Validation Loss: 0.00464821551609889\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006996745228534564\n",
      "Training Loss: 0.007164985500276088\n",
      "Training Loss: 0.0069614716735668485\n",
      "Validation Loss: 0.004628793565357585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006977383826160803\n",
      "Training Loss: 0.007146513558691368\n",
      "Training Loss: 0.006943114824825898\n",
      "Validation Loss: 0.004610721779244251\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006958920883480459\n",
      "Training Loss: 0.00712895919336006\n",
      "Training Loss: 0.006925469675334171\n",
      "Validation Loss: 0.004593829564185122\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006941292606061324\n",
      "Training Loss: 0.007112265178002417\n",
      "Training Loss: 0.006908493946539238\n",
      "Validation Loss: 0.004577979571029042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0069244399026501926\n",
      "Training Loss: 0.007096374670509249\n",
      "Training Loss: 0.006892146851168945\n",
      "Validation Loss: 0.0045630498808871415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006908309656428174\n",
      "Training Loss: 0.007081235592486337\n",
      "Training Loss: 0.0068763917923206465\n",
      "Validation Loss: 0.004548934218342929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006892851236043498\n",
      "Training Loss: 0.0070667980320286\n",
      "Training Loss: 0.006861193364602514\n",
      "Validation Loss: 0.004535552749954415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006878018372226506\n",
      "Training Loss: 0.007053012469550595\n",
      "Training Loss: 0.006846519381506369\n",
      "Validation Loss: 0.004522821894752678\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006863768593175337\n",
      "Training Loss: 0.007039833358721808\n",
      "Training Loss: 0.006832340731634758\n",
      "Validation Loss: 0.0045106814189269885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006850060095312074\n",
      "Training Loss: 0.007027217411668971\n",
      "Training Loss: 0.006818627667380497\n",
      "Validation Loss: 0.004499066808114477\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006836854985449463\n",
      "Training Loss: 0.007015122511074878\n",
      "Training Loss: 0.0068053534539649265\n",
      "Validation Loss: 0.004487931276436142\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006824118070071563\n",
      "Training Loss: 0.0070035098737571385\n",
      "Training Loss: 0.0067924934579059485\n",
      "Validation Loss: 0.004477223603766453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006811814799439162\n",
      "Training Loss: 0.006992342579178512\n",
      "Training Loss: 0.006780022600432858\n",
      "Validation Loss: 0.004466902817798297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006799913959112018\n",
      "Training Loss: 0.006981585988542065\n",
      "Training Loss: 0.006767917342367582\n",
      "Validation Loss: 0.004456933710745044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006788386109983549\n",
      "Training Loss: 0.006971207998576574\n",
      "Training Loss: 0.006756155891343952\n",
      "Validation Loss: 0.004447283915223114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00677720362553373\n",
      "Training Loss: 0.006961177925113589\n",
      "Training Loss: 0.006744717538822442\n",
      "Validation Loss: 0.004437921393474334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006766340445028618\n",
      "Training Loss: 0.006951467647450044\n",
      "Training Loss: 0.006733582399901934\n",
      "Validation Loss: 0.00442881972314476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0067557730455882845\n",
      "Training Loss: 0.006942050948855467\n",
      "Training Loss: 0.00672273118281737\n",
      "Validation Loss: 0.004419958036460945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.00674547984963283\n",
      "Training Loss: 0.006932903766864911\n",
      "Training Loss: 0.006712146414793096\n",
      "Validation Loss: 0.004411311020284598\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006735440008342266\n",
      "Training Loss: 0.006924004085594788\n",
      "Training Loss: 0.00670181120862253\n",
      "Validation Loss: 0.004402863490918463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006725635792827233\n",
      "Training Loss: 0.006915331793716177\n",
      "Training Loss: 0.006691710785380564\n",
      "Validation Loss: 0.004394596417727514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006716051840921864\n",
      "Training Loss: 0.006906868803780526\n",
      "Training Loss: 0.006681830873130821\n",
      "Validation Loss: 0.004386498098355833\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.0067066722654271875\n",
      "Training Loss: 0.006898598514962942\n",
      "Training Loss: 0.00667215957713779\n",
      "Validation Loss: 0.0043785613261717845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006697484239703044\n",
      "Training Loss: 0.006890507264179177\n",
      "Training Loss: 0.006662683941540308\n",
      "Validation Loss: 0.00437076380175507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006688475606497377\n",
      "Training Loss: 0.00688258086447604\n",
      "Training Loss: 0.006653393976739608\n",
      "Validation Loss: 0.004363103008441878\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006679636019980535\n",
      "Training Loss: 0.006874808371649123\n",
      "Training Loss: 0.006644279775791802\n",
      "Validation Loss: 0.004355569264139938\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006670956321759149\n",
      "Training Loss: 0.006867179517284967\n",
      "Training Loss: 0.006635333088343032\n",
      "Validation Loss: 0.004348157874957313\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006662428412819281\n",
      "Training Loss: 0.006859684575465508\n",
      "Training Loss: 0.006626544644823298\n",
      "Validation Loss: 0.004340861472719757\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006654043949674815\n",
      "Training Loss: 0.006852315640426241\n",
      "Training Loss: 0.0066179083776660265\n",
      "Validation Loss: 0.0043336720934420234\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0066457977087702605\n",
      "Training Loss: 0.0068450656731147315\n",
      "Training Loss: 0.006609416822902859\n",
      "Validation Loss: 0.004326586633805562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.006637682414148003\n",
      "Training Loss: 0.0068379283917602155\n",
      "Training Loss: 0.006601064371643588\n",
      "Validation Loss: 0.00431960378511903\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006629694352159277\n",
      "Training Loss: 0.006830897213076241\n",
      "Training Loss: 0.006592845001141541\n",
      "Validation Loss: 0.00431271617005192\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0066218262887559835\n",
      "Training Loss: 0.006823967773816548\n",
      "Training Loss: 0.0065847535972716285\n",
      "Validation Loss: 0.0043059223177162615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006614075673278421\n",
      "Training Loss: 0.006817134434822947\n",
      "Training Loss: 0.006576785517972894\n",
      "Validation Loss: 0.004299218306616158\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006606436686124653\n",
      "Training Loss: 0.0068103929806966335\n",
      "Training Loss: 0.006568935849936679\n",
      "Validation Loss: 0.004292605319871369\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006598907137522474\n",
      "Training Loss: 0.006803740090690553\n",
      "Training Loss: 0.006561199880670756\n",
      "Validation Loss: 0.004286077919316718\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006591481564100832\n",
      "Training Loss: 0.006797171873040497\n",
      "Training Loss: 0.006553574581048451\n",
      "Validation Loss: 0.004279638285450428\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006584158076439053\n",
      "Training Loss: 0.006790685029118322\n",
      "Training Loss: 0.006546055566868745\n",
      "Validation Loss: 0.004273277345725618\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00657693296787329\n",
      "Training Loss: 0.006784276289399713\n",
      "Training Loss: 0.0065386393014341595\n",
      "Validation Loss: 0.004267002062824963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006569802704034373\n",
      "Training Loss: 0.0067779428279027346\n",
      "Training Loss: 0.00653132249310147\n",
      "Validation Loss: 0.004260802945629642\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0065627644630149\n",
      "Training Loss: 0.006771681593963876\n",
      "Training Loss: 0.006524101219256408\n",
      "Validation Loss: 0.004254684301506561\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006555815131869167\n",
      "Training Loss: 0.006765490308171138\n",
      "Training Loss: 0.006516972886165604\n",
      "Validation Loss: 0.004248644558194858\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.0065489521622657774\n",
      "Training Loss: 0.006759366628248244\n",
      "Training Loss: 0.00650993368239142\n",
      "Validation Loss: 0.004242679143450066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006542173182824626\n",
      "Training Loss: 0.006753307851613499\n",
      "Training Loss: 0.006502981854719109\n",
      "Validation Loss: 0.004236790932992243\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006535474824486301\n",
      "Training Loss: 0.006747311738436111\n",
      "Training Loss: 0.006496114264009521\n",
      "Validation Loss: 0.004230976964770868\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006528854308999144\n",
      "Training Loss: 0.006741376965073868\n",
      "Training Loss: 0.006489327164599672\n",
      "Validation Loss: 0.004225232319911586\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006522309750434943\n",
      "Training Loss: 0.006735500439535826\n",
      "Training Loss: 0.0064826189674204214\n",
      "Validation Loss: 0.004219558416326855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006515839095809497\n",
      "Training Loss: 0.006729680527932942\n",
      "Training Loss: 0.0064759873802540825\n",
      "Validation Loss: 0.004213952668026885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006509439201909118\n",
      "Training Loss: 0.00672391644504387\n",
      "Training Loss: 0.0064694292464992035\n",
      "Validation Loss: 0.0042084157774277195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006503108397591859\n",
      "Training Loss: 0.006718205167562701\n",
      "Training Loss: 0.006462942899088375\n",
      "Validation Loss: 0.004202948277554569\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.0064968440221855415\n",
      "Training Loss: 0.006712545676855371\n",
      "Training Loss: 0.006456525883404538\n",
      "Validation Loss: 0.004197545023242618\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006490644433652051\n",
      "Training Loss: 0.006706936337868683\n",
      "Training Loss: 0.006450175843783654\n",
      "Validation Loss: 0.004192205667861978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006484507340355776\n",
      "Training Loss: 0.0067013758688699455\n",
      "Training Loss: 0.006443891289527528\n",
      "Validation Loss: 0.004186926589457297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006478430400020443\n",
      "Training Loss: 0.006695861885673366\n",
      "Training Loss: 0.006437669420847669\n",
      "Validation Loss: 0.004181711202660011\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006472411861177534\n",
      "Training Loss: 0.006690393933095038\n",
      "Training Loss: 0.006431508437963203\n",
      "Validation Loss: 0.004176554518615764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006466451196465641\n",
      "Training Loss: 0.006684969726484269\n",
      "Training Loss: 0.006425407498609275\n",
      "Validation Loss: 0.004171457140983784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006460544567089528\n",
      "Training Loss: 0.006679589215782471\n",
      "Training Loss: 0.006419363649911247\n",
      "Validation Loss: 0.004166414895162949\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.00645469092996791\n",
      "Training Loss: 0.006674249868956395\n",
      "Training Loss: 0.00641337540699169\n",
      "Validation Loss: 0.0041614273619272986\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006448888755985535\n",
      "Training Loss: 0.0066689510073047135\n",
      "Training Loss: 0.0064074409782188015\n",
      "Validation Loss: 0.00415649332545614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.0064431361068272965\n",
      "Training Loss: 0.006663691414287314\n",
      "Training Loss: 0.006401559576042928\n",
      "Validation Loss: 0.004151611827612133\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006437431536032818\n",
      "Training Loss: 0.0066584685910493135\n",
      "Training Loss: 0.006395728478091769\n",
      "Validation Loss: 0.004146779356969081\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006431772850337438\n",
      "Training Loss: 0.0066532826103502885\n",
      "Training Loss: 0.006389947061543353\n",
      "Validation Loss: 0.004142000648622098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006426160251139663\n",
      "Training Loss: 0.006648132472182624\n",
      "Training Loss: 0.006384212475968525\n",
      "Validation Loss: 0.0041372664514772095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00642059009114746\n",
      "Training Loss: 0.006643016574671492\n",
      "Training Loss: 0.006378525759791955\n",
      "Validation Loss: 0.004132580281538742\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006415062621817924\n",
      "Training Loss: 0.00663793358602561\n",
      "Training Loss: 0.006372882695286535\n",
      "Validation Loss: 0.004127936330466961\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006409574798308313\n",
      "Training Loss: 0.00663288222218398\n",
      "Training Loss: 0.006367283457657322\n",
      "Validation Loss: 0.004123340601499161\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006404126142151654\n",
      "Training Loss: 0.006627861731685698\n",
      "Training Loss: 0.006361725135939196\n",
      "Validation Loss: 0.004118781549897924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006398713951930404\n",
      "Training Loss: 0.006622870442224666\n",
      "Training Loss: 0.0063562081597046925\n",
      "Validation Loss: 0.0041142642953213345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006393339108326472\n",
      "Training Loss: 0.0066179074160754685\n",
      "Training Loss: 0.006350729847326875\n",
      "Validation Loss: 0.004109785415289735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.0063879976200405504\n",
      "Training Loss: 0.006612971050199121\n",
      "Training Loss: 0.006345290853059851\n",
      "Validation Loss: 0.004105347741703932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006382690040627494\n",
      "Training Loss: 0.006608061220031231\n",
      "Training Loss: 0.0063398875010898335\n",
      "Validation Loss: 0.004100940423086286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006377413779264316\n",
      "Training Loss: 0.0066031761385966095\n",
      "Training Loss: 0.006334518278599717\n",
      "Validation Loss: 0.0040965689034881386\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006372167822555639\n",
      "Training Loss: 0.006598313779104501\n",
      "Training Loss: 0.006329183006309904\n",
      "Validation Loss: 0.004092231611302646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006366950368974358\n",
      "Training Loss: 0.006593473472166806\n",
      "Training Loss: 0.006323880774434656\n",
      "Validation Loss: 0.004087924955967354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006361760633881204\n",
      "Training Loss: 0.006588653867365793\n",
      "Training Loss: 0.006318609894369729\n",
      "Validation Loss: 0.004083648773977596\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006356597482808865\n",
      "Training Loss: 0.006583854331402108\n",
      "Training Loss: 0.006313368237460964\n",
      "Validation Loss: 0.004079399219049622\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006351458816207014\n",
      "Training Loss: 0.006579072774620727\n",
      "Training Loss: 0.006308155377046205\n",
      "Validation Loss: 0.0040751791211221826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.0063463440397754315\n",
      "Training Loss: 0.006574308935087174\n",
      "Training Loss: 0.006302969571552239\n",
      "Validation Loss: 0.004070982739873482\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006341250744881109\n",
      "Training Loss: 0.006569561136420816\n",
      "Training Loss: 0.00629780984367244\n",
      "Validation Loss: 0.004066811711658211\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006336178781930357\n",
      "Training Loss: 0.006564827354159206\n",
      "Training Loss: 0.006292675173608586\n",
      "Validation Loss: 0.004062662343232987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006331125886063092\n",
      "Training Loss: 0.00656010682345368\n",
      "Training Loss: 0.006287564733647741\n",
      "Validation Loss: 0.004058534881816863\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006326091485098004\n",
      "Training Loss: 0.00655539917293936\n",
      "Training Loss: 0.006282475067419\n",
      "Validation Loss: 0.004054424741430983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.0063210736395558345\n",
      "Training Loss: 0.006550701796077192\n",
      "Training Loss: 0.006277407163288444\n",
      "Validation Loss: 0.0040503344596677536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006316070635220967\n",
      "Training Loss: 0.006546013776678592\n",
      "Training Loss: 0.006272358965361491\n",
      "Validation Loss: 0.004046261536213831\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.0063110830180812625\n",
      "Training Loss: 0.006541333870263771\n",
      "Training Loss: 0.0062673289712984115\n",
      "Validation Loss: 0.004042205712077826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006306106947595254\n",
      "Training Loss: 0.006536661210702732\n",
      "Training Loss: 0.0062623158848145975\n",
      "Validation Loss: 0.004038163439208495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006301143591408618\n",
      "Training Loss: 0.0065319938550237565\n",
      "Training Loss: 0.0062573191273259\n",
      "Validation Loss: 0.004034136671813603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006296189614222385\n",
      "Training Loss: 0.00652733049239032\n",
      "Training Loss: 0.006252337300684303\n",
      "Validation Loss: 0.004030122139799838\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00629124523315113\n",
      "Training Loss: 0.006522670037811622\n",
      "Training Loss: 0.006247368448530324\n",
      "Validation Loss: 0.004026116371636143\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006286308731650933\n",
      "Training Loss: 0.006518011316657067\n",
      "Training Loss: 0.006242412571446039\n",
      "Validation Loss: 0.004022123577240645\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006281378630665131\n",
      "Training Loss: 0.006513353310292586\n",
      "Training Loss: 0.006237467723549344\n",
      "Validation Loss: 0.004018134839722908\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00627645421132911\n",
      "Training Loss: 0.006508694149088115\n",
      "Training Loss: 0.0062325329432496804\n",
      "Validation Loss: 0.0040141550995399025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.00627153372450266\n",
      "Training Loss: 0.006504033334786072\n",
      "Training Loss: 0.006227606172906235\n",
      "Validation Loss: 0.004010179366529239\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006266615391359665\n",
      "Training Loss: 0.006499368366785347\n",
      "Training Loss: 0.0062226876831846314\n",
      "Validation Loss: 0.004006210136426131\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006261698936577886\n",
      "Training Loss: 0.006494698879541829\n",
      "Training Loss: 0.006217774723190814\n",
      "Validation Loss: 0.004002240914171248\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006256783165154048\n",
      "Training Loss: 0.006490023713558913\n",
      "Training Loss: 0.006212867244612425\n",
      "Validation Loss: 0.003998278437464843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006251865951926447\n",
      "Training Loss: 0.006485340551007539\n",
      "Training Loss: 0.00620796381670516\n",
      "Validation Loss: 0.0039943174996643505\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006246947749750689\n",
      "Training Loss: 0.00648064985871315\n",
      "Training Loss: 0.006203063161228784\n",
      "Validation Loss: 0.003990358196256494\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006242025863612071\n",
      "Training Loss: 0.006475948848528788\n",
      "Training Loss: 0.006198163939989172\n",
      "Validation Loss: 0.0039863942120370735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006237099793506787\n",
      "Training Loss: 0.006471236780053005\n",
      "Training Loss: 0.0061932650586823\n",
      "Validation Loss: 0.003982430226509616\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006232168534188532\n",
      "Training Loss: 0.006466512611368671\n",
      "Training Loss: 0.006188365208799951\n",
      "Validation Loss: 0.003978466926393717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006227231278317049\n",
      "Training Loss: 0.006461774698691442\n",
      "Training Loss: 0.006183463555644266\n",
      "Validation Loss: 0.003974498381048148\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006222286412958055\n",
      "Training Loss: 0.006457022321410477\n",
      "Training Loss: 0.006178559261607006\n",
      "Validation Loss: 0.0039705263844461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.0062173331435769795\n",
      "Training Loss: 0.00645225424086675\n",
      "Training Loss: 0.006173650142736733\n",
      "Validation Loss: 0.003966544763959442\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.00621236996317748\n",
      "Training Loss: 0.006447469295235351\n",
      "Training Loss: 0.00616873558901716\n",
      "Validation Loss: 0.003962560567947293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006207396403769962\n",
      "Training Loss: 0.006442665694048628\n",
      "Training Loss: 0.006163814896717667\n",
      "Validation Loss: 0.003958563521122455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.0062024105485761535\n",
      "Training Loss: 0.006437842884333804\n",
      "Training Loss: 0.006158886058838107\n",
      "Validation Loss: 0.00395456240172341\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006197412261972204\n",
      "Training Loss: 0.006432999342214316\n",
      "Training Loss: 0.006153948808205314\n",
      "Validation Loss: 0.003950550343208225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.00619240032043308\n",
      "Training Loss: 0.00642813463578932\n",
      "Training Loss: 0.006149001098237932\n",
      "Validation Loss: 0.0039465288753265495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006187373677967116\n",
      "Training Loss: 0.0064232463634107265\n",
      "Training Loss: 0.0061440424551256\n",
      "Validation Loss: 0.003942495028179641\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006182331706513651\n",
      "Training Loss: 0.006418334536720067\n",
      "Training Loss: 0.006139071900979616\n",
      "Validation Loss: 0.003938450518566487\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006177273017819971\n",
      "Training Loss: 0.0064133971242699775\n",
      "Training Loss: 0.006134086993406527\n",
      "Validation Loss: 0.003934392871026452\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006172196541447192\n",
      "Training Loss: 0.006408433858305216\n",
      "Training Loss: 0.006129087642766535\n",
      "Validation Loss: 0.00393032089066744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006167101667961106\n",
      "Training Loss: 0.0064034431532491\n",
      "Training Loss: 0.0061240722372895105\n",
      "Validation Loss: 0.003926233181813627\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006161987589439377\n",
      "Training Loss: 0.006398423921782523\n",
      "Training Loss: 0.006119040116900578\n",
      "Validation Loss: 0.0039221293762525065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.0061568535235710445\n",
      "Training Loss: 0.006393375358311459\n",
      "Training Loss: 0.006113989405566827\n",
      "Validation Loss: 0.003918009001128585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006151697776513174\n",
      "Training Loss: 0.0063882953010033815\n",
      "Training Loss: 0.0061089206958422435\n",
      "Validation Loss: 0.003913873532561983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006146520396578125\n",
      "Training Loss: 0.006383184334263206\n",
      "Training Loss: 0.006103830367792397\n",
      "Validation Loss: 0.003909717077190538\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006141320670139976\n",
      "Training Loss: 0.006378040282288566\n",
      "Training Loss: 0.0060987191792810334\n",
      "Validation Loss: 0.0039055392668017426\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006136096339323558\n",
      "Training Loss: 0.006372862153220922\n",
      "Training Loss: 0.006093584785121493\n",
      "Validation Loss: 0.0039013429960821954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006130848230677657\n",
      "Training Loss: 0.006367649579187855\n",
      "Training Loss: 0.006088427028735168\n",
      "Validation Loss: 0.003897125763410514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.00612557573535014\n",
      "Training Loss: 0.00636240145075135\n",
      "Training Loss: 0.006083244978217408\n",
      "Validation Loss: 0.0038928837100765928\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006120276807341725\n",
      "Training Loss: 0.006357116318540647\n",
      "Training Loss: 0.006078036761027761\n",
      "Validation Loss: 0.003888621091125847\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006114951632334851\n",
      "Training Loss: 0.0063517930789384995\n",
      "Training Loss: 0.00607280103082303\n",
      "Validation Loss: 0.0038843323892568437\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006109598575858399\n",
      "Training Loss: 0.006346431341953575\n",
      "Training Loss: 0.006067538339993917\n",
      "Validation Loss: 0.0038800205410133752\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006104218597756699\n",
      "Training Loss: 0.006341030653566122\n",
      "Training Loss: 0.006062245590728707\n",
      "Validation Loss: 0.003875681663486646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006098809329560026\n",
      "Training Loss: 0.006335589721566066\n",
      "Training Loss: 0.006056923455325887\n",
      "Validation Loss: 0.0038713130693139738\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006093371367896907\n",
      "Training Loss: 0.006330107168760151\n",
      "Training Loss: 0.006051569461124018\n",
      "Validation Loss: 0.0038669198997360603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006087904214509763\n",
      "Training Loss: 0.00632458389387466\n",
      "Training Loss: 0.00604618433280848\n",
      "Validation Loss: 0.0038624984392728865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006082407613284886\n",
      "Training Loss: 0.006319017672212795\n",
      "Training Loss: 0.006040766388177871\n",
      "Validation Loss: 0.003858046497615954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006076879784232005\n",
      "Training Loss: 0.006313408671412617\n",
      "Training Loss: 0.006035314584732987\n",
      "Validation Loss: 0.0038535626457344868\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006071321066701785\n",
      "Training Loss: 0.006307756460737437\n",
      "Training Loss: 0.006029828299651854\n",
      "Validation Loss: 0.003849047961451239\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.0060657314647687595\n",
      "Training Loss: 0.006302059919107705\n",
      "Training Loss: 0.006024306909530423\n",
      "Validation Loss: 0.0038445029110815167\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0060601112258154895\n",
      "Training Loss: 0.006296319706598297\n",
      "Training Loss: 0.0060187503736233335\n",
      "Validation Loss: 0.0038399270152296413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006054459103033878\n",
      "Training Loss: 0.006290535269072279\n",
      "Training Loss: 0.0060131568374345076\n",
      "Validation Loss: 0.0038353164223797116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006048775424133055\n",
      "Training Loss: 0.006284706627484411\n",
      "Training Loss: 0.006007526306784711\n",
      "Validation Loss: 0.0038306717034900205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.00604306063905824\n",
      "Training Loss: 0.006278833858668804\n",
      "Training Loss: 0.006001858995296061\n",
      "Validation Loss: 0.003825994972348883\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006037314843852073\n",
      "Training Loss: 0.006272916637826711\n",
      "Training Loss: 0.005996154053718783\n",
      "Validation Loss: 0.0038212852269997087\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006031537334783934\n",
      "Training Loss: 0.006266956605250016\n",
      "Training Loss: 0.005990412119426764\n",
      "Validation Loss: 0.0038165401685669015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006025729791726917\n",
      "Training Loss: 0.006260953193996102\n",
      "Training Loss: 0.005984632266918197\n",
      "Validation Loss: 0.0038117570102769423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006019891983014531\n",
      "Training Loss: 0.006254907954717055\n",
      "Training Loss: 0.0059788149624364455\n",
      "Validation Loss: 0.0038069426893057785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.0060140242014313115\n",
      "Training Loss: 0.006248821469489485\n",
      "Training Loss: 0.005972960686776787\n",
      "Validation Loss: 0.0038020945483755865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006008127729292028\n",
      "Training Loss: 0.00624269523890689\n",
      "Training Loss: 0.005967068356112577\n",
      "Validation Loss: 0.0037972117791193003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006002202828531153\n",
      "Training Loss: 0.006236530069727451\n",
      "Training Loss: 0.005961142133455724\n",
      "Validation Loss: 0.0037922959093245143\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005996250661555677\n",
      "Training Loss: 0.006230328069068492\n",
      "Training Loss: 0.005955178862204775\n",
      "Validation Loss: 0.0037873443261866742\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005990272474009544\n",
      "Training Loss: 0.006224090302130208\n",
      "Training Loss: 0.005949181750766002\n",
      "Validation Loss: 0.0037823634953342714\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005984269326436333\n",
      "Training Loss: 0.006217819582670927\n",
      "Training Loss: 0.005943149714148603\n",
      "Validation Loss: 0.003777349174802265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005978242149576545\n",
      "Training Loss: 0.006211517482297495\n",
      "Training Loss: 0.0059370866289827975\n",
      "Validation Loss: 0.0037723045895566767\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005972193339257501\n",
      "Training Loss: 0.00620518607320264\n",
      "Training Loss: 0.005930991893983446\n",
      "Validation Loss: 0.0037672322621474857\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005966123794787563\n",
      "Training Loss: 0.006198828429915011\n",
      "Training Loss: 0.0059248674690024926\n",
      "Validation Loss: 0.003762130044777407\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005960034280433319\n",
      "Training Loss: 0.006192447553621605\n",
      "Training Loss: 0.0059187154954997824\n",
      "Validation Loss: 0.003757001940694669\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005953927842201665\n",
      "Training Loss: 0.006186045289505273\n",
      "Training Loss: 0.005912536677788012\n",
      "Validation Loss: 0.0037518468766546483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005947805106989108\n",
      "Training Loss: 0.006179624550277367\n",
      "Training Loss: 0.005906333702150732\n",
      "Validation Loss: 0.0037466665197509142\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005941668845480308\n",
      "Training Loss: 0.006173189990222454\n",
      "Training Loss: 0.005900109453359619\n",
      "Validation Loss: 0.0037414626809611413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005935520374332554\n",
      "Training Loss: 0.006166743249632418\n",
      "Training Loss: 0.005893863732344471\n",
      "Validation Loss: 0.0037362358161363374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.0059293615666683765\n",
      "Training Loss: 0.006160287861712277\n",
      "Training Loss: 0.005887600181158632\n",
      "Validation Loss: 0.0037309892738520514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005923193546477705\n",
      "Training Loss: 0.006153826972004026\n",
      "Training Loss: 0.0058813206979539245\n",
      "Validation Loss: 0.0037257246786906394\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005917019721819088\n",
      "Training Loss: 0.006147364214994014\n",
      "Training Loss: 0.00587502779555507\n",
      "Validation Loss: 0.0037204445446998385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005910841915756464\n",
      "Training Loss: 0.0061409023229498415\n",
      "Training Loss: 0.005868723631137982\n",
      "Validation Loss: 0.003715150671084987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005904660894302651\n",
      "Training Loss: 0.006134445066563785\n",
      "Training Loss: 0.0058624105644412335\n",
      "Validation Loss: 0.003709840641901148\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005898478587623685\n",
      "Training Loss: 0.006127994969720021\n",
      "Training Loss: 0.005856089896988124\n",
      "Validation Loss: 0.0037045221372894693\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.0058922982856165615\n",
      "Training Loss: 0.006121555167483166\n",
      "Training Loss: 0.005849765233579092\n",
      "Validation Loss: 0.0036991921598824224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005886120485374704\n",
      "Training Loss: 0.006115128825185821\n",
      "Training Loss: 0.005843438475276344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [13:56<20:55, 209.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.003693854137391815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.12367158506065606\n",
      "Training Loss: 0.09538400301709771\n",
      "Training Loss: 0.07874385301023722\n",
      "Validation Loss: 0.07384046723835924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.07065828818827867\n",
      "Training Loss: 0.06875015672296286\n",
      "Training Loss: 0.06637077471241355\n",
      "Validation Loss: 0.06464757362192267\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06262602394446731\n",
      "Training Loss: 0.061051667500287295\n",
      "Training Loss: 0.05881429789587855\n",
      "Validation Loss: 0.0569053040712737\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.05516469279304147\n",
      "Training Loss: 0.0534773659799248\n",
      "Training Loss: 0.05116973373107612\n",
      "Validation Loss: 0.048848481179120835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.047377598164603116\n",
      "Training Loss: 0.045446269894018766\n",
      "Training Loss: 0.04304128714837134\n",
      "Validation Loss: 0.04021775782150164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.039072943087667227\n",
      "Training Loss: 0.036825409210287034\n",
      "Training Loss: 0.03436220138333738\n",
      "Validation Loss: 0.031194097812423546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.030539862564764916\n",
      "Training Loss: 0.028303924654610454\n",
      "Training Loss: 0.02625129815656692\n",
      "Validation Loss: 0.02343087936908509\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.023432753002271055\n",
      "Training Loss: 0.02170416892040521\n",
      "Training Loss: 0.020333325173705816\n",
      "Validation Loss: 0.018070726786227374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.018581681200303138\n",
      "Training Loss: 0.01730650259181857\n",
      "Training Loss: 0.016333843709435313\n",
      "Validation Loss: 0.014284831445580452\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.015071969302371145\n",
      "Training Loss: 0.014019766924902796\n",
      "Training Loss: 0.013133741514757276\n",
      "Validation Loss: 0.011128882903689415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.012148625829722733\n",
      "Training Loss: 0.011538602501386776\n",
      "Training Loss: 0.010953748897882178\n",
      "Validation Loss: 0.009313876823040792\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.010615390512393788\n",
      "Training Loss: 0.010456978066358716\n",
      "Training Loss: 0.010067631481215358\n",
      "Validation Loss: 0.008521618198582463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.009954168933909387\n",
      "Training Loss: 0.009926995321875439\n",
      "Training Loss: 0.009574505363125353\n",
      "Validation Loss: 0.007999020432425516\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.009515410338062793\n",
      "Training Loss: 0.009539879478979855\n",
      "Training Loss: 0.009196305561345071\n",
      "Validation Loss: 0.0075728956078901215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.009160472401417792\n",
      "Training Loss: 0.009213805423351006\n",
      "Training Loss: 0.008873675207141787\n",
      "Validation Loss: 0.007205152572812826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.008857294105691835\n",
      "Training Loss: 0.008932044411776587\n",
      "Training Loss: 0.008598365080542862\n",
      "Validation Loss: 0.006896212512595851\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.008604954327456654\n",
      "Training Loss: 0.008697308109840379\n",
      "Training Loss: 0.008374762458261102\n",
      "Validation Loss: 0.006649735432086784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.008405269996728748\n",
      "Training Loss: 0.008510487830499187\n",
      "Training Loss: 0.008201189278624952\n",
      "Validation Loss: 0.006458455817041437\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.008251888239756226\n",
      "Training Loss: 0.008364686383865774\n",
      "Training Loss: 0.008067952847341076\n",
      "Validation Loss: 0.006308305605701851\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.008133458775701001\n",
      "Training Loss: 0.008249686029739679\n",
      "Training Loss: 0.007963680839166045\n",
      "Validation Loss: 0.006186337184077234\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008039460653671995\n",
      "Training Loss: 0.00815660528605804\n",
      "Training Loss: 0.007879476230591535\n",
      "Validation Loss: 0.006083576221887567\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.007962360394885763\n",
      "Training Loss: 0.00807915988145396\n",
      "Training Loss: 0.0078093899064697325\n",
      "Validation Loss: 0.005994370787959086\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.007897228287765757\n",
      "Training Loss: 0.008013153899228201\n",
      "Training Loss: 0.007749574519693852\n",
      "Validation Loss: 0.0059151425696072285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.007840858107665554\n",
      "Training Loss: 0.007955763010540977\n",
      "Training Loss: 0.007697493028827012\n",
      "Validation Loss: 0.005843532197125089\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.007791112626437098\n",
      "Training Loss: 0.007905037387972698\n",
      "Training Loss: 0.007651412660488859\n",
      "Validation Loss: 0.005777911770973731\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007746518936473876\n",
      "Training Loss: 0.007859588677529245\n",
      "Training Loss: 0.007610109758097679\n",
      "Validation Loss: 0.00571711903328097\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.007706030557164923\n",
      "Training Loss: 0.007818407777231187\n",
      "Training Loss: 0.007572696056449786\n",
      "Validation Loss: 0.005660305597578709\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007668888781918213\n",
      "Training Loss: 0.007780749492812902\n",
      "Training Loss: 0.007538513394538313\n",
      "Validation Loss: 0.005606841926431555\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.007634531146613881\n",
      "Training Loss: 0.007746051219291985\n",
      "Training Loss: 0.0075070660165511074\n",
      "Validation Loss: 0.00555626318523179\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.0076025361893698575\n",
      "Training Loss: 0.007713890820741654\n",
      "Training Loss: 0.007477976643713191\n",
      "Validation Loss: 0.005508226273173278\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007572584082372487\n",
      "Training Loss: 0.007683941498398781\n",
      "Training Loss: 0.007450950428610667\n",
      "Validation Loss: 0.005462476188379727\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007544429952977225\n",
      "Training Loss: 0.007655952591449022\n",
      "Training Loss: 0.007425757714081556\n",
      "Validation Loss: 0.005418823121555066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007517880906816572\n",
      "Training Loss: 0.007629723255522549\n",
      "Training Loss: 0.00740221134503372\n",
      "Validation Loss: 0.005377122832807514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007492782791377977\n",
      "Training Loss: 0.007605087886331603\n",
      "Training Loss: 0.007380155116552487\n",
      "Validation Loss: 0.0053372598355217425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007469007413601503\n",
      "Training Loss: 0.007581905910046771\n",
      "Training Loss: 0.007359455010155216\n",
      "Validation Loss: 0.005299139429103541\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007446442088112235\n",
      "Training Loss: 0.0075600542291067545\n",
      "Training Loss: 0.007339991243788972\n",
      "Validation Loss: 0.0052626718965809\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007424986232072115\n",
      "Training Loss: 0.007539416637737304\n",
      "Training Loss: 0.0073216512077488\n",
      "Validation Loss: 0.0052277783371424405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007404546336038038\n",
      "Training Loss: 0.0075198874273337424\n",
      "Training Loss: 0.007304329070029781\n",
      "Validation Loss: 0.005194379605373807\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007385031518060714\n",
      "Training Loss: 0.007501363750780001\n",
      "Training Loss: 0.007287925126729533\n",
      "Validation Loss: 0.005162404960000448\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007366354028927162\n",
      "Training Loss: 0.007483747721416876\n",
      "Training Loss: 0.0072723397356458\n",
      "Validation Loss: 0.00513177692989578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007348427614197135\n",
      "Training Loss: 0.007466943254694342\n",
      "Training Loss: 0.0072574756341055036\n",
      "Validation Loss: 0.005102430913058434\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007331165464129299\n",
      "Training Loss: 0.007450856702635064\n",
      "Training Loss: 0.007243238625815138\n",
      "Validation Loss: 0.005074300612924684\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007314480530330911\n",
      "Training Loss: 0.0074353964999318125\n",
      "Training Loss: 0.007229533677455038\n",
      "Validation Loss: 0.005047325689732777\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007298285380238667\n",
      "Training Loss: 0.007420471819350496\n",
      "Training Loss: 0.007216267548501492\n",
      "Validation Loss: 0.005021448559755606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007282493211096153\n",
      "Training Loss: 0.007405995848821476\n",
      "Training Loss: 0.007203351404750719\n",
      "Validation Loss: 0.0049966204874845365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007267019853461534\n",
      "Training Loss: 0.0073918860289268196\n",
      "Training Loss: 0.007190698430640623\n",
      "Validation Loss: 0.0049727986857106676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007251789001747966\n",
      "Training Loss: 0.0073780680762138215\n",
      "Training Loss: 0.007178233726881445\n",
      "Validation Loss: 0.004949949115378719\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0072367382352240385\n",
      "Training Loss: 0.007364481266122312\n",
      "Training Loss: 0.007165897169616073\n",
      "Validation Loss: 0.004928050645145724\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007221829069312662\n",
      "Training Loss: 0.007351087618153542\n",
      "Training Loss: 0.0071536533313337715\n",
      "Validation Loss: 0.004907085943517139\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007207057329360396\n",
      "Training Loss: 0.007337875723605975\n",
      "Training Loss: 0.007141497113043443\n",
      "Validation Loss: 0.00488705518446193\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007192459496436641\n",
      "Training Loss: 0.0073248679109383374\n",
      "Training Loss: 0.0071294601331464945\n",
      "Validation Loss: 0.004867954656424189\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007178110225358978\n",
      "Training Loss: 0.007312115377280861\n",
      "Training Loss: 0.007117603475926444\n",
      "Validation Loss: 0.004849770466085482\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007164107178105041\n",
      "Training Loss: 0.007299684484023601\n",
      "Training Loss: 0.00710600468039047\n",
      "Validation Loss: 0.004832475847078048\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007150549333309755\n",
      "Training Loss: 0.007287642643786967\n",
      "Training Loss: 0.007094741846667602\n",
      "Validation Loss: 0.004816019075681989\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007137508703162893\n",
      "Training Loss: 0.0072760402609128505\n",
      "Training Loss: 0.007083869319176301\n",
      "Validation Loss: 0.0048003260535449625\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.00712502192822285\n",
      "Training Loss: 0.0072649019432719794\n",
      "Training Loss: 0.007073412896716036\n",
      "Validation Loss: 0.004785307774613245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0071130850468762215\n",
      "Training Loss: 0.007254226668737829\n",
      "Training Loss: 0.007063367094378919\n",
      "Validation Loss: 0.004770877833211313\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007101667084498331\n",
      "Training Loss: 0.007243993267184124\n",
      "Training Loss: 0.007053703271667473\n",
      "Validation Loss: 0.004756959272925271\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007090720125706867\n",
      "Training Loss: 0.007234165971167386\n",
      "Training Loss: 0.007044380410225131\n",
      "Validation Loss: 0.004743489742231963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007080189709085971\n",
      "Training Loss: 0.007224703440442681\n",
      "Training Loss: 0.007035351684899069\n",
      "Validation Loss: 0.004730424577542947\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007070023503620178\n",
      "Training Loss: 0.007215562895871699\n",
      "Training Loss: 0.007026571758324281\n",
      "Validation Loss: 0.004717728066169186\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007060174215584993\n",
      "Training Loss: 0.007206703347619623\n",
      "Training Loss: 0.007018001624965109\n",
      "Validation Loss: 0.004705378856363423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.0070506004919298\n",
      "Training Loss: 0.00719808700028807\n",
      "Training Loss: 0.007009607694344595\n",
      "Validation Loss: 0.0046933574093181345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007041267552413046\n",
      "Training Loss: 0.007189681607997045\n",
      "Training Loss: 0.007001358462148346\n",
      "Validation Loss: 0.004681653040330439\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007032146560959518\n",
      "Training Loss: 0.007181458700215444\n",
      "Training Loss: 0.006993233422981575\n",
      "Validation Loss: 0.004670253716764993\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007023213716456667\n",
      "Training Loss: 0.007173394070705399\n",
      "Training Loss: 0.006985210702987387\n",
      "Validation Loss: 0.004659144971728995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007014446738176048\n",
      "Training Loss: 0.007165466749574989\n",
      "Training Loss: 0.006977274376549758\n",
      "Validation Loss: 0.004648319594012678\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0070058287051506345\n",
      "Training Loss: 0.007157657912466675\n",
      "Training Loss: 0.006969409364974127\n",
      "Validation Loss: 0.004637764130452166\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006997345522977412\n",
      "Training Loss: 0.007149953512707725\n",
      "Training Loss: 0.006961603770614602\n",
      "Validation Loss: 0.00462746439669072\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006988980955211446\n",
      "Training Loss: 0.007142338852863759\n",
      "Training Loss: 0.006953848493285477\n",
      "Validation Loss: 0.004617408199204404\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0069807243719697\n",
      "Training Loss: 0.00713480245671235\n",
      "Training Loss: 0.006946131543954835\n",
      "Validation Loss: 0.004607586076305238\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.00697256434825249\n",
      "Training Loss: 0.007127333416137844\n",
      "Training Loss: 0.006938446428976021\n",
      "Validation Loss: 0.004597982696487746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006964490126119926\n",
      "Training Loss: 0.007119922147830948\n",
      "Training Loss: 0.006930783306597732\n",
      "Validation Loss: 0.0045885867072959965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.0069564924982842054\n",
      "Training Loss: 0.007112560094101355\n",
      "Training Loss: 0.0069231347960885615\n",
      "Validation Loss: 0.004579384156549873\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006948561914032325\n",
      "Training Loss: 0.007105238392250612\n",
      "Training Loss: 0.006915494375862181\n",
      "Validation Loss: 0.0045703647271049825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006940691416384652\n",
      "Training Loss: 0.007097950432216749\n",
      "Training Loss: 0.006907857061014511\n",
      "Validation Loss: 0.004561518073040113\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006932871625758707\n",
      "Training Loss: 0.007090689409524203\n",
      "Training Loss: 0.006900215176865458\n",
      "Validation Loss: 0.004552833382772763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0069250949763227255\n",
      "Training Loss: 0.007083448361372575\n",
      "Training Loss: 0.006892563000437803\n",
      "Validation Loss: 0.004544298165854527\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00691735471598804\n",
      "Training Loss: 0.00707622189889662\n",
      "Training Loss: 0.006884895278490149\n",
      "Validation Loss: 0.004535906557046044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006909644082188606\n",
      "Training Loss: 0.007069004302611575\n",
      "Training Loss: 0.006877208133228123\n",
      "Validation Loss: 0.004527643507377904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006901955757057294\n",
      "Training Loss: 0.007061790123116225\n",
      "Training Loss: 0.006869495936552994\n",
      "Validation Loss: 0.004519504183652193\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006894283628789708\n",
      "Training Loss: 0.007054573934292421\n",
      "Training Loss: 0.0068617543525761\n",
      "Validation Loss: 0.00451147860162002\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006886622098973021\n",
      "Training Loss: 0.007047351950313896\n",
      "Training Loss: 0.006853978710714728\n",
      "Validation Loss: 0.004503555585410488\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006878963725175709\n",
      "Training Loss: 0.007040119060548022\n",
      "Training Loss: 0.00684616469021421\n",
      "Validation Loss: 0.004495728483653805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006871303846128285\n",
      "Training Loss: 0.007032870559487492\n",
      "Training Loss: 0.006838308891747147\n",
      "Validation Loss: 0.00448798981437648\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.00686363648972474\n",
      "Training Loss: 0.007025601711357013\n",
      "Training Loss: 0.006830407686647959\n",
      "Validation Loss: 0.004480331522030651\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006855955872451886\n",
      "Training Loss: 0.007018310570856556\n",
      "Training Loss: 0.0068224583129631354\n",
      "Validation Loss: 0.004472745437269214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006848257194506004\n",
      "Training Loss: 0.0070109920937102286\n",
      "Training Loss: 0.006814455767744221\n",
      "Validation Loss: 0.004465223518278701\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006840533927315846\n",
      "Training Loss: 0.007003643077332526\n",
      "Training Loss: 0.0068063985591288656\n",
      "Validation Loss: 0.004457761331466602\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006832783811260015\n",
      "Training Loss: 0.006996260749874636\n",
      "Training Loss: 0.006798283134121448\n",
      "Validation Loss: 0.0044503514694175525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006824999777600169\n",
      "Training Loss: 0.006988840826088563\n",
      "Training Loss: 0.006790106421685777\n",
      "Validation Loss: 0.004442982738602153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0068171764153521506\n",
      "Training Loss: 0.006981379895005375\n",
      "Training Loss: 0.00678186475939583\n",
      "Validation Loss: 0.004435654288796143\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.0068093098560348155\n",
      "Training Loss: 0.006973875428084284\n",
      "Training Loss: 0.006773557260166854\n",
      "Validation Loss: 0.004428355312341133\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006801394477952272\n",
      "Training Loss: 0.006966323395026848\n",
      "Training Loss: 0.006765179261565208\n",
      "Validation Loss: 0.004421080540462784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006793426013318822\n",
      "Training Loss: 0.006958721614209935\n",
      "Training Loss: 0.006756729898625053\n",
      "Validation Loss: 0.00441382206738351\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006785400690278038\n",
      "Training Loss: 0.0069510683102998886\n",
      "Training Loss: 0.006748206415795721\n",
      "Validation Loss: 0.004406576600773365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006777312961639836\n",
      "Training Loss: 0.006943359167780727\n",
      "Training Loss: 0.006739606565097347\n",
      "Validation Loss: 0.004399338254464392\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006769159277901053\n",
      "Training Loss: 0.00693559211329557\n",
      "Training Loss: 0.006730927531607449\n",
      "Validation Loss: 0.004392098908160886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006760934696067125\n",
      "Training Loss: 0.00692776472773403\n",
      "Training Loss: 0.006722167859552428\n",
      "Validation Loss: 0.0043848538444262375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006752634987933561\n",
      "Training Loss: 0.006919873737497255\n",
      "Training Loss: 0.006713325278833508\n",
      "Validation Loss: 0.004377595485800229\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00674425512785092\n",
      "Training Loss: 0.006911918054101989\n",
      "Training Loss: 0.00670439810550306\n",
      "Validation Loss: 0.004370321243676995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006735792174004019\n",
      "Training Loss: 0.006903893640264868\n",
      "Training Loss: 0.006695382561301813\n",
      "Validation Loss: 0.004363024178918535\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006727241303306073\n",
      "Training Loss: 0.006895798307377845\n",
      "Training Loss: 0.006686278929118998\n",
      "Validation Loss: 0.004355697120864237\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0067185988917481156\n",
      "Training Loss: 0.006887630845885724\n",
      "Training Loss: 0.0066770837944932285\n",
      "Validation Loss: 0.004348337496604687\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006709860756527633\n",
      "Training Loss: 0.006879387536318973\n",
      "Training Loss: 0.006667796377441846\n",
      "Validation Loss: 0.004340939368304451\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006701022249180824\n",
      "Training Loss: 0.006871066751191392\n",
      "Training Loss: 0.006658414234407246\n",
      "Validation Loss: 0.004333495430575077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006692081151995808\n",
      "Training Loss: 0.006862666364759207\n",
      "Training Loss: 0.00664893546141684\n",
      "Validation Loss: 0.004326003457791057\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006683033376466483\n",
      "Training Loss: 0.006854183954419568\n",
      "Training Loss: 0.006639359791297466\n",
      "Validation Loss: 0.0043184571811835165\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006673875460401177\n",
      "Training Loss: 0.006845619056839496\n",
      "Training Loss: 0.006629685863154009\n",
      "Validation Loss: 0.004310852914049259\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006664606170961634\n",
      "Training Loss: 0.006836969687137753\n",
      "Training Loss: 0.0066199126129504295\n",
      "Validation Loss: 0.004303186706115565\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006655220267130062\n",
      "Training Loss: 0.006828233691630885\n",
      "Training Loss: 0.006610038583748974\n",
      "Validation Loss: 0.004295451859577319\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006645716376369819\n",
      "Training Loss: 0.006819410050520673\n",
      "Training Loss: 0.006600062947836704\n",
      "Validation Loss: 0.004287647406486899\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006636093382257968\n",
      "Training Loss: 0.006810498101403936\n",
      "Training Loss: 0.006589986959006637\n",
      "Validation Loss: 0.0042797672690488816\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0066263497492764145\n",
      "Training Loss: 0.006801497088745236\n",
      "Training Loss: 0.006579810219118371\n",
      "Validation Loss: 0.004271809428307657\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006616483915131539\n",
      "Training Loss: 0.00679240727215074\n",
      "Training Loss: 0.006569533485453576\n",
      "Validation Loss: 0.00426377161351043\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006606495942687616\n",
      "Training Loss: 0.006783229016000405\n",
      "Training Loss: 0.006559157411102206\n",
      "Validation Loss: 0.004255648426387166\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006596385834272951\n",
      "Training Loss: 0.006773961768485606\n",
      "Training Loss: 0.006548684170120396\n",
      "Validation Loss: 0.004247440092574303\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006586154942051507\n",
      "Training Loss: 0.006764607976656407\n",
      "Training Loss: 0.0065381160500692205\n",
      "Validation Loss: 0.00423913978869514\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006575804328313097\n",
      "Training Loss: 0.006755169244715944\n",
      "Training Loss: 0.006527456448529847\n",
      "Validation Loss: 0.004230749193615583\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006565337133361026\n",
      "Training Loss: 0.006745647254865617\n",
      "Training Loss: 0.006516707402770407\n",
      "Validation Loss: 0.0042222673720889465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006554756689001806\n",
      "Training Loss: 0.0067360457696486265\n",
      "Training Loss: 0.006505875868606381\n",
      "Validation Loss: 0.0042136896518059075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006544068533694372\n",
      "Training Loss: 0.006726370372343808\n",
      "Training Loss: 0.006494967170292512\n",
      "Validation Loss: 0.004205020697881583\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006533277098205872\n",
      "Training Loss: 0.0067166225693654266\n",
      "Training Loss: 0.006483986016828567\n",
      "Validation Loss: 0.00419625619052271\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006522388648591004\n",
      "Training Loss: 0.006706808671588078\n",
      "Training Loss: 0.006472939667291939\n",
      "Validation Loss: 0.004187398647701138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006511411330429837\n",
      "Training Loss: 0.006696934944484383\n",
      "Training Loss: 0.006461836148519069\n",
      "Validation Loss: 0.004178447213960455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006500352199655026\n",
      "Training Loss: 0.006687008199514821\n",
      "Training Loss: 0.00645068402402103\n",
      "Validation Loss: 0.004169404593013813\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006489222798845731\n",
      "Training Loss: 0.0066770348418504\n",
      "Training Loss: 0.00643949338409584\n",
      "Validation Loss: 0.004160271041236525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.00647803092142567\n",
      "Training Loss: 0.006667023104382679\n",
      "Training Loss: 0.006428272604825907\n",
      "Validation Loss: 0.004151048873200636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006466788196703419\n",
      "Training Loss: 0.006656980175757781\n",
      "Training Loss: 0.006417032341705636\n",
      "Validation Loss: 0.0041417417448704665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006455506178899668\n",
      "Training Loss: 0.006646915372693911\n",
      "Training Loss: 0.006405784237431362\n",
      "Validation Loss: 0.004132352371731501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006444197174860164\n",
      "Training Loss: 0.006636837586993352\n",
      "Training Loss: 0.006394538364256732\n",
      "Validation Loss: 0.0041228823397789945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00643287286919076\n",
      "Training Loss: 0.006626755204051733\n",
      "Training Loss: 0.006383307206560857\n",
      "Validation Loss: 0.0041133400197978015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006421545540215448\n",
      "Training Loss: 0.006616677865386009\n",
      "Training Loss: 0.006372100639855489\n",
      "Validation Loss: 0.004103725556980065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006410228195600212\n",
      "Training Loss: 0.006606613057665527\n",
      "Training Loss: 0.006360929511720315\n",
      "Validation Loss: 0.004094044990534109\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006398932211450301\n",
      "Training Loss: 0.006596570601686835\n",
      "Training Loss: 0.006349805425852537\n",
      "Validation Loss: 0.004084301398925776\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006387669398682192\n",
      "Training Loss: 0.006586559034185484\n",
      "Training Loss: 0.00633873802551534\n",
      "Validation Loss: 0.004074503546659155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006376452188123949\n",
      "Training Loss: 0.006576584845315665\n",
      "Training Loss: 0.006327736674575135\n",
      "Validation Loss: 0.004064655377466776\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006365291440743021\n",
      "Training Loss: 0.006566656723152846\n",
      "Training Loss: 0.006316811959259212\n",
      "Validation Loss: 0.004054762314471385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006354196394677274\n",
      "Training Loss: 0.006556780942482874\n",
      "Training Loss: 0.006305969483219087\n",
      "Validation Loss: 0.004044828929917447\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00634317648364231\n",
      "Training Loss: 0.006546963168075308\n",
      "Training Loss: 0.006295219666790217\n",
      "Validation Loss: 0.004034866850948736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.0063322403788333755\n",
      "Training Loss: 0.0065372104605194185\n",
      "Training Loss: 0.00628456624574028\n",
      "Validation Loss: 0.0040248776740248035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006321395524428226\n",
      "Training Loss: 0.00652752704336308\n",
      "Training Loss: 0.006274016436655074\n",
      "Validation Loss: 0.004014869621468268\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006310647921054624\n",
      "Training Loss: 0.0065179171517957\n",
      "Training Loss: 0.0062635760713601486\n",
      "Validation Loss: 0.004004850915928758\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00630000427714549\n",
      "Training Loss: 0.006508385088527575\n",
      "Training Loss: 0.0062532473535975444\n",
      "Validation Loss: 0.00399482523953134\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006289469604962505\n",
      "Training Loss: 0.006498933613765984\n",
      "Training Loss: 0.0062430361221777276\n",
      "Validation Loss: 0.0039848045889403294\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006279047997086309\n",
      "Training Loss: 0.006489566687960178\n",
      "Training Loss: 0.006232944076182321\n",
      "Validation Loss: 0.0039747908075073255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006268741831881926\n",
      "Training Loss: 0.006480286009609699\n",
      "Training Loss: 0.0062229730340186504\n",
      "Validation Loss: 0.003964791474654601\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006258554047672078\n",
      "Training Loss: 0.006471093506552279\n",
      "Training Loss: 0.006213124812929891\n",
      "Validation Loss: 0.003954814215258727\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006248486988479272\n",
      "Training Loss: 0.006461989275412634\n",
      "Training Loss: 0.006203400591621175\n",
      "Validation Loss: 0.003944865715679493\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006238540373160504\n",
      "Training Loss: 0.006452976075233891\n",
      "Training Loss: 0.0061938010330777615\n",
      "Validation Loss: 0.003934953800269162\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006228717113262974\n",
      "Training Loss: 0.00644405449857004\n",
      "Training Loss: 0.006184324680943974\n",
      "Validation Loss: 0.003925082070381496\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006219015081878752\n",
      "Training Loss: 0.006435222565196455\n",
      "Training Loss: 0.006174972098669969\n",
      "Validation Loss: 0.003915256695047523\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006209434653283097\n",
      "Training Loss: 0.006426482805982232\n",
      "Training Loss: 0.006165742020239122\n",
      "Validation Loss: 0.003905483059456907\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006199975442141295\n",
      "Training Loss: 0.006417834308231249\n",
      "Training Loss: 0.006156634346698411\n",
      "Validation Loss: 0.00389576652885174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006190636169631034\n",
      "Training Loss: 0.006409276388585568\n",
      "Training Loss: 0.006147648048936389\n",
      "Validation Loss: 0.003886111114328476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006181415915489197\n",
      "Training Loss: 0.0064008095604367555\n",
      "Training Loss: 0.006138780838227831\n",
      "Validation Loss: 0.0038765253122435527\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006172312831040472\n",
      "Training Loss: 0.006392432413995266\n",
      "Training Loss: 0.006130030442145653\n",
      "Validation Loss: 0.003867007508151903\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006163324441295117\n",
      "Training Loss: 0.006384143028408289\n",
      "Training Loss: 0.006121396649978124\n",
      "Validation Loss: 0.0038575653354323398\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006154450085014105\n",
      "Training Loss: 0.006375942897284404\n",
      "Training Loss: 0.006112875856924802\n",
      "Validation Loss: 0.0038481996789721125\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006145685570663773\n",
      "Training Loss: 0.006367828490911051\n",
      "Training Loss: 0.006104467324330471\n",
      "Validation Loss: 0.003838914675439258\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006137031083926559\n",
      "Training Loss: 0.006359798650955781\n",
      "Training Loss: 0.006096167748910375\n",
      "Validation Loss: 0.003829715238802993\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.0061284825624898075\n",
      "Training Loss: 0.00635185428080149\n",
      "Training Loss: 0.00608797615102958\n",
      "Validation Loss: 0.003820600803991194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.00612003768212162\n",
      "Training Loss: 0.006343992809997871\n",
      "Training Loss: 0.006079890533001162\n",
      "Validation Loss: 0.0038115732565396623\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006111695603467524\n",
      "Training Loss: 0.006336213843896985\n",
      "Training Loss: 0.006071908512385562\n",
      "Validation Loss: 0.0038026365205603703\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0061034525523427875\n",
      "Training Loss: 0.006328515111235901\n",
      "Training Loss: 0.00606402829173021\n",
      "Validation Loss: 0.003793792790939948\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.0060953078465536235\n",
      "Training Loss: 0.00632089567487128\n",
      "Training Loss: 0.006056246295338496\n",
      "Validation Loss: 0.003785040593193321\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006087256808532402\n",
      "Training Loss: 0.006313354636076838\n",
      "Training Loss: 0.006048563150106929\n",
      "Validation Loss: 0.0037763847229123283\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006079298964468762\n",
      "Training Loss: 0.006305889913346618\n",
      "Training Loss: 0.006040974498027936\n",
      "Validation Loss: 0.0037678208429722136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006071430590236559\n",
      "Training Loss: 0.006298501004930585\n",
      "Training Loss: 0.006033478991012089\n",
      "Validation Loss: 0.003759352349364879\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006063650331343524\n",
      "Training Loss: 0.0062911867501679805\n",
      "Training Loss: 0.0060260738217039036\n",
      "Validation Loss: 0.0037509820730101022\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006055954598123208\n",
      "Training Loss: 0.006283944755559787\n",
      "Training Loss: 0.006018758202553726\n",
      "Validation Loss: 0.0037427081103865683\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006048344121081755\n",
      "Training Loss: 0.006276775607839227\n",
      "Training Loss: 0.006011530745890923\n",
      "Validation Loss: 0.003734532447094935\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.00604081416677218\n",
      "Training Loss: 0.006269675972871483\n",
      "Training Loss: 0.006004387870198116\n",
      "Validation Loss: 0.003726449511223211\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006033363905153237\n",
      "Training Loss: 0.006262646585237235\n",
      "Training Loss: 0.0059973295172676445\n",
      "Validation Loss: 0.0037184660355665125\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006025990642956458\n",
      "Training Loss: 0.0062556855450384315\n",
      "Training Loss: 0.005990353737724945\n",
      "Validation Loss: 0.0037105776339486827\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006018692989600822\n",
      "Training Loss: 0.006248791093239561\n",
      "Training Loss: 0.00598345811595209\n",
      "Validation Loss: 0.003702785801129374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006011468791984953\n",
      "Training Loss: 0.00624196381890215\n",
      "Training Loss: 0.005976640557637438\n",
      "Validation Loss: 0.0036950893582399476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006004316266044043\n",
      "Training Loss: 0.0062352003972046075\n",
      "Training Loss: 0.005969901771750301\n",
      "Validation Loss: 0.003687487468790535\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005997234156820923\n",
      "Training Loss: 0.006228501545265317\n",
      "Training Loss: 0.005963237002142705\n",
      "Validation Loss: 0.0036799788600608203\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005990219959057868\n",
      "Training Loss: 0.006221864970866591\n",
      "Training Loss: 0.005956647206912749\n",
      "Validation Loss: 0.003672566197176851\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005983272978919558\n",
      "Training Loss: 0.006215292024426162\n",
      "Training Loss: 0.005950130292330868\n",
      "Validation Loss: 0.0036652462646558754\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.0059763909358298405\n",
      "Training Loss: 0.006208778443979099\n",
      "Training Loss: 0.00594368371705059\n",
      "Validation Loss: 0.003658019943134022\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005969573400216177\n",
      "Training Loss: 0.006202325812773779\n",
      "Training Loss: 0.005937308974098414\n",
      "Validation Loss: 0.0036508841678798407\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005962818485568277\n",
      "Training Loss: 0.006195932535920292\n",
      "Training Loss: 0.005931002130964771\n",
      "Validation Loss: 0.003643838544847088\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005956123603973538\n",
      "Training Loss: 0.006189598721684888\n",
      "Training Loss: 0.005924762318609282\n",
      "Validation Loss: 0.003636882699937089\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005949488969054073\n",
      "Training Loss: 0.006183322236174718\n",
      "Training Loss: 0.005918588974163867\n",
      "Validation Loss: 0.003630013411780924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005942911529564299\n",
      "Training Loss: 0.0061771034286357464\n",
      "Training Loss: 0.005912480331026018\n",
      "Validation Loss: 0.0036232356149493977\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.00593639365688432\n",
      "Training Loss: 0.006170940726296976\n",
      "Training Loss: 0.00590643587813247\n",
      "Validation Loss: 0.003616541832374158\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0059299309074413034\n",
      "Training Loss: 0.006164834265364334\n",
      "Training Loss: 0.0059004542313050475\n",
      "Validation Loss: 0.003609935153312353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.00592352453386411\n",
      "Training Loss: 0.0061587823065929114\n",
      "Training Loss: 0.005894534973194822\n",
      "Validation Loss: 0.0036034146323800087\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005917171895271167\n",
      "Training Loss: 0.006152784833684564\n",
      "Training Loss: 0.005888676022877916\n",
      "Validation Loss: 0.0035969752807227706\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.00591087234322913\n",
      "Training Loss: 0.006146840993314981\n",
      "Training Loss: 0.005882876515970565\n",
      "Validation Loss: 0.003590617753340329\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005904625506955199\n",
      "Training Loss: 0.006140951364068314\n",
      "Training Loss: 0.0058771352982148525\n",
      "Validation Loss: 0.003584342628058172\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005898430497036315\n",
      "Training Loss: 0.006135114120552316\n",
      "Training Loss: 0.0058714523160597306\n",
      "Validation Loss: 0.003578149186109789\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005892286639427766\n",
      "Training Loss: 0.006129329154500738\n",
      "Training Loss: 0.005865827052039094\n",
      "Validation Loss: 0.003572032969049963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005886192641919479\n",
      "Training Loss: 0.006123596851248294\n",
      "Training Loss: 0.00586025630356744\n",
      "Validation Loss: 0.0035659939644523382\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005880147445132025\n",
      "Training Loss: 0.006117914727656171\n",
      "Training Loss: 0.0058547418483067305\n",
      "Validation Loss: 0.0035600340702790726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005874152237083763\n",
      "Training Loss: 0.0061122838815208524\n",
      "Training Loss: 0.0058492807409493254\n",
      "Validation Loss: 0.003554151091970545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005868205686565489\n",
      "Training Loss: 0.0061067038308829065\n",
      "Training Loss: 0.005843874159036204\n",
      "Validation Loss: 0.0035483393546068267\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0058623052382608875\n",
      "Training Loss: 0.006101173516362905\n",
      "Training Loss: 0.005838519277749583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [17:25<17:26, 209.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.003542604073986746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.31339340262115\n",
      "Training Loss: 0.23740695334970952\n",
      "Training Loss: 0.16265255603939294\n",
      "Validation Loss: 0.11357928820875253\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.09554436132311821\n",
      "Training Loss: 0.0794837455637753\n",
      "Training Loss: 0.07146460138261318\n",
      "Validation Loss: 0.07123366232668416\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06884023655205965\n",
      "Training Loss: 0.06824174921959639\n",
      "Training Loss: 0.06522225506603718\n",
      "Validation Loss: 0.06510626409579529\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.06270136954262853\n",
      "Training Loss: 0.0618719391990453\n",
      "Training Loss: 0.05892213832587004\n",
      "Validation Loss: 0.05849313254604179\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.05614166893064976\n",
      "Training Loss: 0.055218581221997735\n",
      "Training Loss: 0.05243803579360247\n",
      "Validation Loss: 0.05179796177433448\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.049558071456849574\n",
      "Training Loss: 0.048563128570094705\n",
      "Training Loss: 0.04590923313051462\n",
      "Validation Loss: 0.04505141857969627\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.04293079399503767\n",
      "Training Loss: 0.041815611328929664\n",
      "Training Loss: 0.03921791889704764\n",
      "Validation Loss: 0.038146956352872796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.036195855978876354\n",
      "Training Loss: 0.034987635952420534\n",
      "Training Loss: 0.032510345680639145\n",
      "Validation Loss: 0.03135093643717217\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.029730438143014907\n",
      "Training Loss: 0.028599330438300968\n",
      "Training Loss: 0.026467308327555658\n",
      "Validation Loss: 0.02540168178717742\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.024306077286601068\n",
      "Training Loss: 0.023409434854984285\n",
      "Training Loss: 0.021780869984067978\n",
      "Validation Loss: 0.02083189506083727\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.02032930473331362\n",
      "Training Loss: 0.01965929267462343\n",
      "Training Loss: 0.018504888708703218\n",
      "Validation Loss: 0.017565319032062977\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.017598052602261306\n",
      "Training Loss: 0.017074289610609413\n",
      "Training Loss: 0.016288075768388807\n",
      "Validation Loss: 0.015263220708649815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.015730574512854218\n",
      "Training Loss: 0.015292380806058645\n",
      "Training Loss: 0.014776804309803993\n",
      "Validation Loss: 0.013623672184335548\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.014430298693478108\n",
      "Training Loss: 0.014046811668667942\n",
      "Training Loss: 0.01372507227351889\n",
      "Validation Loss: 0.012427928582817483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.01349600062239915\n",
      "Training Loss: 0.013147975686006248\n",
      "Training Loss: 0.012959241312928497\n",
      "Validation Loss: 0.011510035520147406\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.012783105152193457\n",
      "Training Loss: 0.01245725534390658\n",
      "Training Loss: 0.012356666496489198\n",
      "Validation Loss: 0.010749021589525797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.012188586274860427\n",
      "Training Loss: 0.011876111762830988\n",
      "Training Loss: 0.01182887785253115\n",
      "Validation Loss: 0.010053656181697262\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.01163310852018185\n",
      "Training Loss: 0.011329185778740793\n",
      "Training Loss: 0.011304894811473787\n",
      "Validation Loss: 0.009351139095496763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.011054617898771539\n",
      "Training Loss: 0.01076514803338796\n",
      "Training Loss: 0.010741531525272876\n",
      "Validation Loss: 0.00861228606271233\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.010440810444997624\n",
      "Training Loss: 0.010196130292024463\n",
      "Training Loss: 0.01017194033251144\n",
      "Validation Loss: 0.00790611003379055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.009874674360034988\n",
      "Training Loss: 0.009714583747554571\n",
      "Training Loss: 0.009698688758071512\n",
      "Validation Loss: 0.007348461601140208\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.00945373193710111\n",
      "Training Loss: 0.009376751395175234\n",
      "Training Loss: 0.009361571900080889\n",
      "Validation Loss: 0.006956669126553566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.009167028348892927\n",
      "Training Loss: 0.009142404935555532\n",
      "Training Loss: 0.009118968572001904\n",
      "Validation Loss: 0.006676349046593009\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008959757330594585\n",
      "Training Loss: 0.008965875616995618\n",
      "Training Loss: 0.00893308941507712\n",
      "Validation Loss: 0.006463939221994428\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.00880027082748711\n",
      "Training Loss: 0.008826224918011575\n",
      "Training Loss: 0.0087850047997199\n",
      "Validation Loss: 0.006295896384451705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008673382127890364\n",
      "Training Loss: 0.00871263563632965\n",
      "Training Loss: 0.008663922691484914\n",
      "Validation Loss: 0.006158996085646782\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008569836420938372\n",
      "Training Loss: 0.008617792750010267\n",
      "Training Loss: 0.008562542484141886\n",
      "Validation Loss: 0.006044776016699799\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.00848312962334603\n",
      "Training Loss: 0.008536436944268644\n",
      "Training Loss: 0.008475606673164294\n",
      "Validation Loss: 0.005947343964558723\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.008408591527258978\n",
      "Training Loss: 0.00846484954818152\n",
      "Training Loss: 0.008399340035393834\n",
      "Validation Loss: 0.005862509593723363\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008342950758524238\n",
      "Training Loss: 0.00840048849931918\n",
      "Training Loss: 0.008331105585675687\n",
      "Validation Loss: 0.00578732960438879\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.008283977264072746\n",
      "Training Loss: 0.00834165672888048\n",
      "Training Loss: 0.008269099318422377\n",
      "Validation Loss: 0.005719748503539\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008230177676305175\n",
      "Training Loss: 0.008287240494973957\n",
      "Training Loss: 0.008212111484026537\n",
      "Validation Loss: 0.005658337184531468\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008180556544102728\n",
      "Training Loss: 0.008236508042318746\n",
      "Training Loss: 0.008159321506973356\n",
      "Validation Loss: 0.00560207982296438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008134440672583877\n",
      "Training Loss: 0.00818896888406016\n",
      "Training Loss: 0.008110167727572844\n",
      "Validation Loss: 0.005550242722390241\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008091363380663097\n",
      "Training Loss: 0.008144283981528134\n",
      "Training Loss: 0.008064248508308083\n",
      "Validation Loss: 0.005502285138871311\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008050988988252357\n",
      "Training Loss: 0.008102207776391878\n",
      "Training Loss: 0.0080212620284874\n",
      "Validation Loss: 0.005457773134949502\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008013060541125015\n",
      "Training Loss: 0.008062551494222135\n",
      "Training Loss: 0.007980970968492328\n",
      "Validation Loss: 0.005416365235709072\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007977372580207885\n",
      "Training Loss: 0.008025157157098874\n",
      "Training Loss: 0.00794317183084786\n",
      "Validation Loss: 0.00537777376842549\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007943750543054193\n",
      "Training Loss: 0.007989888391457499\n",
      "Training Loss: 0.007907689863350243\n",
      "Validation Loss: 0.005341752166755163\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007912041709059849\n",
      "Training Loss: 0.007956619339529425\n",
      "Training Loss: 0.007874357955297456\n",
      "Validation Loss: 0.005308076497324313\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007882103392621503\n",
      "Training Loss: 0.007925228086533025\n",
      "Training Loss: 0.00784302204148844\n",
      "Validation Loss: 0.005276544159539881\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007853804057231174\n",
      "Training Loss: 0.007895598987815902\n",
      "Training Loss: 0.00781353604979813\n",
      "Validation Loss: 0.005246972963423207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007827018549432978\n",
      "Training Loss: 0.007867618707241491\n",
      "Training Loss: 0.007785756172379479\n",
      "Validation Loss: 0.005219197633226266\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0078016312757972625\n",
      "Training Loss: 0.0078411783662159\n",
      "Training Loss: 0.007759550980990753\n",
      "Validation Loss: 0.00519306543335486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007777530683670193\n",
      "Training Loss: 0.007816172380698845\n",
      "Training Loss: 0.007734791891416535\n",
      "Validation Loss: 0.005168431524836113\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007754612411372363\n",
      "Training Loss: 0.007792498163180426\n",
      "Training Loss: 0.007711357282241807\n",
      "Validation Loss: 0.005145165304067346\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007732781150843948\n",
      "Training Loss: 0.007770060189068317\n",
      "Training Loss: 0.007689133876701817\n",
      "Validation Loss: 0.0051231502589319695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007711943926988169\n",
      "Training Loss: 0.00774876601411961\n",
      "Training Loss: 0.007668016579700634\n",
      "Validation Loss: 0.005102281462599973\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007692020067479461\n",
      "Training Loss: 0.007728531946195289\n",
      "Training Loss: 0.007647908037761227\n",
      "Validation Loss: 0.005082461548697078\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007672933542635292\n",
      "Training Loss: 0.00770927794626914\n",
      "Training Loss: 0.007628719344502315\n",
      "Validation Loss: 0.005063600076906634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007654615571955219\n",
      "Training Loss: 0.007690930648241192\n",
      "Training Loss: 0.007610368457389996\n",
      "Validation Loss: 0.005045622186944558\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007637001535622403\n",
      "Training Loss: 0.007673422270454466\n",
      "Training Loss: 0.007592779650585726\n",
      "Validation Loss: 0.005028457761470103\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.007620036137523129\n",
      "Training Loss: 0.007656689842697233\n",
      "Training Loss: 0.0075758841715287415\n",
      "Validation Loss: 0.0050120369238213875\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007603665648493916\n",
      "Training Loss: 0.007640675927978009\n",
      "Training Loss: 0.007559621758991853\n",
      "Validation Loss: 0.004996307581411988\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007587843494256959\n",
      "Training Loss: 0.007625326812267304\n",
      "Training Loss: 0.007543934925924986\n",
      "Validation Loss: 0.004981212477524127\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007572526840958744\n",
      "Training Loss: 0.007610594361322001\n",
      "Training Loss: 0.007528772703371942\n",
      "Validation Loss: 0.004966708443002001\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0075576767465099694\n",
      "Training Loss: 0.007596433163853362\n",
      "Training Loss: 0.007514089184114709\n",
      "Validation Loss: 0.004952750452584849\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007543257764191367\n",
      "Training Loss: 0.007582799554802478\n",
      "Training Loss: 0.007499841389944777\n",
      "Validation Loss: 0.004939297161828936\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007529237294802442\n",
      "Training Loss: 0.007569656122941523\n",
      "Training Loss: 0.007485988987609744\n",
      "Validation Loss: 0.004926310754067191\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007515584249049425\n",
      "Training Loss: 0.00755696480977349\n",
      "Training Loss: 0.007472497812705115\n",
      "Validation Loss: 0.004913758744846695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007502271208213642\n",
      "Training Loss: 0.007544690915383398\n",
      "Training Loss: 0.007459335115272552\n",
      "Validation Loss: 0.004901612238314924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007489273056853563\n",
      "Training Loss: 0.007532802172936499\n",
      "Training Loss: 0.007446469854330644\n",
      "Validation Loss: 0.004889839128695763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007476565537508577\n",
      "Training Loss: 0.007521268032724038\n",
      "Training Loss: 0.007433876087889075\n",
      "Validation Loss: 0.004878415894862055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.0074641264323145155\n",
      "Training Loss: 0.007510059528285638\n",
      "Training Loss: 0.007421528163831681\n",
      "Validation Loss: 0.0048673191759212135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007451935433782637\n",
      "Training Loss: 0.007499149584909901\n",
      "Training Loss: 0.0074094032670836895\n",
      "Validation Loss: 0.004856526488019677\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007439973134314642\n",
      "Training Loss: 0.007488513581920415\n",
      "Training Loss: 0.007397481459192932\n",
      "Validation Loss: 0.004846022733790653\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007428222853923217\n",
      "Training Loss: 0.007478127293288708\n",
      "Training Loss: 0.007385743354680016\n",
      "Validation Loss: 0.0048357860611626105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0074166657507885245\n",
      "Training Loss: 0.00746796814375557\n",
      "Training Loss: 0.007374172447016463\n",
      "Validation Loss: 0.004825803793945841\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007405287327710539\n",
      "Training Loss: 0.007458015312440693\n",
      "Training Loss: 0.007362751472974196\n",
      "Validation Loss: 0.004816062970144486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007394072378519923\n",
      "Training Loss: 0.007448249431326985\n",
      "Training Loss: 0.007351467650732957\n",
      "Validation Loss: 0.004806547416531051\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007383005978772417\n",
      "Training Loss: 0.007438651752891019\n",
      "Training Loss: 0.00734030659252312\n",
      "Validation Loss: 0.004797248066444829\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007372075087623671\n",
      "Training Loss: 0.00742920542601496\n",
      "Training Loss: 0.0073292561154812575\n",
      "Validation Loss: 0.004788152638069364\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007361267213709653\n",
      "Training Loss: 0.007419894331833348\n",
      "Training Loss: 0.007318305687513202\n",
      "Validation Loss: 0.004779248929378482\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007350569722475484\n",
      "Training Loss: 0.007410702052293345\n",
      "Training Loss: 0.007307444328907877\n",
      "Validation Loss: 0.004770528103926041\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.0073399707942735405\n",
      "Training Loss: 0.007401615023845807\n",
      "Training Loss: 0.00729666254715994\n",
      "Validation Loss: 0.004761979441692153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00732945981901139\n",
      "Training Loss: 0.007392620311584323\n",
      "Training Loss: 0.007285950963268988\n",
      "Validation Loss: 0.004753591712522372\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007319025412434712\n",
      "Training Loss: 0.007383704073727131\n",
      "Training Loss: 0.007275303471251391\n",
      "Validation Loss: 0.004745358423319425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007308659029658884\n",
      "Training Loss: 0.007374855926027522\n",
      "Training Loss: 0.007264710293384269\n",
      "Validation Loss: 0.004737265217065644\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00729834983823821\n",
      "Training Loss: 0.007366062928922474\n",
      "Training Loss: 0.007254166239290498\n",
      "Validation Loss: 0.004729307221976014\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007288090728688985\n",
      "Training Loss: 0.007357316322159022\n",
      "Training Loss: 0.007243665247806348\n",
      "Validation Loss: 0.004721474825284245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007277873345883563\n",
      "Training Loss: 0.007348606967134401\n",
      "Training Loss: 0.00723320113203954\n",
      "Validation Loss: 0.004713754805349065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0072676897677592936\n",
      "Training Loss: 0.0073399248940404505\n",
      "Training Loss: 0.007222770042717457\n",
      "Validation Loss: 0.0047061400294429465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007257534237578511\n",
      "Training Loss: 0.0073312621412333105\n",
      "Training Loss: 0.007212367133470252\n",
      "Validation Loss: 0.004698623264119489\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007247400066116825\n",
      "Training Loss: 0.007322611229028553\n",
      "Training Loss: 0.007201989651075564\n",
      "Validation Loss: 0.004691191976419075\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.0072372811939567325\n",
      "Training Loss: 0.007313965114299208\n",
      "Training Loss: 0.007191633504698984\n",
      "Validation Loss: 0.004683839664742184\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007227173850405961\n",
      "Training Loss: 0.007305317864520476\n",
      "Training Loss: 0.00718129679095\n",
      "Validation Loss: 0.004676557084535029\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.0072170726675540205\n",
      "Training Loss: 0.0072966631664894525\n",
      "Training Loss: 0.0071709776268107815\n",
      "Validation Loss: 0.004669335407199694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007206975246081129\n",
      "Training Loss: 0.007287996482336894\n",
      "Training Loss: 0.007160673695616424\n",
      "Validation Loss: 0.004662163367264726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007196877938695252\n",
      "Training Loss: 0.007279313006438315\n",
      "Training Loss: 0.007150383905391209\n",
      "Validation Loss: 0.004655036239445293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007186777878087014\n",
      "Training Loss: 0.007270607863320038\n",
      "Training Loss: 0.007140107571613044\n",
      "Validation Loss: 0.004647946598013507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0071766730153467505\n",
      "Training Loss: 0.007261876995908096\n",
      "Training Loss: 0.007129843085422181\n",
      "Validation Loss: 0.004640885418165852\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007166562223574147\n",
      "Training Loss: 0.007253117897780612\n",
      "Training Loss: 0.007119590597576462\n",
      "Validation Loss: 0.004633844488044008\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007156442850828171\n",
      "Training Loss: 0.00724432646878995\n",
      "Training Loss: 0.007109348684316501\n",
      "Validation Loss: 0.0046268177671316135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.007146314759738743\n",
      "Training Loss: 0.007235499940579757\n",
      "Training Loss: 0.007099117985926568\n",
      "Validation Loss: 0.00461979645626159\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007136176251806319\n",
      "Training Loss: 0.007226635355036706\n",
      "Training Loss: 0.007088897465146147\n",
      "Validation Loss: 0.0046127776116959415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007126026577316225\n",
      "Training Loss: 0.007217729332624003\n",
      "Training Loss: 0.007078685910673812\n",
      "Validation Loss: 0.0046057532445967116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007115864999359473\n",
      "Training Loss: 0.007208780099172145\n",
      "Training Loss: 0.007068481753813103\n",
      "Validation Loss: 0.004598714540754393\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007105690428288653\n",
      "Training Loss: 0.007199784643016755\n",
      "Training Loss: 0.0070582849544007335\n",
      "Validation Loss: 0.004591658275202963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.007095501612639055\n",
      "Training Loss: 0.007190739784855396\n",
      "Training Loss: 0.007048094513593241\n",
      "Validation Loss: 0.00458457758205451\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007085298089077696\n",
      "Training Loss: 0.007181643507210538\n",
      "Training Loss: 0.007037907352787443\n",
      "Validation Loss: 0.00457746778049747\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007075077020563186\n",
      "Training Loss: 0.007172492142999544\n",
      "Training Loss: 0.007027721827616915\n",
      "Validation Loss: 0.004570322973899585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.007064837998477742\n",
      "Training Loss: 0.007163282947149127\n",
      "Training Loss: 0.007017534693004563\n",
      "Validation Loss: 0.004563133439619429\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.0070545778842642905\n",
      "Training Loss: 0.007154012440005317\n",
      "Training Loss: 0.007007343438453972\n",
      "Validation Loss: 0.004555900350966481\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0070442946325056256\n",
      "Training Loss: 0.007144677665783092\n",
      "Training Loss: 0.0069971437391359356\n",
      "Validation Loss: 0.004548615047425618\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.007033986047608778\n",
      "Training Loss: 0.007135273937601596\n",
      "Training Loss: 0.0069869325088802725\n",
      "Validation Loss: 0.004541267898572044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.007023647910682484\n",
      "Training Loss: 0.007125797966727987\n",
      "Training Loss: 0.006976704509579577\n",
      "Validation Loss: 0.004533858089498506\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.007013277475489303\n",
      "Training Loss: 0.007116246005753055\n",
      "Training Loss: 0.006966456458321772\n",
      "Validation Loss: 0.004526378189244967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.007002870520809665\n",
      "Training Loss: 0.007106613516807556\n",
      "Training Loss: 0.006956181010464206\n",
      "Validation Loss: 0.004518819191320433\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006992423486663029\n",
      "Training Loss: 0.007096896294970065\n",
      "Training Loss: 0.0069458746013697234\n",
      "Validation Loss: 0.004511176751078933\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006981932180933654\n",
      "Training Loss: 0.007087089199339971\n",
      "Training Loss: 0.006935530651244335\n",
      "Validation Loss: 0.004503440864651014\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.0069713910680729895\n",
      "Training Loss: 0.007077188084367663\n",
      "Training Loss: 0.006925144273554906\n",
      "Validation Loss: 0.004495609015372865\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006960795628838241\n",
      "Training Loss: 0.007067188440123573\n",
      "Training Loss: 0.006914708457770757\n",
      "Validation Loss: 0.004487668846215886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006950141512788832\n",
      "Training Loss: 0.007057085318956524\n",
      "Training Loss: 0.006904217316186987\n",
      "Validation Loss: 0.004479614340208387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006939424113370478\n",
      "Training Loss: 0.007046873882645741\n",
      "Training Loss: 0.006893665403476916\n",
      "Validation Loss: 0.004471433744635015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006928638190729543\n",
      "Training Loss: 0.007036551394267007\n",
      "Training Loss: 0.006883047505980357\n",
      "Validation Loss: 0.004463119529124894\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006917779123177752\n",
      "Training Loss: 0.007026111049344763\n",
      "Training Loss: 0.006872357678366825\n",
      "Validation Loss: 0.004454666906261419\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006906842155149207\n",
      "Training Loss: 0.007015551682561636\n",
      "Training Loss: 0.006861589902546257\n",
      "Validation Loss: 0.004446058687126117\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006895823659142479\n",
      "Training Loss: 0.0070048680668696765\n",
      "Training Loss: 0.006850741249509156\n",
      "Validation Loss: 0.004437288344612743\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.00688471935573034\n",
      "Training Loss: 0.00699405851890333\n",
      "Training Loss: 0.006839807013748213\n",
      "Validation Loss: 0.004428342558977226\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006873525908449665\n",
      "Training Loss: 0.006983119698707014\n",
      "Training Loss: 0.006828783845412545\n",
      "Validation Loss: 0.004419213279903856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006862239816691726\n",
      "Training Loss: 0.006972050254698842\n",
      "Training Loss: 0.006817669196170755\n",
      "Validation Loss: 0.004409886337424304\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006850858778925613\n",
      "Training Loss: 0.006960848986636847\n",
      "Training Loss: 0.00680646279535722\n",
      "Validation Loss: 0.004400350189417224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0068393806560197844\n",
      "Training Loss: 0.006949515655869618\n",
      "Training Loss: 0.006795162811758928\n",
      "Validation Loss: 0.0043905944382939275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006827804244821891\n",
      "Training Loss: 0.0069380499795079235\n",
      "Training Loss: 0.006783770387410186\n",
      "Validation Loss: 0.004380600119475275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.0068161274056183175\n",
      "Training Loss: 0.006926453547785059\n",
      "Training Loss: 0.0067722884938120845\n",
      "Validation Loss: 0.0043703649092329605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0068043505167588595\n",
      "Training Loss: 0.00691472842823714\n",
      "Training Loss: 0.00676071930443868\n",
      "Validation Loss: 0.004359868525140239\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006792472555534914\n",
      "Training Loss: 0.00690287530538626\n",
      "Training Loss: 0.006749066342017614\n",
      "Validation Loss: 0.004349094509471501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006780490924138576\n",
      "Training Loss: 0.006890895328251645\n",
      "Training Loss: 0.006737334305071272\n",
      "Validation Loss: 0.004338036595419929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006768406110350043\n",
      "Training Loss: 0.006878792149946094\n",
      "Training Loss: 0.006725529008544982\n",
      "Validation Loss: 0.004326676548291207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.00675621535803657\n",
      "Training Loss: 0.00686656508827582\n",
      "Training Loss: 0.006713655610801652\n",
      "Validation Loss: 0.004314997221678077\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006743913246900775\n",
      "Training Loss: 0.006854212216567248\n",
      "Training Loss: 0.006701716508832761\n",
      "Validation Loss: 0.004302976582345836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006731493876432068\n",
      "Training Loss: 0.006841731288004666\n",
      "Training Loss: 0.006689717551344074\n",
      "Validation Loss: 0.004290603428262841\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006718947910121642\n",
      "Training Loss: 0.006829114235006273\n",
      "Training Loss: 0.006677657999680378\n",
      "Validation Loss: 0.004277845831546053\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006706259930506348\n",
      "Training Loss: 0.006816350799053908\n",
      "Training Loss: 0.006665535303764045\n",
      "Validation Loss: 0.004264669998397193\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006693411106825806\n",
      "Training Loss: 0.006803423713427037\n",
      "Training Loss: 0.006653345079394057\n",
      "Validation Loss: 0.004251043324677922\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006680372309638187\n",
      "Training Loss: 0.00679030773229897\n",
      "Training Loss: 0.006641074390499852\n",
      "Validation Loss: 0.004236916607910286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00666710901830811\n",
      "Training Loss: 0.006776971737854183\n",
      "Training Loss: 0.006628706001210957\n",
      "Validation Loss: 0.004222237722152907\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006653575923992321\n",
      "Training Loss: 0.006763372994028032\n",
      "Training Loss: 0.00661621275357902\n",
      "Validation Loss: 0.004206942596467568\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006639716477366164\n",
      "Training Loss: 0.006749457427067682\n",
      "Training Loss: 0.0066035591589752585\n",
      "Validation Loss: 0.004190946906266127\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006625458401977084\n",
      "Training Loss: 0.006735157434595749\n",
      "Training Loss: 0.006590698604122735\n",
      "Validation Loss: 0.004174152595838637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006610714500420727\n",
      "Training Loss: 0.0067203918180894105\n",
      "Training Loss: 0.006577573497197591\n",
      "Validation Loss: 0.00415644453958807\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.00659538070380222\n",
      "Training Loss: 0.006705059834057465\n",
      "Training Loss: 0.006564107306185178\n",
      "Validation Loss: 0.004137688163364453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.0065793317498173565\n",
      "Training Loss: 0.0066890473465900865\n",
      "Training Loss: 0.006550213937880471\n",
      "Validation Loss: 0.004117742798217897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006562426760210655\n",
      "Training Loss: 0.006672222741181031\n",
      "Training Loss: 0.00653578850091435\n",
      "Validation Loss: 0.004096441070308511\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006544500197633169\n",
      "Training Loss: 0.006654436105163768\n",
      "Training Loss: 0.006520708504249342\n",
      "Validation Loss: 0.004073622747026174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006525377306970768\n",
      "Training Loss: 0.006635530123021454\n",
      "Training Loss: 0.006504839927656576\n",
      "Validation Loss: 0.004049138308586448\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006504874785314314\n",
      "Training Loss: 0.006615349921630696\n",
      "Training Loss: 0.006488041656557471\n",
      "Validation Loss: 0.004022885840569278\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006482827718136832\n",
      "Training Loss: 0.006593761248514057\n",
      "Training Loss: 0.006470176703296602\n",
      "Validation Loss: 0.003994835809453945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006459108886774629\n",
      "Training Loss: 0.006570678588468581\n",
      "Training Loss: 0.0064511298865545545\n",
      "Validation Loss: 0.003965107300920475\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.00643367639509961\n",
      "Training Loss: 0.006546100811101496\n",
      "Training Loss: 0.006430836142972112\n",
      "Validation Loss: 0.0039340053283442025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006406614271108993\n",
      "Training Loss: 0.006520147449336946\n",
      "Training Loss: 0.006409300940576941\n",
      "Validation Loss: 0.0039020576880554134\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006378169730305671\n",
      "Training Loss: 0.006493078034836799\n",
      "Training Loss: 0.006386624077567831\n",
      "Validation Loss: 0.00387001368138688\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006348755504586734\n",
      "Training Loss: 0.006465279397089034\n",
      "Training Loss: 0.006362991706700996\n",
      "Validation Loss: 0.0038387270145944915\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006318904715008102\n",
      "Training Loss: 0.006437210272997618\n",
      "Training Loss: 0.0063386610487941655\n",
      "Validation Loss: 0.0038090181939848017\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006289182219770737\n",
      "Training Loss: 0.0064093241229420525\n",
      "Training Loss: 0.006313917213119567\n",
      "Validation Loss: 0.003781493500089587\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006260080948704854\n",
      "Training Loss: 0.006381994566181675\n",
      "Training Loss: 0.006289041698910296\n",
      "Validation Loss: 0.0037564595067727095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.00623195924796164\n",
      "Training Loss: 0.006355489150155336\n",
      "Training Loss: 0.006264285780489444\n",
      "Validation Loss: 0.0037339168628206833\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006205022640060633\n",
      "Training Loss: 0.00632996425323654\n",
      "Training Loss: 0.006239866042742506\n",
      "Validation Loss: 0.0037136589637381976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006179358920780942\n",
      "Training Loss: 0.006305501474416815\n",
      "Training Loss: 0.0062159540154971184\n",
      "Validation Loss: 0.003695366197145345\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.0061549690214451405\n",
      "Training Loss: 0.006282126917503774\n",
      "Training Loss: 0.006192680452950299\n",
      "Validation Loss: 0.0036787196373818127\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006131814013933763\n",
      "Training Loss: 0.006259834670345299\n",
      "Training Loss: 0.006170134505955502\n",
      "Validation Loss: 0.003663423218701579\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006109834267990663\n",
      "Training Loss: 0.006238600463839248\n",
      "Training Loss: 0.006148368205176666\n",
      "Validation Loss: 0.0036492408423297357\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006088958867476322\n",
      "Training Loss: 0.006218383195227943\n",
      "Training Loss: 0.006127407106105238\n",
      "Validation Loss: 0.0036359839075967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.0060691210115328435\n",
      "Training Loss: 0.006199140736716799\n",
      "Training Loss: 0.006107257751282304\n",
      "Validation Loss: 0.003623501485961835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006050254389992915\n",
      "Training Loss: 0.006180823941249401\n",
      "Training Loss: 0.0060879102442413565\n",
      "Validation Loss: 0.0036116942529189873\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006032299311482347\n",
      "Training Loss: 0.006163385903928429\n",
      "Training Loss: 0.006069349094759673\n",
      "Validation Loss: 0.0036004740055493507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006015199521789328\n",
      "Training Loss: 0.006146777601679787\n",
      "Training Loss: 0.00605155314435251\n",
      "Validation Loss: 0.00358977031781537\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005998903797008097\n",
      "Training Loss: 0.006130955195403658\n",
      "Training Loss: 0.006034498586086556\n",
      "Validation Loss: 0.0035795314293721\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005983364585554227\n",
      "Training Loss: 0.006115873362869024\n",
      "Training Loss: 0.00601816248963587\n",
      "Validation Loss: 0.0035697156713825513\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005968541128095239\n",
      "Training Loss: 0.00610149179934524\n",
      "Training Loss: 0.006002520638285205\n",
      "Validation Loss: 0.003560282518234355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005954393044812605\n",
      "Training Loss: 0.006087771884631365\n",
      "Training Loss: 0.005987550673307851\n",
      "Validation Loss: 0.003551210305559334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005940885073505342\n",
      "Training Loss: 0.006074677144060843\n",
      "Training Loss: 0.0059732275211717934\n",
      "Validation Loss: 0.0035424688124631562\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005927985801827162\n",
      "Training Loss: 0.006062172368983738\n",
      "Training Loss: 0.005959530756808817\n",
      "Validation Loss: 0.003534033467464693\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005915660940809175\n",
      "Training Loss: 0.006050223775673658\n",
      "Training Loss: 0.0059464369289344175\n",
      "Validation Loss: 0.0035258812687276038\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.0059038813645020125\n",
      "Training Loss: 0.006038798848749138\n",
      "Training Loss: 0.005933922267286107\n",
      "Validation Loss: 0.003517999099907622\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005892616620985791\n",
      "Training Loss: 0.006027865154319443\n",
      "Training Loss: 0.005921963258879259\n",
      "Validation Loss: 0.0035103566302352824\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005881836513290182\n",
      "Training Loss: 0.006017389983753674\n",
      "Training Loss: 0.005910533414571546\n",
      "Validation Loss: 0.003502938545863615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005871512201847509\n",
      "Training Loss: 0.006007344251265749\n",
      "Training Loss: 0.005899606189341284\n",
      "Validation Loss: 0.003495725399220697\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005861612115986645\n",
      "Training Loss: 0.0059976967691909525\n",
      "Training Loss: 0.005889155974145978\n",
      "Validation Loss: 0.003488695589051153\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005852110086707398\n",
      "Training Loss: 0.005988417045446112\n",
      "Training Loss: 0.005879152398556471\n",
      "Validation Loss: 0.003481830147178739\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005842974173137918\n",
      "Training Loss: 0.005979477267246693\n",
      "Training Loss: 0.005869569293572567\n",
      "Validation Loss: 0.0034751089756099736\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.0058341763087082655\n",
      "Training Loss: 0.005970848363358527\n",
      "Training Loss: 0.005860376598429866\n",
      "Validation Loss: 0.0034685168236106884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005825689347693697\n",
      "Training Loss: 0.005962503053015098\n",
      "Training Loss: 0.005851545758196153\n",
      "Validation Loss: 0.00346203366251338\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005817485394654796\n",
      "Training Loss: 0.005954417182947509\n",
      "Training Loss: 0.005843049208633602\n",
      "Validation Loss: 0.0034556437285062384\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005809538693865761\n",
      "Training Loss: 0.005946565270423889\n",
      "Training Loss: 0.0058348585374187675\n",
      "Validation Loss: 0.0034493313797519364\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005801823806250468\n",
      "Training Loss: 0.005938925259397365\n",
      "Training Loss: 0.005826948331086896\n",
      "Validation Loss: 0.003443084814810728\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005794318094849586\n",
      "Training Loss: 0.005931475848774426\n",
      "Training Loss: 0.005819292960804887\n",
      "Validation Loss: 0.003436891037677781\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.0057870005199220035\n",
      "Training Loss: 0.005924198023276404\n",
      "Training Loss: 0.005811870464240201\n",
      "Validation Loss: 0.003430736388519323\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005779849481768906\n",
      "Training Loss: 0.005917074218741618\n",
      "Training Loss: 0.00580465585924685\n",
      "Validation Loss: 0.0034246133170169096\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005772848149063065\n",
      "Training Loss: 0.005910088169621304\n",
      "Training Loss: 0.005797630744054914\n",
      "Validation Loss: 0.0034185143565664882\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005765979844145477\n",
      "Training Loss: 0.005903225684305653\n",
      "Training Loss: 0.005790775536443107\n",
      "Validation Loss: 0.0034124299650357807\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005759228368988261\n",
      "Training Loss: 0.005896473568864167\n",
      "Training Loss: 0.005784072625683621\n",
      "Validation Loss: 0.00340635547273154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.00575258148368448\n",
      "Training Loss: 0.005889820536249317\n",
      "Training Loss: 0.005777506537269801\n",
      "Validation Loss: 0.003400286454890509\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005746025278931484\n",
      "Training Loss: 0.005883255744120106\n",
      "Training Loss: 0.00577106221287977\n",
      "Validation Loss: 0.0033942160276392622\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005739549691788852\n",
      "Training Loss: 0.00587677008472383\n",
      "Training Loss: 0.005764726936467924\n",
      "Validation Loss: 0.0033881464800431154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005733146606944501\n",
      "Training Loss: 0.005870354909566231\n",
      "Training Loss: 0.005758489274885505\n",
      "Validation Loss: 0.0033820702359498886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.00572680528042838\n",
      "Training Loss: 0.005864004522445612\n",
      "Training Loss: 0.00575233934505377\n",
      "Validation Loss: 0.0033759896933190066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005720519826281816\n",
      "Training Loss: 0.005857711636926979\n",
      "Training Loss: 0.005746266915812157\n",
      "Validation Loss: 0.0033698999218307853\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005714282140834257\n",
      "Training Loss: 0.005851470257621259\n",
      "Training Loss: 0.005740264758933336\n",
      "Validation Loss: 0.0033638038894219136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005708088560495526\n",
      "Training Loss: 0.005845276708714664\n",
      "Training Loss: 0.0057343239651527255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [20:55<13:57, 209.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.003357702456453941\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.14474970284849406\n",
      "Training Loss: 0.11537657858803868\n",
      "Training Loss: 0.08901927284896374\n",
      "Validation Loss: 0.07859949633646547\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.07490004677325487\n",
      "Training Loss: 0.07320536620914936\n",
      "Training Loss: 0.07082241892814636\n",
      "Validation Loss: 0.06999522642222013\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06853696726262569\n",
      "Training Loss: 0.06752120630815625\n",
      "Training Loss: 0.0655137419141829\n",
      "Validation Loss: 0.06439042719227545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.06309849020093679\n",
      "Training Loss: 0.06193971975706518\n",
      "Training Loss: 0.05993739474564791\n",
      "Validation Loss: 0.05826587910146526\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.05710138272494078\n",
      "Training Loss: 0.05559478835202754\n",
      "Training Loss: 0.05344319382682443\n",
      "Validation Loss: 0.051224830089492746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.05016360393725336\n",
      "Training Loss: 0.04829808668233454\n",
      "Training Loss: 0.046006709653884176\n",
      "Validation Loss: 0.04349412901975801\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.042513607246801255\n",
      "Training Loss: 0.0404343892633915\n",
      "Training Loss: 0.0381187947280705\n",
      "Validation Loss: 0.035639895209937954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.034788856180384756\n",
      "Training Loss: 0.032765369340777395\n",
      "Training Loss: 0.030625692466273904\n",
      "Validation Loss: 0.028493799520342537\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.027883093520067632\n",
      "Training Loss: 0.026195113440044226\n",
      "Training Loss: 0.02438107233028859\n",
      "Validation Loss: 0.02272919842743137\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.02240981358103454\n",
      "Training Loss: 0.02118582867551595\n",
      "Training Loss: 0.01970640951767564\n",
      "Validation Loss: 0.018470148461755743\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.01842259251046926\n",
      "Training Loss: 0.017678601420484483\n",
      "Training Loss: 0.01651299307588488\n",
      "Validation Loss: 0.015534353617767102\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.015743504522833972\n",
      "Training Loss: 0.015382154784165323\n",
      "Training Loss: 0.014488933689426631\n",
      "Validation Loss: 0.013490526735510551\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.013959075773600489\n",
      "Training Loss: 0.013770453368779271\n",
      "Training Loss: 0.013082789690233767\n",
      "Validation Loss: 0.011920983489770233\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.012642877229955047\n",
      "Training Loss: 0.012533455854281783\n",
      "Training Loss: 0.012000021140556783\n",
      "Validation Loss: 0.010672886301292463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.011615761978318914\n",
      "Training Loss: 0.011562858975958079\n",
      "Training Loss: 0.011133920091670007\n",
      "Validation Loss: 0.009668924990947243\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.010798880654620006\n",
      "Training Loss: 0.010798299813177436\n",
      "Training Loss: 0.010438037945423275\n",
      "Validation Loss: 0.008865969467468643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.010150996906450018\n",
      "Training Loss: 0.010199414563830942\n",
      "Training Loss: 0.009884059369796886\n",
      "Validation Loss: 0.008230748841423918\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.009642700105905533\n",
      "Training Loss: 0.009733953922986984\n",
      "Training Loss: 0.009448772361502052\n",
      "Validation Loss: 0.007732395422801877\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.009248555224621669\n",
      "Training Loss: 0.009374294804874807\n",
      "Training Loss: 0.00911077679367736\n",
      "Validation Loss: 0.007342215219324225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.008945239452878014\n",
      "Training Loss: 0.009096591116394848\n",
      "Training Loss: 0.008849997809156775\n",
      "Validation Loss: 0.007034662781346045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008711545909754932\n",
      "Training Loss: 0.008880567604210228\n",
      "Training Loss: 0.00864814940141514\n",
      "Validation Loss: 0.006788058008925383\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.008529075375990942\n",
      "Training Loss: 0.008709593054372817\n",
      "Training Loss: 0.008489558650180697\n",
      "Validation Loss: 0.006585026621619721\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008382991426624358\n",
      "Training Loss: 0.00857081272173673\n",
      "Training Loss: 0.008361863533500581\n",
      "Validation Loss: 0.00641261117030563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008262334930477663\n",
      "Training Loss: 0.008454976284410805\n",
      "Training Loss: 0.008256089555798098\n",
      "Validation Loss: 0.0062618607856605325\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.008159641589736566\n",
      "Training Loss: 0.00835585909895599\n",
      "Training Loss: 0.008166171375196428\n",
      "Validation Loss: 0.006127013139468566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008070150093408302\n",
      "Training Loss: 0.008269466281635687\n",
      "Training Loss: 0.008088192295981572\n",
      "Validation Loss: 0.006004576908759354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.007990942661417649\n",
      "Training Loss: 0.008193280234700069\n",
      "Training Loss: 0.008019661249127239\n",
      "Validation Loss: 0.005892504570388308\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007920228082221002\n",
      "Training Loss: 0.008125666179694235\n",
      "Training Loss: 0.007958947563311084\n",
      "Validation Loss: 0.005789586998197888\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.007856846440117806\n",
      "Training Loss: 0.008065477703930811\n",
      "Training Loss: 0.007904904620954768\n",
      "Validation Loss: 0.005695055293840136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.007799963488942012\n",
      "Training Loss: 0.008011824096320197\n",
      "Training Loss: 0.007856649491004645\n",
      "Validation Loss: 0.005608346890082604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007748899657744914\n",
      "Training Loss: 0.00796394336502999\n",
      "Training Loss: 0.007813442314509303\n",
      "Validation Loss: 0.005528973157810612\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007703052016440779\n",
      "Training Loss: 0.007921147076413036\n",
      "Training Loss: 0.007774630272760988\n",
      "Validation Loss: 0.005456465042688036\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007661855178885162\n",
      "Training Loss: 0.007882799234939739\n",
      "Training Loss: 0.0077396250155288724\n",
      "Validation Loss: 0.005390341168667158\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.00762477740412578\n",
      "Training Loss: 0.007848315594019368\n",
      "Training Loss: 0.007707897392101586\n",
      "Validation Loss: 0.0053301256979302915\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007591324767563492\n",
      "Training Loss: 0.007817167008761316\n",
      "Training Loss: 0.007678976462921127\n",
      "Validation Loss: 0.005275330336220311\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.007561041848966852\n",
      "Training Loss: 0.007788879970321432\n",
      "Training Loss: 0.007652447770815343\n",
      "Validation Loss: 0.005225478674107221\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007533517953706905\n",
      "Training Loss: 0.007763039867859334\n",
      "Training Loss: 0.007627955938223749\n",
      "Validation Loss: 0.005180114312516002\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007508391288574785\n",
      "Training Loss: 0.00773929052753374\n",
      "Training Loss: 0.007605198642704636\n",
      "Validation Loss: 0.005138803201497354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.007485344149172306\n",
      "Training Loss: 0.007717329388251528\n",
      "Training Loss: 0.007583923840429634\n",
      "Validation Loss: 0.005101149738243038\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.00746410530176945\n",
      "Training Loss: 0.007696901357267052\n",
      "Training Loss: 0.007563921643886715\n",
      "Validation Loss: 0.0050667760935559705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007444441189290956\n",
      "Training Loss: 0.007677795855561271\n",
      "Training Loss: 0.007545021113473922\n",
      "Validation Loss: 0.005035347328782918\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007426151983672753\n",
      "Training Loss: 0.007659835832891986\n",
      "Training Loss: 0.0075270824506878855\n",
      "Validation Loss: 0.005006568553449398\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.00740907289669849\n",
      "Training Loss: 0.007642877727048472\n",
      "Training Loss: 0.007509991225088015\n",
      "Validation Loss: 0.0049801596278178225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007393062026239932\n",
      "Training Loss: 0.007626801909646019\n",
      "Training Loss: 0.007493654357967898\n",
      "Validation Loss: 0.004955880130703948\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007377999318996444\n",
      "Training Loss: 0.007611508612753824\n",
      "Training Loss: 0.00747799487086013\n",
      "Validation Loss: 0.004933510235971196\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007363782688044012\n",
      "Training Loss: 0.007596915735630319\n",
      "Training Loss: 0.007462951565394178\n",
      "Validation Loss: 0.0049128617150865994\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.00735032630385831\n",
      "Training Loss: 0.007582955413963646\n",
      "Training Loss: 0.0074484714260324835\n",
      "Validation Loss: 0.004893762956823359\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007337555692065507\n",
      "Training Loss: 0.007569568355102092\n",
      "Training Loss: 0.007434510121820495\n",
      "Validation Loss: 0.004876059333511283\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.0073254073213320225\n",
      "Training Loss: 0.00755670627229847\n",
      "Training Loss: 0.007421031472040341\n",
      "Validation Loss: 0.004859617407030813\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.00731382503407076\n",
      "Training Loss: 0.007544327036011964\n",
      "Training Loss: 0.007408002601005137\n",
      "Validation Loss: 0.004844322285923605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007302761634346098\n",
      "Training Loss: 0.007532393459696323\n",
      "Training Loss: 0.00739539603702724\n",
      "Validation Loss: 0.004830064424679855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007292172374436631\n",
      "Training Loss: 0.007520874321926385\n",
      "Training Loss: 0.007383186041843146\n",
      "Validation Loss: 0.004816746949401339\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00728202048339881\n",
      "Training Loss: 0.00750974043039605\n",
      "Training Loss: 0.007371351298643276\n",
      "Validation Loss: 0.004804289537440106\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007272272637346759\n",
      "Training Loss: 0.007498967568390072\n",
      "Training Loss: 0.0073598711739759895\n",
      "Validation Loss: 0.004792614119896519\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0072628967941273\n",
      "Training Loss: 0.00748853350058198\n",
      "Training Loss: 0.007348727498902008\n",
      "Validation Loss: 0.0047816531693437294\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007253867400577292\n",
      "Training Loss: 0.007478416790254414\n",
      "Training Loss: 0.0073379040625877676\n",
      "Validation Loss: 0.004771348932663795\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007245158707955852\n",
      "Training Loss: 0.007468599785352126\n",
      "Training Loss: 0.007327383717056364\n",
      "Validation Loss: 0.0047616437548415715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007236747562419623\n",
      "Training Loss: 0.007459064567228779\n",
      "Training Loss: 0.007317152802133933\n",
      "Validation Loss: 0.0047524865550575045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007228613557526842\n",
      "Training Loss: 0.007449794670101255\n",
      "Training Loss: 0.007307196310721338\n",
      "Validation Loss: 0.004743834716147545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007220737491734326\n",
      "Training Loss: 0.0074407772149425\n",
      "Training Loss: 0.007297502249712124\n",
      "Validation Loss: 0.004735649105559072\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007213101993547752\n",
      "Training Loss: 0.007431997373932972\n",
      "Training Loss: 0.007288058378035203\n",
      "Validation Loss: 0.004727889936066787\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007205690735718235\n",
      "Training Loss: 0.007423443815205246\n",
      "Training Loss: 0.007278852919116616\n",
      "Validation Loss: 0.0047205227441573\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007198489404981956\n",
      "Training Loss: 0.007415103865787387\n",
      "Training Loss: 0.007269875003257767\n",
      "Validation Loss: 0.00471351875431752\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007191483388887718\n",
      "Training Loss: 0.0074069664545822885\n",
      "Training Loss: 0.0072611134580802175\n",
      "Validation Loss: 0.004706848765922229\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0071846598933916535\n",
      "Training Loss: 0.00739902141271159\n",
      "Training Loss: 0.007252559637418016\n",
      "Validation Loss: 0.004700490403030947\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007178008070914075\n",
      "Training Loss: 0.007391259317519144\n",
      "Training Loss: 0.007244202987058088\n",
      "Validation Loss: 0.004694411956856885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007171515766531229\n",
      "Training Loss: 0.007383670747512952\n",
      "Training Loss: 0.00723603532765992\n",
      "Validation Loss: 0.004688598572020157\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0071651729894801975\n",
      "Training Loss: 0.007376246969215572\n",
      "Training Loss: 0.007228047031676396\n",
      "Validation Loss: 0.0046830259569596204\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007158969742013141\n",
      "Training Loss: 0.007368979521561414\n",
      "Training Loss: 0.007220230643870309\n",
      "Validation Loss: 0.004677675511384529\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007152898265048862\n",
      "Training Loss: 0.007361861462704837\n",
      "Training Loss: 0.007212578252656385\n",
      "Validation Loss: 0.004672534452500135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007146950258174911\n",
      "Training Loss: 0.007354885950917378\n",
      "Training Loss: 0.007205081685679033\n",
      "Validation Loss: 0.004667581361849279\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.0071411170077044514\n",
      "Training Loss: 0.007348043869715184\n",
      "Training Loss: 0.007197734049987048\n",
      "Validation Loss: 0.004662803412817963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007135391958290711\n",
      "Training Loss: 0.007341330881463364\n",
      "Training Loss: 0.007190528992796316\n",
      "Validation Loss: 0.004658191271905861\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007129768647719175\n",
      "Training Loss: 0.0073347402329090984\n",
      "Training Loss: 0.007183459074003622\n",
      "Validation Loss: 0.004653725657334781\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007124240816337988\n",
      "Training Loss: 0.007328264809912071\n",
      "Training Loss: 0.007176518030464649\n",
      "Validation Loss: 0.004649396343522945\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007118801731849089\n",
      "Training Loss: 0.007321900309761986\n",
      "Training Loss: 0.007169699858641252\n",
      "Validation Loss: 0.00464519503751372\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.007113446720177307\n",
      "Training Loss: 0.00731564118876122\n",
      "Training Loss: 0.007162998692365363\n",
      "Validation Loss: 0.004641107829638202\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00710817068698816\n",
      "Training Loss: 0.00730948165175505\n",
      "Training Loss: 0.007156407898291945\n",
      "Validation Loss: 0.004637129416459062\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.0071029694296885285\n",
      "Training Loss: 0.0073034174868371335\n",
      "Training Loss: 0.007149922985117883\n",
      "Validation Loss: 0.0046332451544984585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007097836515167728\n",
      "Training Loss: 0.007297443673014641\n",
      "Training Loss: 0.007143538477830589\n",
      "Validation Loss: 0.004629454823352103\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007092770041199401\n",
      "Training Loss: 0.007291554912226275\n",
      "Training Loss: 0.0071372487128246575\n",
      "Validation Loss: 0.004625742827290983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.007087763905292377\n",
      "Training Loss: 0.007285748170688749\n",
      "Training Loss: 0.007131048992741853\n",
      "Validation Loss: 0.004622109021122954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.007082816130714491\n",
      "Training Loss: 0.007280018816236407\n",
      "Training Loss: 0.007124933715676889\n",
      "Validation Loss: 0.0046185416043893\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007077922027092427\n",
      "Training Loss: 0.007274362606694922\n",
      "Training Loss: 0.0071188996417913585\n",
      "Validation Loss: 0.0046150364711608615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.00707307887962088\n",
      "Training Loss: 0.007268776573473587\n",
      "Training Loss: 0.007112941141240298\n",
      "Validation Loss: 0.004611589257825124\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007068284042179585\n",
      "Training Loss: 0.007263255961006508\n",
      "Training Loss: 0.007107054963707924\n",
      "Validation Loss: 0.00460819347324687\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.00706353502930142\n",
      "Training Loss: 0.007257798603968695\n",
      "Training Loss: 0.007101236326852813\n",
      "Validation Loss: 0.004604844532101258\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007058827920118347\n",
      "Training Loss: 0.007252400800352916\n",
      "Training Loss: 0.007095481881406158\n",
      "Validation Loss: 0.0046015406796562205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.0070541612047236414\n",
      "Training Loss: 0.007247059268411249\n",
      "Training Loss: 0.007089787275763228\n",
      "Validation Loss: 0.004598273715171753\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007049532341770828\n",
      "Training Loss: 0.0072417717264033855\n",
      "Training Loss: 0.00708414880442433\n",
      "Validation Loss: 0.0045950427092873395\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007044939697952941\n",
      "Training Loss: 0.007236534286057576\n",
      "Training Loss: 0.007078564439434558\n",
      "Validation Loss: 0.0045918452061629025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007040381077677012\n",
      "Training Loss: 0.007231346233747899\n",
      "Training Loss: 0.0070730299432761965\n",
      "Validation Loss: 0.00458867651975473\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007035854823188856\n",
      "Training Loss: 0.007226203398313373\n",
      "Training Loss: 0.007067542857257649\n",
      "Validation Loss: 0.00458553544339839\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.0070313594327308234\n",
      "Training Loss: 0.0072211045946460216\n",
      "Training Loss: 0.007062100337352603\n",
      "Validation Loss: 0.004582421677553335\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007026893747970462\n",
      "Training Loss: 0.007216047635301948\n",
      "Training Loss: 0.00705669927992858\n",
      "Validation Loss: 0.004579328618593137\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007022456076228991\n",
      "Training Loss: 0.007211030364269391\n",
      "Training Loss: 0.007051338693127036\n",
      "Validation Loss: 0.004576260099721173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007018046110169962\n",
      "Training Loss: 0.007206050872337073\n",
      "Training Loss: 0.007046014884253964\n",
      "Validation Loss: 0.004573212003889965\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007013662795070558\n",
      "Training Loss: 0.007201108825393021\n",
      "Training Loss: 0.00704072771477513\n",
      "Validation Loss: 0.004570184068184011\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.00700930472346954\n",
      "Training Loss: 0.007196201721671969\n",
      "Training Loss: 0.007035474308067933\n",
      "Validation Loss: 0.004567174737346934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.007004972029244527\n",
      "Training Loss: 0.007191328946501017\n",
      "Training Loss: 0.007030254349811003\n",
      "Validation Loss: 0.004564185038842051\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.007000664573861286\n",
      "Training Loss: 0.007186489689629525\n",
      "Training Loss: 0.007025065558264032\n",
      "Validation Loss: 0.004561213696678954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006996381185017526\n",
      "Training Loss: 0.007181682568043471\n",
      "Training Loss: 0.007019907722715288\n",
      "Validation Loss: 0.004558262146428604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006992122726514936\n",
      "Training Loss: 0.007176907205721363\n",
      "Training Loss: 0.007014779801247642\n",
      "Validation Loss: 0.004555328614392391\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006987888612784445\n",
      "Training Loss: 0.007172162936767563\n",
      "Training Loss: 0.007009679934708401\n",
      "Validation Loss: 0.004552412930525463\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0069836788531392816\n",
      "Training Loss: 0.007167449360713363\n",
      "Training Loss: 0.007004610263975337\n",
      "Validation Loss: 0.004549518480028413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006979493912076577\n",
      "Training Loss: 0.0071627660281956195\n",
      "Training Loss: 0.006999567620223388\n",
      "Validation Loss: 0.004546642725308834\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006975333805894479\n",
      "Training Loss: 0.007158111752942204\n",
      "Training Loss: 0.00699455345980823\n",
      "Validation Loss: 0.00454378624582726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.00697119836579077\n",
      "Training Loss: 0.0071534876769874245\n",
      "Training Loss: 0.006989567034179345\n",
      "Validation Loss: 0.004540952051377572\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006967088271630928\n",
      "Training Loss: 0.007148892796831206\n",
      "Training Loss: 0.006984608200145886\n",
      "Validation Loss: 0.004538137937916882\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006963003595592454\n",
      "Training Loss: 0.00714432651293464\n",
      "Training Loss: 0.006979676916962489\n",
      "Validation Loss: 0.004535347705293614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006958944794023409\n",
      "Training Loss: 0.0071397891908418385\n",
      "Training Loss: 0.006974772139219567\n",
      "Validation Loss: 0.0045325779656910994\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006954911377979442\n",
      "Training Loss: 0.0071352798363659535\n",
      "Training Loss: 0.00696989529998973\n",
      "Validation Loss: 0.004529831397315759\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006950903938850388\n",
      "Training Loss: 0.007130799024598673\n",
      "Training Loss: 0.0069650449242908506\n",
      "Validation Loss: 0.004527105221896317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006946922269416973\n",
      "Training Loss: 0.00712634610128589\n",
      "Training Loss: 0.006960221956251189\n",
      "Validation Loss: 0.004524403653275012\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006942967229988426\n",
      "Training Loss: 0.0071219202782958745\n",
      "Training Loss: 0.00695542513160035\n",
      "Validation Loss: 0.004521724076031216\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006939037857810035\n",
      "Training Loss: 0.007117521655745804\n",
      "Training Loss: 0.006950655024265871\n",
      "Validation Loss: 0.0045190665228658595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006935133680235595\n",
      "Training Loss: 0.007113148916978389\n",
      "Training Loss: 0.006945910757640377\n",
      "Validation Loss: 0.004516429278287995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006931254812516272\n",
      "Training Loss: 0.007108801519498229\n",
      "Training Loss: 0.006941192485392093\n",
      "Validation Loss: 0.004513813288662624\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006927400503773242\n",
      "Training Loss: 0.0071044792304746805\n",
      "Training Loss: 0.006936499566072598\n",
      "Validation Loss: 0.004511218389831065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006923571194056422\n",
      "Training Loss: 0.007100181285059079\n",
      "Training Loss: 0.006931830110261217\n",
      "Validation Loss: 0.004508644169761559\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006919764241902157\n",
      "Training Loss: 0.0070959058497101065\n",
      "Training Loss: 0.006927184866508469\n",
      "Validation Loss: 0.0045060865728843835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006915980378398672\n",
      "Training Loss: 0.007091652769595384\n",
      "Training Loss: 0.006922562044346705\n",
      "Validation Loss: 0.004503546215939137\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006912218360230327\n",
      "Training Loss: 0.007087419831659645\n",
      "Training Loss: 0.00691796111408621\n",
      "Validation Loss: 0.004501022852360784\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006908477020915598\n",
      "Training Loss: 0.00708320680540055\n",
      "Training Loss: 0.006913379353936762\n",
      "Validation Loss: 0.004498512647637909\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006904754617717117\n",
      "Training Loss: 0.0070790118305012585\n",
      "Training Loss: 0.006908818345982582\n",
      "Validation Loss: 0.0044960154062189325\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006901050269370899\n",
      "Training Loss: 0.007074833738151938\n",
      "Training Loss: 0.006904275030829013\n",
      "Validation Loss: 0.004493528650027145\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006897362894378602\n",
      "Training Loss: 0.00707066991366446\n",
      "Training Loss: 0.0068997483304701745\n",
      "Validation Loss: 0.004491053138378213\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006893690412398428\n",
      "Training Loss: 0.007066520312801004\n",
      "Training Loss: 0.00689523653476499\n",
      "Validation Loss: 0.0044885838176699335\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006890031130751595\n",
      "Training Loss: 0.007062381451250985\n",
      "Training Loss: 0.006890736463246867\n",
      "Validation Loss: 0.004486119795166835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006886383442906663\n",
      "Training Loss: 0.0070582526514772325\n",
      "Training Loss: 0.006886249590897933\n",
      "Validation Loss: 0.004483658038184382\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006882745090406388\n",
      "Training Loss: 0.007054131148615852\n",
      "Training Loss: 0.006881771378684789\n",
      "Validation Loss: 0.00448119762455626\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006879114886978641\n",
      "Training Loss: 0.007050015052082017\n",
      "Training Loss: 0.006877301029162481\n",
      "Validation Loss: 0.0044787346877241405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006875490237725899\n",
      "Training Loss: 0.007045902349054813\n",
      "Training Loss: 0.006872835074318573\n",
      "Validation Loss: 0.004476264896122425\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006871868912130594\n",
      "Training Loss: 0.007041791091905907\n",
      "Training Loss: 0.006868372990866191\n",
      "Validation Loss: 0.004473789892645992\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006868248610990122\n",
      "Training Loss: 0.007037678234046325\n",
      "Training Loss: 0.006863911020336673\n",
      "Validation Loss: 0.004471305619109045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006864626868627965\n",
      "Training Loss: 0.007033561636926606\n",
      "Training Loss: 0.006859447311144322\n",
      "Validation Loss: 0.004468807380965628\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006861001185607165\n",
      "Training Loss: 0.007029438964091242\n",
      "Training Loss: 0.006854979461641051\n",
      "Validation Loss: 0.004466292723029685\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006857368773780763\n",
      "Training Loss: 0.007025306441355497\n",
      "Training Loss: 0.006850503629539162\n",
      "Validation Loss: 0.004463760443868932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006853727306006477\n",
      "Training Loss: 0.007021162289893254\n",
      "Training Loss: 0.006846017698408104\n",
      "Validation Loss: 0.004461202319198696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006850073125679046\n",
      "Training Loss: 0.007017003135988489\n",
      "Training Loss: 0.006841518376022577\n",
      "Validation Loss: 0.004458621038343716\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006846404603566043\n",
      "Training Loss: 0.007012826325371861\n",
      "Training Loss: 0.006837003505206667\n",
      "Validation Loss: 0.004456012653647347\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006842718009720556\n",
      "Training Loss: 0.00700862823985517\n",
      "Training Loss: 0.006832468381617218\n",
      "Validation Loss: 0.004453367059213224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006839009370305575\n",
      "Training Loss: 0.007004405584884807\n",
      "Training Loss: 0.006827909571584314\n",
      "Validation Loss: 0.0044506882530574296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006835275756311603\n",
      "Training Loss: 0.007000154991401359\n",
      "Training Loss: 0.006823324423166923\n",
      "Validation Loss: 0.004447968133850798\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.0068315135082229975\n",
      "Training Loss: 0.006995871891267598\n",
      "Training Loss: 0.0068187073001172395\n",
      "Validation Loss: 0.004445202324900441\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0068277188763022426\n",
      "Training Loss: 0.006991552967811003\n",
      "Training Loss: 0.006814054973074235\n",
      "Validation Loss: 0.004442387039438308\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006823887545033358\n",
      "Training Loss: 0.006987193872919306\n",
      "Training Loss: 0.006809362739440985\n",
      "Validation Loss: 0.00443951643119158\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006820015562698245\n",
      "Training Loss: 0.006982790466863662\n",
      "Training Loss: 0.00680462654796429\n",
      "Validation Loss: 0.004436586850082104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006816098217386752\n",
      "Training Loss: 0.006978336486499756\n",
      "Training Loss: 0.006799839895102196\n",
      "Validation Loss: 0.004433591728454477\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006812130009057\n",
      "Training Loss: 0.006973827928304672\n",
      "Training Loss: 0.00679499875579495\n",
      "Validation Loss: 0.00443052352351147\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.00680810623394791\n",
      "Training Loss: 0.006969259126344695\n",
      "Training Loss: 0.006790096035110764\n",
      "Validation Loss: 0.004427378304600925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006804020788986236\n",
      "Training Loss: 0.00696462263353169\n",
      "Training Loss: 0.006785125490278005\n",
      "Validation Loss: 0.004424148309829362\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006799867518129759\n",
      "Training Loss: 0.0069599128398112955\n",
      "Training Loss: 0.006780081009492278\n",
      "Validation Loss: 0.004420822076865712\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006795640031341464\n",
      "Training Loss: 0.0069551227591000495\n",
      "Training Loss: 0.0067749554774491115\n",
      "Validation Loss: 0.004417393486057356\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006791329385014251\n",
      "Training Loss: 0.006950243655592203\n",
      "Training Loss: 0.006769739213632419\n",
      "Validation Loss: 0.0044138499913643\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.0067869286274071785\n",
      "Training Loss: 0.006945267552509904\n",
      "Training Loss: 0.0067644244822440665\n",
      "Validation Loss: 0.00441018152155317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006782427617581561\n",
      "Training Loss: 0.006940183787373826\n",
      "Training Loss: 0.006759000541060232\n",
      "Validation Loss: 0.0044063747509937275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006777816926478409\n",
      "Training Loss: 0.0069349824055097995\n",
      "Training Loss: 0.006753458483726718\n",
      "Validation Loss: 0.004402415001545143\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006773084609303624\n",
      "Training Loss: 0.006929650959791616\n",
      "Training Loss: 0.006747785835177638\n",
      "Validation Loss: 0.0043982875048120115\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006768218412180432\n",
      "Training Loss: 0.006924177359323948\n",
      "Training Loss: 0.006741970311850309\n",
      "Validation Loss: 0.00439397402086787\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006763205485185608\n",
      "Training Loss: 0.0069185459718573835\n",
      "Training Loss: 0.006735998311196454\n",
      "Validation Loss: 0.004389452107062333\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006758028728654608\n",
      "Training Loss: 0.006912742072017863\n",
      "Training Loss: 0.00672985503973905\n",
      "Validation Loss: 0.004384700533295615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006752672681468539\n",
      "Training Loss: 0.006906749499030411\n",
      "Training Loss: 0.006723525797133334\n",
      "Validation Loss: 0.0043796969295134035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006747119801002554\n",
      "Training Loss: 0.006900548993144184\n",
      "Training Loss: 0.006716993701411411\n",
      "Validation Loss: 0.00437441299894427\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.00674134990607854\n",
      "Training Loss: 0.006894122035009787\n",
      "Training Loss: 0.006710243114503101\n",
      "Validation Loss: 0.004368820198905769\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006735343804466538\n",
      "Training Loss: 0.006887450281064958\n",
      "Training Loss: 0.00670325591461733\n",
      "Validation Loss: 0.004362892250928065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006729080689838156\n",
      "Training Loss: 0.006880513506475836\n",
      "Training Loss: 0.006696016419446096\n",
      "Validation Loss: 0.004356594110729194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006722538149333559\n",
      "Training Loss: 0.0068732932140119374\n",
      "Training Loss: 0.006688508014776744\n",
      "Validation Loss: 0.004349900557387495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.00671569628699217\n",
      "Training Loss: 0.006865770978620276\n",
      "Training Loss: 0.006680715012480505\n",
      "Validation Loss: 0.0043427802928399\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006708533248165623\n",
      "Training Loss: 0.006857928074896336\n",
      "Training Loss: 0.006672622413607315\n",
      "Validation Loss: 0.004335202376771551\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006701027592062019\n",
      "Training Loss: 0.006849747794913128\n",
      "Training Loss: 0.006664215032360516\n",
      "Validation Loss: 0.004327136989201555\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006693158353446051\n",
      "Training Loss: 0.00684121299884282\n",
      "Training Loss: 0.006655478171887807\n",
      "Validation Loss: 0.004318557522688689\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.0066849027742864565\n",
      "Training Loss: 0.0068323059275280686\n",
      "Training Loss: 0.006646396078285761\n",
      "Validation Loss: 0.004309431247523033\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006676237977226265\n",
      "Training Loss: 0.00682300683693029\n",
      "Training Loss: 0.006636951976688578\n",
      "Validation Loss: 0.004299731331280946\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006667139385244809\n",
      "Training Loss: 0.006813296877080575\n",
      "Training Loss: 0.006627125641098246\n",
      "Validation Loss: 0.004289422759111301\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006657576928846538\n",
      "Training Loss: 0.00680315064266324\n",
      "Training Loss: 0.006616897062631324\n",
      "Validation Loss: 0.004278472740480446\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006647520598489791\n",
      "Training Loss: 0.006792542260373011\n",
      "Training Loss: 0.006606240193359553\n",
      "Validation Loss: 0.004266833336474479\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006636932104011066\n",
      "Training Loss: 0.006781440026825294\n",
      "Training Loss: 0.006595125959720462\n",
      "Validation Loss: 0.004254465476018533\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006625769517850131\n",
      "Training Loss: 0.006769807453965768\n",
      "Training Loss: 0.006583523028530181\n",
      "Validation Loss: 0.004241308961272909\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.00661398540774826\n",
      "Training Loss: 0.006757604143349454\n",
      "Training Loss: 0.006571393824415282\n",
      "Validation Loss: 0.004227304904397284\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.00660152428143192\n",
      "Training Loss: 0.006744782323949039\n",
      "Training Loss: 0.006558696852298453\n",
      "Validation Loss: 0.004212382687726633\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006588321119779721\n",
      "Training Loss: 0.006731286009307951\n",
      "Training Loss: 0.006545381515752524\n",
      "Validation Loss: 0.004196463584418545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006574302579392679\n",
      "Training Loss: 0.0067170511686708775\n",
      "Training Loss: 0.006531393915647641\n",
      "Validation Loss: 0.004179449926596135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006559383416897617\n",
      "Training Loss: 0.006702004699036479\n",
      "Training Loss: 0.006516669293632731\n",
      "Validation Loss: 0.004161234461085013\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.00654346400348004\n",
      "Training Loss: 0.0066860600316431375\n",
      "Training Loss: 0.006501132453558966\n",
      "Validation Loss: 0.004141697841251709\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006526429111254402\n",
      "Training Loss: 0.006669118406716734\n",
      "Training Loss: 0.006484699395950883\n",
      "Validation Loss: 0.00412069745047876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006508148206630721\n",
      "Training Loss: 0.006651067322818563\n",
      "Training Loss: 0.006467273415764794\n",
      "Validation Loss: 0.004098084498212536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006488472323981114\n",
      "Training Loss: 0.006631777434377\n",
      "Training Loss: 0.006448744698427617\n",
      "Validation Loss: 0.0040736870324825135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006467231608112343\n",
      "Training Loss: 0.00661110682412982\n",
      "Training Loss: 0.006428990751737728\n",
      "Validation Loss: 0.004047338274605782\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006444245147868059\n",
      "Training Loss: 0.006588903807569295\n",
      "Training Loss: 0.006407887251116335\n",
      "Validation Loss: 0.004018880023354183\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006419330257922411\n",
      "Training Loss: 0.006565022885333747\n",
      "Training Loss: 0.006385312669444829\n",
      "Validation Loss: 0.003988189505690479\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006392316782148555\n",
      "Training Loss: 0.006539336404530332\n",
      "Training Loss: 0.006361166475107894\n",
      "Validation Loss: 0.003955218014228838\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.0063630805700086055\n",
      "Training Loss: 0.006511770726647228\n",
      "Training Loss: 0.006335393068147823\n",
      "Validation Loss: 0.003920052195977671\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.0063315890025114644\n",
      "Training Loss: 0.006482345564290881\n",
      "Training Loss: 0.006308023635065183\n",
      "Validation Loss: 0.003882973914667755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.0062979552487377076\n",
      "Training Loss: 0.0064512203144840895\n",
      "Training Loss: 0.006279205837054178\n",
      "Validation Loss: 0.0038445235989998314\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006262498296564445\n",
      "Training Loss: 0.006418734901817515\n",
      "Training Loss: 0.006249230495886877\n",
      "Validation Loss: 0.003805509786738941\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006225760401575826\n",
      "Training Loss: 0.006385411486262455\n",
      "Training Loss: 0.0062185264215804635\n",
      "Validation Loss: 0.003766958097524397\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006188487848849036\n",
      "Training Loss: 0.0063519066094886514\n",
      "Training Loss: 0.006187609793851152\n",
      "Validation Loss: 0.003729943398600758\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006151521083083935\n",
      "Training Loss: 0.00631890346412547\n",
      "Training Loss: 0.00615699625806883\n",
      "Validation Loss: 0.003695380101582098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0061156468786066395\n",
      "Training Loss: 0.006286992724053562\n",
      "Training Loss: 0.006127125428756699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [24:24<10:28, 209.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0036638340326758585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.35781688429415226\n",
      "Training Loss: 0.2847414759546518\n",
      "Training Loss: 0.2081135391071439\n",
      "Validation Loss: 0.14658499730939276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.11533252825960517\n",
      "Training Loss: 0.08152299644425512\n",
      "Training Loss: 0.06504999289289116\n",
      "Validation Loss: 0.06343265666804287\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.0606618938408792\n",
      "Training Loss: 0.06021534016355872\n",
      "Training Loss: 0.057026184909045693\n",
      "Validation Loss: 0.057039280432496176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.054701414685696365\n",
      "Training Loss: 0.05389295492321253\n",
      "Training Loss: 0.05066628459841013\n",
      "Validation Loss: 0.04981912191161949\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.04732491278089583\n",
      "Training Loss: 0.04557389213703573\n",
      "Training Loss: 0.04149929059669375\n",
      "Validation Loss: 0.039095764247219215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.036254579927772285\n",
      "Training Loss: 0.03355313193053007\n",
      "Training Loss: 0.02951980567537248\n",
      "Validation Loss: 0.026951575269817972\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.025367324943654238\n",
      "Training Loss: 0.023463357975706457\n",
      "Training Loss: 0.020771809997968377\n",
      "Validation Loss: 0.018709231884836145\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.018533002659678458\n",
      "Training Loss: 0.01752102264901623\n",
      "Training Loss: 0.015919789627660066\n",
      "Validation Loss: 0.014355563694673978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.014993733847513796\n",
      "Training Loss: 0.01456362979952246\n",
      "Training Loss: 0.01355639934190549\n",
      "Validation Loss: 0.012188646230804786\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.01320533206453547\n",
      "Training Loss: 0.013021786257158965\n",
      "Training Loss: 0.01229446508572437\n",
      "Validation Loss: 0.010966106066794207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.012177914308849722\n",
      "Training Loss: 0.012096838839352132\n",
      "Training Loss: 0.01151542306644842\n",
      "Validation Loss: 0.010180727110422227\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.011509085381403565\n",
      "Training Loss: 0.011476279555354267\n",
      "Training Loss: 0.010978407611837611\n",
      "Validation Loss: 0.009627035281390705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.011031650728546083\n",
      "Training Loss: 0.011025584884919226\n",
      "Training Loss: 0.01058010765700601\n",
      "Validation Loss: 0.009212309019797053\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.010668902939651162\n",
      "Training Loss: 0.010679461110848934\n",
      "Training Loss: 0.010269713104935362\n",
      "Validation Loss: 0.008887797306302223\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.010380808006739245\n",
      "Training Loss: 0.010402407922083513\n",
      "Training Loss: 0.010018990624230355\n",
      "Validation Loss: 0.008625300046516939\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.01014464555075392\n",
      "Training Loss: 0.010173916380153969\n",
      "Training Loss: 0.009811203667195513\n",
      "Validation Loss: 0.008407669956795871\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.009946771467803046\n",
      "Training Loss: 0.009981518888380378\n",
      "Training Loss: 0.00963584366487339\n",
      "Validation Loss: 0.008223846382095237\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.009778399916831404\n",
      "Training Loss: 0.00981709395768121\n",
      "Training Loss: 0.009485835140803828\n",
      "Validation Loss: 0.00806619864600721\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.009633409461239353\n",
      "Training Loss: 0.009674953495850786\n",
      "Training Loss: 0.009356096726842225\n",
      "Validation Loss: 0.007929188024064296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.009507279383251443\n",
      "Training Loss: 0.009550888526719063\n",
      "Training Loss: 0.009242807978298516\n",
      "Validation Loss: 0.0078086468725977985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.009396520612062887\n",
      "Training Loss: 0.009441646724008023\n",
      "Training Loss: 0.00914298658957705\n",
      "Validation Loss: 0.007701366459636876\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.009298376778606326\n",
      "Training Loss: 0.009344655515160411\n",
      "Training Loss: 0.00905426020734012\n",
      "Validation Loss: 0.007604842188014659\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.009210640212986619\n",
      "Training Loss: 0.009257841691141949\n",
      "Training Loss: 0.008974717975361273\n",
      "Validation Loss: 0.0075171044255931225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.009131528316065668\n",
      "Training Loss: 0.009179524483624846\n",
      "Training Loss: 0.008902802902739496\n",
      "Validation Loss: 0.0074365924287775765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.00905958927120082\n",
      "Training Loss: 0.009108323508407921\n",
      "Training Loss: 0.008837245923932641\n",
      "Validation Loss: 0.007362060830928385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.00899364216136746\n",
      "Training Loss: 0.009043106165481731\n",
      "Training Loss: 0.00877700203913264\n",
      "Validation Loss: 0.007292506672190816\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008932718535652385\n",
      "Training Loss: 0.008982936828397214\n",
      "Training Loss: 0.008721214156830684\n",
      "Validation Loss: 0.007227122490660528\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.00887601956143044\n",
      "Training Loss: 0.008927039527334273\n",
      "Training Loss: 0.008669175603426994\n",
      "Validation Loss: 0.007165246149995939\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.00882288656779565\n",
      "Training Loss: 0.008874769418034702\n",
      "Training Loss: 0.008620297615416349\n",
      "Validation Loss: 0.007106334805551372\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.008772771020885558\n",
      "Training Loss: 0.008825583114521578\n",
      "Training Loss: 0.008574089735047892\n",
      "Validation Loss: 0.007049931218491846\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.00872520859586075\n",
      "Training Loss: 0.008779021725058556\n",
      "Training Loss: 0.008530137191992254\n",
      "Validation Loss: 0.00699565447526827\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.008679807060398162\n",
      "Training Loss: 0.008734693162841723\n",
      "Training Loss: 0.008488084694836289\n",
      "Validation Loss: 0.006943167800743008\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.008636227038223296\n",
      "Training Loss: 0.008692253029439599\n",
      "Training Loss: 0.008447623669635505\n",
      "Validation Loss: 0.006892183103619583\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.008594169860007241\n",
      "Training Loss: 0.008651398313231766\n",
      "Training Loss: 0.008408476929180324\n",
      "Validation Loss: 0.006842428624755546\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.008553361609810963\n",
      "Training Loss: 0.008611848156433552\n",
      "Training Loss: 0.008370389932533726\n",
      "Validation Loss: 0.006793657253663777\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.008513550587231293\n",
      "Training Loss: 0.00857334147905931\n",
      "Training Loss: 0.008333121449686587\n",
      "Validation Loss: 0.0067456190217956066\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.008474491715896875\n",
      "Training Loss: 0.008535621761111542\n",
      "Training Loss: 0.008296430972404779\n",
      "Validation Loss: 0.0066980653131129535\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.008435934017179535\n",
      "Training Loss: 0.008498424170538784\n",
      "Training Loss: 0.008260065576760098\n",
      "Validation Loss: 0.0066507233717489275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.008397607040824368\n",
      "Training Loss: 0.008461462301202118\n",
      "Training Loss: 0.008223745031282306\n",
      "Validation Loss: 0.0066032855316285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.008359204115113244\n",
      "Training Loss: 0.00842440958833322\n",
      "Training Loss: 0.008187146827112883\n",
      "Validation Loss: 0.006555392546840849\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.00832036170293577\n",
      "Training Loss: 0.008386877074372023\n",
      "Training Loss: 0.008149875659728422\n",
      "Validation Loss: 0.006506610205240046\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.008280627597123384\n",
      "Training Loss: 0.008348372988402844\n",
      "Training Loss: 0.008111432547448203\n",
      "Validation Loss: 0.006456398231165714\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.00823942266171798\n",
      "Training Loss: 0.00830826960853301\n",
      "Training Loss: 0.008071177785750479\n",
      "Validation Loss: 0.006404096679238791\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.008196010825922712\n",
      "Training Loss: 0.00826575340121053\n",
      "Training Loss: 0.008028300382429734\n",
      "Validation Loss: 0.006348931569386315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.00814949265215546\n",
      "Training Loss: 0.008219832279719412\n",
      "Training Loss: 0.007981867849593982\n",
      "Validation Loss: 0.006290103749665065\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.008098920301999896\n",
      "Training Loss: 0.008169484173413366\n",
      "Training Loss: 0.00793107388773933\n",
      "Validation Loss: 0.006227055089705194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.008043678934918717\n",
      "Training Loss: 0.008114119849633426\n",
      "Training Loss: 0.007875819114269688\n",
      "Validation Loss: 0.006159959835774695\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007984208131674678\n",
      "Training Loss: 0.008054369514575228\n",
      "Training Loss: 0.00781747915665619\n",
      "Validation Loss: 0.006090225211051659\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007922612261027098\n",
      "Training Loss: 0.007992565515451133\n",
      "Training Loss: 0.007758935175370425\n",
      "Validation Loss: 0.006020374045435214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.00786214071791619\n",
      "Training Loss: 0.007931986717740074\n",
      "Training Loss: 0.007703253104118631\n",
      "Validation Loss: 0.005952903012002117\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007805465040728449\n",
      "Training Loss: 0.00787519839592278\n",
      "Training Loss: 0.007652094529476017\n",
      "Validation Loss: 0.005889149446804286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007753508309833706\n",
      "Training Loss: 0.0078231694502756\n",
      "Training Loss: 0.0076055421936325725\n",
      "Validation Loss: 0.005829294845252559\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.0077058413811028\n",
      "Training Loss: 0.007775691482238472\n",
      "Training Loss: 0.007562943325610832\n",
      "Validation Loss: 0.005773021076973318\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007661639052676037\n",
      "Training Loss: 0.007732140385778621\n",
      "Training Loss: 0.007523591774515808\n",
      "Validation Loss: 0.005719967348647587\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007620206363499165\n",
      "Training Loss: 0.0076918882026802745\n",
      "Training Loss: 0.007486936894711107\n",
      "Validation Loss: 0.0056698187385172995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0075810720189474524\n",
      "Training Loss: 0.007654422230552882\n",
      "Training Loss: 0.0074525685491971675\n",
      "Validation Loss: 0.005622308731361637\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007543925089994446\n",
      "Training Loss: 0.007619334915652871\n",
      "Training Loss: 0.007420170035911724\n",
      "Validation Loss: 0.005577191679079211\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007508552676299587\n",
      "Training Loss: 0.007586303201969713\n",
      "Training Loss: 0.00738949025166221\n",
      "Validation Loss: 0.005534257500411503\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007474799503106624\n",
      "Training Loss: 0.007555072642862797\n",
      "Training Loss: 0.00736033076653257\n",
      "Validation Loss: 0.00549332434939367\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007442544088698924\n",
      "Training Loss: 0.00752543976996094\n",
      "Training Loss: 0.007332533446606249\n",
      "Validation Loss: 0.005454243193485178\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007411691157612949\n",
      "Training Loss: 0.007497245664708317\n",
      "Training Loss: 0.007305973069742322\n",
      "Validation Loss: 0.005416891228909908\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007382159130647778\n",
      "Training Loss: 0.007470365040935576\n",
      "Training Loss: 0.007280551998410374\n",
      "Validation Loss: 0.005381181217426581\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007353881596354767\n",
      "Training Loss: 0.007444699441548437\n",
      "Training Loss: 0.007256192824570462\n",
      "Validation Loss: 0.005347029104556763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007326797705609352\n",
      "Training Loss: 0.007420168833341449\n",
      "Training Loss: 0.007232831952860579\n",
      "Validation Loss: 0.0053143783268424566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.007300856444053352\n",
      "Training Loss: 0.007396710310131311\n",
      "Training Loss: 0.007210419059265405\n",
      "Validation Loss: 0.005283166534068544\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007276009371271357\n",
      "Training Loss: 0.00737426761072129\n",
      "Training Loss: 0.007188911457778886\n",
      "Validation Loss: 0.005253350080953639\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.007252213405445218\n",
      "Training Loss: 0.007352794121252373\n",
      "Training Loss: 0.007168272354174405\n",
      "Validation Loss: 0.005224874995261682\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007229427398415283\n",
      "Training Loss: 0.007332246261648833\n",
      "Training Loss: 0.0071484692534431815\n",
      "Validation Loss: 0.0051976983040424715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007207611195044592\n",
      "Training Loss: 0.0073125838954001664\n",
      "Training Loss: 0.007129473405657336\n",
      "Validation Loss: 0.005171773004936829\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0071867249289061874\n",
      "Training Loss: 0.007293766774237156\n",
      "Training Loss: 0.0071112549141980706\n",
      "Validation Loss: 0.005147045389278217\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.00716672952636145\n",
      "Training Loss: 0.007275756181916222\n",
      "Training Loss: 0.007093788107158616\n",
      "Validation Loss: 0.005123469858231504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007147587159415707\n",
      "Training Loss: 0.007258515156572685\n",
      "Training Loss: 0.007077045548940077\n",
      "Validation Loss: 0.005100992827393701\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007129257208434865\n",
      "Training Loss: 0.007242003453429788\n",
      "Training Loss: 0.007060998968081549\n",
      "Validation Loss: 0.005079559136176838\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007111698508961126\n",
      "Training Loss: 0.007226181920850649\n",
      "Training Loss: 0.007045619833515957\n",
      "Validation Loss: 0.005059116811025888\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007094872819725424\n",
      "Training Loss: 0.007211012728512287\n",
      "Training Loss: 0.007030881081009284\n",
      "Validation Loss: 0.00503960586404114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007078739241696894\n",
      "Training Loss: 0.007196454117074608\n",
      "Training Loss: 0.007016751893097535\n",
      "Validation Loss: 0.005020973510684424\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.0070632569107692686\n",
      "Training Loss: 0.007182471173582599\n",
      "Training Loss: 0.007003203089116141\n",
      "Validation Loss: 0.005003163386973437\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.007048387670656666\n",
      "Training Loss: 0.007169024245813489\n",
      "Training Loss: 0.006990206009941175\n",
      "Validation Loss: 0.004986120319239837\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007034092765534297\n",
      "Training Loss: 0.0071560776978731155\n",
      "Training Loss: 0.006977731499355286\n",
      "Validation Loss: 0.004969793314956589\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.0070203359250444915\n",
      "Training Loss: 0.007143597429385409\n",
      "Training Loss: 0.006965750849340111\n",
      "Validation Loss: 0.004954130159235779\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.007007080898620188\n",
      "Training Loss: 0.007131549358600751\n",
      "Training Loss: 0.006954235404264182\n",
      "Validation Loss: 0.004939081921671214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0069942943938076495\n",
      "Training Loss: 0.007119903047569096\n",
      "Training Loss: 0.006943158119684085\n",
      "Validation Loss: 0.004924603954370898\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006981943651335314\n",
      "Training Loss: 0.007108629180584103\n",
      "Training Loss: 0.006932493021013215\n",
      "Validation Loss: 0.00491065412414459\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006969999293796718\n",
      "Training Loss: 0.007097699304576963\n",
      "Training Loss: 0.006922214295482263\n",
      "Validation Loss: 0.004897189357977235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006958432531682775\n",
      "Training Loss: 0.007087089649867266\n",
      "Training Loss: 0.006912299058167264\n",
      "Validation Loss: 0.004884174035396427\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006947216625558212\n",
      "Training Loss: 0.007076776154572144\n",
      "Training Loss: 0.0069027228769846264\n",
      "Validation Loss: 0.004871571742605125\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006936327234143391\n",
      "Training Loss: 0.007066736478591338\n",
      "Training Loss: 0.006893465265166015\n",
      "Validation Loss: 0.004859349623667809\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006925740194274113\n",
      "Training Loss: 0.0070569515030365435\n",
      "Training Loss: 0.006884504612535238\n",
      "Validation Loss: 0.004847476179083663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006915434391703457\n",
      "Training Loss: 0.007047403106698766\n",
      "Training Loss: 0.006875821887515485\n",
      "Validation Loss: 0.004835928921075015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006905390509637073\n",
      "Training Loss: 0.007038073758594692\n",
      "Training Loss: 0.006867398746544495\n",
      "Validation Loss: 0.00482467730762437\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006895589338382706\n",
      "Training Loss: 0.007028948676306755\n",
      "Training Loss: 0.006859219869365916\n",
      "Validation Loss: 0.004813700650159395\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006886013826588168\n",
      "Training Loss: 0.007020012773573399\n",
      "Training Loss: 0.006851266914745793\n",
      "Validation Loss: 0.004802977377182564\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006876649581827223\n",
      "Training Loss: 0.0070112544170115145\n",
      "Training Loss: 0.0068435270572081205\n",
      "Validation Loss: 0.004792488960345181\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006867479833308607\n",
      "Training Loss: 0.0070026617078110575\n",
      "Training Loss: 0.006835986652877182\n",
      "Validation Loss: 0.004782216081244013\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006858492869650945\n",
      "Training Loss: 0.006994224630761892\n",
      "Training Loss: 0.006828631599200889\n",
      "Validation Loss: 0.004772142733426325\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006849676585989073\n",
      "Training Loss: 0.006985931657254696\n",
      "Training Loss: 0.00682145097409375\n",
      "Validation Loss: 0.004762256134751389\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006841018415288999\n",
      "Training Loss: 0.006977775091072545\n",
      "Training Loss: 0.006814434918342158\n",
      "Validation Loss: 0.0047525405070189845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006832507009385154\n",
      "Training Loss: 0.006969745621317997\n",
      "Training Loss: 0.006807571233948693\n",
      "Validation Loss: 0.004742983487314346\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0068241333507467064\n",
      "Training Loss: 0.006961836159462109\n",
      "Training Loss: 0.006800851970911026\n",
      "Validation Loss: 0.0047335758755569545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006815889365971089\n",
      "Training Loss: 0.006954039147822186\n",
      "Training Loss: 0.006794267871882766\n",
      "Validation Loss: 0.004724306595940771\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006807763954857365\n",
      "Training Loss: 0.006946348097408191\n",
      "Training Loss: 0.006787810898385942\n",
      "Validation Loss: 0.004715163991237164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006799751239595935\n",
      "Training Loss: 0.006938757665921003\n",
      "Training Loss: 0.006781474092276767\n",
      "Validation Loss: 0.004706141226951116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006791843458777294\n",
      "Training Loss: 0.006931261108256877\n",
      "Training Loss: 0.00677525021485053\n",
      "Validation Loss: 0.004697229974808988\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0067840326461009685\n",
      "Training Loss: 0.006923853724729269\n",
      "Training Loss: 0.006769132514018565\n",
      "Validation Loss: 0.00468842154748089\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.00677631332888268\n",
      "Training Loss: 0.006916529689915478\n",
      "Training Loss: 0.006763116524089128\n",
      "Validation Loss: 0.004679711443356375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006768679757369682\n",
      "Training Loss: 0.006909285549772904\n",
      "Training Loss: 0.006757194583769888\n",
      "Validation Loss: 0.004671088634372762\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006761125596240163\n",
      "Training Loss: 0.006902115929406137\n",
      "Training Loss: 0.006751363879302517\n",
      "Validation Loss: 0.004662551238264344\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006753645201679319\n",
      "Training Loss: 0.006895017303759232\n",
      "Training Loss: 0.006745617383858189\n",
      "Validation Loss: 0.0046540911223090595\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006746235621394589\n",
      "Training Loss: 0.006887984989443794\n",
      "Training Loss: 0.0067399537796154615\n",
      "Validation Loss: 0.004645706380042509\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006738889968255534\n",
      "Training Loss: 0.006881015648832545\n",
      "Training Loss: 0.006734366616001353\n",
      "Validation Loss: 0.004637389026550848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006731604913948103\n",
      "Training Loss: 0.006874106540344656\n",
      "Training Loss: 0.006728852487867698\n",
      "Validation Loss: 0.0046291332790010685\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0067243756970856335\n",
      "Training Loss: 0.006867253514938057\n",
      "Training Loss: 0.006723409505793825\n",
      "Validation Loss: 0.004620939625945109\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006717198577243835\n",
      "Training Loss: 0.006860452509718016\n",
      "Training Loss: 0.006718033370561898\n",
      "Validation Loss: 0.004612798845719839\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006710070171393454\n",
      "Training Loss: 0.0068537020578514785\n",
      "Training Loss: 0.006712719943607226\n",
      "Validation Loss: 0.004604711144995154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.0067029856576118615\n",
      "Training Loss: 0.00684699768666178\n",
      "Training Loss: 0.00670746764051728\n",
      "Validation Loss: 0.004596668400205253\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006695942807709798\n",
      "Training Loss: 0.006840337398461998\n",
      "Training Loss: 0.006702273017726839\n",
      "Validation Loss: 0.004588669763741952\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.00668893817695789\n",
      "Training Loss: 0.0068337180605158206\n",
      "Training Loss: 0.006697134625865147\n",
      "Validation Loss: 0.0045807135358107475\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.00668196766404435\n",
      "Training Loss: 0.006827136744977907\n",
      "Training Loss: 0.006692048227414489\n",
      "Validation Loss: 0.004572790843994472\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006675027797464281\n",
      "Training Loss: 0.006820590967545286\n",
      "Training Loss: 0.006687013471964747\n",
      "Validation Loss: 0.004564900392028137\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006668115898501128\n",
      "Training Loss: 0.006814076533773914\n",
      "Training Loss: 0.006682025706395507\n",
      "Validation Loss: 0.004557039937281751\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006661228344310075\n",
      "Training Loss: 0.006807592116529122\n",
      "Training Loss: 0.00667708472115919\n",
      "Validation Loss: 0.00454920543922802\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006654361739056185\n",
      "Training Loss: 0.006801134747220204\n",
      "Training Loss: 0.006672186439391226\n",
      "Validation Loss: 0.004541394856020683\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006647513905772939\n",
      "Training Loss: 0.006794701780891046\n",
      "Training Loss: 0.006667329593328759\n",
      "Validation Loss: 0.004533600561148121\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006640680824639276\n",
      "Training Loss: 0.0067882897099480035\n",
      "Training Loss: 0.006662511689355597\n",
      "Validation Loss: 0.0045258236285089775\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006633858438581228\n",
      "Training Loss: 0.006781895565800369\n",
      "Training Loss: 0.006657730698352679\n",
      "Validation Loss: 0.0045180579377966145\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006627044671913609\n",
      "Training Loss: 0.006775517059722915\n",
      "Training Loss: 0.006652984226820991\n",
      "Validation Loss: 0.004510302197324175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006620236244052649\n",
      "Training Loss: 0.006769150675972923\n",
      "Training Loss: 0.006648269894067198\n",
      "Validation Loss: 0.004502552345635767\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.00661342844250612\n",
      "Training Loss: 0.006762794201495126\n",
      "Training Loss: 0.006643585438141599\n",
      "Validation Loss: 0.0044948046083896935\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006606618677033112\n",
      "Training Loss: 0.00675644350820221\n",
      "Training Loss: 0.006638928059255704\n",
      "Validation Loss: 0.004487056194888323\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006599802422570065\n",
      "Training Loss: 0.00675009484984912\n",
      "Training Loss: 0.00663429616484791\n",
      "Validation Loss: 0.00447929957149069\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006592975581297651\n",
      "Training Loss: 0.00674374565249309\n",
      "Training Loss: 0.00662968575488776\n",
      "Validation Loss: 0.004471532997853133\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006586135188117623\n",
      "Training Loss: 0.006737391721690074\n",
      "Training Loss: 0.006625094551127404\n",
      "Validation Loss: 0.0044637516702085815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.0065792757784947755\n",
      "Training Loss: 0.006731030235532671\n",
      "Training Loss: 0.006620519818970934\n",
      "Validation Loss: 0.004455953335462745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.0065723930147942155\n",
      "Training Loss: 0.006724655030993744\n",
      "Training Loss: 0.00661595850251615\n",
      "Validation Loss: 0.004448130997577996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.00656548157450743\n",
      "Training Loss: 0.006718264133669436\n",
      "Training Loss: 0.006611405496951193\n",
      "Validation Loss: 0.004440278449263214\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006558537791715935\n",
      "Training Loss: 0.0067118521756492555\n",
      "Training Loss: 0.006606859173625707\n",
      "Validation Loss: 0.004432391827230164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006551554342731834\n",
      "Training Loss: 0.006705414458410814\n",
      "Training Loss: 0.0066023162263445555\n",
      "Validation Loss: 0.004424468338819158\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.0065445278584957125\n",
      "Training Loss: 0.006698946453398093\n",
      "Training Loss: 0.006597770726075396\n",
      "Validation Loss: 0.00441650017635541\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006537452477496117\n",
      "Training Loss: 0.006692443884676322\n",
      "Training Loss: 0.006593219978967681\n",
      "Validation Loss: 0.004408478836942297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006530319420853629\n",
      "Training Loss: 0.006685900914017111\n",
      "Training Loss: 0.006588658427353948\n",
      "Validation Loss: 0.004400398255864765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.0065231239434797314\n",
      "Training Loss: 0.006679311894113198\n",
      "Training Loss: 0.006584081207402051\n",
      "Validation Loss: 0.004392254102865255\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006515858470229432\n",
      "Training Loss: 0.006672671401174739\n",
      "Training Loss: 0.006579483808018267\n",
      "Validation Loss: 0.004384037440778751\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006508516424801201\n",
      "Training Loss: 0.006665974642382935\n",
      "Training Loss: 0.006574860356049612\n",
      "Validation Loss: 0.004375736498577374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006501090556848794\n",
      "Training Loss: 0.006659214636310935\n",
      "Training Loss: 0.006570205962052569\n",
      "Validation Loss: 0.004367349108189261\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006493573079351336\n",
      "Training Loss: 0.006652385159395635\n",
      "Training Loss: 0.006565512751694769\n",
      "Validation Loss: 0.004358861282117288\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006485955493990332\n",
      "Training Loss: 0.0066454813012387605\n",
      "Training Loss: 0.0065607751137577\n",
      "Validation Loss: 0.004350266157743636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.00647822929895483\n",
      "Training Loss: 0.00663849514676258\n",
      "Training Loss: 0.0065559871506411585\n",
      "Validation Loss: 0.004341553686725582\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006470385789871216\n",
      "Training Loss: 0.006631421794882044\n",
      "Training Loss: 0.006551140231313184\n",
      "Validation Loss: 0.0043327149024672724\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006462417193688452\n",
      "Training Loss: 0.006624254466732964\n",
      "Training Loss: 0.006546227592043579\n",
      "Validation Loss: 0.004323734088900259\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006454313847934827\n",
      "Training Loss: 0.006616987545276061\n",
      "Training Loss: 0.006541241689119488\n",
      "Validation Loss: 0.004314607291173692\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006446067246142775\n",
      "Training Loss: 0.0066096136928535994\n",
      "Training Loss: 0.006536173513159156\n",
      "Validation Loss: 0.004305316718671931\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.00643766712397337\n",
      "Training Loss: 0.006602126341313123\n",
      "Training Loss: 0.006531014913925901\n",
      "Validation Loss: 0.004295853631089578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.00642910513561219\n",
      "Training Loss: 0.006594520711805671\n",
      "Training Loss: 0.00652575705666095\n",
      "Validation Loss: 0.004286205364009261\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006420371347339824\n",
      "Training Loss: 0.006586790042929351\n",
      "Training Loss: 0.006520390441874042\n",
      "Validation Loss: 0.004276361003821569\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006411457941867411\n",
      "Training Loss: 0.006578928130911663\n",
      "Training Loss: 0.006514905862277373\n",
      "Validation Loss: 0.004266302476066761\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006402355870231986\n",
      "Training Loss: 0.0065709306241478775\n",
      "Training Loss: 0.006509295705473051\n",
      "Validation Loss: 0.0042560258661161355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006393056779634207\n",
      "Training Loss: 0.006562791678588837\n",
      "Training Loss: 0.006503547909669578\n",
      "Validation Loss: 0.004245513768571042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006383553367340937\n",
      "Training Loss: 0.006554506175452843\n",
      "Training Loss: 0.006497654154663905\n",
      "Validation Loss: 0.004234755097161142\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.00637383776018396\n",
      "Training Loss: 0.006546069709584117\n",
      "Training Loss: 0.006491605018964037\n",
      "Validation Loss: 0.004223737640049978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006363903186284005\n",
      "Training Loss: 0.006537478982936591\n",
      "Training Loss: 0.006485389999579638\n",
      "Validation Loss: 0.004212454564377582\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006353747440734878\n",
      "Training Loss: 0.006528729849960655\n",
      "Training Loss: 0.006479000760009512\n",
      "Validation Loss: 0.0042008918793767355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.0063433633255772295\n",
      "Training Loss: 0.006519821232650429\n",
      "Training Loss: 0.006472428714623675\n",
      "Validation Loss: 0.0041890419389152625\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.006332750412402674\n",
      "Training Loss: 0.006510750714223832\n",
      "Training Loss: 0.006465664429124445\n",
      "Validation Loss: 0.004176900348314764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.006321906608063727\n",
      "Training Loss: 0.006501516245771199\n",
      "Training Loss: 0.006458701019873843\n",
      "Validation Loss: 0.004164453262981194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006310831147711724\n",
      "Training Loss: 0.006492118808673695\n",
      "Training Loss: 0.0064515299908816815\n",
      "Validation Loss: 0.0041517025171098915\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006299526103539392\n",
      "Training Loss: 0.0064825588464736935\n",
      "Training Loss: 0.006444147469010204\n",
      "Validation Loss: 0.004138643032572954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006287995490711182\n",
      "Training Loss: 0.00647283808211796\n",
      "Training Loss: 0.006436544201569631\n",
      "Validation Loss: 0.004125273066410648\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006276244487380609\n",
      "Training Loss: 0.006462959677446633\n",
      "Training Loss: 0.0064287184673594315\n",
      "Validation Loss: 0.004111598862866149\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006264279388124123\n",
      "Training Loss: 0.006452927882783115\n",
      "Training Loss: 0.006420666406629607\n",
      "Validation Loss: 0.004097617652824442\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006252110116183758\n",
      "Training Loss: 0.0064427470555528995\n",
      "Training Loss: 0.006412385623552837\n",
      "Validation Loss: 0.004083340954535191\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.00623974587302655\n",
      "Training Loss: 0.006432423064252362\n",
      "Training Loss: 0.006403876164113171\n",
      "Validation Loss: 0.0040687810236541035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.00622719821636565\n",
      "Training Loss: 0.006421961238374934\n",
      "Training Loss: 0.006395136928185821\n",
      "Validation Loss: 0.004053949749586434\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006214480476919562\n",
      "Training Loss: 0.006411370665300637\n",
      "Training Loss: 0.006386171323247254\n",
      "Validation Loss: 0.004038862950100532\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006201607030816376\n",
      "Training Loss: 0.00640065569547005\n",
      "Training Loss: 0.0063769815780688075\n",
      "Validation Loss: 0.004023540095985982\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006188596488209441\n",
      "Training Loss: 0.006389826429076493\n",
      "Training Loss: 0.0063675746286753565\n",
      "Validation Loss: 0.004008010190354367\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006175464589614421\n",
      "Training Loss: 0.006378889646148309\n",
      "Training Loss: 0.006357954507111571\n",
      "Validation Loss: 0.0039922953733724405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.0061622279579751195\n",
      "Training Loss: 0.00636785458191298\n",
      "Training Loss: 0.006348131550475955\n",
      "Validation Loss: 0.003976431497689232\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006148908354807645\n",
      "Training Loss: 0.00635672984062694\n",
      "Training Loss: 0.006338114768150263\n",
      "Validation Loss: 0.003960444649493092\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.0061355232493951915\n",
      "Training Loss: 0.006345523555064574\n",
      "Training Loss: 0.006327916464651934\n",
      "Validation Loss: 0.003944380167973741\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.006122093717567623\n",
      "Training Loss: 0.006334246334154159\n",
      "Training Loss: 0.006317549546947703\n",
      "Validation Loss: 0.003928271251772478\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.0061086384393274785\n",
      "Training Loss: 0.0063229087699437515\n",
      "Training Loss: 0.006307028643204831\n",
      "Validation Loss: 0.003912156716242872\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006095179951516911\n",
      "Training Loss: 0.006311522304895334\n",
      "Training Loss: 0.006296370684285648\n",
      "Validation Loss: 0.003896072414033952\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006081736797932535\n",
      "Training Loss: 0.006300098655046895\n",
      "Training Loss: 0.006285591493942775\n",
      "Validation Loss: 0.0038800540823604535\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006068330531707034\n",
      "Training Loss: 0.006288652972434647\n",
      "Training Loss: 0.006274711238802411\n",
      "Validation Loss: 0.0038641404816383766\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0060549808398354795\n",
      "Training Loss: 0.006277198969037272\n",
      "Training Loss: 0.006263749239733443\n",
      "Validation Loss: 0.0038483551493453468\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006041709437267855\n",
      "Training Loss: 0.00626575575908646\n",
      "Training Loss: 0.006252724842051975\n",
      "Validation Loss: 0.0038327294963626506\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.0060285379260312764\n",
      "Training Loss: 0.0062543412100058045\n",
      "Training Loss: 0.006241659923107363\n",
      "Validation Loss: 0.00381728854250121\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.0060154892306309195\n",
      "Training Loss: 0.006242975757922977\n",
      "Training Loss: 0.006230576369562186\n",
      "Validation Loss: 0.0038020543971889016\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0060025856515858326\n",
      "Training Loss: 0.006231681373901665\n",
      "Training Loss: 0.006219496723497286\n",
      "Validation Loss: 0.003787046574707551\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.0059898520109709355\n",
      "Training Loss: 0.0062204833788564425\n",
      "Training Loss: 0.006208444314543158\n",
      "Validation Loss: 0.003772286401297688\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005977309636073187\n",
      "Training Loss: 0.006209403386455961\n",
      "Training Loss: 0.006197441713884473\n",
      "Validation Loss: 0.0037577890786420804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005964980832068249\n",
      "Training Loss: 0.006198466389323584\n",
      "Training Loss: 0.006186510942061432\n",
      "Validation Loss: 0.0037435694124151983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005952887795865535\n",
      "Training Loss: 0.006187696362612769\n",
      "Training Loss: 0.006175676715793088\n",
      "Validation Loss: 0.003729640773047557\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.0059410478023346515\n",
      "Training Loss: 0.006177114729071036\n",
      "Training Loss: 0.006164955889689736\n",
      "Validation Loss: 0.00371601659048537\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005929477899335325\n",
      "Training Loss: 0.00616674107091967\n",
      "Training Loss: 0.006154368745628744\n",
      "Validation Loss: 0.0037027016115960864\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.0059181894327048215\n",
      "Training Loss: 0.0061565904080634936\n",
      "Training Loss: 0.006143931874539703\n",
      "Validation Loss: 0.0036897101933973725\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005907195039326325\n",
      "Training Loss: 0.0061466784268850464\n",
      "Training Loss: 0.006133660337654874\n",
      "Validation Loss: 0.0036770463446966174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005896502264076844\n",
      "Training Loss: 0.006137013785191812\n",
      "Training Loss: 0.0061235656752251085\n",
      "Validation Loss: 0.0036647136485350603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.0058861151163000615\n",
      "Training Loss: 0.006127602807246149\n",
      "Training Loss: 0.006113657878013328\n",
      "Validation Loss: 0.003652718220968218\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005876035753171891\n",
      "Training Loss: 0.006118450743961148\n",
      "Training Loss: 0.006103944220230914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [27:53<06:58, 209.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.003641058832768016\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.08738390009850264\n",
      "Training Loss: 0.07767786322161556\n",
      "Training Loss: 0.0733176319859922\n",
      "Validation Loss: 0.07264441496619348\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.07014258027076721\n",
      "Training Loss: 0.06856327883899212\n",
      "Training Loss: 0.06517884524539114\n",
      "Validation Loss: 0.06326371002314465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06051460418850183\n",
      "Training Loss: 0.057833385448902846\n",
      "Training Loss: 0.05350334221497178\n",
      "Validation Loss: 0.05019434681685453\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.04747259550727904\n",
      "Training Loss: 0.044062169119715694\n",
      "Training Loss: 0.03971734980121255\n",
      "Validation Loss: 0.03612153737523248\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.03434171902947128\n",
      "Training Loss: 0.0316038923105225\n",
      "Training Loss: 0.02878717066254467\n",
      "Validation Loss: 0.026357612143574138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.02573683235794306\n",
      "Training Loss: 0.024003015449270606\n",
      "Training Loss: 0.02246307517401874\n",
      "Validation Loss: 0.02074778378742297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.020811453349888324\n",
      "Training Loss: 0.019560092808678747\n",
      "Training Loss: 0.018648412232287227\n",
      "Validation Loss: 0.01716883800494788\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.017691749953664838\n",
      "Training Loss: 0.01672842351952568\n",
      "Training Loss: 0.01615729710785672\n",
      "Validation Loss: 0.014723174378610728\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.015587408761493862\n",
      "Training Loss: 0.0148284423770383\n",
      "Training Loss: 0.014444317650049925\n",
      "Validation Loss: 0.012964394113593054\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.014084542714990676\n",
      "Training Loss: 0.013475264424923807\n",
      "Training Loss: 0.01318609988084063\n",
      "Validation Loss: 0.011615309487567858\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.012935968991369009\n",
      "Training Loss: 0.012443596054799855\n",
      "Training Loss: 0.012200178417842836\n",
      "Validation Loss: 0.01052615571808949\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.012010290635516866\n",
      "Training Loss: 0.011616011378355324\n",
      "Training Loss: 0.011396062137791887\n",
      "Validation Loss: 0.009625778614616628\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.011242961341049523\n",
      "Training Loss: 0.01093573929858394\n",
      "Training Loss: 0.01073218547855504\n",
      "Validation Loss: 0.008879142987353497\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.010605292957043275\n",
      "Training Loss: 0.010377900486346334\n",
      "Training Loss: 0.01019075865857303\n",
      "Validation Loss: 0.008266944049731986\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.010085211775731295\n",
      "Training Loss: 0.009929834274807946\n",
      "Training Loss: 0.00976020096684806\n",
      "Validation Loss: 0.007773409161999235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.009672397113172337\n",
      "Training Loss: 0.009578236315865069\n",
      "Training Loss: 0.009425266239559279\n",
      "Validation Loss: 0.007381062829092647\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.009351511989952996\n",
      "Training Loss: 0.00930569745018147\n",
      "Training Loss: 0.009166693310253321\n",
      "Validation Loss: 0.00707130693945657\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.009103732432704419\n",
      "Training Loss: 0.009093907327624037\n",
      "Training Loss: 0.008965539799537509\n",
      "Validation Loss: 0.006826434939775323\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.008911004123510792\n",
      "Training Loss: 0.008927069413475692\n",
      "Training Loss: 0.008806108930148184\n",
      "Validation Loss: 0.006630811712108134\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.008758386655244976\n",
      "Training Loss: 0.008792779364157469\n",
      "Training Loss: 0.008676360308891162\n",
      "Validation Loss: 0.006471528013739107\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008634374146349728\n",
      "Training Loss: 0.008681638229172678\n",
      "Training Loss: 0.008567377579165623\n",
      "Validation Loss: 0.006338611073802445\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.00853046617237851\n",
      "Training Loss: 0.008586725257337093\n",
      "Training Loss: 0.008472769857617095\n",
      "Validation Loss: 0.006224744107568993\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.008440583352930844\n",
      "Training Loss: 0.008503098090877757\n",
      "Training Loss: 0.00838809214765206\n",
      "Validation Loss: 0.0061247023947411374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.008360496232053266\n",
      "Training Loss: 0.00842732741846703\n",
      "Training Loss: 0.008310338725568727\n",
      "Validation Loss: 0.0060348049789834554\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.00828732141526416\n",
      "Training Loss: 0.00835707744001411\n",
      "Training Loss: 0.008237535152584314\n",
      "Validation Loss: 0.005952481306001042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.008219118991401047\n",
      "Training Loss: 0.008290761673124506\n",
      "Training Loss: 0.00816839344217442\n",
      "Validation Loss: 0.005875914449819227\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.008154578681569546\n",
      "Training Loss: 0.008227290709037334\n",
      "Training Loss: 0.008102089907042682\n",
      "Validation Loss: 0.005803818985809352\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.008092807987704873\n",
      "Training Loss: 0.008165891178650781\n",
      "Training Loss: 0.008038089727051556\n",
      "Validation Loss: 0.005735240738022696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.00803316522622481\n",
      "Training Loss: 0.008105964440619572\n",
      "Training Loss: 0.007976021229987964\n",
      "Validation Loss: 0.005669414764270186\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.007975139641202986\n",
      "Training Loss: 0.008046975662000477\n",
      "Training Loss: 0.00791559240897186\n",
      "Validation Loss: 0.005605678984455848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007918268154608086\n",
      "Training Loss: 0.00798838962917216\n",
      "Training Loss: 0.007856550731230528\n",
      "Validation Loss: 0.00554343753133304\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007862098221667111\n",
      "Training Loss: 0.00792966891429387\n",
      "Training Loss: 0.007798709673807025\n",
      "Validation Loss: 0.00548223613465749\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.007806227500550449\n",
      "Training Loss: 0.007870385020505638\n",
      "Training Loss: 0.0077420710865408185\n",
      "Validation Loss: 0.005421967414375185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.007750461440300569\n",
      "Training Loss: 0.0078105083340778945\n",
      "Training Loss: 0.0076870130095630885\n",
      "Validation Loss: 0.005363110224637883\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007695053927600384\n",
      "Training Loss: 0.007750743794022128\n",
      "Training Loss: 0.007634366237325594\n",
      "Validation Loss: 0.0053066499289282166\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.0076408304669894275\n",
      "Training Loss: 0.007692534141242504\n",
      "Training Loss: 0.007585155681008473\n",
      "Validation Loss: 0.005253583326245125\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007588953130180016\n",
      "Training Loss: 0.00763751462684013\n",
      "Training Loss: 0.007540115778101608\n",
      "Validation Loss: 0.005204396114391641\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007540421559242532\n",
      "Training Loss: 0.007586834552930668\n",
      "Training Loss: 0.0074994306056760255\n",
      "Validation Loss: 0.005159082914064272\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.0074957556137815115\n",
      "Training Loss: 0.007540917204460129\n",
      "Training Loss: 0.007462836925406009\n",
      "Validation Loss: 0.005117392754103737\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.0074550000275485214\n",
      "Training Loss: 0.007499628970399499\n",
      "Training Loss: 0.00742985486285761\n",
      "Validation Loss: 0.005078993827952177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007417898256098851\n",
      "Training Loss: 0.007462549862684682\n",
      "Training Loss: 0.007399969582911581\n",
      "Validation Loss: 0.005043547580887177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.007384064594516531\n",
      "Training Loss: 0.007429168265080079\n",
      "Training Loss: 0.007372716126265004\n",
      "Validation Loss: 0.005010735189929353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007353100443724543\n",
      "Training Loss: 0.0073989808419719335\n",
      "Training Loss: 0.0073477067227941005\n",
      "Validation Loss: 0.004980262498413161\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0073246352153364565\n",
      "Training Loss: 0.0073715336283203215\n",
      "Training Loss: 0.007324629510985687\n",
      "Validation Loss: 0.00495187170526243\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007298349753255024\n",
      "Training Loss: 0.007346431381301954\n",
      "Training Loss: 0.0073032321408391\n",
      "Validation Loss: 0.004925331755494176\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007273976807482541\n",
      "Training Loss: 0.007323337636189535\n",
      "Training Loss: 0.007283307791221887\n",
      "Validation Loss: 0.004900440841839973\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007251286836108193\n",
      "Training Loss: 0.0073019644513260575\n",
      "Training Loss: 0.007264685238478705\n",
      "Validation Loss: 0.004877023150349099\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0072300871519837525\n",
      "Training Loss: 0.007282071434892714\n",
      "Training Loss: 0.007247217386029661\n",
      "Validation Loss: 0.004854921500716525\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007210209508193657\n",
      "Training Loss: 0.007263454495696351\n",
      "Training Loss: 0.007230775926727801\n",
      "Validation Loss: 0.004834004220673082\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0071915060991887\n",
      "Training Loss: 0.007245939687127247\n",
      "Training Loss: 0.0072152461169753225\n",
      "Validation Loss: 0.004814150161763883\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.007173844993812964\n",
      "Training Loss: 0.007229379079071805\n",
      "Training Loss: 0.0072005254973191764\n",
      "Validation Loss: 0.004795251631313998\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.007157105753431097\n",
      "Training Loss: 0.007213644828880206\n",
      "Training Loss: 0.0071865195780992505\n",
      "Validation Loss: 0.004777213933961361\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.0071411821618676185\n",
      "Training Loss: 0.00719862797879614\n",
      "Training Loss: 0.0071731451770756394\n",
      "Validation Loss: 0.004759954620843355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.00712597741628997\n",
      "Training Loss: 0.00718423482729122\n",
      "Training Loss: 0.00716032502357848\n",
      "Validation Loss: 0.004743395306515309\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007111405152827501\n",
      "Training Loss: 0.00717038270784542\n",
      "Training Loss: 0.007147990969242528\n",
      "Validation Loss: 0.004727473101850724\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007097387043759226\n",
      "Training Loss: 0.007157000892329961\n",
      "Training Loss: 0.007136081675998867\n",
      "Validation Loss: 0.004712130714160798\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0070838557172100995\n",
      "Training Loss: 0.007144028876209632\n",
      "Training Loss: 0.007124543443787843\n",
      "Validation Loss: 0.0046973152267955915\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007070749377598986\n",
      "Training Loss: 0.007131413362221792\n",
      "Training Loss: 0.0071133269625715916\n",
      "Validation Loss: 0.004682977949373759\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.007058014189824462\n",
      "Training Loss: 0.0071191070845816285\n",
      "Training Loss: 0.007102389993378893\n",
      "Validation Loss: 0.004669078126068363\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.007045601425925269\n",
      "Training Loss: 0.007107070903293789\n",
      "Training Loss: 0.007091695228591561\n",
      "Validation Loss: 0.004655582734169125\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.007033470567548648\n",
      "Training Loss: 0.00709526871680282\n",
      "Training Loss: 0.00708120874594897\n",
      "Validation Loss: 0.004642455257852091\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007021582734305412\n",
      "Training Loss: 0.007083668116247281\n",
      "Training Loss: 0.007070901435799897\n",
      "Validation Loss: 0.004629661819604592\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007009904832812026\n",
      "Training Loss: 0.007072241118876264\n",
      "Training Loss: 0.007060745012713596\n",
      "Validation Loss: 0.004617176409104441\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006998406486818567\n",
      "Training Loss: 0.007060962571995333\n",
      "Training Loss: 0.0070507169468328355\n",
      "Validation Loss: 0.004604972793663109\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.00698706045979634\n",
      "Training Loss: 0.007049808196024969\n",
      "Training Loss: 0.007040794491767883\n",
      "Validation Loss: 0.004593025520836328\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.0069758419028948995\n",
      "Training Loss: 0.007038757491391152\n",
      "Training Loss: 0.007030957997776568\n",
      "Validation Loss: 0.004581308484815186\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0069647275714669376\n",
      "Training Loss: 0.007027790013235062\n",
      "Training Loss: 0.007021188136423007\n",
      "Validation Loss: 0.00456980299767044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.006953697891440243\n",
      "Training Loss: 0.007016887739300728\n",
      "Training Loss: 0.007011469440767542\n",
      "Validation Loss: 0.004558484776997206\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0069427321536932145\n",
      "Training Loss: 0.007006032506469637\n",
      "Training Loss: 0.0070017848385032265\n",
      "Validation Loss: 0.004547338191749465\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006931812114780769\n",
      "Training Loss: 0.006995208971202374\n",
      "Training Loss: 0.00699211927363649\n",
      "Validation Loss: 0.00453633680714692\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006920921123819426\n",
      "Training Loss: 0.006984399933135137\n",
      "Training Loss: 0.006982459583086893\n",
      "Validation Loss: 0.004525466692592069\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006910042378585786\n",
      "Training Loss: 0.006973591467831284\n",
      "Training Loss: 0.006972791503649205\n",
      "Validation Loss: 0.0045147105178984116\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006899160821922124\n",
      "Training Loss: 0.006962767986115068\n",
      "Training Loss: 0.00696310154395178\n",
      "Validation Loss: 0.004504048160873772\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.0068882611277513205\n",
      "Training Loss: 0.006951914919773117\n",
      "Training Loss: 0.006953378280159086\n",
      "Validation Loss: 0.004493466108838578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006877328598638996\n",
      "Training Loss: 0.006941019467776641\n",
      "Training Loss: 0.006943608442088589\n",
      "Validation Loss: 0.004482945102251271\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006866349691990763\n",
      "Training Loss: 0.006930066916393116\n",
      "Training Loss: 0.006933781336992979\n",
      "Validation Loss: 0.00447247240409841\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006855312158586457\n",
      "Training Loss: 0.00691904415958561\n",
      "Training Loss: 0.006923885102150962\n",
      "Validation Loss: 0.004462030411467709\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.006844199580373242\n",
      "Training Loss: 0.006907937388168648\n",
      "Training Loss: 0.006913907439447939\n",
      "Validation Loss: 0.004451608210095738\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006833001564955339\n",
      "Training Loss: 0.006896734203910455\n",
      "Training Loss: 0.006903838196303695\n",
      "Validation Loss: 0.004441188725517288\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006821704455651343\n",
      "Training Loss: 0.006885420334292576\n",
      "Training Loss: 0.006893665161915124\n",
      "Validation Loss: 0.00443075831097896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006810294728493318\n",
      "Training Loss: 0.0068739826837554575\n",
      "Training Loss: 0.006883377924095839\n",
      "Validation Loss: 0.004420304632996827\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0067987616604659705\n",
      "Training Loss: 0.006862409407040105\n",
      "Training Loss: 0.00687296649790369\n",
      "Validation Loss: 0.00440981321225173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006787090399302542\n",
      "Training Loss: 0.0068506851547863335\n",
      "Training Loss: 0.0068624183116480705\n",
      "Validation Loss: 0.0043992749238753\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.00677527085179463\n",
      "Training Loss: 0.006838796896627173\n",
      "Training Loss: 0.006851723087020218\n",
      "Validation Loss: 0.0043886722257789855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006763288602232933\n",
      "Training Loss: 0.006826731279725209\n",
      "Training Loss: 0.006840868482831866\n",
      "Validation Loss: 0.004377993471852472\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006751131775090471\n",
      "Training Loss: 0.006814474484417588\n",
      "Training Loss: 0.0068298447865527126\n",
      "Validation Loss: 0.004367224006845584\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006738788012880832\n",
      "Training Loss: 0.006802012418629602\n",
      "Training Loss: 0.006818639601115137\n",
      "Validation Loss: 0.0043563552225646925\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006726246102480218\n",
      "Training Loss: 0.006789331334875897\n",
      "Training Loss: 0.006807242750655859\n",
      "Validation Loss: 0.004345375321821173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006713492376729846\n",
      "Training Loss: 0.006776416986249387\n",
      "Training Loss: 0.006795642289798707\n",
      "Validation Loss: 0.004334269465585606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.0067005148599855605\n",
      "Training Loss: 0.0067632554937154055\n",
      "Training Loss: 0.00678382703801617\n",
      "Validation Loss: 0.004323023886765155\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0066873021726496515\n",
      "Training Loss: 0.006749833595240489\n",
      "Training Loss: 0.00677178559708409\n",
      "Validation Loss: 0.004311631636411538\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006673843013122678\n",
      "Training Loss: 0.0067361354606691745\n",
      "Training Loss: 0.006759507192764431\n",
      "Validation Loss: 0.004300077876803371\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006660125623457133\n",
      "Training Loss: 0.006722148292465135\n",
      "Training Loss: 0.006746981330215931\n",
      "Validation Loss: 0.0042883509140869795\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006646139462245629\n",
      "Training Loss: 0.006707858761074021\n",
      "Training Loss: 0.006734195726457983\n",
      "Validation Loss: 0.004276439208103072\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006631872509606182\n",
      "Training Loss: 0.006693252990371548\n",
      "Training Loss: 0.006721141132293269\n",
      "Validation Loss: 0.004264335543717771\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.0066173174814321105\n",
      "Training Loss: 0.006678319228813052\n",
      "Training Loss: 0.006707807690836489\n",
      "Validation Loss: 0.004252022710429986\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006602463892195373\n",
      "Training Loss: 0.006663044384913519\n",
      "Training Loss: 0.00669418582576327\n",
      "Validation Loss: 0.004239494532341494\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006587306304136291\n",
      "Training Loss: 0.00664741902728565\n",
      "Training Loss: 0.006680268062045798\n",
      "Validation Loss: 0.004226740894399667\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006571837790543214\n",
      "Training Loss: 0.006631433601141907\n",
      "Training Loss: 0.006666045885067433\n",
      "Validation Loss: 0.004213750193005406\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006556053980020806\n",
      "Training Loss: 0.006615080262999982\n",
      "Training Loss: 0.006651514914119616\n",
      "Validation Loss: 0.00420052033726105\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006539952452294528\n",
      "Training Loss: 0.006598353076842613\n",
      "Training Loss: 0.006636668614810333\n",
      "Validation Loss: 0.004187037063673599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006523532728897408\n",
      "Training Loss: 0.006581248337752186\n",
      "Training Loss: 0.0066215061082039025\n",
      "Validation Loss: 0.004173299636472058\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006506798388436437\n",
      "Training Loss: 0.006563765329774469\n",
      "Training Loss: 0.006606026185909286\n",
      "Validation Loss: 0.004159299502400368\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006489755768561736\n",
      "Training Loss: 0.006545908198459074\n",
      "Training Loss: 0.0065902315673884\n",
      "Validation Loss: 0.004145041074122439\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006472413837909699\n",
      "Training Loss: 0.0065276828565401955\n",
      "Training Loss: 0.00657412733649835\n",
      "Validation Loss: 0.004130520243747055\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0064547866070643065\n",
      "Training Loss: 0.006509100749972276\n",
      "Training Loss: 0.006557723006699235\n",
      "Validation Loss: 0.004115739176075989\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006436893756035716\n",
      "Training Loss: 0.0064901797519996765\n",
      "Training Loss: 0.006541032831883058\n",
      "Validation Loss: 0.0041007063753139015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006418755936902016\n",
      "Training Loss: 0.006470941657316871\n",
      "Training Loss: 0.006524074043845758\n",
      "Validation Loss: 0.004085434087960238\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006400406133616343\n",
      "Training Loss: 0.006451415928313509\n",
      "Training Loss: 0.006506868262076751\n",
      "Validation Loss: 0.00406993434665034\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006381875940132886\n",
      "Training Loss: 0.006431638416252099\n",
      "Training Loss: 0.006489445123588666\n",
      "Validation Loss: 0.004054228288613343\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006363208141410723\n",
      "Training Loss: 0.006411654526600614\n",
      "Training Loss: 0.006471837714780122\n",
      "Validation Loss: 0.004038340325095806\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006344447850715369\n",
      "Training Loss: 0.006391512213740498\n",
      "Training Loss: 0.006454088445752859\n",
      "Validation Loss: 0.0040223037554579\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006325651103397831\n",
      "Training Loss: 0.00637127296125982\n",
      "Training Loss: 0.00643624151009135\n",
      "Validation Loss: 0.004006150176649222\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006306872928980738\n",
      "Training Loss: 0.006351001573493704\n",
      "Training Loss: 0.0064183487871196125\n",
      "Validation Loss: 0.003989927089391171\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006288178579416126\n",
      "Training Loss: 0.006330772223882377\n",
      "Training Loss: 0.006400468630017713\n",
      "Validation Loss: 0.003973681368509286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.0062696333206258715\n",
      "Training Loss: 0.006310662011383101\n",
      "Training Loss: 0.006382661876268685\n",
      "Validation Loss: 0.003957465364927363\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006251304000616073\n",
      "Training Loss: 0.006290752388304099\n",
      "Training Loss: 0.006364990052534267\n",
      "Validation Loss: 0.003941333935114691\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006233258576830849\n",
      "Training Loss: 0.006271124214981683\n",
      "Training Loss: 0.006347519785631448\n",
      "Validation Loss: 0.003925346799477349\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.0062155623419675975\n",
      "Training Loss: 0.006251862282515504\n",
      "Training Loss: 0.006330315266968683\n",
      "Validation Loss: 0.003909558515598098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.0061982747435104105\n",
      "Training Loss: 0.006233041347004473\n",
      "Training Loss: 0.006313435276970267\n",
      "Validation Loss: 0.003894031298964211\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0061814504861831664\n",
      "Training Loss: 0.006214732679072767\n",
      "Training Loss: 0.0062969366228207944\n",
      "Validation Loss: 0.0038788142219039327\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006165134138427675\n",
      "Training Loss: 0.006197000032407232\n",
      "Training Loss: 0.006280867604073137\n",
      "Validation Loss: 0.0038639565882681126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006149359041592107\n",
      "Training Loss: 0.006179889039485715\n",
      "Training Loss: 0.006265264260582626\n",
      "Validation Loss: 0.0038494914606621677\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.00613414692808874\n",
      "Training Loss: 0.006163438984658569\n",
      "Training Loss: 0.006250154736917466\n",
      "Validation Loss: 0.003835458260714966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006119506981922314\n",
      "Training Loss: 0.006147672189981677\n",
      "Training Loss: 0.006235561260255053\n",
      "Validation Loss: 0.0038218751760314773\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006105440555838868\n",
      "Training Loss: 0.006132597260293551\n",
      "Training Loss: 0.006221485872520134\n",
      "Validation Loss: 0.0038087597140372634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0060919354774523525\n",
      "Training Loss: 0.0061182106827618555\n",
      "Training Loss: 0.006207926309434697\n",
      "Validation Loss: 0.0037961092334779585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.0060789701755857095\n",
      "Training Loss: 0.006104497521882877\n",
      "Training Loss: 0.00619487254822161\n",
      "Validation Loss: 0.0037839274861316166\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006066519519081339\n",
      "Training Loss: 0.006091432569082827\n",
      "Training Loss: 0.0061823063081828875\n",
      "Validation Loss: 0.0037722023598996276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006054552395362407\n",
      "Training Loss: 0.006078984345658682\n",
      "Training Loss: 0.00617020079633221\n",
      "Validation Loss: 0.003760918427789793\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006043031339650042\n",
      "Training Loss: 0.0060671143274521454\n",
      "Training Loss: 0.00615852860501036\n",
      "Validation Loss: 0.0037500562268607527\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006031922330730594\n",
      "Training Loss: 0.0060557821602560576\n",
      "Training Loss: 0.006147263121092692\n",
      "Validation Loss: 0.003739598529933941\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006021190356696024\n",
      "Training Loss: 0.006044947046902962\n",
      "Training Loss: 0.006136370417661965\n",
      "Validation Loss: 0.003729521359050165\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.0060108007572125645\n",
      "Training Loss: 0.006034567327587865\n",
      "Training Loss: 0.00612582077155821\n",
      "Validation Loss: 0.0037198002552195045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006000723125762306\n",
      "Training Loss: 0.006024601116660051\n",
      "Training Loss: 0.006115586076048203\n",
      "Validation Loss: 0.00371041460115766\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.005990926895756274\n",
      "Training Loss: 0.006015011733397841\n",
      "Training Loss: 0.0061056378099601715\n",
      "Validation Loss: 0.0037013344052038415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.0059813877660781145\n",
      "Training Loss: 0.006005761272972449\n",
      "Training Loss: 0.006095951008028351\n",
      "Validation Loss: 0.0036925464296933223\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005972082695225253\n",
      "Training Loss: 0.0059968203364405785\n",
      "Training Loss: 0.00608650287555065\n",
      "Validation Loss: 0.0036840201201822527\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.0059629896807018665\n",
      "Training Loss: 0.005988157406100072\n",
      "Training Loss: 0.006077272596303374\n",
      "Validation Loss: 0.003675740488851966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005954094182816334\n",
      "Training Loss: 0.0059797480684937905\n",
      "Training Loss: 0.006068243286572397\n",
      "Validation Loss: 0.0036676862751908097\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.005945378332980909\n",
      "Training Loss: 0.0059715663053793834\n",
      "Training Loss: 0.006059395864140242\n",
      "Validation Loss: 0.0036598416179381937\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.005936830287100747\n",
      "Training Loss: 0.0059635915351100265\n",
      "Training Loss: 0.006050718788173981\n",
      "Validation Loss: 0.0036521865747570783\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005928438566625119\n",
      "Training Loss: 0.00595580591878388\n",
      "Training Loss: 0.006042196595808491\n",
      "Validation Loss: 0.00364471064799392\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005920195079525002\n",
      "Training Loss: 0.0059481934370705855\n",
      "Training Loss: 0.006033819759031758\n",
      "Validation Loss: 0.003637391684074583\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005912088086479344\n",
      "Training Loss: 0.00594073801417835\n",
      "Training Loss: 0.006025578015833161\n",
      "Validation Loss: 0.0036302231833616042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005904111861600541\n",
      "Training Loss: 0.005933427940472029\n",
      "Training Loss: 0.006017461526207626\n",
      "Validation Loss: 0.0036231889248841305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.0058962594933109356\n",
      "Training Loss: 0.005926251570344903\n",
      "Training Loss: 0.006009464436210692\n",
      "Validation Loss: 0.0036162783295632956\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.0058885250752791765\n",
      "Training Loss: 0.005919198706978932\n",
      "Training Loss: 0.006001578485011123\n",
      "Validation Loss: 0.0036094801440269926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.0058809023746289315\n",
      "Training Loss: 0.005912261140183546\n",
      "Training Loss: 0.005993796647526323\n",
      "Validation Loss: 0.0036027833026064707\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005873386700986884\n",
      "Training Loss: 0.005905429536360316\n",
      "Training Loss: 0.005986114476691\n",
      "Validation Loss: 0.003596181512334164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005865973312756978\n",
      "Training Loss: 0.005898698628880083\n",
      "Training Loss: 0.005978528278064914\n",
      "Validation Loss: 0.0035896671679540634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.00585865906032268\n",
      "Training Loss: 0.0058920598175609485\n",
      "Training Loss: 0.005971032195375301\n",
      "Validation Loss: 0.0035832303470220376\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005851439057732932\n",
      "Training Loss: 0.005885510084335692\n",
      "Training Loss: 0.0059636224899441\n",
      "Validation Loss: 0.003576861055478914\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005844309889362194\n",
      "Training Loss: 0.005879041780717671\n",
      "Training Loss: 0.005956295095384121\n",
      "Validation Loss: 0.0035705606566265938\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005837268905015663\n",
      "Training Loss: 0.005872652191319503\n",
      "Training Loss: 0.0059490484278649095\n",
      "Validation Loss: 0.0035643188689647013\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005830312664620578\n",
      "Training Loss: 0.005866338147316128\n",
      "Training Loss: 0.005941878468729555\n",
      "Validation Loss: 0.0035581315221432388\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005823439819505438\n",
      "Training Loss: 0.005860097114928067\n",
      "Training Loss: 0.00593478366441559\n",
      "Validation Loss: 0.0035519937960446675\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.00581664668105077\n",
      "Training Loss: 0.005853921279776841\n",
      "Training Loss: 0.005927759901387617\n",
      "Validation Loss: 0.003545900255446898\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005809931000112556\n",
      "Training Loss: 0.005847811849671416\n",
      "Training Loss: 0.005920809260569513\n",
      "Validation Loss: 0.003539847821230069\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005803290281328373\n",
      "Training Loss: 0.005841764268698171\n",
      "Training Loss: 0.0059139264118857685\n",
      "Validation Loss: 0.0035338356245303883\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005796724302927032\n",
      "Training Loss: 0.005835778061882593\n",
      "Training Loss: 0.0059071110479999335\n",
      "Validation Loss: 0.003527858593760142\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.0057902292243670675\n",
      "Training Loss: 0.005829850609879941\n",
      "Training Loss: 0.005900363588589244\n",
      "Validation Loss: 0.0035219124915324192\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0057838044973323125\n",
      "Training Loss: 0.005823980047716759\n",
      "Training Loss: 0.005893678551656194\n",
      "Validation Loss: 0.003515997828308786\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005777451316243969\n",
      "Training Loss: 0.005818166263634339\n",
      "Training Loss: 0.005887059349333867\n",
      "Validation Loss: 0.0035101083837177477\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005771163533790968\n",
      "Training Loss: 0.005812406488112174\n",
      "Training Loss: 0.005880502871586941\n",
      "Validation Loss: 0.0035042487185584443\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005764945185510442\n",
      "Training Loss: 0.005806698653032072\n",
      "Training Loss: 0.005874009092221968\n",
      "Validation Loss: 0.003498412001278942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005758789769606665\n",
      "Training Loss: 0.005801044261897914\n",
      "Training Loss: 0.00586757724173367\n",
      "Validation Loss: 0.003492599096491912\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005752700197626836\n",
      "Training Loss: 0.00579543924832251\n",
      "Training Loss: 0.005861205931869336\n",
      "Validation Loss: 0.0034868073573838293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005746674807160162\n",
      "Training Loss: 0.005789886863785796\n",
      "Training Loss: 0.00585489502933342\n",
      "Validation Loss: 0.0034810373025914924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005740714624989778\n",
      "Training Loss: 0.00578438323282171\n",
      "Training Loss: 0.005848645018995739\n",
      "Validation Loss: 0.003475286830425932\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005734815013711341\n",
      "Training Loss: 0.005778928103973157\n",
      "Training Loss: 0.00584245448699221\n",
      "Validation Loss: 0.0034695500082838654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005728977473918349\n",
      "Training Loss: 0.005773520902148448\n",
      "Training Loss: 0.005836323010153137\n",
      "Validation Loss: 0.0034638373020989296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005723201528890058\n",
      "Training Loss: 0.005768164566834457\n",
      "Training Loss: 0.005830251310835592\n",
      "Validation Loss: 0.00345814034664543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005717486382927746\n",
      "Training Loss: 0.005762853330234066\n",
      "Training Loss: 0.00582423779880628\n",
      "Validation Loss: 0.0034524620447851967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.00571183152962476\n",
      "Training Loss: 0.005757589327404275\n",
      "Training Loss: 0.005818282402469776\n",
      "Validation Loss: 0.003446800612355321\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005706237024860457\n",
      "Training Loss: 0.005752374092699028\n",
      "Training Loss: 0.005812386677134782\n",
      "Validation Loss: 0.0034411545693115698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.0057007013191469015\n",
      "Training Loss: 0.00574720309174154\n",
      "Training Loss: 0.0058065470185829325\n",
      "Validation Loss: 0.0034355285977735444\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.0056952268397435545\n",
      "Training Loss: 0.0057420795783400536\n",
      "Training Loss: 0.005800765589810908\n",
      "Validation Loss: 0.0034299163743618123\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005689807840390131\n",
      "Training Loss: 0.0057370012800674886\n",
      "Training Loss: 0.005795042001409456\n",
      "Validation Loss: 0.0034243199497518873\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.0056844480009749536\n",
      "Training Loss: 0.005731969171320088\n",
      "Training Loss: 0.005789373713778332\n",
      "Validation Loss: 0.0034187394047150745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005679146305192262\n",
      "Training Loss: 0.005726981272455305\n",
      "Training Loss: 0.0057837641792139035\n",
      "Validation Loss: 0.003413178554469238\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005673903640126809\n",
      "Training Loss: 0.005722039411193691\n",
      "Training Loss: 0.005778209720738232\n",
      "Validation Loss: 0.003407636121061913\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005668716706568375\n",
      "Training Loss: 0.005717142231296748\n",
      "Training Loss: 0.0057727138738846405\n",
      "Validation Loss: 0.0034021096934533017\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005663586010923609\n",
      "Training Loss: 0.005712289173388853\n",
      "Training Loss: 0.005767270975629799\n",
      "Validation Loss: 0.0033965983524201753\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005658512181835249\n",
      "Training Loss: 0.005707480820710771\n",
      "Training Loss: 0.005761886104592122\n",
      "Validation Loss: 0.0033911063353494436\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005653493121499196\n",
      "Training Loss: 0.005702718029497191\n",
      "Training Loss: 0.005756555082043633\n",
      "Validation Loss: 0.003385635036935904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005648530320031569\n",
      "Training Loss: 0.005697997162933462\n",
      "Training Loss: 0.0057512791419867424\n",
      "Validation Loss: 0.0033801766092826225\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005643623286159709\n",
      "Training Loss: 0.005693320505670272\n",
      "Training Loss: 0.005746059284429066\n",
      "Validation Loss: 0.0033747413433732444\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005638768554199487\n",
      "Training Loss: 0.005688686331850477\n",
      "Training Loss: 0.005740891395253129\n",
      "Validation Loss: 0.0033693245626473275\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005633969905320555\n",
      "Training Loss: 0.0056840966118033975\n",
      "Training Loss: 0.005735778731759638\n",
      "Validation Loss: 0.0033639248338229745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005629225671291351\n",
      "Training Loss: 0.005679549167398363\n",
      "Training Loss: 0.005730718518607319\n",
      "Validation Loss: 0.003358551445273222\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.0056245322735048835\n",
      "Training Loss: 0.005675043329247274\n",
      "Training Loss: 0.005725712608546018\n",
      "Validation Loss: 0.0033531893467206213\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005619892668910324\n",
      "Training Loss: 0.00567058140586596\n",
      "Training Loss: 0.005720758837414905\n",
      "Validation Loss: 0.0033478547136817295\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005615304852835834\n",
      "Training Loss: 0.0056661620707018304\n",
      "Training Loss: 0.005715856266324409\n",
      "Validation Loss: 0.003342539683544192\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005610769161721691\n",
      "Training Loss: 0.005661780853988603\n",
      "Training Loss: 0.0057110063894651834\n",
      "Validation Loss: 0.0033372525162367003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.0056062843231484295\n",
      "Training Loss: 0.005657442168449052\n",
      "Training Loss: 0.005706209256313741\n",
      "Validation Loss: 0.0033319773573658607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005601850302191451\n",
      "Training Loss: 0.00565314779698383\n",
      "Training Loss: 0.005701460801064968\n",
      "Validation Loss: 0.00332673059238822\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005597468003979884\n",
      "Training Loss: 0.005648892052704468\n",
      "Training Loss: 0.005696764109306969\n",
      "Validation Loss: 0.003321506588895062\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005593134597875178\n",
      "Training Loss: 0.005644674663199112\n",
      "Training Loss: 0.005692116654245183\n",
      "Validation Loss: 0.0033163071438026604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.00558884869213216\n",
      "Training Loss: 0.005640498545253649\n",
      "Training Loss: 0.005687517696642317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [31:23<03:29, 209.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.003311127384671983\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.08266917198896408\n",
      "Training Loss: 0.06911155117675662\n",
      "Training Loss: 0.05962848545983434\n",
      "Validation Loss: 0.05710290977208132\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.05482303250581026\n",
      "Training Loss: 0.05333246934227645\n",
      "Training Loss: 0.049972866494208575\n",
      "Validation Loss: 0.047785054379550926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.04541149022057653\n",
      "Training Loss: 0.04279415626078844\n",
      "Training Loss: 0.03867092501372099\n",
      "Validation Loss: 0.03550991060191326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.03340893993154168\n",
      "Training Loss: 0.030666815717704594\n",
      "Training Loss: 0.0273026310140267\n",
      "Validation Loss: 0.024833266094871118\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.02380149175412953\n",
      "Training Loss: 0.022104605026543142\n",
      "Training Loss: 0.020194929633289574\n",
      "Validation Loss: 0.018603914741719706\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.01849970801267773\n",
      "Training Loss: 0.01754908630391583\n",
      "Training Loss: 0.01645540901692584\n",
      "Validation Loss: 0.01518299704511765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.015610412140376865\n",
      "Training Loss: 0.015035074958577753\n",
      "Training Loss: 0.01431112297810614\n",
      "Validation Loss: 0.013094693571433759\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.013821840782184154\n",
      "Training Loss: 0.013472262611612678\n",
      "Training Loss: 0.012933342657051981\n",
      "Validation Loss: 0.01168614711999642\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.01261104154633358\n",
      "Training Loss: 0.012410475960932673\n",
      "Training Loss: 0.011971842763014138\n",
      "Validation Loss: 0.01065180983833885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.011737425085157157\n",
      "Training Loss: 0.011627651851158589\n",
      "Training Loss: 0.011246220857137813\n",
      "Validation Loss: 0.009839022673373476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.011063758415402844\n",
      "Training Loss: 0.011009655895177274\n",
      "Training Loss: 0.010664824813138693\n",
      "Validation Loss: 0.009172242745662924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.010516412854194641\n",
      "Training Loss: 0.010497976157348604\n",
      "Training Loss: 0.010177875115768984\n",
      "Validation Loss: 0.008605362008995471\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.01005364089854993\n",
      "Training Loss: 0.01005874624243006\n",
      "Training Loss: 0.00975545852445066\n",
      "Validation Loss: 0.008110040534559763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.009650722366059199\n",
      "Training Loss: 0.009672764369752259\n",
      "Training Loss: 0.00938108524424024\n",
      "Validation Loss: 0.007671176879719067\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.009294820391805842\n",
      "Training Loss: 0.009330843391362578\n",
      "Training Loss: 0.009047831679927185\n",
      "Validation Loss: 0.0072829464392924915\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.008981191673083231\n",
      "Training Loss: 0.009030279760481789\n",
      "Training Loss: 0.008754962650127708\n",
      "Validation Loss: 0.006945145148993208\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.008709806121187285\n",
      "Training Loss: 0.008771798748057336\n",
      "Training Loss: 0.008504637653240934\n",
      "Validation Loss: 0.006659467044035286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.00848207864095457\n",
      "Training Loss: 0.00855653449660167\n",
      "Training Loss: 0.008298577446257695\n",
      "Validation Loss: 0.006425975564228936\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.00829792250180617\n",
      "Training Loss: 0.008383522911462932\n",
      "Training Loss: 0.008135536381741986\n",
      "Validation Loss: 0.006240973682038151\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.00815403059241362\n",
      "Training Loss: 0.008248602733947336\n",
      "Training Loss: 0.008010599234839901\n",
      "Validation Loss: 0.0060970975201200235\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.008044093461940065\n",
      "Training Loss: 0.008145125029841438\n",
      "Training Loss: 0.007916413615457713\n",
      "Validation Loss: 0.005985212457376752\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.007960442894836888\n",
      "Training Loss: 0.008065680854488165\n",
      "Training Loss: 0.007845195818226784\n",
      "Validation Loss: 0.005896581822137735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.007895819939440116\n",
      "Training Loss: 0.008003610770683735\n",
      "Training Loss: 0.007790221736067906\n",
      "Validation Loss: 0.005824138635371843\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.00784438958275132\n",
      "Training Loss: 0.007953715935582296\n",
      "Training Loss: 0.007746402827324346\n",
      "Validation Loss: 0.005762780210777615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.007801935402676463\n",
      "Training Loss: 0.007912289696978405\n",
      "Training Loss: 0.00771019394393079\n",
      "Validation Loss: 0.0057090618922322825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007765607793116942\n",
      "Training Loss: 0.007876817078795284\n",
      "Training Loss: 0.007679236731491983\n",
      "Validation Loss: 0.00566073569333118\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.007733549514086917\n",
      "Training Loss: 0.007845635451376438\n",
      "Training Loss: 0.007652002283139154\n",
      "Validation Loss: 0.005616359735755355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007704567252658307\n",
      "Training Loss: 0.007817644473398104\n",
      "Training Loss: 0.007627495343331248\n",
      "Validation Loss: 0.005574997717232098\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.007677893555955962\n",
      "Training Loss: 0.007792110730661079\n",
      "Training Loss: 0.00760506623541005\n",
      "Validation Loss: 0.0055360323239348076\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.0076530245842877775\n",
      "Training Loss: 0.007768529606983066\n",
      "Training Loss: 0.007584276625420898\n",
      "Validation Loss: 0.005499038681796009\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.007629622489912435\n",
      "Training Loss: 0.007746546647977084\n",
      "Training Loss: 0.007564826383022591\n",
      "Validation Loss: 0.005463719154770873\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.007607451162766665\n",
      "Training Loss: 0.007725903278915211\n",
      "Training Loss: 0.007546497093280777\n",
      "Validation Loss: 0.005429851033808559\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.0075863381684757765\n",
      "Training Loss: 0.00770640155300498\n",
      "Training Loss: 0.007529129877220839\n",
      "Validation Loss: 0.005397261060209254\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.00756615538499318\n",
      "Training Loss: 0.007687891036039218\n",
      "Training Loss: 0.007512601709458977\n",
      "Validation Loss: 0.0053658214890858515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.007546802771976217\n",
      "Training Loss: 0.007670252415118739\n",
      "Training Loss: 0.007496816702187061\n",
      "Validation Loss: 0.005335420479323045\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.00752820011577569\n",
      "Training Loss: 0.007653386512538418\n",
      "Training Loss: 0.007481696655740961\n",
      "Validation Loss: 0.005305970704387037\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.007510282956063747\n",
      "Training Loss: 0.007637216246221215\n",
      "Training Loss: 0.007467178539372981\n",
      "Validation Loss: 0.005277400851390963\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.007492997895460576\n",
      "Training Loss: 0.007621675482951105\n",
      "Training Loss: 0.007453209726372733\n",
      "Validation Loss: 0.005249650768276429\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.0074763002968393265\n",
      "Training Loss: 0.007606710735708475\n",
      "Training Loss: 0.00743974655168131\n",
      "Validation Loss: 0.0052226751531327794\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.007460154101718217\n",
      "Training Loss: 0.00759227835573256\n",
      "Training Loss: 0.0074267528706695885\n",
      "Validation Loss: 0.005196433414945777\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.007444528992054984\n",
      "Training Loss: 0.007578341745538637\n",
      "Training Loss: 0.007414196922909468\n",
      "Validation Loss: 0.005170895360943893\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0074293987895362075\n",
      "Training Loss: 0.007564868208719417\n",
      "Training Loss: 0.007402053084224463\n",
      "Validation Loss: 0.00514604297356701\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.007414743595290929\n",
      "Training Loss: 0.0075518362794537094\n",
      "Training Loss: 0.007390298282261938\n",
      "Validation Loss: 0.005121857455010746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.007400545310229063\n",
      "Training Loss: 0.007539222093764692\n",
      "Training Loss: 0.007378913190914318\n",
      "Validation Loss: 0.0050983282925791285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.007386788961011917\n",
      "Training Loss: 0.007527009180048481\n",
      "Training Loss: 0.0073678818892221895\n",
      "Validation Loss: 0.0050754503999690245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.007373460757080465\n",
      "Training Loss: 0.0075151807453949\n",
      "Training Loss: 0.007357188442256302\n",
      "Validation Loss: 0.005053217906708924\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.007360550393350422\n",
      "Training Loss: 0.007503723229747266\n",
      "Training Loss: 0.007346819119993598\n",
      "Validation Loss: 0.005031630501485943\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.007348044923273847\n",
      "Training Loss: 0.007492623808793723\n",
      "Training Loss: 0.0073367605707608165\n",
      "Validation Loss: 0.005010687694440104\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.007335934775182977\n",
      "Training Loss: 0.0074818692030385135\n",
      "Training Loss: 0.007327000740915537\n",
      "Validation Loss: 0.004990386728228729\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.007324207948986441\n",
      "Training Loss: 0.0074714476242661474\n",
      "Training Loss: 0.0073175267328042535\n",
      "Validation Loss: 0.004970731116894088\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.00731285412563011\n",
      "Training Loss: 0.007461346301715821\n",
      "Training Loss: 0.007308326888596639\n",
      "Validation Loss: 0.0049517154366594185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.0073018617648631335\n",
      "Training Loss: 0.007451552777783945\n",
      "Training Loss: 0.007299388786777854\n",
      "Validation Loss: 0.00493333916888792\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.0072912171855568884\n",
      "Training Loss: 0.007442053776467219\n",
      "Training Loss: 0.007290700286393986\n",
      "Validation Loss: 0.004915598829622182\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.007280909279361367\n",
      "Training Loss: 0.007432835631771013\n",
      "Training Loss: 0.007282247688854113\n",
      "Validation Loss: 0.004898483811987543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.007270924417534843\n",
      "Training Loss: 0.007423884337767959\n",
      "Training Loss: 0.007274019313044846\n",
      "Validation Loss: 0.004881989579710565\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.007261248534778133\n",
      "Training Loss: 0.007415187233127654\n",
      "Training Loss: 0.007266002626856789\n",
      "Validation Loss: 0.0048661059242139535\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.007251868383027613\n",
      "Training Loss: 0.007406729466747492\n",
      "Training Loss: 0.0072581860574427995\n",
      "Validation Loss: 0.004850819784091011\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.007242769749136641\n",
      "Training Loss: 0.007398497413378209\n",
      "Training Loss: 0.007250555729260668\n",
      "Validation Loss: 0.00483611782913337\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.0072339392243884505\n",
      "Training Loss: 0.007390478741144762\n",
      "Training Loss: 0.007243102174252272\n",
      "Validation Loss: 0.004821986084924278\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.0072253638261463495\n",
      "Training Loss: 0.007382660025032237\n",
      "Training Loss: 0.007235812402796\n",
      "Validation Loss: 0.004808407376933676\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.0072170287428889425\n",
      "Training Loss: 0.007375027075177058\n",
      "Training Loss: 0.00722867725067772\n",
      "Validation Loss: 0.004795364724221022\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.007208922590361908\n",
      "Training Loss: 0.007367570274509489\n",
      "Training Loss: 0.007221686274278909\n",
      "Validation Loss: 0.004782844803118137\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.007201032405719161\n",
      "Training Loss: 0.007360277879051864\n",
      "Training Loss: 0.0072148301918059585\n",
      "Validation Loss: 0.004770822490806158\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.007193347042193637\n",
      "Training Loss: 0.007353138258913532\n",
      "Training Loss: 0.007208100854186341\n",
      "Validation Loss: 0.004759285799123012\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.00718585500260815\n",
      "Training Loss: 0.007346143583999947\n",
      "Training Loss: 0.007201490750303492\n",
      "Validation Loss: 0.004748212989796413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.007178546724608168\n",
      "Training Loss: 0.007339284025365486\n",
      "Training Loss: 0.007194992851000279\n",
      "Validation Loss: 0.004737583070789369\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0071714116749353705\n",
      "Training Loss: 0.007332552344305441\n",
      "Training Loss: 0.0071886013576295225\n",
      "Validation Loss: 0.0047273825071856715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.007164440987398848\n",
      "Training Loss: 0.007325940477894619\n",
      "Training Loss: 0.007182309931376949\n",
      "Validation Loss: 0.004717589705251157\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.007157627162523567\n",
      "Training Loss: 0.0073194423306267705\n",
      "Training Loss: 0.007176114270696417\n",
      "Validation Loss: 0.004708185754036217\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.007150961043080315\n",
      "Training Loss: 0.007313051734818146\n",
      "Training Loss: 0.0071700097457505765\n",
      "Validation Loss: 0.004699153369135653\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.007144436776870862\n",
      "Training Loss: 0.0073067628685384986\n",
      "Training Loss: 0.007163992807036266\n",
      "Validation Loss: 0.004690476062738996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.007138045857427642\n",
      "Training Loss: 0.007300571835367009\n",
      "Training Loss: 0.007158059594221413\n",
      "Validation Loss: 0.004682137511848482\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.007131783387158066\n",
      "Training Loss: 0.00729447407880798\n",
      "Training Loss: 0.007152207599719986\n",
      "Validation Loss: 0.004674117719189504\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.007125642760656774\n",
      "Training Loss: 0.007288465471938252\n",
      "Training Loss: 0.007146433079615235\n",
      "Validation Loss: 0.0046664040386572144\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.007119618859142065\n",
      "Training Loss: 0.007282542800530791\n",
      "Training Loss: 0.007140734220156446\n",
      "Validation Loss: 0.004658979638427329\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.007113706032978371\n",
      "Training Loss: 0.007276701827067882\n",
      "Training Loss: 0.007135108630172908\n",
      "Validation Loss: 0.004651827373400624\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.0071078992553520945\n",
      "Training Loss: 0.0072709406504873186\n",
      "Training Loss: 0.007129553658305667\n",
      "Validation Loss: 0.004644937478424458\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0071021937381010505\n",
      "Training Loss: 0.007265255491947755\n",
      "Training Loss: 0.00712406707520131\n",
      "Validation Loss: 0.004638288925490813\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.007096584821119904\n",
      "Training Loss: 0.007259643542347476\n",
      "Training Loss: 0.007118647714378312\n",
      "Validation Loss: 0.00463187463615571\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.007091067800065503\n",
      "Training Loss: 0.007254102707374841\n",
      "Training Loss: 0.007113292252179235\n",
      "Validation Loss: 0.004625678658249966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0070856389089021836\n",
      "Training Loss: 0.007248630557442084\n",
      "Training Loss: 0.007107998697902076\n",
      "Validation Loss: 0.004619687123961872\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0070802931417711075\n",
      "Training Loss: 0.007243223887635395\n",
      "Training Loss: 0.007102765819872729\n",
      "Validation Loss: 0.004613891868522454\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.0070750275591854005\n",
      "Training Loss: 0.007237880572210998\n",
      "Training Loss: 0.0070975910336710515\n",
      "Validation Loss: 0.004608276458404791\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.007069837176240981\n",
      "Training Loss: 0.007232598260743544\n",
      "Training Loss: 0.007092472478980198\n",
      "Validation Loss: 0.004602835813845937\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.0070647188497241584\n",
      "Training Loss: 0.007227374620269984\n",
      "Training Loss: 0.0070874076196923855\n",
      "Validation Loss: 0.004597551600740741\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.007059667079010979\n",
      "Training Loss: 0.007222206828882918\n",
      "Training Loss: 0.007082394570461475\n",
      "Validation Loss: 0.00459241942669987\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.00705467990366742\n",
      "Training Loss: 0.007217092450009659\n",
      "Training Loss: 0.007077430061181076\n",
      "Validation Loss: 0.004587428219187377\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.007049752749735489\n",
      "Training Loss: 0.007212029050569981\n",
      "Training Loss: 0.007072513003950007\n",
      "Validation Loss: 0.004582568145032679\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.007044881847687066\n",
      "Training Loss: 0.007207014936720952\n",
      "Training Loss: 0.007067640940658748\n",
      "Validation Loss: 0.004577829975378438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.007040063929744065\n",
      "Training Loss: 0.007202046504244208\n",
      "Training Loss: 0.007062810516217723\n",
      "Validation Loss: 0.004573204139315555\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.007035295085515827\n",
      "Training Loss: 0.007197121903300285\n",
      "Training Loss: 0.0070580207847524435\n",
      "Validation Loss: 0.004568684379193472\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.007030572303337976\n",
      "Training Loss: 0.00719223887543194\n",
      "Training Loss: 0.007053268299787305\n",
      "Validation Loss: 0.004564259657793249\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.007025891864905134\n",
      "Training Loss: 0.0071873943135142325\n",
      "Training Loss: 0.0070485509815625845\n",
      "Validation Loss: 0.004559926618037097\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.0070212507352698595\n",
      "Training Loss: 0.007182586132548749\n",
      "Training Loss: 0.007043866484891624\n",
      "Validation Loss: 0.004555675138332201\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.007016645160038024\n",
      "Training Loss: 0.007177811660803854\n",
      "Training Loss: 0.007039211800438352\n",
      "Validation Loss: 0.004551497892361511\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.007012071873759851\n",
      "Training Loss: 0.007173067922703922\n",
      "Training Loss: 0.0070345853245817125\n",
      "Validation Loss: 0.004547390012918145\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.007007528389804066\n",
      "Training Loss: 0.00716835300787352\n",
      "Training Loss: 0.0070299841859377925\n",
      "Validation Loss: 0.004543345242351545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.007003011489287019\n",
      "Training Loss: 0.0071636649989522995\n",
      "Training Loss: 0.007025407020701095\n",
      "Validation Loss: 0.004539356394304654\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006998518018517644\n",
      "Training Loss: 0.007159000184619799\n",
      "Training Loss: 0.007020850269473158\n",
      "Validation Loss: 0.004535419752643433\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006994045503670349\n",
      "Training Loss: 0.00715435786289163\n",
      "Training Loss: 0.007016311298939399\n",
      "Validation Loss: 0.00453152563200552\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006989590128650889\n",
      "Training Loss: 0.007149733527330682\n",
      "Training Loss: 0.007011788764502853\n",
      "Validation Loss: 0.004527671843390452\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006985150395194068\n",
      "Training Loss: 0.0071451258554589\n",
      "Training Loss: 0.007007279683020898\n",
      "Validation Loss: 0.004523853528747607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006980722872540355\n",
      "Training Loss: 0.007140532658668235\n",
      "Training Loss: 0.0070027823053533216\n",
      "Validation Loss: 0.0045200659968011245\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006976305554853752\n",
      "Training Loss: 0.007135951054515317\n",
      "Training Loss: 0.006998293452197686\n",
      "Validation Loss: 0.004516302349616177\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006971895524766296\n",
      "Training Loss: 0.007131378945196047\n",
      "Training Loss: 0.006993812849977985\n",
      "Validation Loss: 0.004512560959994333\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006967490537790582\n",
      "Training Loss: 0.007126814104849472\n",
      "Training Loss: 0.006989336114493198\n",
      "Validation Loss: 0.0045088361817924826\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.00696308798273094\n",
      "Training Loss: 0.007122254318092019\n",
      "Training Loss: 0.006984862465178594\n",
      "Validation Loss: 0.004505124892725536\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006958685124991462\n",
      "Training Loss: 0.007117697352077812\n",
      "Training Loss: 0.0069803890114417295\n",
      "Validation Loss: 0.004501423655925423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006954281599028036\n",
      "Training Loss: 0.007113141011213884\n",
      "Training Loss: 0.006975914239301346\n",
      "Validation Loss: 0.0044977263602417675\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.0069498736562673006\n",
      "Training Loss: 0.0071085828798823055\n",
      "Training Loss: 0.006971436021267437\n",
      "Validation Loss: 0.0044940318003181745\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.00694545938516967\n",
      "Training Loss: 0.007104021008126437\n",
      "Training Loss: 0.006966952128568665\n",
      "Validation Loss: 0.004490334187435468\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006941036866046488\n",
      "Training Loss: 0.007099453514674678\n",
      "Training Loss: 0.006962460989016108\n",
      "Validation Loss: 0.0044866348839145195\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006936604512156918\n",
      "Training Loss: 0.007094877894269302\n",
      "Training Loss: 0.006957959621795453\n",
      "Validation Loss: 0.004482925220083947\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006932160396827385\n",
      "Training Loss: 0.00709029242862016\n",
      "Training Loss: 0.006953447107225657\n",
      "Validation Loss: 0.004479208604034999\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006927702265093103\n",
      "Training Loss: 0.0070856942981481555\n",
      "Training Loss: 0.006948920824797824\n",
      "Validation Loss: 0.004475475250994473\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006923227967927232\n",
      "Training Loss: 0.007081082592485473\n",
      "Training Loss: 0.006944379875203594\n",
      "Validation Loss: 0.004471729094230601\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.00691873635398224\n",
      "Training Loss: 0.007076455224305392\n",
      "Training Loss: 0.006939821884152479\n",
      "Validation Loss: 0.004467961452299666\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006914225614164025\n",
      "Training Loss: 0.007071808682521805\n",
      "Training Loss: 0.006935244519263506\n",
      "Validation Loss: 0.0044641727241530515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.00690969337709248\n",
      "Training Loss: 0.007067142963642255\n",
      "Training Loss: 0.006930646603577771\n",
      "Validation Loss: 0.004460358824790194\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006905138953588903\n",
      "Training Loss: 0.007062455414561555\n",
      "Training Loss: 0.006926026785513386\n",
      "Validation Loss: 0.004456518258470414\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006900560251669958\n",
      "Training Loss: 0.007057743606856093\n",
      "Training Loss: 0.006921383023145608\n",
      "Validation Loss: 0.004452647305135563\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.0068959550361614675\n",
      "Training Loss: 0.0070530068525113164\n",
      "Training Loss: 0.006916712936363183\n",
      "Validation Loss: 0.00444874433235506\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006891322619048878\n",
      "Training Loss: 0.007048242484452203\n",
      "Training Loss: 0.006912016022251919\n",
      "Validation Loss: 0.004444809172700128\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006886662057368085\n",
      "Training Loss: 0.0070434485992882405\n",
      "Training Loss: 0.006907289907685481\n",
      "Validation Loss: 0.004440836531889698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006881969579262659\n",
      "Training Loss: 0.0070386235520709306\n",
      "Training Loss: 0.0069025331223383545\n",
      "Validation Loss: 0.004436825188870929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0068772457458544526\n",
      "Training Loss: 0.00703376570949331\n",
      "Training Loss: 0.006897743777954019\n",
      "Validation Loss: 0.004432769782652848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006872487425571307\n",
      "Training Loss: 0.007028873045928776\n",
      "Training Loss: 0.006892921616672538\n",
      "Validation Loss: 0.004428670839720479\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006867694716202095\n",
      "Training Loss: 0.007023943436797709\n",
      "Training Loss: 0.006888063794467598\n",
      "Validation Loss: 0.004424525470619373\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006862864880240523\n",
      "Training Loss: 0.0070189760671928525\n",
      "Training Loss: 0.006883169180364348\n",
      "Validation Loss: 0.004420328119460995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006857995854807087\n",
      "Training Loss: 0.0070139682816807184\n",
      "Training Loss: 0.006878236557822675\n",
      "Validation Loss: 0.004416079686829046\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006853087503113784\n",
      "Training Loss: 0.007008917656494304\n",
      "Training Loss: 0.006873263686429709\n",
      "Validation Loss: 0.004411776165551181\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.0068481382075697185\n",
      "Training Loss: 0.007003823613049462\n",
      "Training Loss: 0.006868249602266587\n",
      "Validation Loss: 0.00440741520116022\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.00684314533660654\n",
      "Training Loss: 0.006998684001155197\n",
      "Training Loss: 0.0068631937785539774\n",
      "Validation Loss: 0.00440299419327796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006838108223164454\n",
      "Training Loss: 0.006993496417999267\n",
      "Training Loss: 0.006858093410846778\n",
      "Validation Loss: 0.004398508083724072\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006833024601801299\n",
      "Training Loss: 0.006988260024227202\n",
      "Training Loss: 0.006852947533479892\n",
      "Validation Loss: 0.004393956608275015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.0068278937949799\n",
      "Training Loss: 0.0069829722051508725\n",
      "Training Loss: 0.006847755449125543\n",
      "Validation Loss: 0.004389332583843825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006822712435387075\n",
      "Training Loss: 0.006977630569599569\n",
      "Training Loss: 0.006842514906311408\n",
      "Validation Loss: 0.004384637244563717\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006817480102181434\n",
      "Training Loss: 0.006972233895212412\n",
      "Training Loss: 0.0068372244009515274\n",
      "Validation Loss: 0.004379864420422636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006812194451340474\n",
      "Training Loss: 0.00696677895844914\n",
      "Training Loss: 0.006831882529659197\n",
      "Validation Loss: 0.004375012306983091\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006806853671441786\n",
      "Training Loss: 0.006961264690617099\n",
      "Training Loss: 0.006826488458900712\n",
      "Validation Loss: 0.004370076138411094\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006801455585518852\n",
      "Training Loss: 0.006955688251182437\n",
      "Training Loss: 0.0068210399005329235\n",
      "Validation Loss: 0.004365049241754237\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006795997878652997\n",
      "Training Loss: 0.006950047767022624\n",
      "Training Loss: 0.006815535499481484\n",
      "Validation Loss: 0.004359931583657568\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006790478880866431\n",
      "Training Loss: 0.006944340055342764\n",
      "Training Loss: 0.006809973331401125\n",
      "Validation Loss: 0.004354714956186879\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006784896319149993\n",
      "Training Loss: 0.00693856266210787\n",
      "Training Loss: 0.006804352905601263\n",
      "Validation Loss: 0.004349398513042023\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006779246679507196\n",
      "Training Loss: 0.00693271316238679\n",
      "Training Loss: 0.006798671009019017\n",
      "Validation Loss: 0.00434397168070329\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006773527678451501\n",
      "Training Loss: 0.006926787672564387\n",
      "Training Loss: 0.006792926508933306\n",
      "Validation Loss: 0.004338432406860121\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006767735924804583\n",
      "Training Loss: 0.006920783687382937\n",
      "Training Loss: 0.006787116938503459\n",
      "Validation Loss: 0.004332774361919905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.00676186868455261\n",
      "Training Loss: 0.006914697617758065\n",
      "Training Loss: 0.006781240020645782\n",
      "Validation Loss: 0.004326989207144701\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006755922263255343\n",
      "Training Loss: 0.006908525413600728\n",
      "Training Loss: 0.006775294500403106\n",
      "Validation Loss: 0.004321073440264576\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006749892616062425\n",
      "Training Loss: 0.006902262176154182\n",
      "Training Loss: 0.006769276526756585\n",
      "Validation Loss: 0.004315015600910515\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006743775563081727\n",
      "Training Loss: 0.006895904722623527\n",
      "Training Loss: 0.006763184424489737\n",
      "Validation Loss: 0.004308810549149855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006737566709052772\n",
      "Training Loss: 0.006889447261346504\n",
      "Training Loss: 0.006757014209870249\n",
      "Validation Loss: 0.004302448299425664\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.0067312602698802945\n",
      "Training Loss: 0.006882884424412623\n",
      "Training Loss: 0.00675076300627552\n",
      "Validation Loss: 0.004295916927669699\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006724850508617237\n",
      "Training Loss: 0.006876210187328979\n",
      "Training Loss: 0.006744427763624117\n",
      "Validation Loss: 0.004289207094495468\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006718331435113214\n",
      "Training Loss: 0.006869417204288766\n",
      "Training Loss: 0.006738003661157563\n",
      "Validation Loss: 0.004282307396433578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006711695863632485\n",
      "Training Loss: 0.006862498186528683\n",
      "Training Loss: 0.006731485666241497\n",
      "Validation Loss: 0.004275202668753316\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006704934929148294\n",
      "Training Loss: 0.006855445287656039\n",
      "Training Loss: 0.0067248694703448565\n",
      "Validation Loss: 0.004267879963847233\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006698040260234847\n",
      "Training Loss: 0.006848248526221141\n",
      "Training Loss: 0.0067181493435055015\n",
      "Validation Loss: 0.00426032181614696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.0066910018876660616\n",
      "Training Loss: 0.006840897805523128\n",
      "Training Loss: 0.006711318378802389\n",
      "Validation Loss: 0.004252506222502653\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006683807013323531\n",
      "Training Loss: 0.006833380541065708\n",
      "Training Loss: 0.0067043695389293135\n",
      "Validation Loss: 0.004244417000530559\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.0066764433559728786\n",
      "Training Loss: 0.006825683058705181\n",
      "Training Loss: 0.006697294091572985\n",
      "Validation Loss: 0.0042360291961486335\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006668896332266741\n",
      "Training Loss: 0.006817790616769344\n",
      "Training Loss: 0.006690083011053502\n",
      "Validation Loss: 0.004227315868366133\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0066611481324071065\n",
      "Training Loss: 0.006809685480548069\n",
      "Training Loss: 0.006682724502170458\n",
      "Validation Loss: 0.004218248450955941\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.00665317929990124\n",
      "Training Loss: 0.006801347916480154\n",
      "Training Loss: 0.006675206656800583\n",
      "Validation Loss: 0.004208792241687855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006644967566826381\n",
      "Training Loss: 0.006792753608897329\n",
      "Training Loss: 0.0066675148881040515\n",
      "Validation Loss: 0.004198908670465305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006636487208306789\n",
      "Training Loss: 0.006783878562273458\n",
      "Training Loss: 0.006659631534712389\n",
      "Validation Loss: 0.004188554526275296\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006627707313164137\n",
      "Training Loss: 0.006774691028986126\n",
      "Training Loss: 0.006651537065627053\n",
      "Validation Loss: 0.004177683648480667\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006618593714665622\n",
      "Training Loss: 0.0067651574953924865\n",
      "Training Loss: 0.0066432091034948825\n",
      "Validation Loss: 0.004166236858856812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006609107244876213\n",
      "Training Loss: 0.006755237546749413\n",
      "Training Loss: 0.006634620111435651\n",
      "Validation Loss: 0.004154151559969557\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.0065991991845658045\n",
      "Training Loss: 0.006744884632062167\n",
      "Training Loss: 0.0066257385967765006\n",
      "Validation Loss: 0.004141355503685354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006588816872099414\n",
      "Training Loss: 0.006734046421479433\n",
      "Training Loss: 0.006616527497535571\n",
      "Validation Loss: 0.004127769496846484\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006577896447270177\n",
      "Training Loss: 0.006722660743398592\n",
      "Training Loss: 0.0066069425793830305\n",
      "Validation Loss: 0.004113299780645606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.0065663648647023365\n",
      "Training Loss: 0.006710659259697422\n",
      "Training Loss: 0.006596934219123796\n",
      "Validation Loss: 0.004097850625717154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006554138981737196\n",
      "Training Loss: 0.00669796317233704\n",
      "Training Loss: 0.0065864419867284595\n",
      "Validation Loss: 0.004081314441413106\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006541124832001515\n",
      "Training Loss: 0.006684484129073099\n",
      "Training Loss: 0.006575397597625852\n",
      "Validation Loss: 0.00406358214134476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006527215512469411\n",
      "Training Loss: 0.006670128152472898\n",
      "Training Loss: 0.006563723030267283\n",
      "Validation Loss: 0.004044543854301021\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006512296459404752\n",
      "Training Loss: 0.006654793951893226\n",
      "Training Loss: 0.0065513308718800544\n",
      "Validation Loss: 0.004024105926771554\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006496247892500833\n",
      "Training Loss: 0.006638383822282776\n",
      "Training Loss: 0.006538130106637254\n",
      "Validation Loss: 0.004002207722724154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006478956650244072\n",
      "Training Loss: 0.006620811048196629\n",
      "Training Loss: 0.006524024716345594\n",
      "Validation Loss: 0.003978841102242553\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.0064603239041753115\n",
      "Training Loss: 0.006602013555821031\n",
      "Training Loss: 0.006508926938986406\n",
      "Validation Loss: 0.003954091647592781\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006440292682964355\n",
      "Training Loss: 0.006581976126180961\n",
      "Training Loss: 0.006492768524913118\n",
      "Validation Loss: 0.00392816427524798\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006418867880129255\n",
      "Training Loss: 0.006560749458149075\n",
      "Training Loss: 0.00647551785223186\n",
      "Validation Loss: 0.0039014307660406476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006396147282794118\n",
      "Training Loss: 0.006538469274528324\n",
      "Training Loss: 0.006457191103836521\n",
      "Validation Loss: 0.003874419665806456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.006372335185296833\n",
      "Training Loss: 0.006515359411714598\n",
      "Training Loss: 0.006437871227972209\n",
      "Validation Loss: 0.003847798946041488\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0063477451406652105\n",
      "Training Loss: 0.006491718081524596\n",
      "Training Loss: 0.0064176996366586535\n",
      "Validation Loss: 0.003822282354316015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.006322765467921272\n",
      "Training Loss: 0.0064678762608673425\n",
      "Training Loss: 0.006396874118363485\n",
      "Validation Loss: 0.003798517072180893\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006297808417584747\n",
      "Training Loss: 0.00644415375078097\n",
      "Training Loss: 0.006375623374478892\n",
      "Validation Loss: 0.0037769585431898746\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006273246540804394\n",
      "Training Loss: 0.0064208234858233485\n",
      "Training Loss: 0.006354189091362059\n",
      "Validation Loss: 0.0037578223037711355\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.006249376755440608\n",
      "Training Loss: 0.006398090174188837\n",
      "Training Loss: 0.0063328019483014945\n",
      "Validation Loss: 0.003741090560692959\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.0062263990915380416\n",
      "Training Loss: 0.006376096123130992\n",
      "Training Loss: 0.006311667429981753\n",
      "Validation Loss: 0.0037265746478838953\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006204426599433646\n",
      "Training Loss: 0.006354931064415723\n",
      "Training Loss: 0.006290953523712233\n",
      "Validation Loss: 0.003714000379792258\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006183506534434855\n",
      "Training Loss: 0.006334647194016725\n",
      "Training Loss: 0.006270789378322661\n",
      "Validation Loss: 0.003703068578851231\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006163639098522253\n",
      "Training Loss: 0.006315264500444755\n",
      "Training Loss: 0.006251262193545699\n",
      "Validation Loss: 0.003693497610890589\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006144794836291112\n",
      "Training Loss: 0.00629678342025727\n",
      "Training Loss: 0.006232423586770892\n",
      "Validation Loss: 0.0036850434209984005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.00612692725553643\n",
      "Training Loss: 0.006279183949809522\n",
      "Training Loss: 0.006214296680409462\n",
      "Validation Loss: 0.003677497835688586\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006109978385502472\n",
      "Training Loss: 0.006262436436954886\n",
      "Training Loss: 0.006196881186915562\n",
      "Validation Loss: 0.00367068344847498\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006093887016177178\n",
      "Training Loss: 0.006246500340057537\n",
      "Training Loss: 0.006180163475219161\n",
      "Validation Loss: 0.003664463280380986\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006078588942764327\n",
      "Training Loss: 0.006231332146562636\n",
      "Training Loss: 0.006164117451990023\n",
      "Validation Loss: 0.0036587174013884886\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.006064021249767393\n",
      "Training Loss: 0.006216883073793724\n",
      "Training Loss: 0.0061487105069682\n",
      "Validation Loss: 0.0036533516884017527\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0060501235444098715\n",
      "Training Loss: 0.006203103634761647\n",
      "Training Loss: 0.006133906489703804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [34:52<00:00, 209.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.003648286801762879\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Shape of the data after splitting into sequences: (28500, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Loss: 0.46319680146873\n",
      "Training Loss: 0.3513029595836997\n",
      "Training Loss: 0.27977054096758364\n",
      "Validation Loss: 0.17813762826740406\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.13864984806627034\n",
      "Training Loss: 0.07669134989148006\n",
      "Training Loss: 0.06946284654550254\n",
      "Validation Loss: 0.060813385302598555\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.056825466863811014\n",
      "Training Loss: 0.053380514876917\n",
      "Training Loss: 0.05778562389314175\n",
      "Validation Loss: 0.055509092056014564\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.05199162468779832\n",
      "Training Loss: 0.049017340186983345\n",
      "Training Loss: 0.05241599593311548\n",
      "Validation Loss: 0.050191205978560985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.04686880961759016\n",
      "Training Loss: 0.04389813920948654\n",
      "Training Loss: 0.046495922897011044\n",
      "Validation Loss: 0.044151432299463264\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.0408857792452909\n",
      "Training Loss: 0.03779811636544764\n",
      "Training Loss: 0.039497067374177276\n",
      "Validation Loss: 0.03699148336446352\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.033703945371089505\n",
      "Training Loss: 0.0305122198513709\n",
      "Training Loss: 0.031284891916438934\n",
      "Validation Loss: 0.028827842366829348\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.02558437226631213\n",
      "Training Loss: 0.02256040192907676\n",
      "Training Loss: 0.022649026273284106\n",
      "Validation Loss: 0.02074542149294461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.017760710561851738\n",
      "Training Loss: 0.015369354407303035\n",
      "Training Loss: 0.015245076448190958\n",
      "Validation Loss: 0.014363212857573304\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.011844892327790148\n",
      "Training Loss: 0.010334542861091905\n",
      "Training Loss: 0.010268966128351167\n",
      "Validation Loss: 0.010266968820196985\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.008265994078465155\n",
      "Training Loss: 0.007489093939075247\n",
      "Training Loss: 0.007488030386157334\n",
      "Validation Loss: 0.007820402675575126\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.006314053683272505\n",
      "Training Loss: 0.006017773824278265\n",
      "Training Loss: 0.006047454855870455\n",
      "Validation Loss: 0.006328007180814035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.005240682646690403\n",
      "Training Loss: 0.005222659468417987\n",
      "Training Loss: 0.005256766460370273\n",
      "Validation Loss: 0.005398278322144087\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.004589850406046025\n",
      "Training Loss: 0.004700601185904815\n",
      "Training Loss: 0.0047157839720603075\n",
      "Validation Loss: 0.004759514545420122\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.004109886786754942\n",
      "Training Loss: 0.004272837225580588\n",
      "Training Loss: 0.004269044646061957\n",
      "Validation Loss: 0.004264396494332952\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.0037113902062992565\n",
      "Training Loss: 0.00389987672038842\n",
      "Training Loss: 0.0038858828248339704\n",
      "Validation Loss: 0.0038592600361103966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.0033747043120092714\n",
      "Training Loss: 0.003578351413598284\n",
      "Training Loss: 0.0035590316070010885\n",
      "Validation Loss: 0.0035234985783502587\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.003091359707832453\n",
      "Training Loss: 0.0033038440969539807\n",
      "Training Loss: 0.0032797572822892106\n",
      "Validation Loss: 0.0032440144087978002\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.0028525503030687105\n",
      "Training Loss: 0.0030692838915274477\n",
      "Training Loss: 0.0030393227338208818\n",
      "Validation Loss: 0.003008297868229951\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.002649585356703028\n",
      "Training Loss: 0.0028674775452236646\n",
      "Training Loss: 0.0028306975559098647\n",
      "Validation Loss: 0.0028059092550433836\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.002474985017615836\n",
      "Training Loss: 0.0026920632031396962\n",
      "Training Loss: 0.0026483295418438502\n",
      "Validation Loss: 0.002629708262288608\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0023228698990715203\n",
      "Training Loss: 0.0025378027130500413\n",
      "Training Loss: 0.0024878691721823996\n",
      "Validation Loss: 0.002474992795980776\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.0021889730606926604\n",
      "Training Loss: 0.0024007222201908006\n",
      "Training Loss: 0.002346121539885644\n",
      "Validation Loss: 0.002338590507861227\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.0020704535215918442\n",
      "Training Loss: 0.0022780693227832673\n",
      "Training Loss: 0.0022208914993098005\n",
      "Validation Loss: 0.0022183158595162037\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0019654933497076856\n",
      "Training Loss: 0.002167990648158593\n",
      "Training Loss: 0.002110521019785665\n",
      "Validation Loss: 0.0021124236600269467\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.0018727313457929996\n",
      "Training Loss: 0.0020689966305508277\n",
      "Training Loss: 0.002013229081348982\n",
      "Validation Loss: 0.0020189438917114283\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.001790588998410385\n",
      "Training Loss: 0.0019792737017269245\n",
      "Training Loss: 0.0019263848147238604\n",
      "Validation Loss: 0.0019349794144208512\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.0017166203788656275\n",
      "Training Loss: 0.0018959853900014424\n",
      "Training Loss: 0.0018459343109861947\n",
      "Validation Loss: 0.0018561432809108512\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.0016470380162354559\n",
      "Training Loss: 0.0018147216280340217\n",
      "Training Loss: 0.001766155041696038\n",
      "Validation Loss: 0.0017763319839932694\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.001576560187531868\n",
      "Training Loss: 0.0017293285457708407\n",
      "Training Loss: 0.0016800045577110723\n",
      "Validation Loss: 0.0016882484180334788\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.001499020193296019\n",
      "Training Loss: 0.0016329946398036554\n",
      "Training Loss: 0.0015809501454350538\n",
      "Validation Loss: 0.001585892310073034\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.0014097612805198877\n",
      "Training Loss: 0.0015220173686975613\n",
      "Training Loss: 0.0014673046220559627\n",
      "Validation Loss: 0.0014697576456394548\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.0013100221098284238\n",
      "Training Loss: 0.0014011299663980025\n",
      "Training Loss: 0.001346501644293312\n",
      "Validation Loss: 0.001350063198178044\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.0012089414107322228\n",
      "Training Loss: 0.0012832956288184505\n",
      "Training Loss: 0.001232248288433766\n",
      "Validation Loss: 0.0012410578438606678\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.0011182248375553171\n",
      "Training Loss: 0.0011814626640989445\n",
      "Training Loss: 0.0011359826345869806\n",
      "Validation Loss: 0.001152326286511412\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.0010452917925431392\n",
      "Training Loss: 0.0011017201424692758\n",
      "Training Loss: 0.0010615178124862724\n",
      "Validation Loss: 0.001085058362334391\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0009903708583442494\n",
      "Training Loss: 0.0010408811045635956\n",
      "Training Loss: 0.0010035854569287038\n",
      "Validation Loss: 0.0010313553994932222\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.0009454517948324792\n",
      "Training Loss: 0.00098445235453255\n",
      "Training Loss: 0.0009458553191507235\n",
      "Validation Loss: 0.0009710362308274097\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.0008899565737374359\n",
      "Training Loss: 0.0008997607851051726\n",
      "Training Loss: 0.0008570814310951392\n",
      "Validation Loss: 0.0008728488455029477\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.0007953879888373195\n",
      "Training Loss: 0.0007709103514935123\n",
      "Training Loss: 0.0007491360286803684\n",
      "Validation Loss: 0.0007792588119300899\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.000712787460815889\n",
      "Training Loss: 0.0006912025636847829\n",
      "Training Loss: 0.0006933262685197406\n",
      "Validation Loss: 0.0007306717528622818\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0006715667768867206\n",
      "Training Loss: 0.0006507464624155545\n",
      "Training Loss: 0.0006585902634105877\n",
      "Validation Loss: 0.0006938599121333048\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0006403688645968942\n",
      "Training Loss: 0.0006196517320495331\n",
      "Training Loss: 0.0006299560588377062\n",
      "Validation Loss: 0.0006632428876094238\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0006135736600026575\n",
      "Training Loss: 0.0005931455785321304\n",
      "Training Loss: 0.0006046037431951845\n",
      "Validation Loss: 0.0006365286525773157\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.0005894465959681839\n",
      "Training Loss: 0.000569418151571881\n",
      "Training Loss: 0.0005813234591914806\n",
      "Validation Loss: 0.0006123140951836994\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.0005670564975662273\n",
      "Training Loss: 0.0005474832282197895\n",
      "Training Loss: 0.0005594194350851466\n",
      "Validation Loss: 0.0005897434398576387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0005458566617107863\n",
      "Training Loss: 0.0005267534931044793\n",
      "Training Loss: 0.0005384553154726745\n",
      "Validation Loss: 0.0005682767570781354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0005255127050077136\n",
      "Training Loss: 0.0005068667919113068\n",
      "Training Loss: 0.0005181513924617321\n",
      "Validation Loss: 0.0005475613678539999\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.000505817135945108\n",
      "Training Loss: 0.0004875954958697548\n",
      "Training Loss: 0.0004983315764548024\n",
      "Validation Loss: 0.0005273698929506302\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.00048664402314898327\n",
      "Training Loss: 0.00046879973335308024\n",
      "Training Loss: 0.0004788958751305472\n",
      "Validation Loss: 0.0005075589085280309\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.0004679245125043963\n",
      "Training Loss: 0.0004504031025135191\n",
      "Training Loss: 0.00045979785383678975\n",
      "Validation Loss: 0.00048805237208163427\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.00044963144276152886\n",
      "Training Loss: 0.0004323761050181929\n",
      "Training Loss: 0.00044103719512349925\n",
      "Validation Loss: 0.00046882359474489276\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.0004317708717826463\n",
      "Training Loss: 0.0004147298925090581\n",
      "Training Loss: 0.00042265061507350764\n",
      "Validation Loss: 0.0004498969507305569\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.00041437833943746226\n",
      "Training Loss: 0.00039751090211211706\n",
      "Training Loss: 0.0004047089286177652\n",
      "Validation Loss: 0.00043133657801375735\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.00039751545191393236\n",
      "Training Loss: 0.00038079741163528526\n",
      "Training Loss: 0.00038731169595848766\n",
      "Validation Loss: 0.000413247250057129\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.00038126529672808827\n",
      "Training Loss: 0.00036469290680543056\n",
      "Training Loss: 0.00037058043912111313\n",
      "Validation Loss: 0.00039576165735514916\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.00036572606602931047\n",
      "Training Loss: 0.00034931634942040544\n",
      "Training Loss: 0.00035464676257106474\n",
      "Validation Loss: 0.00037902135399021106\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.0003509986704648327\n",
      "Training Loss: 0.00033478757341072197\n",
      "Training Loss: 0.00033963681176828685\n",
      "Validation Loss: 0.000363167004170566\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00033717474460900123\n",
      "Training Loss: 0.00032121088333951774\n",
      "Training Loss: 0.00032565447778324595\n",
      "Validation Loss: 0.0003483102080056423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.0003243227117491188\n",
      "Training Loss: 0.00030866003024129893\n",
      "Training Loss: 0.00031276687441277317\n",
      "Validation Loss: 0.0003345230873216341\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.00031247881746821805\n",
      "Training Loss: 0.0002971673027786892\n",
      "Training Loss: 0.0003009958195980289\n",
      "Validation Loss: 0.0003218283162915326\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.00030164213356329126\n",
      "Training Loss: 0.00028672101529082284\n",
      "Training Loss: 0.0002903197132764035\n",
      "Validation Loss: 0.0003102068052960042\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.0002917788199010829\n",
      "Training Loss: 0.00027727300141123124\n",
      "Training Loss: 0.000280678442613862\n",
      "Validation Loss: 0.0002996029366697891\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.00028282871207920837\n",
      "Training Loss: 0.0002687463416805258\n",
      "Training Loss: 0.0002719865570907132\n",
      "Validation Loss: 0.00028993884340063357\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0002747150649156538\n",
      "Training Loss: 0.00026104865024535685\n",
      "Training Loss: 0.0002641439036960946\n",
      "Validation Loss: 0.0002811267880266375\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.00026735281506262254\n",
      "Training Loss: 0.00025408180543308847\n",
      "Training Loss: 0.00025704594529088356\n",
      "Validation Loss: 0.0002730777583217766\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0002606558120896807\n",
      "Training Loss: 0.00024775047382718184\n",
      "Training Loss: 0.000250591727635765\n",
      "Validation Loss: 0.00026570580086443215\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0002545401937641145\n",
      "Training Loss: 0.00024196557758841665\n",
      "Training Loss: 0.00024468715109833284\n",
      "Validation Loss: 0.0002589309159692461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0002489283167051326\n",
      "Training Loss: 0.0002366465514569427\n",
      "Training Loss: 0.0002392476903332863\n",
      "Validation Loss: 0.0002526807258025942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.00024374933760554996\n",
      "Training Loss: 0.00023172389966930496\n",
      "Training Loss: 0.0002342000763383112\n",
      "Validation Loss: 0.0002468901603718764\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.00023893998644780367\n",
      "Training Loss: 0.00022713697766448603\n",
      "Training Loss: 0.00022948116969928378\n",
      "Validation Loss: 0.00024150058870260038\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.00023444507914973655\n",
      "Training Loss: 0.00022283495385636343\n",
      "Training Loss: 0.0002250384059152566\n",
      "Validation Loss: 0.00023645959904723476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.00023021628370770486\n",
      "Training Loss: 0.00021877497638342903\n",
      "Training Loss: 0.00022082728475652402\n",
      "Validation Loss: 0.0002317210372270535\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.00022621250896918355\n",
      "Training Loss: 0.00021492147581739118\n",
      "Training Loss: 0.00021681122481822967\n",
      "Validation Loss: 0.00022724291548921642\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.00022239851525228006\n",
      "Training Loss: 0.0002112446166938753\n",
      "Training Loss: 0.00021296057831932558\n",
      "Validation Loss: 0.0002229895084322001\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00021874525164093938\n",
      "Training Loss: 0.00020772053558175684\n",
      "Training Loss: 0.00020925109216477723\n",
      "Validation Loss: 0.00021893033196003793\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00021522820861719083\n",
      "Training Loss: 0.0002043289791618008\n",
      "Training Loss: 0.00020566243321809452\n",
      "Validation Loss: 0.0002150373710607168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00021182668726396514\n",
      "Training Loss: 0.00020105370298551862\n",
      "Training Loss: 0.00020217883396981052\n",
      "Validation Loss: 0.00021128832929789905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00020852438059591803\n",
      "Training Loss: 0.00019788169098319485\n",
      "Training Loss: 0.00019878711667843164\n",
      "Validation Loss: 0.00020766411999533518\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.00020530742145638214\n",
      "Training Loss: 0.00019480149407172576\n",
      "Training Loss: 0.00019547671410691692\n",
      "Validation Loss: 0.00020414839088792516\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0002021647701440088\n",
      "Training Loss: 0.00019180421284545446\n",
      "Training Loss: 0.0001922392938286066\n",
      "Validation Loss: 0.00020072799329835597\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0001990878598735435\n",
      "Training Loss: 0.00018888237655119157\n",
      "Training Loss: 0.00018906804547441425\n",
      "Validation Loss: 0.00019739226128590774\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.00019606963114711108\n",
      "Training Loss: 0.00018603045029522036\n",
      "Training Loss: 0.00018595772933622357\n",
      "Validation Loss: 0.00019413268471887501\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.00019310471790959127\n",
      "Training Loss: 0.00018324305085116066\n",
      "Training Loss: 0.00018290401549165837\n",
      "Validation Loss: 0.00019094288090650734\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.00019018950461031637\n",
      "Training Loss: 0.0001805167880593217\n",
      "Training Loss: 0.00017990406093304045\n",
      "Validation Loss: 0.00018781735789059567\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.0001873211079146131\n",
      "Training Loss: 0.00017784819370717742\n",
      "Training Loss: 0.0001769556786530302\n",
      "Validation Loss: 0.00018475222344257805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.0001844975530548254\n",
      "Training Loss: 0.00017523518023153883\n",
      "Training Loss: 0.00017405678670911584\n",
      "Validation Loss: 0.00018174486900902133\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.00018171841555158607\n",
      "Training Loss: 0.00017267599809201784\n",
      "Training Loss: 0.00017120699940278427\n",
      "Validation Loss: 0.00017879354989295434\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.00017898313486512053\n",
      "Training Loss: 0.0001701689913898008\n",
      "Training Loss: 0.0001684058407408884\n",
      "Validation Loss: 0.00017589735976757817\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.00017629247769946233\n",
      "Training Loss: 0.00016771387325206888\n",
      "Training Loss: 0.0001656535515212454\n",
      "Validation Loss: 0.0001730563282272373\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0001736475025245454\n",
      "Training Loss: 0.00016531000537725049\n",
      "Training Loss: 0.00016295137691486162\n",
      "Validation Loss: 0.00017027073179919723\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.00017104955983086257\n",
      "Training Loss: 0.00016295732622893412\n",
      "Training Loss: 0.00016029983613407238\n",
      "Validation Loss: 0.0001675412677899801\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.0001685005199396983\n",
      "Training Loss: 0.0001606563745190215\n",
      "Training Loss: 0.00015770049394632225\n",
      "Validation Loss: 0.00016486944497976286\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.0001660027554498811\n",
      "Training Loss: 0.0001584073149388132\n",
      "Training Loss: 0.00015515519757173024\n",
      "Validation Loss: 0.0001622564001142942\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.00016355820936951205\n",
      "Training Loss: 0.00015621128306520405\n",
      "Training Loss: 0.00015266555397829506\n",
      "Validation Loss: 0.0001597042062487898\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.0001611692691585631\n",
      "Training Loss: 0.00015406875694679912\n",
      "Training Loss: 0.00015023359719634755\n",
      "Validation Loss: 0.0001572145666317881\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.00015883856598520652\n",
      "Training Loss: 0.00015198036513538682\n",
      "Training Loss: 0.00014786137751798378\n",
      "Validation Loss: 0.00015478937822397998\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.00015656778385164216\n",
      "Training Loss: 0.00014994707071309677\n",
      "Training Loss: 0.00014555034817021807\n",
      "Validation Loss: 0.00015243028075234138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0001543593845963187\n",
      "Training Loss: 0.00014796933543038905\n",
      "Training Loss: 0.00014330247038742526\n",
      "Validation Loss: 0.0001501389151905713\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.00015221476090118813\n",
      "Training Loss: 0.00014604810858145356\n",
      "Training Loss: 0.00014111959091678728\n",
      "Validation Loss: 0.0001479167415944564\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0001501354945867206\n",
      "Training Loss: 0.00014418354252484277\n",
      "Training Loss: 0.0001390021341649117\n",
      "Validation Loss: 0.0001457651334031867\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.00014812274544965477\n",
      "Training Loss: 0.00014237557425076375\n",
      "Training Loss: 0.00013695170884602703\n",
      "Validation Loss: 0.00014368475912602519\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.00014617675296904053\n",
      "Training Loss: 0.00014062441738133203\n",
      "Training Loss: 0.00013496837200364098\n",
      "Validation Loss: 0.0001416760656825369\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.00014429842004574312\n",
      "Training Loss: 0.00013893004359488259\n",
      "Training Loss: 0.0001330523384967819\n",
      "Validation Loss: 0.00013973932765836937\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.00014248699602831038\n",
      "Training Loss: 0.00013729132215303252\n",
      "Training Loss: 0.00013120344578055666\n",
      "Validation Loss: 0.00013787384298492714\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0001407420678606286\n",
      "Training Loss: 0.0001357077242937521\n",
      "Training Loss: 0.00012942099703650456\n",
      "Validation Loss: 0.00013607899304230833\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.00013906245907492122\n",
      "Training Loss: 0.00013417840347756282\n",
      "Training Loss: 0.00012770372870363645\n",
      "Validation Loss: 0.0001343535519951596\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0001374471250710485\n",
      "Training Loss: 0.00013270174902572763\n",
      "Training Loss: 0.0001260504359015613\n",
      "Validation Loss: 0.0001326961910001884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.000135894175609792\n",
      "Training Loss: 0.00013127668356901267\n",
      "Training Loss: 0.00012445949521861622\n",
      "Validation Loss: 0.00013110493577943508\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.00013440181088299143\n",
      "Training Loss: 0.0001299013650532288\n",
      "Training Loss: 0.00012292920426261843\n",
      "Validation Loss: 0.00012957724966020508\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.00013296770048327745\n",
      "Training Loss: 0.0001285746153371292\n",
      "Training Loss: 0.00012145712827987153\n",
      "Validation Loss: 0.0001281112785200298\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.00013158969613868976\n",
      "Training Loss: 0.0001272945439268369\n",
      "Training Loss: 0.00012004106090898858\n",
      "Validation Loss: 0.00012670398064074107\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.00013026563723542495\n",
      "Training Loss: 0.00012605927640834124\n",
      "Training Loss: 0.00011867919620272005\n",
      "Validation Loss: 0.00012535327440356636\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.00012899249521069577\n",
      "Training Loss: 0.0001248671358007414\n",
      "Training Loss: 0.00011736877053408534\n",
      "Validation Loss: 0.00012405554817438786\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.00012776803335782461\n",
      "Training Loss: 0.00012371611679554918\n",
      "Training Loss: 0.00011610753164859489\n",
      "Validation Loss: 0.0001228085487362866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.00012659009672916\n",
      "Training Loss: 0.00012260440777026816\n",
      "Training Loss: 0.00011489324886497343\n",
      "Validation Loss: 0.00012160893538237544\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.00012545559121463157\n",
      "Training Loss: 0.0001215306364611024\n",
      "Training Loss: 0.00011372339689842192\n",
      "Validation Loss: 0.00012045398173367165\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.000124362703022598\n",
      "Training Loss: 0.0001204925242200261\n",
      "Training Loss: 0.0001125958415104833\n",
      "Validation Loss: 0.0001193409059422347\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.000123308720831119\n",
      "Training Loss: 0.00011948882036449504\n",
      "Training Loss: 0.00011150835374792223\n",
      "Validation Loss: 0.00011826681781784511\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.00012229168260091683\n",
      "Training Loss: 0.00011851767498228582\n",
      "Training Loss: 0.00011045870820453274\n",
      "Validation Loss: 0.00011722948858753145\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.00012130908540257223\n",
      "Training Loss: 0.00011757764128560666\n",
      "Training Loss: 0.00010944475185169722\n",
      "Validation Loss: 0.00011622592380641191\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.00012035910722261179\n",
      "Training Loss: 0.0001166668999576359\n",
      "Training Loss: 0.00010846497159946012\n",
      "Validation Loss: 0.00011525407145005844\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.00011943974207042629\n",
      "Training Loss: 0.0001157840652649611\n",
      "Training Loss: 0.00010751735337180435\n",
      "Validation Loss: 0.00011431186134502517\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.00011854926499836438\n",
      "Training Loss: 0.00011492799364532403\n",
      "Training Loss: 0.00010660021172952838\n",
      "Validation Loss: 0.00011339693892193863\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.00011768589654820972\n",
      "Training Loss: 0.00011409691451262916\n",
      "Training Loss: 0.00010571175649602082\n",
      "Validation Loss: 0.00011250743994147915\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00011684786687055749\n",
      "Training Loss: 0.00011328989229696163\n",
      "Training Loss: 0.000104850752177299\n",
      "Validation Loss: 0.00011164172825333532\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.000116033860308562\n",
      "Training Loss: 0.00011250547710005777\n",
      "Training Loss: 0.00010401551005998044\n",
      "Validation Loss: 0.00011079833294909835\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.00011524242167070042\n",
      "Training Loss: 0.00011174288661095489\n",
      "Training Loss: 0.00010320469695216162\n",
      "Validation Loss: 0.00010997545582489529\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00011447230484918691\n",
      "Training Loss: 0.00011100034886112554\n",
      "Training Loss: 0.00010241705902444664\n",
      "Validation Loss: 0.00010917216361653363\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.00011372223439138906\n",
      "Training Loss: 0.00011027752968402638\n",
      "Training Loss: 0.00010165177513044909\n",
      "Validation Loss: 0.00010838688620914878\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.00011299113610675704\n",
      "Training Loss: 0.00010957308310025837\n",
      "Training Loss: 0.00010090712004966917\n",
      "Validation Loss: 0.00010761867562312831\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00011227785952542036\n",
      "Training Loss: 0.00010888621857702675\n",
      "Training Loss: 0.00010018227905675303\n",
      "Validation Loss: 0.00010686618249534414\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.00011158142619933642\n",
      "Training Loss: 0.00010821573834618903\n",
      "Training Loss: 9.94765524774266e-05\n",
      "Validation Loss: 0.00010612892193227268\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.00011090089834397077\n",
      "Training Loss: 0.00010756120194855611\n",
      "Training Loss: 9.878860284516122e-05\n",
      "Validation Loss: 0.00010540579389496261\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.00011023572963040352\n",
      "Training Loss: 0.00010692171602386225\n",
      "Training Loss: 9.811783760596882e-05\n",
      "Validation Loss: 0.00010469617894598874\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.00010958451536225766\n",
      "Training Loss: 0.00010629645983499358\n",
      "Training Loss: 9.746321275088121e-05\n",
      "Validation Loss: 0.00010399960557877196\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00010894711415403436\n",
      "Training Loss: 0.0001056847071959055\n",
      "Training Loss: 9.682428652013186e-05\n",
      "Validation Loss: 0.00010331516346064909\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.00010832266390480072\n",
      "Training Loss: 0.0001050857708651165\n",
      "Training Loss: 9.620015842301655e-05\n",
      "Validation Loss: 0.00010264232434892211\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.00010771055565783171\n",
      "Training Loss: 0.00010449942351442587\n",
      "Training Loss: 9.559008598444053e-05\n",
      "Validation Loss: 0.00010198091454032202\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00010711006973906478\n",
      "Training Loss: 0.00010392462468189478\n",
      "Training Loss: 9.499363934082794e-05\n",
      "Validation Loss: 0.00010133007772003949\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.00010652081007719971\n",
      "Training Loss: 0.0001033610377999139\n",
      "Training Loss: 9.441000447623083e-05\n",
      "Validation Loss: 0.00010068970055890327\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.00010594230082460853\n",
      "Training Loss: 0.0001028082108223316\n",
      "Training Loss: 9.383862883623806e-05\n",
      "Validation Loss: 0.00010005939745159788\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00010537375525018433\n",
      "Training Loss: 0.00010226560692899511\n",
      "Training Loss: 9.327920932264533e-05\n",
      "Validation Loss: 9.943868254400983e-05\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00010481503335540765\n",
      "Training Loss: 0.0001017326123201201\n",
      "Training Loss: 9.273100566133508e-05\n",
      "Validation Loss: 9.882747404293022e-05\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.00010426574322991655\n",
      "Training Loss: 0.00010120909851139004\n",
      "Training Loss: 9.219369778293185e-05\n",
      "Validation Loss: 9.822516142193809e-05\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.00010372532748988306\n",
      "Training Loss: 0.00010069446616398637\n",
      "Training Loss: 9.166697145701619e-05\n",
      "Validation Loss: 9.763174786937443e-05\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.00010319352718852316\n",
      "Training Loss: 0.00010018836734161596\n",
      "Training Loss: 9.114995400523185e-05\n",
      "Validation Loss: 9.704658954474274e-05\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.00010266984892041364\n",
      "Training Loss: 9.969029439162114e-05\n",
      "Training Loss: 9.064279869562597e-05\n",
      "Validation Loss: 9.647027430862967e-05\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.00010215410990440433\n",
      "Training Loss: 9.920026426698315e-05\n",
      "Training Loss: 9.014482713610051e-05\n",
      "Validation Loss: 9.590171593822816e-05\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.00010164603679186258\n",
      "Training Loss: 9.871764602394251e-05\n",
      "Training Loss: 8.965575236288714e-05\n",
      "Validation Loss: 9.534134590191233e-05\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.00010114534128206287\n",
      "Training Loss: 9.82424638732482e-05\n",
      "Training Loss: 8.917524830394541e-05\n",
      "Validation Loss: 9.478856567041608e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.00010065174729334103\n",
      "Training Loss: 9.777408803529397e-05\n",
      "Training Loss: 8.870282756106462e-05\n",
      "Validation Loss: 9.424338585110459e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.0001001649308591368\n",
      "Training Loss: 9.731253807331086e-05\n",
      "Training Loss: 8.823856624076142e-05\n",
      "Validation Loss: 9.370585482092077e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 9.968472355012637e-05\n",
      "Training Loss: 9.68572922738531e-05\n",
      "Training Loss: 8.778186322160763e-05\n",
      "Validation Loss: 9.317551325438309e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 9.921104325258056e-05\n",
      "Training Loss: 9.640835362006327e-05\n",
      "Training Loss: 8.733269010917866e-05\n",
      "Validation Loss: 9.265246855361868e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 9.874330662114517e-05\n",
      "Training Loss: 9.596551024515065e-05\n",
      "Training Loss: 8.689071779372171e-05\n",
      "Validation Loss: 9.213638624939622e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 9.828163402289647e-05\n",
      "Training Loss: 9.552838863783109e-05\n",
      "Training Loss: 8.645556490591844e-05\n",
      "Validation Loss: 9.162712799633063e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 9.782587815152511e-05\n",
      "Training Loss: 9.509690746199339e-05\n",
      "Training Loss: 8.602715446613729e-05\n",
      "Validation Loss: 9.112439321153248e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 9.737573757774954e-05\n",
      "Training Loss: 9.467093134844617e-05\n",
      "Training Loss: 8.560514974305989e-05\n",
      "Validation Loss: 9.062850912915167e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 9.69311085600566e-05\n",
      "Training Loss: 9.425008222024189e-05\n",
      "Training Loss: 8.518960394212626e-05\n",
      "Validation Loss: 9.013915946985087e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 9.649160912431398e-05\n",
      "Training Loss: 9.383453098052997e-05\n",
      "Training Loss: 8.477995537759852e-05\n",
      "Validation Loss: 8.965635433004321e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 9.605755268239591e-05\n",
      "Training Loss: 9.342359959191527e-05\n",
      "Training Loss: 8.437639100520755e-05\n",
      "Validation Loss: 8.91796020103234e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 9.562847881170456e-05\n",
      "Training Loss: 9.301774452069367e-05\n",
      "Training Loss: 8.397868141400977e-05\n",
      "Validation Loss: 8.870906427811984e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 9.520431312921573e-05\n",
      "Training Loss: 9.261636740120593e-05\n",
      "Training Loss: 8.358638736353896e-05\n",
      "Validation Loss: 8.82446079384444e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 9.478508033225808e-05\n",
      "Training Loss: 9.221956563123967e-05\n",
      "Training Loss: 8.319952748024662e-05\n",
      "Validation Loss: 8.77863229393012e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 9.437042541321716e-05\n",
      "Training Loss: 9.182703146507265e-05\n",
      "Training Loss: 8.28180277676438e-05\n",
      "Validation Loss: 8.733391433871178e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 9.396025506703154e-05\n",
      "Training Loss: 9.143896305431554e-05\n",
      "Training Loss: 8.244162508162845e-05\n",
      "Validation Loss: 8.688716890615426e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 9.355468499506969e-05\n",
      "Training Loss: 9.105496836127714e-05\n",
      "Training Loss: 8.207031356505468e-05\n",
      "Validation Loss: 8.64461828436862e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 9.315346690073057e-05\n",
      "Training Loss: 9.067506540304749e-05\n",
      "Training Loss: 8.170374195287877e-05\n",
      "Validation Loss: 8.601079150210946e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 9.275652767428256e-05\n",
      "Training Loss: 9.029905940224126e-05\n",
      "Training Loss: 8.134215898280672e-05\n",
      "Validation Loss: 8.558078563632836e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 9.236388252702455e-05\n",
      "Training Loss: 8.992687755380757e-05\n",
      "Training Loss: 8.098506297756103e-05\n",
      "Validation Loss: 8.515636983155561e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 9.197534350732894e-05\n",
      "Training Loss: 8.955870891441008e-05\n",
      "Training Loss: 8.063259548180213e-05\n",
      "Validation Loss: 8.473699542744807e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 9.159086775525793e-05\n",
      "Training Loss: 8.919397403587936e-05\n",
      "Training Loss: 8.028453172300942e-05\n",
      "Validation Loss: 8.432319219956002e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 9.121032664552331e-05\n",
      "Training Loss: 8.883303404218168e-05\n",
      "Training Loss: 7.994091119144287e-05\n",
      "Validation Loss: 8.391456452791521e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 9.083388778435619e-05\n",
      "Training Loss: 8.847552295264905e-05\n",
      "Training Loss: 7.960140521390713e-05\n",
      "Validation Loss: 8.351086120803322e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 9.046110014423903e-05\n",
      "Training Loss: 8.812162366666598e-05\n",
      "Training Loss: 7.926621337901451e-05\n",
      "Validation Loss: 8.311247025692333e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 9.009224489091139e-05\n",
      "Training Loss: 8.77711139855819e-05\n",
      "Training Loss: 7.893501335274777e-05\n",
      "Validation Loss: 8.271889233133713e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 8.972709153113101e-05\n",
      "Training Loss: 8.742402814277738e-05\n",
      "Training Loss: 7.860794716179953e-05\n",
      "Validation Loss: 8.233035509108325e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 8.936567032378661e-05\n",
      "Training Loss: 8.708005380867689e-05\n",
      "Training Loss: 7.828471964785422e-05\n",
      "Validation Loss: 8.194658760893096e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 8.900788227037992e-05\n",
      "Training Loss: 8.673940955304715e-05\n",
      "Training Loss: 7.796538358888937e-05\n",
      "Validation Loss: 8.156729880569953e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 8.865377534675645e-05\n",
      "Training Loss: 8.640179095891653e-05\n",
      "Training Loss: 7.764993406453869e-05\n",
      "Validation Loss: 8.119284469235997e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 8.830313955513702e-05\n",
      "Training Loss: 8.606760295151616e-05\n",
      "Training Loss: 7.733800426649395e-05\n",
      "Validation Loss: 8.082297929658114e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 8.795603481303261e-05\n",
      "Training Loss: 8.573623247684736e-05\n",
      "Training Loss: 7.702989782956137e-05\n",
      "Validation Loss: 8.045737313602402e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 8.761248553128098e-05\n",
      "Training Loss: 8.540809182704833e-05\n",
      "Training Loss: 7.672531821299344e-05\n",
      "Validation Loss: 8.009666020002902e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 8.7272534310614e-05\n",
      "Training Loss: 8.508303964845254e-05\n",
      "Training Loss: 7.642434921763197e-05\n",
      "Validation Loss: 7.974037952806795e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 8.693581158695451e-05\n",
      "Training Loss: 8.47607661944494e-05\n",
      "Training Loss: 7.612689762936497e-05\n",
      "Validation Loss: 7.938817138943869e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 8.660260451506474e-05\n",
      "Training Loss: 8.444163831882179e-05\n",
      "Training Loss: 7.583268085909368e-05\n",
      "Validation Loss: 7.904023292989162e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 8.627265916402393e-05\n",
      "Training Loss: 8.412532179136178e-05\n",
      "Training Loss: 7.554209267254919e-05\n",
      "Validation Loss: 7.86967315782042e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 8.594616237587615e-05\n",
      "Training Loss: 8.381210645893588e-05\n",
      "Training Loss: 7.525491443630017e-05\n",
      "Validation Loss: 7.83575971499988e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 8.562299719869771e-05\n",
      "Training Loss: 8.350145324584446e-05\n",
      "Training Loss: 7.497093321035209e-05\n",
      "Validation Loss: 7.802226960091992e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 8.530316902579216e-05\n",
      "Training Loss: 8.319385817230796e-05\n",
      "Training Loss: 7.469019162272162e-05\n",
      "Validation Loss: 7.769144903750714e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 8.498651064201113e-05\n",
      "Training Loss: 8.288897359307157e-05\n",
      "Training Loss: 7.441269247465243e-05\n",
      "Validation Loss: 7.736410513511085e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 8.467316027690685e-05\n",
      "Training Loss: 8.258683050371473e-05\n",
      "Training Loss: 7.413854985315993e-05\n",
      "Validation Loss: 7.70412527058755e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 8.436313160018472e-05\n",
      "Training Loss: 8.228748859437474e-05\n",
      "Training Loss: 7.386744375253328e-05\n",
      "Validation Loss: 7.672211433621189e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 8.405627602769527e-05\n",
      "Training Loss: 8.19909858637402e-05\n",
      "Training Loss: 7.359940581409318e-05\n",
      "Validation Loss: 7.640681190862911e-05\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 8.375272107969068e-05\n",
      "Training Loss: 8.169715943040501e-05\n",
      "Training Loss: 7.333457431741409e-05\n",
      "Validation Loss: 7.609545626907134e-05\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 8.345241702045314e-05\n",
      "Training Loss: 8.140608852045262e-05\n",
      "Training Loss: 7.307279161977931e-05\n",
      "Validation Loss: 7.578778558525485e-05\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 8.31551758437854e-05\n",
      "Training Loss: 8.11177187551948e-05\n",
      "Training Loss: 7.28141162198881e-05\n",
      "Validation Loss: 7.548402514617061e-05\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 8.286116410090472e-05\n",
      "Training Loss: 8.083201063527668e-05\n",
      "Training Loss: 7.255826379150676e-05\n",
      "Validation Loss: 7.51836451353882e-05\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 8.257035057340545e-05\n",
      "Training Loss: 8.054904503296712e-05\n",
      "Training Loss: 7.230552214423369e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [03:30<31:31, 210.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.488727145898614e-05\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.32609655782580377\n",
      "Training Loss: 0.20665806390345096\n",
      "Training Loss: 0.1419255573209375\n",
      "Validation Loss: 0.08765663522599118\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.07363508716225624\n",
      "Training Loss: 0.05795313835609704\n",
      "Training Loss: 0.06030548152513802\n",
      "Validation Loss: 0.05861127781524752\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.0555778981000185\n",
      "Training Loss: 0.05284082598052919\n",
      "Training Loss: 0.05540403552353382\n",
      "Validation Loss: 0.05437690204825629\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.0511993880616501\n",
      "Training Loss: 0.04855707806535065\n",
      "Training Loss: 0.05062406519427896\n",
      "Validation Loss: 0.04955686318028844\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.04628934550564736\n",
      "Training Loss: 0.043759415848180654\n",
      "Training Loss: 0.04533389708958566\n",
      "Validation Loss: 0.04422220703884122\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.040894558185245844\n",
      "Training Loss: 0.03845569983590394\n",
      "Training Loss: 0.03950892081018537\n",
      "Validation Loss: 0.038335477294965406\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.03498515198240057\n",
      "Training Loss: 0.032628130004741254\n",
      "Training Loss: 0.03317608956713229\n",
      "Validation Loss: 0.03198985012478373\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.028697636864380913\n",
      "Training Loss: 0.026503323507495225\n",
      "Training Loss: 0.026669327276758848\n",
      "Validation Loss: 0.0256334551730392\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.022535225717001596\n",
      "Training Loss: 0.020680512321414424\n",
      "Training Loss: 0.020677511531393977\n",
      "Validation Loss: 0.019981548292143794\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.01722545955111855\n",
      "Training Loss: 0.015862818654859438\n",
      "Training Loss: 0.015865607385057955\n",
      "Validation Loss: 0.015550922738450967\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.013233846209623153\n",
      "Training Loss: 0.012367202412569896\n",
      "Training Loss: 0.012428975683869794\n",
      "Validation Loss: 0.012356698914598464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.010509846828354057\n",
      "Training Loss: 0.010031820870935917\n",
      "Training Loss: 0.010135024301707745\n",
      "Validation Loss: 0.010138059065028439\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.008725578864105046\n",
      "Training Loss: 0.008507763739908114\n",
      "Training Loss: 0.008619651363696903\n",
      "Validation Loss: 0.008601246205461996\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.007534018105652649\n",
      "Training Loss: 0.007469121543690563\n",
      "Training Loss: 0.007564889028435573\n",
      "Validation Loss: 0.007507596778411293\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.006679591277206782\n",
      "Training Loss: 0.006695468629477546\n",
      "Training Loss: 0.006762634402839467\n",
      "Validation Loss: 0.006708221744191362\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.006010529644263443\n",
      "Training Loss: 0.006061284883180633\n",
      "Training Loss: 0.0060923512896988545\n",
      "Validation Loss: 0.006120856534794308\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.005438160228368361\n",
      "Training Loss: 0.005493028974160552\n",
      "Training Loss: 0.005484076165594161\n",
      "Validation Loss: 0.0056955010507234866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.004910692572011612\n",
      "Training Loss: 0.004953150808578357\n",
      "Training Loss: 0.004911651128204539\n",
      "Validation Loss: 0.005371610279550797\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.004414212314004544\n",
      "Training Loss: 0.0044495413027470935\n",
      "Training Loss: 0.0043958538310835136\n",
      "Validation Loss: 0.005044316925275861\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.003965803281171247\n",
      "Training Loss: 0.004015960372053087\n",
      "Training Loss: 0.003964669330161996\n",
      "Validation Loss: 0.004637857447142891\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.0035790109040681273\n",
      "Training Loss: 0.003662576628266834\n",
      "Training Loss: 0.003615788210881874\n",
      "Validation Loss: 0.004172421397976159\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0032521344785345716\n",
      "Training Loss: 0.0033745805447688328\n",
      "Training Loss: 0.0033295968867605554\n",
      "Validation Loss: 0.003713141905740322\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.002975697911460884\n",
      "Training Loss: 0.003131514188426081\n",
      "Training Loss: 0.0030861364101292567\n",
      "Validation Loss: 0.003311060198315869\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.002739212516316911\n",
      "Training Loss: 0.0029178725616657176\n",
      "Training Loss: 0.0028710767030133864\n",
      "Validation Loss: 0.0029837728185014107\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0025330849252350163\n",
      "Training Loss: 0.0027231910152477213\n",
      "Training Loss: 0.0026741246154415423\n",
      "Validation Loss: 0.002726216615423602\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.00234840244622319\n",
      "Training Loss: 0.002539435276703443\n",
      "Training Loss: 0.00248716196132591\n",
      "Validation Loss: 0.002522432943806052\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.0021772854405571708\n",
      "Training Loss: 0.0023599706569802947\n",
      "Training Loss: 0.00230403494584607\n",
      "Validation Loss: 0.0023527658551675008\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.0020136320449091727\n",
      "Training Loss: 0.00217995311395498\n",
      "Training Loss: 0.0021214840150787493\n",
      "Validation Loss: 0.0021994371540070084\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.0018538967647327808\n",
      "Training Loss: 0.001997817579831462\n",
      "Training Loss: 0.0019408570011728444\n",
      "Validation Loss: 0.0020521402050311982\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.0016989715254749171\n",
      "Training Loss: 0.0018190799937292468\n",
      "Training Loss: 0.0017708151263650506\n",
      "Validation Loss: 0.001913658088515251\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.0015569330836297012\n",
      "Training Loss: 0.0016572599946812262\n",
      "Training Loss: 0.0016239599155960605\n",
      "Validation Loss: 0.0017942553592108065\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.001437280559912324\n",
      "Training Loss: 0.0015215883748896887\n",
      "Training Loss: 0.001503777881152928\n",
      "Validation Loss: 0.0016938830837685867\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.0013387653857353144\n",
      "Training Loss: 0.0014042033565056045\n",
      "Training Loss: 0.0013972892613674048\n",
      "Validation Loss: 0.0015915712924396848\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.0012446241873840336\n",
      "Training Loss: 0.0012779606529511511\n",
      "Training Loss: 0.0012738704311777837\n",
      "Validation Loss: 0.0014356351001358204\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.0011188012338243425\n",
      "Training Loss: 0.001096690174890682\n",
      "Training Loss: 0.0010916571169218513\n",
      "Validation Loss: 0.0011857088409649376\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.0009539795718592358\n",
      "Training Loss: 0.0009139503500045975\n",
      "Training Loss: 0.0009404894079489168\n",
      "Validation Loss: 0.0010412648956885226\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0008596552841481752\n",
      "Training Loss: 0.0008253906536265276\n",
      "Training Loss: 0.0008605053825885988\n",
      "Validation Loss: 0.0009845057605396893\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.0008021500664983705\n",
      "Training Loss: 0.0007674488853808726\n",
      "Training Loss: 0.0008033122916822322\n",
      "Validation Loss: 0.0009481952482510065\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.0007579770607753744\n",
      "Training Loss: 0.0007216955451440299\n",
      "Training Loss: 0.0007570566394133493\n",
      "Validation Loss: 0.0009191842791245036\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.0007212855505622428\n",
      "Training Loss: 0.0006832561749615707\n",
      "Training Loss: 0.0007178865264722844\n",
      "Validation Loss: 0.0008939277450848381\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.0006895643692769227\n",
      "Training Loss: 0.0006500197858986212\n",
      "Training Loss: 0.000683937150388374\n",
      "Validation Loss: 0.0008708441813458071\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0006614915496174945\n",
      "Training Loss: 0.0006208178940141807\n",
      "Training Loss: 0.0006540578060958068\n",
      "Validation Loss: 0.0008490748450538049\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0006362693701157696\n",
      "Training Loss: 0.0005948931101011113\n",
      "Training Loss: 0.0006274555348500144\n",
      "Validation Loss: 0.0008281890146422535\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0006133919374042307\n",
      "Training Loss: 0.0005717203378299018\n",
      "Training Loss: 0.0006035699868516531\n",
      "Validation Loss: 0.0008080759634436475\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.0005925281450254261\n",
      "Training Loss: 0.0005509162862290395\n",
      "Training Loss: 0.0005819937717024004\n",
      "Validation Loss: 0.0007887462864785581\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.0005734444105837611\n",
      "Training Loss: 0.0005321797741635237\n",
      "Training Loss: 0.0005624189224181464\n",
      "Validation Loss: 0.0007702474024707944\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0005559620869826176\n",
      "Training Loss: 0.0005152577392436797\n",
      "Training Loss: 0.0005445958399650408\n",
      "Validation Loss: 0.000752611028218872\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0005399234454671386\n",
      "Training Loss: 0.0004999236365983962\n",
      "Training Loss: 0.0005283070095902076\n",
      "Validation Loss: 0.0007357964311952682\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.0005251781013794243\n",
      "Training Loss: 0.0004859679860965116\n",
      "Training Loss: 0.0005133581292466261\n",
      "Validation Loss: 0.0007197124850177572\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0005115794542507501\n",
      "Training Loss: 0.00047319599158072377\n",
      "Training Loss: 0.0004995713502285071\n",
      "Validation Loss: 0.0007042563535496095\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.0004989859445049661\n",
      "Training Loss: 0.0004614313837373629\n",
      "Training Loss: 0.000486785043758573\n",
      "Validation Loss: 0.000689325122417113\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.00048726237793744075\n",
      "Training Loss: 0.0004505129471363034\n",
      "Training Loss: 0.00047485302631685046\n",
      "Validation Loss: 0.000674816291894387\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00047628578093281246\n",
      "Training Loss: 0.00044030254663084633\n",
      "Training Loss: 0.0004636478043539682\n",
      "Validation Loss: 0.0006606585705954336\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.00046594516483310143\n",
      "Training Loss: 0.00043067941576737214\n",
      "Training Loss: 0.0004530590325157391\n",
      "Validation Loss: 0.0006467765854855377\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.00045614522423420567\n",
      "Training Loss: 0.00042154500941251173\n",
      "Training Loss: 0.00044299470966507214\n",
      "Validation Loss: 0.0006331245322042051\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0004468054906101315\n",
      "Training Loss: 0.0004128165507063386\n",
      "Training Loss: 0.00043337910741684025\n",
      "Validation Loss: 0.0006196582449120705\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.00043785923386167267\n",
      "Training Loss: 0.00040443082947604124\n",
      "Training Loss: 0.0004241480791097274\n",
      "Validation Loss: 0.0006063397092240459\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.00042925065023155187\n",
      "Training Loss: 0.00039633306943869683\n",
      "Training Loss: 0.0004152486355451401\n",
      "Validation Loss: 0.0005931334908586649\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00042093278643733355\n",
      "Training Loss: 0.00038848156535095765\n",
      "Training Loss: 0.0004066375656839227\n",
      "Validation Loss: 0.0005800119072906516\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.00041286730905994775\n",
      "Training Loss: 0.00038084165465988916\n",
      "Training Loss: 0.000398278390566702\n",
      "Validation Loss: 0.0005669576172560605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.00040502327770809644\n",
      "Training Loss: 0.0003733856115286471\n",
      "Training Loss: 0.00039014194109768144\n",
      "Validation Loss: 0.0005539653130664406\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.00039737607421557185\n",
      "Training Loss: 0.0003660930877595092\n",
      "Training Loss: 0.00038220489637751597\n",
      "Validation Loss: 0.000541027912435454\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.00038990573564660734\n",
      "Training Loss: 0.00035894773631298447\n",
      "Training Loss: 0.0003744494134662091\n",
      "Validation Loss: 0.0005281507909468642\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.00038259979413851397\n",
      "Training Loss: 0.0003519394195973291\n",
      "Training Loss: 0.0003668639305760735\n",
      "Validation Loss: 0.0005153417236971528\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0003754487793048611\n",
      "Training Loss: 0.0003450624086690368\n",
      "Training Loss: 0.00035944140137871727\n",
      "Validation Loss: 0.0005026157658689085\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.00036844947277131723\n",
      "Training Loss: 0.00033831487944553374\n",
      "Training Loss: 0.0003521780759183457\n",
      "Validation Loss: 0.0004899879278444418\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.00036159986950224266\n",
      "Training Loss: 0.00033169760321470675\n",
      "Training Loss: 0.0003450743633584352\n",
      "Validation Loss: 0.00047748532323454496\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0003549029749774491\n",
      "Training Loss: 0.0003252143421559595\n",
      "Training Loss: 0.00033813211375672837\n",
      "Validation Loss: 0.0004651237151561321\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.00034836064969567815\n",
      "Training Loss: 0.0003188688416412333\n",
      "Training Loss: 0.00033135489153210076\n",
      "Validation Loss: 0.0004529329580078268\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.00034197805154690287\n",
      "Training Loss: 0.0003126659952249611\n",
      "Training Loss: 0.0003247473176452331\n",
      "Validation Loss: 0.0004409324986257805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.00033575780635146656\n",
      "Training Loss: 0.0003066107210906921\n",
      "Training Loss: 0.00031831274111027597\n",
      "Validation Loss: 0.00042915274254196804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.0003297040878896951\n",
      "Training Loss: 0.00030070575110585196\n",
      "Training Loss: 0.00031205451545247343\n",
      "Validation Loss: 0.0004176131935330406\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.0003238184064321104\n",
      "Training Loss: 0.0002949525575240841\n",
      "Training Loss: 0.0003059741115794168\n",
      "Validation Loss: 0.00040632739432255497\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.0003180999400865403\n",
      "Training Loss: 0.00028935135964275105\n",
      "Training Loss: 0.0003000701362907421\n",
      "Validation Loss: 0.000395311518015374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.000312546680252126\n",
      "Training Loss: 0.00028389843897457467\n",
      "Training Loss: 0.00029433915322442774\n",
      "Validation Loss: 0.00038458351161935254\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00030715405999217184\n",
      "Training Loss: 0.0002785882640455384\n",
      "Training Loss: 0.0002887756010022713\n",
      "Validation Loss: 0.0003741396708545416\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00030191499114152977\n",
      "Training Loss: 0.00027341359287675005\n",
      "Training Loss: 0.0002833712748542894\n",
      "Validation Loss: 0.0003639868549460393\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00029682075026357777\n",
      "Training Loss: 0.0002683639619863243\n",
      "Training Loss: 0.0002781159340884187\n",
      "Validation Loss: 0.00035412662614004825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.0002918613714791718\n",
      "Training Loss: 0.0002634286114698625\n",
      "Training Loss: 0.0002729974040994421\n",
      "Validation Loss: 0.00034455646695523374\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.00028702430914563594\n",
      "Training Loss: 0.00025859419005428206\n",
      "Training Loss: 0.000268002571319812\n",
      "Validation Loss: 0.00033526585177875523\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.00028229777642991394\n",
      "Training Loss: 0.0002538484757315018\n",
      "Training Loss: 0.000263116979804181\n",
      "Validation Loss: 0.00032624465018854835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0002776688955418649\n",
      "Training Loss: 0.0002491787049439154\n",
      "Training Loss: 0.0002583271756520844\n",
      "Validation Loss: 0.0003174869922853698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.00027312583159073257\n",
      "Training Loss: 0.00024457292867737125\n",
      "Training Loss: 0.00025361999239976286\n",
      "Validation Loss: 0.00030898004990192703\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.0002686572007587529\n",
      "Training Loss: 0.00024002090009162203\n",
      "Training Loss: 0.0002489829240221297\n",
      "Validation Loss: 0.0003007143140973895\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.00026425370157085126\n",
      "Training Loss: 0.00023551539892650907\n",
      "Training Loss: 0.00024440640590910333\n",
      "Validation Loss: 0.0002926801790135358\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.0002599083375025657\n",
      "Training Loss: 0.00023105021431547358\n",
      "Training Loss: 0.00023988213273696602\n",
      "Validation Loss: 0.0002848701751032279\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.000255615554524411\n",
      "Training Loss: 0.00022662410250632093\n",
      "Training Loss: 0.00023540607689938043\n",
      "Validation Loss: 0.0002772791630878202\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.0002513737004846917\n",
      "Training Loss: 0.00022223792744625827\n",
      "Training Loss: 0.0002309762084041722\n",
      "Validation Loss: 0.00026990361764322015\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.0002471834524840233\n",
      "Training Loss: 0.0002178963974074577\n",
      "Training Loss: 0.00022659348116576438\n",
      "Validation Loss: 0.0002627418286332552\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.00024304782982653705\n",
      "Training Loss: 0.00021360787068260833\n",
      "Training Loss: 0.00022226317832974019\n",
      "Validation Loss: 0.0002557984828374092\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00023897414048406062\n",
      "Training Loss: 0.00020938298239343568\n",
      "Training Loss: 0.00021799275626108283\n",
      "Validation Loss: 0.0002490729634343317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.00023496989993873285\n",
      "Training Loss: 0.0002052343664763612\n",
      "Training Loss: 0.00021379206707933918\n",
      "Validation Loss: 0.00024257028151501044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.00023104543877707328\n",
      "Training Loss: 0.00020117661078984385\n",
      "Training Loss: 0.00020967305817976012\n",
      "Validation Loss: 0.00023629406136473682\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.00022721150362485786\n",
      "Training Loss: 0.00019722447053936774\n",
      "Training Loss: 0.00020564787673720274\n",
      "Validation Loss: 0.0002302495818542646\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.00022347949216054984\n",
      "Training Loss: 0.0001933924571494572\n",
      "Training Loss: 0.00020172975104287616\n",
      "Validation Loss: 0.0002244419093391586\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.00021986105992255033\n",
      "Training Loss: 0.0001896946466877125\n",
      "Training Loss: 0.00019793107225268613\n",
      "Validation Loss: 0.0002188758464828343\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.00021636572091665585\n",
      "Training Loss: 0.00018614289630932034\n",
      "Training Loss: 0.00019426330967689865\n",
      "Validation Loss: 0.00021355574255026982\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.0002130032925924752\n",
      "Training Loss: 0.0001827476172547904\n",
      "Training Loss: 0.00019073581919656134\n",
      "Validation Loss: 0.00020848392625666339\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.00020978079328415333\n",
      "Training Loss: 0.00017951610694581178\n",
      "Training Loss: 0.0001873564465131494\n",
      "Validation Loss: 0.0002036645735520477\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.00020670345638791333\n",
      "Training Loss: 0.00017645353756961412\n",
      "Training Loss: 0.00018413065259665018\n",
      "Validation Loss: 0.00019909519834798405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00020377440112497424\n",
      "Training Loss: 0.00017356231248413678\n",
      "Training Loss: 0.00018106136418282403\n",
      "Validation Loss: 0.00019477661236338422\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.00020099438250326785\n",
      "Training Loss: 0.0001708410530773108\n",
      "Training Loss: 0.00017814902825193712\n",
      "Validation Loss: 0.00019070219762428413\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.00019836193900118815\n",
      "Training Loss: 0.00016828785363031784\n",
      "Training Loss: 0.00017539261853016797\n",
      "Validation Loss: 0.0001868672717249497\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.00019587333295930875\n",
      "Training Loss: 0.0001658960391796427\n",
      "Training Loss: 0.0001727878650308412\n",
      "Validation Loss: 0.00018326169655859742\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.00019352343642822233\n",
      "Training Loss: 0.00016365913910703966\n",
      "Training Loss: 0.00017033002735843182\n",
      "Validation Loss: 0.00017987757472725575\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.00019130564131046414\n",
      "Training Loss: 0.00016156826497535804\n",
      "Training Loss: 0.0001680125235725427\n",
      "Validation Loss: 0.00017670168569331121\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.00018921294937172207\n",
      "Training Loss: 0.00015961465009240784\n",
      "Training Loss: 0.00016582826008743724\n",
      "Validation Loss: 0.00017372342327699912\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.00018723670864346787\n",
      "Training Loss: 0.0001577875908697024\n",
      "Training Loss: 0.00016376927560486366\n",
      "Validation Loss: 0.00017093153655878268\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.00018536903537096804\n",
      "Training Loss: 0.00015607814118993703\n",
      "Training Loss: 0.00016182770625164267\n",
      "Validation Loss: 0.00016831400344937024\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.0001836017611276475\n",
      "Training Loss: 0.000154475517992978\n",
      "Training Loss: 0.00015999495626601857\n",
      "Validation Loss: 0.00016585575086241222\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.00018192673331213883\n",
      "Training Loss: 0.00015297119007300352\n",
      "Training Loss: 0.0001582638081345067\n",
      "Validation Loss: 0.0001635478332941551\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.00018033643727903835\n",
      "Training Loss: 0.0001515561223095574\n",
      "Training Loss: 0.00015662591849832097\n",
      "Validation Loss: 0.00016137816631915363\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.00017882381202070973\n",
      "Training Loss: 0.00015022196590507519\n",
      "Training Loss: 0.0001550743982261338\n",
      "Validation Loss: 0.00015933667265435975\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0001773823200892366\n",
      "Training Loss: 0.00014896122611389727\n",
      "Training Loss: 0.00015360252551545274\n",
      "Validation Loss: 0.0001574132840094821\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.00017600536881218432\n",
      "Training Loss: 0.00014776672280277124\n",
      "Training Loss: 0.00015220336912534548\n",
      "Validation Loss: 0.00015559957825920148\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.00017468812218794484\n",
      "Training Loss: 0.00014663267667856418\n",
      "Training Loss: 0.00015087194095031009\n",
      "Validation Loss: 0.00015388602295889143\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.00017342497460049345\n",
      "Training Loss: 0.00014555272931829677\n",
      "Training Loss: 0.00014960212021833286\n",
      "Validation Loss: 0.00015226432915778275\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.00017221138668901403\n",
      "Training Loss: 0.00014452277751843213\n",
      "Training Loss: 0.00014838897830486532\n",
      "Validation Loss: 0.00015072869546508698\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.0001710431791980227\n",
      "Training Loss: 0.00014353717391713872\n",
      "Training Loss: 0.00014722861180416658\n",
      "Validation Loss: 0.00014927068181084784\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.00016991682730804313\n",
      "Training Loss: 0.0001425922828275361\n",
      "Training Loss: 0.0001461162385021453\n",
      "Validation Loss: 0.0001478840323021829\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.00016882870926565374\n",
      "Training Loss: 0.00014168427856930065\n",
      "Training Loss: 0.0001450478337937966\n",
      "Validation Loss: 0.00014656393070478944\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.00016777571039710892\n",
      "Training Loss: 0.00014080946550166118\n",
      "Training Loss: 0.00014402049924683525\n",
      "Validation Loss: 0.00014530492646801495\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.00016675497241521952\n",
      "Training Loss: 0.0001399653309272253\n",
      "Training Loss: 0.0001430308103772404\n",
      "Validation Loss: 0.00014410146991650344\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.0001657640286430251\n",
      "Training Loss: 0.0001391488385525008\n",
      "Training Loss: 0.00014207562922820217\n",
      "Validation Loss: 0.00014294875190796702\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.00016480070638863253\n",
      "Training Loss: 0.00013835792775353185\n",
      "Training Loss: 0.00014115235557255802\n",
      "Validation Loss: 0.00014184356192610843\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00016386260880608462\n",
      "Training Loss: 0.00013758967512330855\n",
      "Training Loss: 0.0001402592811245995\n",
      "Validation Loss: 0.00014078164568172213\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.00016294804538119934\n",
      "Training Loss: 0.0001368426182671101\n",
      "Training Loss: 0.00013939315200332204\n",
      "Validation Loss: 0.00013976019334713168\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.00016205550846279949\n",
      "Training Loss: 0.00013611489834147505\n",
      "Training Loss: 0.000138552460612118\n",
      "Validation Loss: 0.0001387746898636282\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00016118314993946115\n",
      "Training Loss: 0.0001354046650340024\n",
      "Training Loss: 0.0001377352324743697\n",
      "Validation Loss: 0.00013782346556244387\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.00016032919962526647\n",
      "Training Loss: 0.0001347108018489962\n",
      "Training Loss: 0.0001369397691451013\n",
      "Validation Loss: 0.000136903498018961\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.00015949321508742286\n",
      "Training Loss: 0.00013403153934632428\n",
      "Training Loss: 0.00013616461459605488\n",
      "Validation Loss: 0.00013601163120481206\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00015867305965002742\n",
      "Training Loss: 0.00013336620469999615\n",
      "Training Loss: 0.00013540797077439494\n",
      "Validation Loss: 0.00013514647132447616\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.00015786845153343166\n",
      "Training Loss: 0.00013271281100969646\n",
      "Training Loss: 0.00013466904219967547\n",
      "Validation Loss: 0.00013430608454696033\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.00015707770005974454\n",
      "Training Loss: 0.00013207171301473863\n",
      "Training Loss: 0.0001339462366013322\n",
      "Validation Loss: 0.00013348826174029108\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.0001563005400566908\n",
      "Training Loss: 0.0001314409354927193\n",
      "Training Loss: 0.00013323878943992894\n",
      "Validation Loss: 0.00013269130511289813\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.00015553553681456832\n",
      "Training Loss: 0.00013082006867989548\n",
      "Training Loss: 0.0001325452704259078\n",
      "Validation Loss: 0.0001319134687824015\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00015478259108931525\n",
      "Training Loss: 0.0001302081599351368\n",
      "Training Loss: 0.00013186501955715358\n",
      "Validation Loss: 0.0001311540250304113\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.0001540404408297036\n",
      "Training Loss: 0.0001296050531163928\n",
      "Training Loss: 0.00013119710623868741\n",
      "Validation Loss: 0.00013041054481117897\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.00015330849917518207\n",
      "Training Loss: 0.00012900957532110624\n",
      "Training Loss: 0.00013054076436674222\n",
      "Validation Loss: 0.00012968351424451306\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00015258658979291795\n",
      "Training Loss: 0.00012842146426919498\n",
      "Training Loss: 0.00012989542265131603\n",
      "Validation Loss: 0.00012897035995092153\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.00015187400658760452\n",
      "Training Loss: 0.0001278400436603988\n",
      "Training Loss: 0.0001292597955580277\n",
      "Validation Loss: 0.0001282707366494271\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.0001511694907094352\n",
      "Training Loss: 0.00012726540455332725\n",
      "Training Loss: 0.00012863416635809698\n",
      "Validation Loss: 0.0001275835107621839\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00015047395550936925\n",
      "Training Loss: 0.0001266963416856015\n",
      "Training Loss: 0.00012801740602299104\n",
      "Validation Loss: 0.00012690776934045408\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00014978587152654653\n",
      "Training Loss: 0.00012613324426638428\n",
      "Training Loss: 0.00012740929658320966\n",
      "Validation Loss: 0.000126243481747551\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.00014910585570760305\n",
      "Training Loss: 0.00012557495532746542\n",
      "Training Loss: 0.0001268090401572408\n",
      "Validation Loss: 0.00012558927597495036\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.00014843216187728102\n",
      "Training Loss: 0.00012502203022449977\n",
      "Training Loss: 0.0001262160412989033\n",
      "Validation Loss: 0.00012494451102304922\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.0001477654353766411\n",
      "Training Loss: 0.00012447388138753013\n",
      "Training Loss: 0.00012563062109620659\n",
      "Validation Loss: 0.00012430895060716783\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.00014710576080688044\n",
      "Training Loss: 0.0001239300252382236\n",
      "Training Loss: 0.00012505210415838519\n",
      "Validation Loss: 0.0001236820408143757\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.00014645184073742713\n",
      "Training Loss: 0.00012339054002950435\n",
      "Training Loss: 0.00012447997636627406\n",
      "Validation Loss: 0.0001230632448179799\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.00014580412318537128\n",
      "Training Loss: 0.00012285544362384827\n",
      "Training Loss: 0.00012391384010697947\n",
      "Validation Loss: 0.00012245204488045715\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.0001451620589614322\n",
      "Training Loss: 0.00012232383372975163\n",
      "Training Loss: 0.00012335364539467264\n",
      "Validation Loss: 0.00012184817050377645\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.00014452552679358632\n",
      "Training Loss: 0.00012179616669527604\n",
      "Training Loss: 0.00012279926480914582\n",
      "Validation Loss: 0.00012125115056968231\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.00014389426834895858\n",
      "Training Loss: 0.00012127245756346383\n",
      "Training Loss: 0.00012225010672409553\n",
      "Validation Loss: 0.00012066065849804351\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.0001432684831888764\n",
      "Training Loss: 0.00012075199078026344\n",
      "Training Loss: 0.00012170625383078005\n",
      "Validation Loss: 0.00012007627847605761\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.00014264744786487426\n",
      "Training Loss: 0.0001202346183345071\n",
      "Training Loss: 0.00012116707968743867\n",
      "Validation Loss: 0.00011949790647388123\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.00014203106764398398\n",
      "Training Loss: 0.00011972085174420499\n",
      "Training Loss: 0.00012063281485097832\n",
      "Validation Loss: 0.00011892553308999606\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.0001414200200815685\n",
      "Training Loss: 0.00011921022072783672\n",
      "Training Loss: 0.00012010316830128431\n",
      "Validation Loss: 0.00011835843982358771\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.00014081312288908521\n",
      "Training Loss: 0.00011870298963913228\n",
      "Training Loss: 0.00011957802196775446\n",
      "Validation Loss: 0.00011779649627968549\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.0001402107974536193\n",
      "Training Loss: 0.00011819840101452428\n",
      "Training Loss: 0.00011905703786396771\n",
      "Validation Loss: 0.00011723976260756818\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00013961278136775945\n",
      "Training Loss: 0.00011769733928304049\n",
      "Training Loss: 0.00011853999270897475\n",
      "Validation Loss: 0.00011668788150866553\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.00013901909054766292\n",
      "Training Loss: 0.00011719888048901339\n",
      "Training Loss: 0.00011802729573901161\n",
      "Validation Loss: 0.00011614078935922459\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.00013842963502611383\n",
      "Training Loss: 0.00011670363794110017\n",
      "Training Loss: 0.00011751837753763539\n",
      "Validation Loss: 0.00011559844589622148\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0001378441520319029\n",
      "Training Loss: 0.00011621106821621652\n",
      "Training Loss: 0.00011701300585627906\n",
      "Validation Loss: 0.00011506037201011015\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.00013726270168263\n",
      "Training Loss: 0.00011572168959901319\n",
      "Training Loss: 0.00011651166663796175\n",
      "Validation Loss: 0.00011452662603118001\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.00013668533370946533\n",
      "Training Loss: 0.00011523501110787038\n",
      "Training Loss: 0.00011601374304518685\n",
      "Validation Loss: 0.0001139971929987542\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.00013611165388283553\n",
      "Training Loss: 0.00011475108303784509\n",
      "Training Loss: 0.00011551943873200799\n",
      "Validation Loss: 0.00011347179701914775\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.0001355418371167616\n",
      "Training Loss: 0.00011426998109527631\n",
      "Training Loss: 0.0001150284506911703\n",
      "Validation Loss: 0.00011295077391050551\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.00013497635407475173\n",
      "Training Loss: 0.00011379192435924779\n",
      "Training Loss: 0.00011454114124717307\n",
      "Validation Loss: 0.00011243337894995236\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.00013441392407912645\n",
      "Training Loss: 0.00011331638963383739\n",
      "Training Loss: 0.00011405697256122949\n",
      "Validation Loss: 0.00011192000393554296\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.00013385550108068855\n",
      "Training Loss: 0.00011284381411314825\n",
      "Training Loss: 0.00011357606321325875\n",
      "Validation Loss: 0.00011141038014735959\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.00013330052024684846\n",
      "Training Loss: 0.0001123739998183737\n",
      "Training Loss: 0.00011309847113807336\n",
      "Validation Loss: 0.00011090452747165717\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.00013274931185151218\n",
      "Training Loss: 0.0001119070404638478\n",
      "Training Loss: 0.00011262366049777484\n",
      "Validation Loss: 0.00011040240969215232\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.0001322016004269244\n",
      "Training Loss: 0.00011144266565679572\n",
      "Training Loss: 0.00011215221065867809\n",
      "Validation Loss: 0.00010990382498277356\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.000131657589081442\n",
      "Training Loss: 0.0001109813955554273\n",
      "Training Loss: 0.00011168393406478572\n",
      "Validation Loss: 0.00010940874184843471\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.0001311166382402007\n",
      "Training Loss: 0.00011052268049752456\n",
      "Training Loss: 0.00011121866089524701\n",
      "Validation Loss: 0.00010891742541195087\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.00013057957728960902\n",
      "Training Loss: 0.00011006696811818984\n",
      "Training Loss: 0.00011075636384703103\n",
      "Validation Loss: 0.00010842948518330467\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.000130045924361184\n",
      "Training Loss: 0.00010961404235786176\n",
      "Training Loss: 0.00011029712108211243\n",
      "Validation Loss: 0.00010794501129444296\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.00012951589571457588\n",
      "Training Loss: 0.00010916423192611547\n",
      "Training Loss: 0.0001098407066456275\n",
      "Validation Loss: 0.00010746398396130123\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.00012898881926957982\n",
      "Training Loss: 0.00010871673433939578\n",
      "Training Loss: 0.00010938704535874421\n",
      "Validation Loss: 0.00010698640007729082\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.00012846525070926872\n",
      "Training Loss: 0.00010827240188518772\n",
      "Training Loss: 0.00010893659964494873\n",
      "Validation Loss: 0.00010651198638523954\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.0001279454525320034\n",
      "Training Loss: 0.00010783104464280768\n",
      "Training Loss: 0.00010848873869690579\n",
      "Validation Loss: 0.0001060410605133982\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.00012742886776322848\n",
      "Training Loss: 0.00010739220815594308\n",
      "Training Loss: 0.00010804404610098572\n",
      "Validation Loss: 0.000105573475593701\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.0001269155027148372\n",
      "Training Loss: 0.0001069565902434988\n",
      "Training Loss: 0.0001076018724052119\n",
      "Validation Loss: 0.00010510923577507885\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.00012640559920328086\n",
      "Training Loss: 0.0001065239113631833\n",
      "Training Loss: 0.00010716263890572009\n",
      "Validation Loss: 0.00010464817604044808\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.00012589895219207393\n",
      "Training Loss: 0.00010609427186864196\n",
      "Training Loss: 0.00010672633330614189\n",
      "Validation Loss: 0.00010419047888145348\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.00012539591116365046\n",
      "Training Loss: 0.00010566693328655673\n",
      "Training Loss: 0.00010629284524839022\n",
      "Validation Loss: 0.0001037360480755994\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.00012489597269450315\n",
      "Training Loss: 0.00010524315806833329\n",
      "Training Loss: 0.00010586197649899986\n",
      "Validation Loss: 0.00010328473994316105\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.00012439950336556649\n",
      "Training Loss: 0.00010482187333764159\n",
      "Training Loss: 0.00010543363876422518\n",
      "Validation Loss: 0.00010283662252251734\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.00012390613375828252\n",
      "Training Loss: 0.00010440397622005549\n",
      "Training Loss: 0.00010500839724045363\n",
      "Validation Loss: 0.0001023918535752324\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.00012341630303126295\n",
      "Training Loss: 0.00010398874908787548\n",
      "Training Loss: 0.00010458591268616146\n",
      "Validation Loss: 0.0001019502349949659\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.00012292968247493263\n",
      "Training Loss: 0.00010357668057622504\n",
      "Training Loss: 0.00010416617978989962\n",
      "Validation Loss: 0.00010151164425540914\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.00012244619121702272\n",
      "Training Loss: 0.0001031677745959314\n",
      "Training Loss: 0.00010374865834819502\n",
      "Validation Loss: 0.00010107630920031348\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.00012196604429846048\n",
      "Training Loss: 0.00010276167626216193\n",
      "Training Loss: 0.00010333460440961061\n",
      "Validation Loss: 0.0001006443035294069\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.00012148952966526849\n",
      "Training Loss: 0.00010235882155029685\n",
      "Training Loss: 0.00010292295533872675\n",
      "Validation Loss: 0.00010021531225095132\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.00012101576641725841\n",
      "Training Loss: 0.00010195891660259804\n",
      "Training Loss: 0.00010251396794046741\n",
      "Validation Loss: 9.978938478423188e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.00012054560927936109\n",
      "Training Loss: 0.00010156168240428087\n",
      "Training Loss: 0.00010210776819803869\n",
      "Validation Loss: 9.936683679544341e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.00012007875702693127\n",
      "Training Loss: 0.00010116814133652952\n",
      "Training Loss: 0.00010170409923375701\n",
      "Validation Loss: 9.894721000144661e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.00011961493586568395\n",
      "Training Loss: 0.00010077731574710925\n",
      "Training Loss: 0.00010130322949407856\n",
      "Validation Loss: 9.853064293156933e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.00011915456554561387\n",
      "Training Loss: 0.00010038958642326179\n",
      "Training Loss: 0.00010090532232425176\n",
      "Validation Loss: 9.811733091257257e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.00011869728008605307\n",
      "Training Loss: 0.00010000497873988933\n",
      "Training Loss: 0.00010050985078123631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [07:00<28:00, 210.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 9.770718418039497e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.06076240804046393\n",
      "Training Loss: 0.056388432793319225\n",
      "Training Loss: 0.05744657861068845\n",
      "Validation Loss: 0.05615203540897771\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.0523445313051343\n",
      "Training Loss: 0.04822463855147362\n",
      "Training Loss: 0.04845275606028736\n",
      "Validation Loss: 0.04609164407246568\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.04234950332902372\n",
      "Training Loss: 0.03790320529602468\n",
      "Training Loss: 0.03695725426077843\n",
      "Validation Loss: 0.03370872358111351\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.030319114127196373\n",
      "Training Loss: 0.026021135980263354\n",
      "Training Loss: 0.024535432709380984\n",
      "Validation Loss: 0.021351152620267835\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.018765877325786277\n",
      "Training Loss: 0.01571339948626701\n",
      "Training Loss: 0.014828847511671484\n",
      "Validation Loss: 0.01307134747013366\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.011416246610442613\n",
      "Training Loss: 0.010103340072091668\n",
      "Training Loss: 0.010008620219305158\n",
      "Validation Loss: 0.009452545827024438\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.008274957158137112\n",
      "Training Loss: 0.0078102905745618045\n",
      "Training Loss: 0.00788956596981734\n",
      "Validation Loss: 0.0076683785339942976\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.006741170362074627\n",
      "Training Loss: 0.006559447689214721\n",
      "Training Loss: 0.00662268647341989\n",
      "Validation Loss: 0.006466622429041799\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.005707964057219215\n",
      "Training Loss: 0.005636465374846011\n",
      "Training Loss: 0.005657560637919232\n",
      "Validation Loss: 0.00550004085552031\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.004862532679107971\n",
      "Training Loss: 0.004839843689114787\n",
      "Training Loss: 0.004823486687382683\n",
      "Validation Loss: 0.00465000609558578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.00411026609770488\n",
      "Training Loss: 0.004125219339621253\n",
      "Training Loss: 0.004089173917309381\n",
      "Validation Loss: 0.003909662507181422\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.003456200370565057\n",
      "Training Loss: 0.00351938649546355\n",
      "Training Loss: 0.0034839487093267963\n",
      "Validation Loss: 0.003317433432283487\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.002942381391039817\n",
      "Training Loss: 0.003056849752028938\n",
      "Training Loss: 0.0030297065019840373\n",
      "Validation Loss: 0.002887750375398508\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.0025712741940515115\n",
      "Training Loss: 0.0027145543167716823\n",
      "Training Loss: 0.0026897339028073477\n",
      "Validation Loss: 0.002576334669458774\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.002292791533109266\n",
      "Training Loss: 0.002441013565403409\n",
      "Training Loss: 0.002416206203924958\n",
      "Validation Loss: 0.002337337921509582\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.002067285096709384\n",
      "Training Loss: 0.0022098113273386843\n",
      "Training Loss: 0.002186195707472507\n",
      "Validation Loss: 0.0021471571412036887\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.0018781082282657735\n",
      "Training Loss: 0.0020115168049233034\n",
      "Training Loss: 0.0019895345726399683\n",
      "Validation Loss: 0.0019922126064945557\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.001717058982612798\n",
      "Training Loss: 0.001840254171838751\n",
      "Training Loss: 0.001819480607227888\n",
      "Validation Loss: 0.0018626202074117602\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.0015781775808864041\n",
      "Training Loss: 0.0016907513566548004\n",
      "Training Loss: 0.0016704940832278227\n",
      "Validation Loss: 0.0017509289567222755\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.0014564746587711853\n",
      "Training Loss: 0.0015582524353521877\n",
      "Training Loss: 0.0015378755082201677\n",
      "Validation Loss: 0.0016517958823222603\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.0013477758342924063\n",
      "Training Loss: 0.0014387021915172227\n",
      "Training Loss: 0.0014177119829400908\n",
      "Validation Loss: 0.0015615153177432047\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0012486792235722533\n",
      "Training Loss: 0.0013287978676089552\n",
      "Training Loss: 0.0013068102252145764\n",
      "Validation Loss: 0.0014773966550625588\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.0011564429047575687\n",
      "Training Loss: 0.001225880478596082\n",
      "Training Loss: 0.0012025222391821445\n",
      "Validation Loss: 0.001397137437440884\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.0010687511343712686\n",
      "Training Loss: 0.0011276467850257178\n",
      "Training Loss: 0.001102412686595926\n",
      "Validation Loss: 0.0013181545802397297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0009832884062780068\n",
      "Training Loss: 0.0010315366961003746\n",
      "Training Loss: 0.0010035005107783945\n",
      "Validation Loss: 0.0012360311349071667\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.0008965506642562104\n",
      "Training Loss: 0.0009327829520043451\n",
      "Training Loss: 0.0008995969560055528\n",
      "Validation Loss: 0.0011380980266428166\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.000799142511878017\n",
      "Training Loss: 0.0008160820939883706\n",
      "Training Loss: 0.0007706305728061125\n",
      "Validation Loss: 0.0009736135058998381\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.0006588650065714319\n",
      "Training Loss: 0.0006372472372822812\n",
      "Training Loss: 0.0005964473537824233\n",
      "Validation Loss: 0.0007122288738623303\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.000506914259021869\n",
      "Training Loss: 0.000506481554621132\n",
      "Training Loss: 0.0005194599120659405\n",
      "Validation Loss: 0.0006262656189913317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.00045939979820104783\n",
      "Training Loss: 0.00046511169100995177\n",
      "Training Loss: 0.0004892377035139361\n",
      "Validation Loss: 0.0006027283378173022\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.0004350801099280943\n",
      "Training Loss: 0.00044034555390680906\n",
      "Training Loss: 0.00046757174626691265\n",
      "Validation Loss: 0.0005903010465220412\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.00041730968096089784\n",
      "Training Loss: 0.0004215591639876948\n",
      "Training Loss: 0.0004498200128728058\n",
      "Validation Loss: 0.0005799405921147247\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.00040291329394676723\n",
      "Training Loss: 0.0004059191643682425\n",
      "Training Loss: 0.0004344696656335145\n",
      "Validation Loss: 0.0005692923752188138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.0003905804874011665\n",
      "Training Loss: 0.0003922582772793248\n",
      "Training Loss: 0.0004207929826225154\n",
      "Validation Loss: 0.0005578765205959162\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.00037961347898090024\n",
      "Training Loss: 0.0003799787516800279\n",
      "Training Loss: 0.0004083667026134208\n",
      "Validation Loss: 0.000545807439327669\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.00036958782853616865\n",
      "Training Loss: 0.0003687164938673959\n",
      "Training Loss: 0.000396899417428358\n",
      "Validation Loss: 0.0005333010285335405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0003602188523655059\n",
      "Training Loss: 0.0003582200748132891\n",
      "Training Loss: 0.00038616849342361094\n",
      "Validation Loss: 0.000520548104044322\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.000351300938273198\n",
      "Training Loss: 0.00034829621610697357\n",
      "Training Loss: 0.0003759912589521264\n",
      "Validation Loss: 0.000507647028795229\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.00034267317674675725\n",
      "Training Loss: 0.00033878753094541026\n",
      "Training Loss: 0.00036621252074837686\n",
      "Validation Loss: 0.0004946095410314535\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.00033420545176340963\n",
      "Training Loss: 0.00032956247956462904\n",
      "Training Loss: 0.0003566969986422919\n",
      "Validation Loss: 0.0004813914912232208\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.00032578758109593766\n",
      "Training Loss: 0.00032051116257207467\n",
      "Training Loss: 0.0003473305500665447\n",
      "Validation Loss: 0.00046789912841141516\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.00031732980194647096\n",
      "Training Loss: 0.00031154786205661366\n",
      "Training Loss: 0.00033801868914451916\n",
      "Validation Loss: 0.0004540288831000059\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0003087633812901913\n",
      "Training Loss: 0.0003026117660738237\n",
      "Training Loss: 0.00032869052734895376\n",
      "Validation Loss: 0.0004396923542025321\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.000300042645612848\n",
      "Training Loss: 0.00029366781205681036\n",
      "Training Loss: 0.0003192992155891261\n",
      "Validation Loss: 0.00042483949931757003\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.0002911466701334575\n",
      "Training Loss: 0.00028470251234466557\n",
      "Training Loss: 0.00030982216718257406\n",
      "Validation Loss: 0.00040948112062610205\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.00028207984041728196\n",
      "Training Loss: 0.00027572100987526937\n",
      "Training Loss: 0.0003002585404465208\n",
      "Validation Loss: 0.00039367283199190454\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0002728651564939355\n",
      "Training Loss: 0.0002667419477438671\n",
      "Training Loss: 0.0002906264685225324\n",
      "Validation Loss: 0.0003775216343834227\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.000263543690834922\n",
      "Training Loss: 0.0002577982819639146\n",
      "Training Loss: 0.0002809622230779496\n",
      "Validation Loss: 0.00036115929604706805\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.0002541739282241906\n",
      "Training Loss: 0.00024893462359614204\n",
      "Training Loss: 0.00027131776529131456\n",
      "Validation Loss: 0.0003447490613080242\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0002448271321463835\n",
      "Training Loss: 0.0002402052028446633\n",
      "Training Loss: 0.0002617552617084584\n",
      "Validation Loss: 0.00032847575731169995\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.00023558446211609406\n",
      "Training Loss: 0.00023166925075202016\n",
      "Training Loss: 0.00025234508866560644\n",
      "Validation Loss: 0.0003125271992921421\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.0002265301062107028\n",
      "Training Loss: 0.0002233850857919606\n",
      "Training Loss: 0.00024315800957992905\n",
      "Validation Loss: 0.00029708116791594323\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00021774540273327147\n",
      "Training Loss: 0.0002154057389634545\n",
      "Training Loss: 0.00023426073752489174\n",
      "Validation Loss: 0.000282298249372064\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.0002093024517671438\n",
      "Training Loss: 0.00020777326817551512\n",
      "Training Loss: 0.00022571082230570026\n",
      "Validation Loss: 0.0002683053746213875\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.00020125934403722567\n",
      "Training Loss: 0.00020051870591487387\n",
      "Training Loss: 0.00021755452882644022\n",
      "Validation Loss: 0.000255187299740867\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.00019365818043979744\n",
      "Training Loss: 0.00019365996586202528\n",
      "Training Loss: 0.00020982567000828566\n",
      "Validation Loss: 0.0002429944071398781\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0001865254853237275\n",
      "Training Loss: 0.00018720422356636847\n",
      "Training Loss: 0.00020254565079085295\n",
      "Validation Loss: 0.00023174339530465004\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.00017987195360319675\n",
      "Training Loss: 0.00018114896396582481\n",
      "Training Loss: 0.00019572485416574636\n",
      "Validation Loss: 0.00022141799304795876\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00017369576290548138\n",
      "Training Loss: 0.00017548526153404964\n",
      "Training Loss: 0.00018936456361188904\n",
      "Validation Loss: 0.0002119850054277981\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.00016798560511801952\n",
      "Training Loss: 0.00017019817605614662\n",
      "Training Loss: 0.00018345762086028117\n",
      "Validation Loss: 0.00020339331001267648\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.0001627219926066914\n",
      "Training Loss: 0.00016526974615771906\n",
      "Training Loss: 0.00017798966668124193\n",
      "Validation Loss: 0.00019558315684304708\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.00015787990660925288\n",
      "Training Loss: 0.00016067912176367827\n",
      "Training Loss: 0.00017294200324613485\n",
      "Validation Loss: 0.00018849248960342788\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.0001534306129464369\n",
      "Training Loss: 0.0001564044405131426\n",
      "Training Loss: 0.00016828978174089572\n",
      "Validation Loss: 0.00018205391032179147\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.00014934421424548817\n",
      "Training Loss: 0.00015242277707329776\n",
      "Training Loss: 0.000164005468814139\n",
      "Validation Loss: 0.0001762024263228065\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.00014558924270485107\n",
      "Training Loss: 0.00014871099349875294\n",
      "Training Loss: 0.00016005990693884086\n",
      "Validation Loss: 0.00017087736660470725\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.0001421351282285599\n",
      "Training Loss: 0.0001452466334376368\n",
      "Training Loss: 0.00015642270240277866\n",
      "Validation Loss: 0.00016602218346725863\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.00013895211548856424\n",
      "Training Loss: 0.00014200755035744805\n",
      "Training Loss: 0.00015306282048186404\n",
      "Validation Loss: 0.00016158142871778653\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.00013601199522327077\n",
      "Training Loss: 0.0001389726607067132\n",
      "Training Loss: 0.0001499509534733079\n",
      "Validation Loss: 0.00015751005124135253\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.00013328857694347107\n",
      "Training Loss: 0.0001361228338100773\n",
      "Training Loss: 0.00014705903235153527\n",
      "Validation Loss: 0.0001537650612876502\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.00013075765298168563\n",
      "Training Loss: 0.00013343949510272068\n",
      "Training Loss: 0.00014436067644055584\n",
      "Validation Loss: 0.00015030978962447458\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.00012839754328183517\n",
      "Training Loss: 0.00013090627912788478\n",
      "Training Loss: 0.0001418328941508662\n",
      "Validation Loss: 0.00014711310721170602\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.00012618918462521832\n",
      "Training Loss: 0.00012850883217652154\n",
      "Training Loss: 0.0001394546776828065\n",
      "Validation Loss: 0.00014414535387382538\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.00012411516541249056\n",
      "Training Loss: 0.00012623358358723635\n",
      "Training Loss: 0.00013720755459871724\n",
      "Validation Loss: 0.00014138508464547862\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.00012216065523944054\n",
      "Training Loss: 0.00012406948290845322\n",
      "Training Loss: 0.0001350757545696979\n",
      "Validation Loss: 0.00013880832446967983\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.00012031287654963307\n",
      "Training Loss: 0.00012200650250633771\n",
      "Training Loss: 0.0001330460616009077\n",
      "Validation Loss: 0.00013639808766425487\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00011856073607077633\n",
      "Training Loss: 0.00012003604988422011\n",
      "Training Loss: 0.0001311069616531313\n",
      "Validation Loss: 0.0001341379091254464\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00011689501064552133\n",
      "Training Loss: 0.00011815106515314255\n",
      "Training Loss: 0.0001292491900676396\n",
      "Validation Loss: 0.0001320146253378931\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00011530754090586015\n",
      "Training Loss: 0.00011634513765784504\n",
      "Training Loss: 0.00012746503321977798\n",
      "Validation Loss: 0.00013001432616261666\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00011379185886880805\n",
      "Training Loss: 0.00011461294800028554\n",
      "Training Loss: 0.00012574791795486818\n",
      "Validation Loss: 0.00012812691135438267\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.0001123426082494916\n",
      "Training Loss: 0.00011295048941974529\n",
      "Training Loss: 0.00012409286289766897\n",
      "Validation Loss: 0.0001263436967409883\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.00011095487403508741\n",
      "Training Loss: 0.00011135371490127\n",
      "Training Loss: 0.00012249598654307192\n",
      "Validation Loss: 0.000124655320597116\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.00010962477968064377\n",
      "Training Loss: 0.00010981942175931181\n",
      "Training Loss: 0.00012095359519662452\n",
      "Validation Loss: 0.00012305509857612654\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.00010834927232281189\n",
      "Training Loss: 0.00010834487350621203\n",
      "Training Loss: 0.00011946275662921835\n",
      "Validation Loss: 0.00012153486870906153\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.00010712523408756169\n",
      "Training Loss: 0.00010692714307879214\n",
      "Training Loss: 0.00011802111264842097\n",
      "Validation Loss: 0.00012008904690283293\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.00010595038551855396\n",
      "Training Loss: 0.00010556404437920719\n",
      "Training Loss: 0.00011662676302876207\n",
      "Validation Loss: 0.00011871287013368754\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.00010482224694442265\n",
      "Training Loss: 0.00010425329804093054\n",
      "Training Loss: 0.00011527784465215519\n",
      "Validation Loss: 0.00011740126870443398\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.00010373880257702695\n",
      "Training Loss: 0.00010299280221261142\n",
      "Training Loss: 0.00011397239368307054\n",
      "Validation Loss: 0.0001161483092964568\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.0001026979053199284\n",
      "Training Loss: 0.00010178005619309261\n",
      "Training Loss: 0.0001127089837245876\n",
      "Validation Loss: 0.00011495100409703217\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.00010169777617193177\n",
      "Training Loss: 0.00010061326095637923\n",
      "Training Loss: 0.00011148587971547386\n",
      "Validation Loss: 0.00011380445739837086\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.00010073621670017018\n",
      "Training Loss: 9.949000015694764e-05\n",
      "Training Loss: 0.00011030154355466948\n",
      "Validation Loss: 0.00011270614958335868\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 9.98112783918259e-05\n",
      "Training Loss: 9.840811754656898e-05\n",
      "Training Loss: 0.00010915431823377731\n",
      "Validation Loss: 0.00011165119652238051\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 9.892115652291977e-05\n",
      "Training Loss: 9.736529509609681e-05\n",
      "Training Loss: 0.00010804270041262499\n",
      "Validation Loss: 0.00011063755809619181\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 9.806371193462837e-05\n",
      "Training Loss: 9.635954605982988e-05\n",
      "Training Loss: 0.00010696496763557662\n",
      "Validation Loss: 0.00010966109868931699\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 9.723687460109431e-05\n",
      "Training Loss: 9.538884696667083e-05\n",
      "Training Loss: 0.00010591962816761225\n",
      "Validation Loss: 0.00010871968445346202\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 9.643877549933677e-05\n",
      "Training Loss: 9.445098547985254e-05\n",
      "Training Loss: 0.00010490501228559878\n",
      "Validation Loss: 0.00010781096507037474\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 9.56676551027158e-05\n",
      "Training Loss: 9.354418502880436e-05\n",
      "Training Loss: 0.00010391982221335638\n",
      "Validation Loss: 0.00010693218403533782\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 9.492169151144481e-05\n",
      "Training Loss: 9.26664719281689e-05\n",
      "Training Loss: 0.00010296239964191045\n",
      "Validation Loss: 0.00010608089387767067\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 9.41992736170505e-05\n",
      "Training Loss: 9.181599325529533e-05\n",
      "Training Loss: 0.00010203176509094192\n",
      "Validation Loss: 0.00010525605918364102\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 9.349858304631197e-05\n",
      "Training Loss: 9.099152025555668e-05\n",
      "Training Loss: 0.00010112627218404668\n",
      "Validation Loss: 0.00010445618873960443\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 9.281811559731069e-05\n",
      "Training Loss: 9.019129959597194e-05\n",
      "Training Loss: 0.00010024495439211023\n",
      "Validation Loss: 0.00010367889563292082\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 9.215666202635475e-05\n",
      "Training Loss: 8.941411872910976e-05\n",
      "Training Loss: 9.938662968124845e-05\n",
      "Validation Loss: 0.00010292299970735278\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 9.151296833806555e-05\n",
      "Training Loss: 8.86586316391913e-05\n",
      "Training Loss: 9.855042616436549e-05\n",
      "Validation Loss: 0.00010218831471064824\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 9.088582480671903e-05\n",
      "Training Loss: 8.792377077952551e-05\n",
      "Training Loss: 9.773532314284238e-05\n",
      "Validation Loss: 0.00010147249891876904\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 9.027430151036242e-05\n",
      "Training Loss: 8.720862060727086e-05\n",
      "Training Loss: 9.694048886558448e-05\n",
      "Validation Loss: 0.00010077549082330386\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 8.967735149099099e-05\n",
      "Training Loss: 8.651227990412736e-05\n",
      "Training Loss: 9.616526859645092e-05\n",
      "Validation Loss: 0.00010009679814236201\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 8.909417431368638e-05\n",
      "Training Loss: 8.583398173868773e-05\n",
      "Training Loss: 9.540899959574745e-05\n",
      "Validation Loss: 9.943510291593024e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 8.852408799612022e-05\n",
      "Training Loss: 8.517314897289907e-05\n",
      "Training Loss: 9.467093242164993e-05\n",
      "Validation Loss: 9.879031601352775e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 8.796626941148133e-05\n",
      "Training Loss: 8.452888798274216e-05\n",
      "Training Loss: 9.39504689995374e-05\n",
      "Validation Loss: 9.81613176632339e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 8.742032729969651e-05\n",
      "Training Loss: 8.390070856876264e-05\n",
      "Training Loss: 9.324715499133163e-05\n",
      "Validation Loss: 9.754764194828536e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 8.68857714749538e-05\n",
      "Training Loss: 8.328804659868183e-05\n",
      "Training Loss: 9.256039025785867e-05\n",
      "Validation Loss: 9.694884899003677e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 8.636198522253835e-05\n",
      "Training Loss: 8.269049130831263e-05\n",
      "Training Loss: 9.188971314870287e-05\n",
      "Validation Loss: 9.636461015432953e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 8.584855907884048e-05\n",
      "Training Loss: 8.210749192585353e-05\n",
      "Training Loss: 9.123479002482781e-05\n",
      "Validation Loss: 9.579419689503135e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 8.534514695384132e-05\n",
      "Training Loss: 8.153873216542706e-05\n",
      "Training Loss: 9.0594892108129e-05\n",
      "Validation Loss: 9.523685710090003e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 8.485145194754295e-05\n",
      "Training Loss: 8.098363935459929e-05\n",
      "Training Loss: 8.996985623525688e-05\n",
      "Validation Loss: 9.469224129093215e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 8.436703518782451e-05\n",
      "Training Loss: 8.044195666116138e-05\n",
      "Training Loss: 8.935915100664715e-05\n",
      "Validation Loss: 9.415991865077221e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 8.389163105221087e-05\n",
      "Training Loss: 7.99131502299133e-05\n",
      "Training Loss: 8.876242347469088e-05\n",
      "Validation Loss: 9.363913280783767e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 8.342496545083123e-05\n",
      "Training Loss: 7.939696411995101e-05\n",
      "Training Loss: 8.817918167551397e-05\n",
      "Validation Loss: 9.312946602814633e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 8.296659971620102e-05\n",
      "Training Loss: 7.889290702223662e-05\n",
      "Training Loss: 8.760915020502579e-05\n",
      "Validation Loss: 9.263035163518467e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 8.251655745425524e-05\n",
      "Training Loss: 7.84008301616268e-05\n",
      "Training Loss: 8.70517177918373e-05\n",
      "Validation Loss: 9.214134496779592e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 8.207459713048593e-05\n",
      "Training Loss: 7.79201834302512e-05\n",
      "Training Loss: 8.650680097161967e-05\n",
      "Validation Loss: 9.166149179231418e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 8.164029383351589e-05\n",
      "Training Loss: 7.745071442514018e-05\n",
      "Training Loss: 8.597380093306129e-05\n",
      "Validation Loss: 9.119070410235324e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 8.121345773815846e-05\n",
      "Training Loss: 7.699198310547217e-05\n",
      "Training Loss: 8.545256614524987e-05\n",
      "Validation Loss: 9.072803265113957e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 8.079410570644541e-05\n",
      "Training Loss: 7.654376204300207e-05\n",
      "Training Loss: 8.494234547470115e-05\n",
      "Validation Loss: 9.027347972773844e-05\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 8.038199413931579e-05\n",
      "Training Loss: 7.610584081703564e-05\n",
      "Training Loss: 8.444315251836087e-05\n",
      "Validation Loss: 8.982577827318279e-05\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 7.997666701612615e-05\n",
      "Training Loss: 7.567761314021482e-05\n",
      "Training Loss: 8.395455334721191e-05\n",
      "Validation Loss: 8.938504126919643e-05\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 7.957809796607762e-05\n",
      "Training Loss: 7.525889599492074e-05\n",
      "Training Loss: 8.347612005309202e-05\n",
      "Validation Loss: 8.895074281324187e-05\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 7.918616948700219e-05\n",
      "Training Loss: 7.484950598154682e-05\n",
      "Training Loss: 8.300756461721903e-05\n",
      "Validation Loss: 8.85222569481454e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 7.88006615039194e-05\n",
      "Training Loss: 7.444894144100545e-05\n",
      "Training Loss: 8.254870517703239e-05\n",
      "Validation Loss: 8.809939869903512e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 7.842152946068382e-05\n",
      "Training Loss: 7.405702804135217e-05\n",
      "Training Loss: 8.209918937609472e-05\n",
      "Validation Loss: 8.76818145790433e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 7.804853154084412e-05\n",
      "Training Loss: 7.36734527254157e-05\n",
      "Training Loss: 8.165855386323529e-05\n",
      "Validation Loss: 8.726875038202421e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 7.76815428662303e-05\n",
      "Training Loss: 7.329797508646153e-05\n",
      "Training Loss: 8.122673826619575e-05\n",
      "Validation Loss: 8.686014687841277e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 7.732042643510795e-05\n",
      "Training Loss: 7.293033628229751e-05\n",
      "Training Loss: 8.080310322839068e-05\n",
      "Validation Loss: 8.645579142017248e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 7.696516296164191e-05\n",
      "Training Loss: 7.257029921674984e-05\n",
      "Training Loss: 8.038785585995356e-05\n",
      "Validation Loss: 8.6055372387849e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 7.661546463623381e-05\n",
      "Training Loss: 7.221774017125426e-05\n",
      "Training Loss: 7.99805292444944e-05\n",
      "Validation Loss: 8.565852075141234e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 7.627131200479199e-05\n",
      "Training Loss: 7.18720714758092e-05\n",
      "Training Loss: 7.958095686262823e-05\n",
      "Validation Loss: 8.526526239569484e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 7.593248686589504e-05\n",
      "Training Loss: 7.153338154239464e-05\n",
      "Training Loss: 7.91889091487974e-05\n",
      "Validation Loss: 8.487564978525605e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 7.559903650417255e-05\n",
      "Training Loss: 7.12014326109056e-05\n",
      "Training Loss: 7.880404391471529e-05\n",
      "Validation Loss: 8.448856125765263e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 7.527084943831141e-05\n",
      "Training Loss: 7.087587996466028e-05\n",
      "Training Loss: 7.842624397198961e-05\n",
      "Validation Loss: 8.410532155834149e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 7.49476755345313e-05\n",
      "Training Loss: 7.055666785163339e-05\n",
      "Training Loss: 7.805519107023428e-05\n",
      "Validation Loss: 8.372454755026206e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 7.462974463123828e-05\n",
      "Training Loss: 7.024347880360437e-05\n",
      "Training Loss: 7.769104490762402e-05\n",
      "Validation Loss: 8.334695624224867e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 7.431658622408577e-05\n",
      "Training Loss: 6.993631604927941e-05\n",
      "Training Loss: 7.733321177511243e-05\n",
      "Validation Loss: 8.297217046991422e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 7.400829379093921e-05\n",
      "Training Loss: 6.96349370173266e-05\n",
      "Training Loss: 7.698189950133382e-05\n",
      "Validation Loss: 8.260012719199111e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 7.370472327238531e-05\n",
      "Training Loss: 6.933911443411489e-05\n",
      "Training Loss: 7.663650401809719e-05\n",
      "Validation Loss: 8.223108497066972e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 7.340600401221308e-05\n",
      "Training Loss: 6.90486154962855e-05\n",
      "Training Loss: 7.629716302290035e-05\n",
      "Validation Loss: 8.186428341728418e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 7.311190430300485e-05\n",
      "Training Loss: 6.876345754790236e-05\n",
      "Training Loss: 7.596369027851324e-05\n",
      "Validation Loss: 8.15011319011748e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 7.282225707967882e-05\n",
      "Training Loss: 6.848343920864863e-05\n",
      "Training Loss: 7.563590859717806e-05\n",
      "Validation Loss: 8.114042642508016e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 7.253714589751325e-05\n",
      "Training Loss: 6.820831010372785e-05\n",
      "Training Loss: 7.531365646173072e-05\n",
      "Validation Loss: 8.0782332772512e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 7.225633553844091e-05\n",
      "Training Loss: 6.79380611563829e-05\n",
      "Training Loss: 7.499671545701858e-05\n",
      "Validation Loss: 8.042727648480037e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 7.197989666110516e-05\n",
      "Training Loss: 6.767249858057766e-05\n",
      "Training Loss: 7.468518355381093e-05\n",
      "Validation Loss: 8.007549179259758e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 7.170769054482663e-05\n",
      "Training Loss: 6.741145116393455e-05\n",
      "Training Loss: 7.437869428940758e-05\n",
      "Validation Loss: 7.972637384105081e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 7.143974849441293e-05\n",
      "Training Loss: 6.715500902828354e-05\n",
      "Training Loss: 7.407718400827435e-05\n",
      "Validation Loss: 7.93800561215004e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 7.117574044059439e-05\n",
      "Training Loss: 6.690276609333523e-05\n",
      "Training Loss: 7.37805543394643e-05\n",
      "Validation Loss: 7.90372271112165e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 7.091583507644828e-05\n",
      "Training Loss: 6.665465469268383e-05\n",
      "Training Loss: 7.34887757243996e-05\n",
      "Validation Loss: 7.869729592477617e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 7.065991484751067e-05\n",
      "Training Loss: 6.641070036494057e-05\n",
      "Training Loss: 7.320164104385185e-05\n",
      "Validation Loss: 7.83605820094196e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 7.040786678544464e-05\n",
      "Training Loss: 6.617072950575676e-05\n",
      "Training Loss: 7.291895092748746e-05\n",
      "Validation Loss: 7.802729913727786e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 7.01596021235673e-05\n",
      "Training Loss: 6.593461024749558e-05\n",
      "Training Loss: 7.26407755882974e-05\n",
      "Validation Loss: 7.769706140668327e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 6.991508091005017e-05\n",
      "Training Loss: 6.570231502337265e-05\n",
      "Training Loss: 7.236688544253412e-05\n",
      "Validation Loss: 7.737057810087473e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 6.967424992581073e-05\n",
      "Training Loss: 6.547360296281112e-05\n",
      "Training Loss: 7.209718926787901e-05\n",
      "Validation Loss: 7.704714194976455e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 6.943708050584974e-05\n",
      "Training Loss: 6.524851726680936e-05\n",
      "Training Loss: 7.183154636550171e-05\n",
      "Validation Loss: 7.672707532323882e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 6.920338080362853e-05\n",
      "Training Loss: 6.502674211333214e-05\n",
      "Training Loss: 7.157007877140132e-05\n",
      "Validation Loss: 7.641043150338039e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 6.897310299336823e-05\n",
      "Training Loss: 6.480836057562556e-05\n",
      "Training Loss: 7.13125028869399e-05\n",
      "Validation Loss: 7.609746568009593e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 6.874623828025506e-05\n",
      "Training Loss: 6.459337806518307e-05\n",
      "Training Loss: 7.105867643076635e-05\n",
      "Validation Loss: 7.578777750199295e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 6.85227218741602e-05\n",
      "Training Loss: 6.438141922444629e-05\n",
      "Training Loss: 7.080858496919972e-05\n",
      "Validation Loss: 7.548206056608148e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 6.830239233522661e-05\n",
      "Training Loss: 6.417261697606591e-05\n",
      "Training Loss: 7.056217111312435e-05\n",
      "Validation Loss: 7.517943075154005e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 6.808519813148451e-05\n",
      "Training Loss: 6.396685548679671e-05\n",
      "Training Loss: 7.03193777189881e-05\n",
      "Validation Loss: 7.488078328728836e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 6.787127510733626e-05\n",
      "Training Loss: 6.376393110258505e-05\n",
      "Training Loss: 7.007986010648892e-05\n",
      "Validation Loss: 7.458522293418764e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 6.766036837916544e-05\n",
      "Training Loss: 6.356399865580897e-05\n",
      "Training Loss: 6.984382077462214e-05\n",
      "Validation Loss: 7.429345768787755e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 6.745241232920307e-05\n",
      "Training Loss: 6.336670517157473e-05\n",
      "Training Loss: 6.961101415072335e-05\n",
      "Validation Loss: 7.400534523798485e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 6.724733919782012e-05\n",
      "Training Loss: 6.317216662864667e-05\n",
      "Training Loss: 6.93815260183328e-05\n",
      "Validation Loss: 7.372081606436961e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 6.70451060045707e-05\n",
      "Training Loss: 6.298019830865087e-05\n",
      "Training Loss: 6.915518575624446e-05\n",
      "Validation Loss: 7.34397049149102e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 6.684566778631052e-05\n",
      "Training Loss: 6.279071197241138e-05\n",
      "Training Loss: 6.893190587106801e-05\n",
      "Validation Loss: 7.316227252845289e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 6.66489291347716e-05\n",
      "Training Loss: 6.260382469918113e-05\n",
      "Training Loss: 6.871164127915108e-05\n",
      "Validation Loss: 7.288835292732411e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 6.645485898843618e-05\n",
      "Training Loss: 6.241930391297501e-05\n",
      "Training Loss: 6.849414652606357e-05\n",
      "Validation Loss: 7.261796019336319e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 6.626345556014713e-05\n",
      "Training Loss: 6.223694364052789e-05\n",
      "Training Loss: 6.82797229183052e-05\n",
      "Validation Loss: 7.235105095695759e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 6.60745011941799e-05\n",
      "Training Loss: 6.205705944921647e-05\n",
      "Training Loss: 6.80678937715129e-05\n",
      "Validation Loss: 7.208750254873204e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 6.588806289755666e-05\n",
      "Training Loss: 6.187934925947047e-05\n",
      "Training Loss: 6.785889148886781e-05\n",
      "Validation Loss: 7.182793522363508e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 6.570394107711763e-05\n",
      "Training Loss: 6.170386744997813e-05\n",
      "Training Loss: 6.765253970115736e-05\n",
      "Validation Loss: 7.157115130999198e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 6.552232617195841e-05\n",
      "Training Loss: 6.15305236169661e-05\n",
      "Training Loss: 6.74487259129819e-05\n",
      "Validation Loss: 7.131804968492033e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 6.534299131317312e-05\n",
      "Training Loss: 6.135907877251156e-05\n",
      "Training Loss: 6.724744567691231e-05\n",
      "Validation Loss: 7.106792965939522e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 6.516580346669798e-05\n",
      "Training Loss: 6.118970677562174e-05\n",
      "Training Loss: 6.704874524075421e-05\n",
      "Validation Loss: 7.082117603142783e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 6.499089608951181e-05\n",
      "Training Loss: 6.1022155050523e-05\n",
      "Training Loss: 6.68523086278583e-05\n",
      "Validation Loss: 7.057756099814296e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 6.481809468596111e-05\n",
      "Training Loss: 6.0856624468215156e-05\n",
      "Training Loss: 6.66582359599488e-05\n",
      "Validation Loss: 7.033732450263722e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 6.464733288112256e-05\n",
      "Training Loss: 6.069286175261368e-05\n",
      "Training Loss: 6.646663212450221e-05\n",
      "Validation Loss: 7.010021436962121e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 6.447864304163886e-05\n",
      "Training Loss: 6.0530827431648505e-05\n",
      "Training Loss: 6.627704456604988e-05\n",
      "Validation Loss: 6.986581502133049e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 6.431202117255452e-05\n",
      "Training Loss: 6.037065281816467e-05\n",
      "Training Loss: 6.608975419567287e-05\n",
      "Validation Loss: 6.963478749690858e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 6.414728859454044e-05\n",
      "Training Loss: 6.021218368005066e-05\n",
      "Training Loss: 6.590449740087934e-05\n",
      "Validation Loss: 6.940671370470676e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 6.398441433475455e-05\n",
      "Training Loss: 6.005524604915991e-05\n",
      "Training Loss: 6.572157156824688e-05\n",
      "Validation Loss: 6.91814642716585e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 6.382333356214076e-05\n",
      "Training Loss: 5.9900012693105965e-05\n",
      "Training Loss: 6.554060796588601e-05\n",
      "Validation Loss: 6.895921396381311e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 6.366424639509205e-05\n",
      "Training Loss: 5.974624336886336e-05\n",
      "Training Loss: 6.536165094075841e-05\n",
      "Validation Loss: 6.873951615795776e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 6.350680740752068e-05\n",
      "Training Loss: 5.959420970611973e-05\n",
      "Training Loss: 6.518454436445609e-05\n",
      "Validation Loss: 6.852293312619898e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 6.335119630875851e-05\n",
      "Training Loss: 5.944360741523269e-05\n",
      "Training Loss: 6.500946715277678e-05\n",
      "Validation Loss: 6.830884675958699e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 6.319730615587105e-05\n",
      "Training Loss: 5.929448419010441e-05\n",
      "Training Loss: 6.483622979430948e-05\n",
      "Validation Loss: 6.809737398439568e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 6.30448513970805e-05\n",
      "Training Loss: 5.914675541134784e-05\n",
      "Training Loss: 6.46648002521033e-05\n",
      "Validation Loss: 6.788855964177959e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 6.289418948995262e-05\n",
      "Training Loss: 5.900037284845894e-05\n",
      "Training Loss: 6.44952505672336e-05\n",
      "Validation Loss: 6.768258853288683e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 6.274504013845217e-05\n",
      "Training Loss: 5.885556279281445e-05\n",
      "Training Loss: 6.432746013615542e-05\n",
      "Validation Loss: 6.747895699181447e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 6.259750505819284e-05\n",
      "Training Loss: 5.871193731763924e-05\n",
      "Training Loss: 6.416137754058581e-05\n",
      "Validation Loss: 6.727754351415794e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 6.245136295547126e-05\n",
      "Training Loss: 5.856968492480519e-05\n",
      "Training Loss: 6.399696036623936e-05\n",
      "Validation Loss: 6.707870425395485e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 6.230676170275729e-05\n",
      "Training Loss: 5.8428660977369873e-05\n",
      "Training Loss: 6.383411522620008e-05\n",
      "Validation Loss: 6.688214185753291e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 6.216362317672974e-05\n",
      "Training Loss: 5.828896282764617e-05\n",
      "Training Loss: 6.367315264014905e-05\n",
      "Validation Loss: 6.668793477647444e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 6.202195630066853e-05\n",
      "Training Loss: 5.8150409313384446e-05\n",
      "Training Loss: 6.35135663105757e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [10:30<24:30, 210.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.64960073796526e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.20891592305153608\n",
      "Training Loss: 0.13788685437291862\n",
      "Training Loss: 0.10570238150656223\n",
      "Validation Loss: 0.08196611010752032\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.07168320969678461\n",
      "Training Loss: 0.05933031349442899\n",
      "Training Loss: 0.05948264635168016\n",
      "Validation Loss: 0.056753923352598476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.05339712098240852\n",
      "Training Loss: 0.049003495573997496\n",
      "Training Loss: 0.050315964017063376\n",
      "Validation Loss: 0.04828730128245073\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.04536451277323067\n",
      "Training Loss: 0.04058245059568435\n",
      "Training Loss: 0.04017015636898577\n",
      "Validation Loss: 0.03676794812585531\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.03402501073665917\n",
      "Training Loss: 0.029098626571940258\n",
      "Training Loss: 0.027700777840800585\n",
      "Validation Loss: 0.024373219361570612\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.022414735681377352\n",
      "Training Loss: 0.018927912335493603\n",
      "Training Loss: 0.01796398377744481\n",
      "Validation Loss: 0.01593428224539698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.014670061436190736\n",
      "Training Loss: 0.012732397775398568\n",
      "Training Loss: 0.012315580520080402\n",
      "Validation Loss: 0.011383558906682799\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.010409008195856585\n",
      "Training Loss: 0.009435181479202583\n",
      "Training Loss: 0.009291032090550289\n",
      "Validation Loss: 0.00908391651425385\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.008159418647992425\n",
      "Training Loss: 0.007688253304222598\n",
      "Training Loss: 0.007647194423479959\n",
      "Validation Loss: 0.007900918799902448\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.006931959647336044\n",
      "Training Loss: 0.006700179325416684\n",
      "Training Loss: 0.006691994527354837\n",
      "Validation Loss: 0.007222796204301079\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.0061958342744037505\n",
      "Training Loss: 0.006070792290265672\n",
      "Training Loss: 0.006068464402342215\n",
      "Validation Loss: 0.006743633312570831\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.005682324703084305\n",
      "Training Loss: 0.005602810208802112\n",
      "Training Loss: 0.0055938806489575655\n",
      "Validation Loss: 0.006319641678468481\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.005256347564281896\n",
      "Training Loss: 0.005194908423000015\n",
      "Training Loss: 0.00517050493392162\n",
      "Validation Loss: 0.005882418229181864\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.004845181462005712\n",
      "Training Loss: 0.0047890030988492075\n",
      "Training Loss: 0.0047410355124156926\n",
      "Validation Loss: 0.005395147707470263\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.004404831423889846\n",
      "Training Loss: 0.004348533584270627\n",
      "Training Loss: 0.0042701123916776855\n",
      "Validation Loss: 0.004836588377248119\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.0039090932623366825\n",
      "Training Loss: 0.0038556518836412577\n",
      "Training Loss: 0.003746763995732181\n",
      "Validation Loss: 0.004212276811261525\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.0033647748548537495\n",
      "Training Loss: 0.003336911853402853\n",
      "Training Loss: 0.0032197011285461487\n",
      "Validation Loss: 0.0035975341229919303\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.0028575595589063595\n",
      "Training Loss: 0.0028989856888074427\n",
      "Training Loss: 0.002809557912114542\n",
      "Validation Loss: 0.003106882522965708\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.0025038932214374652\n",
      "Training Loss: 0.0026188077716506088\n",
      "Training Loss: 0.002556336982524954\n",
      "Validation Loss: 0.002745964583349571\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.0022856830674572847\n",
      "Training Loss: 0.002437141627306119\n",
      "Training Loss: 0.002388084831472952\n",
      "Validation Loss: 0.002474809879714393\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.002132322952093091\n",
      "Training Loss: 0.0022972848886274734\n",
      "Training Loss: 0.0022574246587464585\n",
      "Validation Loss: 0.0022765118860916937\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0020126560700009575\n",
      "Training Loss: 0.002180626948538702\n",
      "Training Loss: 0.0021478343158378268\n",
      "Validation Loss: 0.002135585823483514\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.0019144614739343524\n",
      "Training Loss: 0.0020798464765539393\n",
      "Training Loss: 0.0020522836272721177\n",
      "Validation Loss: 0.0020351925002492104\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.001830839515750995\n",
      "Training Loss: 0.001990513902128441\n",
      "Training Loss: 0.00196661366862827\n",
      "Validation Loss: 0.0019601373581720643\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0017570634017465636\n",
      "Training Loss: 0.0019093943377083632\n",
      "Training Loss: 0.0018879077583551407\n",
      "Validation Loss: 0.0018987317770337582\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.0016898111278715077\n",
      "Training Loss: 0.0018340969638666138\n",
      "Training Loss: 0.0018141059833578766\n",
      "Validation Loss: 0.001843261446972545\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.001626849044987466\n",
      "Training Loss: 0.0017629657861834857\n",
      "Training Loss: 0.0017438197685987688\n",
      "Validation Loss: 0.0017893703067753716\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.0015667524750460871\n",
      "Training Loss: 0.001694921422022162\n",
      "Training Loss: 0.0016761445946758614\n",
      "Validation Loss: 0.001735022047428634\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.0015086304540454876\n",
      "Training Loss: 0.001629271177953342\n",
      "Training Loss: 0.0016104471015569288\n",
      "Validation Loss: 0.00167951034463738\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.001451893479388673\n",
      "Training Loss: 0.0015655200949549909\n",
      "Training Loss: 0.0015462177897279617\n",
      "Validation Loss: 0.0016227170605581317\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.0013961035826650914\n",
      "Training Loss: 0.001503250190653489\n",
      "Training Loss: 0.001482986784831155\n",
      "Validation Loss: 0.0015647098902100173\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.0013409024814609437\n",
      "Training Loss: 0.0014420735195744784\n",
      "Training Loss: 0.0014203300245571881\n",
      "Validation Loss: 0.0015055524213749686\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.0012860021778033114\n",
      "Training Loss: 0.0013816442131064832\n",
      "Training Loss: 0.00135791028602398\n",
      "Validation Loss: 0.0014452977731216992\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.0012312087677128147\n",
      "Training Loss: 0.0013216944053419865\n",
      "Training Loss: 0.0012955210909422021\n",
      "Validation Loss: 0.0013840243120918447\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.0011764381366083398\n",
      "Training Loss: 0.0012620661754044705\n",
      "Training Loss: 0.001233110866160132\n",
      "Validation Loss: 0.001321886533026301\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.00112171739252517\n",
      "Training Loss: 0.0012027195743576158\n",
      "Training Loss: 0.0011707767691405025\n",
      "Validation Loss: 0.001259124309695692\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0010671729664318264\n",
      "Training Loss: 0.0011437290112371558\n",
      "Training Loss: 0.0011087403268902562\n",
      "Validation Loss: 0.0011960752520924747\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.001013002875406528\n",
      "Training Loss: 0.0010852574539603665\n",
      "Training Loss: 0.0010473054001340642\n",
      "Validation Loss: 0.0011331453596325487\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.0009594518803351093\n",
      "Training Loss: 0.0010275352145981743\n",
      "Training Loss: 0.0009868263806129107\n",
      "Validation Loss: 0.0010707807817739654\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.0009067877739289543\n",
      "Training Loss: 0.0009708358874922851\n",
      "Training Loss: 0.0009276732502621599\n",
      "Validation Loss: 0.0010094742032011259\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.0008552905338729033\n",
      "Training Loss: 0.000915469511601259\n",
      "Training Loss: 0.0008702182537672343\n",
      "Validation Loss: 0.0009497287806634938\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0008052467691595666\n",
      "Training Loss: 0.000861765777481196\n",
      "Training Loss: 0.0008148137483658501\n",
      "Validation Loss: 0.0008920727362575041\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0007569185172542348\n",
      "Training Loss: 0.0008100257310434245\n",
      "Training Loss: 0.0007617255290097092\n",
      "Validation Loss: 0.0008369231448780871\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0007104308760244748\n",
      "Training Loss: 0.0007603449773159809\n",
      "Training Loss: 0.0007109238042176003\n",
      "Validation Loss: 0.0007843402846457353\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.0006654368991803495\n",
      "Training Loss: 0.0007121334405383095\n",
      "Training Loss: 0.0006615509553375887\n",
      "Validation Loss: 0.0007332673104541386\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.0006202666203535045\n",
      "Training Loss: 0.0006628479195569525\n",
      "Training Loss: 0.0006106028002250241\n",
      "Validation Loss: 0.0006795470756341264\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0005698375305291847\n",
      "Training Loss: 0.000604883378255181\n",
      "Training Loss: 0.0005501638612622628\n",
      "Validation Loss: 0.0006116520093241838\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0005023002099596852\n",
      "Training Loss: 0.0005224511764390627\n",
      "Training Loss: 0.00046891077134205263\n",
      "Validation Loss: 0.0005164556986416214\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.00041365956516528965\n",
      "Training Loss: 0.00042468199055292646\n",
      "Training Loss: 0.0003933421698457096\n",
      "Validation Loss: 0.00043235314284644893\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.00035654284030897545\n",
      "Training Loss: 0.00036952390652004395\n",
      "Training Loss: 0.00035199323021515736\n",
      "Validation Loss: 0.0003794002529574438\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.0003282918510012678\n",
      "Training Loss: 0.00033838170442322733\n",
      "Training Loss: 0.0003232435081372387\n",
      "Validation Loss: 0.00034088044392345263\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.00030692669864947677\n",
      "Training Loss: 0.00031478409700866903\n",
      "Training Loss: 0.0003000250681725447\n",
      "Validation Loss: 0.0003114247526119992\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00028862818497145783\n",
      "Training Loss: 0.00029503114046747214\n",
      "Training Loss: 0.0002805026756323059\n",
      "Validation Loss: 0.00028865766641880084\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.00027267984278296354\n",
      "Training Loss: 0.0002781456807133509\n",
      "Training Loss: 0.0002640424691708176\n",
      "Validation Loss: 0.0002711212288344991\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0002588685978480498\n",
      "Training Loss: 0.0002637638372107176\n",
      "Training Loss: 0.0002502811703743646\n",
      "Validation Loss: 0.0002578225703269055\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0002470604317204561\n",
      "Training Loss: 0.00025163403995975386\n",
      "Training Loss: 0.00023887489776825533\n",
      "Validation Loss: 0.0002479762471839524\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.00023708763870672556\n",
      "Training Loss: 0.0002414743761801219\n",
      "Training Loss: 0.00022944425683817826\n",
      "Validation Loss: 0.0002408680239783239\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.00022871861492603784\n",
      "Training Loss: 0.0002329578242097341\n",
      "Training Loss: 0.00022159056825330481\n",
      "Validation Loss: 0.00023582451684162376\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00022167710660141892\n",
      "Training Loss: 0.00022574618284124882\n",
      "Training Loss: 0.00021493977616046322\n",
      "Validation Loss: 0.00023223424526232325\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.00021568000254774234\n",
      "Training Loss: 0.0002195296119862178\n",
      "Training Loss: 0.00020917458416079172\n",
      "Validation Loss: 0.00022958081995619046\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.00021047073552836083\n",
      "Training Loss: 0.00021405226085335015\n",
      "Training Loss: 0.00020404994713317138\n",
      "Validation Loss: 0.0002274616052070567\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.00020584055078870733\n",
      "Training Loss: 0.00020912054045766127\n",
      "Training Loss: 0.00019938856032240438\n",
      "Validation Loss: 0.0002255855975572865\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.00020163077306278865\n",
      "Training Loss: 0.00020459631859921502\n",
      "Training Loss: 0.00019506933051161468\n",
      "Validation Loss: 0.00022375698584286245\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.00019772757514147088\n",
      "Training Loss: 0.00020038514961925102\n",
      "Training Loss: 0.00019101252204563934\n",
      "Validation Loss: 0.00022185552623933913\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.00019405322662350956\n",
      "Training Loss: 0.0001964246888019261\n",
      "Training Loss: 0.00018716716815106338\n",
      "Validation Loss: 0.0002198166251249593\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.00019055697570365736\n",
      "Training Loss: 0.0001926741819079325\n",
      "Training Loss: 0.00018350272188399685\n",
      "Validation Loss: 0.0002176164250639058\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0001872067488693574\n",
      "Training Loss: 0.00018910896516899812\n",
      "Training Loss: 0.00018000171783569385\n",
      "Validation Loss: 0.00021525288950337954\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.00018398382884697638\n",
      "Training Loss: 0.00018571321312265353\n",
      "Training Loss: 0.0001766543481426197\n",
      "Validation Loss: 0.00021274524467086205\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.00018087917333104998\n",
      "Training Loss: 0.0001824784513155464\n",
      "Training Loss: 0.00017345695523545145\n",
      "Validation Loss: 0.0002101209781327536\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.00017788912018659175\n",
      "Training Loss: 0.00017939948142156937\n",
      "Training Loss: 0.00017040805105352774\n",
      "Validation Loss: 0.00020741142778594592\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0001750136552072945\n",
      "Training Loss: 0.00017647363523792592\n",
      "Training Loss: 0.00016750808868891908\n",
      "Validation Loss: 0.00020465205928436324\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.00017225492218130966\n",
      "Training Loss: 0.00017369862316627405\n",
      "Training Loss: 0.00016475721324241022\n",
      "Validation Loss: 0.00020187271366012283\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.00016961482557235286\n",
      "Training Loss: 0.00017107260933698853\n",
      "Training Loss: 0.00016215566320170182\n",
      "Validation Loss: 0.00019910532774814415\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.00016709629371689517\n",
      "Training Loss: 0.0001685936046851566\n",
      "Training Loss: 0.00015970218404618209\n",
      "Validation Loss: 0.00019637459894858715\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.00016470062828375376\n",
      "Training Loss: 0.00016625865249807247\n",
      "Training Loss: 0.00015739536594992388\n",
      "Validation Loss: 0.00019370459465769751\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00016242845724264042\n",
      "Training Loss: 0.00016406361502959043\n",
      "Training Loss: 0.00015523162968747784\n",
      "Validation Loss: 0.00019111296023766045\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00016027908233809285\n",
      "Training Loss: 0.0001620038698638382\n",
      "Training Loss: 0.00015320711207095884\n",
      "Validation Loss: 0.00018861554572855783\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00015825083110030392\n",
      "Training Loss: 0.0001600739908099058\n",
      "Training Loss: 0.0001513159652313334\n",
      "Validation Loss: 0.00018622058963996954\n",
      "Validation Accuracy: 0.22823033707865167\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00015634044592843565\n",
      "Training Loss: 0.0001582670309653622\n",
      "Training Loss: 0.0001495519349555252\n",
      "Validation Loss: 0.0001839369306457883\n",
      "Validation Accuracy: 0.22823033707865167\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.0001545435856496624\n",
      "Training Loss: 0.0001565764450606366\n",
      "Training Loss: 0.00014790800314585796\n",
      "Validation Loss: 0.0001817658553437887\n",
      "Validation Accuracy: 0.22823033707865167\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0001528554666947457\n",
      "Training Loss: 0.00015499491748414585\n",
      "Training Loss: 0.00014637637686973904\n",
      "Validation Loss: 0.0001797090379808692\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.00015127035663681454\n",
      "Training Loss: 0.00015351433165051277\n",
      "Training Loss: 0.00014494946159175016\n",
      "Validation Loss: 0.00017776600296283402\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.00014978237075411016\n",
      "Training Loss: 0.00015212761261864215\n",
      "Training Loss: 0.00014361875842951123\n",
      "Validation Loss: 0.00017593140282872228\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.00014838494651485236\n",
      "Training Loss: 0.00015082674715813482\n",
      "Training Loss: 0.00014237673367460956\n",
      "Validation Loss: 0.00017420325026107727\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.00014707176816955325\n",
      "Training Loss: 0.00014960447053454117\n",
      "Training Loss: 0.00014121546291789855\n",
      "Validation Loss: 0.0001725741168954985\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.00014583636741008377\n",
      "Training Loss: 0.00014845394252915867\n",
      "Training Loss: 0.0001401274301679223\n",
      "Validation Loss: 0.0001710389405662283\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.00014467271713328956\n",
      "Training Loss: 0.00014736819724930683\n",
      "Training Loss: 0.0001391056232023402\n",
      "Validation Loss: 0.00016959120796272646\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.00014357486936205532\n",
      "Training Loss: 0.0001463411188342434\n",
      "Training Loss: 0.0001381436933115765\n",
      "Validation Loss: 0.00016822352282882123\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.0001425369983917335\n",
      "Training Loss: 0.0001453672338357137\n",
      "Training Loss: 0.00013723528993068613\n",
      "Validation Loss: 0.00016693058939146408\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.00014155357136587553\n",
      "Training Loss: 0.00014444084643400857\n",
      "Training Loss: 0.00013637507572639151\n",
      "Validation Loss: 0.00016570437139634624\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00014062002888749704\n",
      "Training Loss: 0.00014355735278513747\n",
      "Training Loss: 0.00013555782214098145\n",
      "Validation Loss: 0.00016454163750076372\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.00013973151421851072\n",
      "Training Loss: 0.0001427122178938589\n",
      "Training Loss: 0.00013477919756041957\n",
      "Validation Loss: 0.00016343392032948113\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.00013888388634768488\n",
      "Training Loss: 0.00014190164600222488\n",
      "Training Loss: 0.00013403506254690002\n",
      "Validation Loss: 0.00016237697716124796\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.00013807334136799909\n",
      "Training Loss: 0.00014112231643593988\n",
      "Training Loss: 0.0001333217497995065\n",
      "Validation Loss: 0.00016136852634977158\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.00013729666468861979\n",
      "Training Loss: 0.00014037083132279805\n",
      "Training Loss: 0.00013263588463814812\n",
      "Validation Loss: 0.00016040080289498332\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.0001365503720808192\n",
      "Training Loss: 0.00013964483423478669\n",
      "Training Loss: 0.00013197507365475758\n",
      "Validation Loss: 0.0001594720042192605\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.0001358319253540685\n",
      "Training Loss: 0.0001389415457379073\n",
      "Training Loss: 0.0001313364806082973\n",
      "Validation Loss: 0.00015857764301881105\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.00013513846386558725\n",
      "Training Loss: 0.00013825918826114501\n",
      "Training Loss: 0.00013071814209979492\n",
      "Validation Loss: 0.0001577141225793179\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.00013446816239593318\n",
      "Training Loss: 0.00013759547162408125\n",
      "Training Loss: 0.00013011787303184974\n",
      "Validation Loss: 0.00015687978148899888\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.00013381878030486406\n",
      "Training Loss: 0.00013694896066226647\n",
      "Training Loss: 0.00012953437630130793\n",
      "Validation Loss: 0.0001560724481402012\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00013318846272341033\n",
      "Training Loss: 0.0001363184220281255\n",
      "Training Loss: 0.00012896598364022793\n",
      "Validation Loss: 0.0001552885496085366\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.0001325758468829008\n",
      "Training Loss: 0.00013570216725383944\n",
      "Training Loss: 0.00012841121997553273\n",
      "Validation Loss: 0.00015452507348404806\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.00013197916175158752\n",
      "Training Loss: 0.00013509941247320966\n",
      "Training Loss: 0.00012786932917151716\n",
      "Validation Loss: 0.0001537835561178868\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0001313975680841395\n",
      "Training Loss: 0.0001345088754533208\n",
      "Training Loss: 0.00012733902849504375\n",
      "Validation Loss: 0.0001530594124304709\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.00013082959941129957\n",
      "Training Loss: 0.0001339301037842233\n",
      "Training Loss: 0.00012681958864050103\n",
      "Validation Loss: 0.00015235293838568293\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.00013027435797994258\n",
      "Training Loss: 0.00013336188570065133\n",
      "Training Loss: 0.00012631023902940798\n",
      "Validation Loss: 0.00015166137882771907\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.00012973093301297923\n",
      "Training Loss: 0.00013280402204145502\n",
      "Training Loss: 0.00012581024131577578\n",
      "Validation Loss: 0.00015098641370570292\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.00012919877577587612\n",
      "Training Loss: 0.0001322555258502689\n",
      "Training Loss: 0.00012531891183243716\n",
      "Validation Loss: 0.00015032312770150446\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.00012867672825450427\n",
      "Training Loss: 0.00013171583634175477\n",
      "Training Loss: 0.00012483621510909871\n",
      "Validation Loss: 0.000149674532100986\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.00012816451641811\n",
      "Training Loss: 0.00013118463345563214\n",
      "Training Loss: 0.00012436102666470107\n",
      "Validation Loss: 0.00014903692881693787\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.00012766121744789416\n",
      "Training Loss: 0.00013066159106529086\n",
      "Training Loss: 0.00012389330973746837\n",
      "Validation Loss: 0.00014841038697095734\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0001271666081811418\n",
      "Training Loss: 0.00013014623353228673\n",
      "Training Loss: 0.00012343263970251427\n",
      "Validation Loss: 0.00014779443417762777\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.00012668018322983698\n",
      "Training Loss: 0.00012963827450221288\n",
      "Training Loss: 0.00012297852972551482\n",
      "Validation Loss: 0.00014718940735875048\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.00012620132912161353\n",
      "Training Loss: 0.00012913723272504286\n",
      "Training Loss: 0.00012253088574652792\n",
      "Validation Loss: 0.0001465933332787081\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.00012572977924719454\n",
      "Training Loss: 0.0001286426651495276\n",
      "Training Loss: 0.00012208909509354272\n",
      "Validation Loss: 0.0001460062208893811\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.00012526523285487202\n",
      "Training Loss: 0.0001281548556562484\n",
      "Training Loss: 0.00012165329162598937\n",
      "Validation Loss: 0.00014542748578421882\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.00012480760557991743\n",
      "Training Loss: 0.0001276732095357147\n",
      "Training Loss: 0.00012122294419896207\n",
      "Validation Loss: 0.00014485616524775088\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.00012435577074938918\n",
      "Training Loss: 0.00012719764821667924\n",
      "Training Loss: 0.00012079798723789281\n",
      "Validation Loss: 0.00014429296216609617\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.00012391024570206354\n",
      "Training Loss: 0.00012672765529714525\n",
      "Training Loss: 0.00012037804124702234\n",
      "Validation Loss: 0.00014373714248505892\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.00012347059651801829\n",
      "Training Loss: 0.0001262633624173759\n",
      "Training Loss: 0.00011996313673080294\n",
      "Validation Loss: 0.00014318854627663825\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.00012303644242820156\n",
      "Training Loss: 0.00012580464070197195\n",
      "Training Loss: 0.00011955301961279474\n",
      "Validation Loss: 0.00014264649622275735\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.0001226078051058721\n",
      "Training Loss: 0.0001253509735397529\n",
      "Training Loss: 0.00011914752261873219\n",
      "Validation Loss: 0.00014211095007789828\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.00012218431639666961\n",
      "Training Loss: 0.00012490261705352167\n",
      "Training Loss: 0.00011874635425556335\n",
      "Validation Loss: 0.0001415809844903389\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.00012176590155831945\n",
      "Training Loss: 0.0001244590670830803\n",
      "Training Loss: 0.0001183496457633737\n",
      "Validation Loss: 0.00014105657107158162\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.00012135219198171399\n",
      "Training Loss: 0.00012402035016748414\n",
      "Training Loss: 0.0001179573039553361\n",
      "Validation Loss: 0.00014053940307594224\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00012094326895748963\n",
      "Training Loss: 0.00012358630781818646\n",
      "Training Loss: 0.00011756880008761073\n",
      "Validation Loss: 0.00014002605728799178\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.00012053890089191554\n",
      "Training Loss: 0.00012315691164985765\n",
      "Training Loss: 0.00011718434683643863\n",
      "Validation Loss: 0.00013951918250374926\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.00012013899995508837\n",
      "Training Loss: 0.00012273193446162622\n",
      "Training Loss: 0.00011680361663820804\n",
      "Validation Loss: 0.00013901631601594304\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00011974342091889412\n",
      "Training Loss: 0.0001223115492211946\n",
      "Training Loss: 0.00011642665300314548\n",
      "Validation Loss: 0.00013851813342585098\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.0001193518946365657\n",
      "Training Loss: 0.00012189512193799601\n",
      "Training Loss: 0.00011605327103097806\n",
      "Validation Loss: 0.00013802532752317878\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.00011896458593582793\n",
      "Training Loss: 0.00012148304896072659\n",
      "Training Loss: 0.00011568329966394231\n",
      "Validation Loss: 0.00013753598389311253\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00011858127555569809\n",
      "Training Loss: 0.00012107487972571107\n",
      "Training Loss: 0.00011531701466083177\n",
      "Validation Loss: 0.0001370515174437131\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.00011820162529147638\n",
      "Training Loss: 0.00012067086165188811\n",
      "Training Loss: 0.00011495421660583815\n",
      "Validation Loss: 0.00013657178367773237\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.00011782600575315882\n",
      "Training Loss: 0.00012027064992253144\n",
      "Training Loss: 0.00011459437051598797\n",
      "Validation Loss: 0.00013609617096509293\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.00011745410699404602\n",
      "Training Loss: 0.0001198743109125644\n",
      "Training Loss: 0.00011423767913584016\n",
      "Validation Loss: 0.00013562399395555127\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.00011708566508787043\n",
      "Training Loss: 0.00011948160206884495\n",
      "Training Loss: 0.00011388446226192173\n",
      "Validation Loss: 0.0001351560804527907\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00011672087814986299\n",
      "Training Loss: 0.0001190926118488278\n",
      "Training Loss: 0.0001135340672772145\n",
      "Validation Loss: 0.0001346913216499222\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.0001163595918842475\n",
      "Training Loss: 0.00011870709377035383\n",
      "Training Loss: 0.00011318661539917229\n",
      "Validation Loss: 0.00013423072536926576\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.00011600184444432671\n",
      "Training Loss: 0.00011832515027890622\n",
      "Training Loss: 0.00011284213309409097\n",
      "Validation Loss: 0.00013377377802222898\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00011564721153263235\n",
      "Training Loss: 0.00011794685126915284\n",
      "Training Loss: 0.00011250042076426326\n",
      "Validation Loss: 0.00013331946344775398\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.00011529597765729704\n",
      "Training Loss: 0.00011757169500924647\n",
      "Training Loss: 0.00011216144610443735\n",
      "Validation Loss: 0.00013286876704758348\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.00011494807637063787\n",
      "Training Loss: 0.00011719988448930962\n",
      "Training Loss: 0.00011182522463059286\n",
      "Validation Loss: 0.00013242199684406912\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00011460326798442111\n",
      "Training Loss: 0.00011683128827826295\n",
      "Training Loss: 0.00011149180912980228\n",
      "Validation Loss: 0.00013197709341419682\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00011426147831116396\n",
      "Training Loss: 0.00011646609455965517\n",
      "Training Loss: 0.00011116102878077072\n",
      "Validation Loss: 0.00013153690343550693\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.0001139229685668397\n",
      "Training Loss: 0.00011610389208726701\n",
      "Training Loss: 0.00011083268464062712\n",
      "Validation Loss: 0.00013109913465434169\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.00011358731457221438\n",
      "Training Loss: 0.00011574497086257906\n",
      "Training Loss: 0.00011050691640775768\n",
      "Validation Loss: 0.00013066410007994502\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.00011325478206344997\n",
      "Training Loss: 0.00011538904725057364\n",
      "Training Loss: 0.00011018351635357249\n",
      "Validation Loss: 0.00013023263967665974\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.00011292524915916146\n",
      "Training Loss: 0.00011503612959131714\n",
      "Training Loss: 0.00010986264449456939\n",
      "Validation Loss: 0.00012980394254309737\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.00011259847927249212\n",
      "Training Loss: 0.00011468609577605093\n",
      "Training Loss: 0.00010954428544209804\n",
      "Validation Loss: 0.00012937804448677964\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.00011227465287447558\n",
      "Training Loss: 0.00011433899243456835\n",
      "Training Loss: 0.00010922799659965677\n",
      "Validation Loss: 0.00012895455240161458\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.00011195369896995545\n",
      "Training Loss: 0.00011399460070606438\n",
      "Training Loss: 0.00010891418201936176\n",
      "Validation Loss: 0.0001285343960977586\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.00011163540088091395\n",
      "Training Loss: 0.00011365326841769274\n",
      "Training Loss: 0.0001086026573102572\n",
      "Validation Loss: 0.00012811667654480854\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.00011131996118820098\n",
      "Training Loss: 0.00011331452796184749\n",
      "Training Loss: 0.0001082933397628949\n",
      "Validation Loss: 0.0001277013922712224\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.00011100713016276131\n",
      "Training Loss: 0.00011297852173811407\n",
      "Training Loss: 0.00010798622233778587\n",
      "Validation Loss: 0.00012728842579889807\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.00011069706362832222\n",
      "Training Loss: 0.00011264525380283886\n",
      "Training Loss: 0.00010768126765469787\n",
      "Validation Loss: 0.0001268785943241536\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.00011038960927180597\n",
      "Training Loss: 0.00011231471840801532\n",
      "Training Loss: 0.00010737855858678813\n",
      "Validation Loss: 0.00012647141883963343\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.00011008468188265396\n",
      "Training Loss: 0.00011198667940334417\n",
      "Training Loss: 0.00010707774663387681\n",
      "Validation Loss: 0.00012606678585266155\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.00010978254926158115\n",
      "Training Loss: 0.00011166120020789094\n",
      "Training Loss: 0.00010677906413548044\n",
      "Validation Loss: 0.0001256650017914192\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.00010948277121315186\n",
      "Training Loss: 0.00011133848399367707\n",
      "Training Loss: 0.00010648237963323482\n",
      "Validation Loss: 0.00012526432692538947\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00010918553288320254\n",
      "Training Loss: 0.0001110179565512226\n",
      "Training Loss: 0.00010618777108902578\n",
      "Validation Loss: 0.00012486542994777797\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.00010889064197726839\n",
      "Training Loss: 0.00011070007514717872\n",
      "Training Loss: 0.00010589510309728212\n",
      "Validation Loss: 0.00012447048369252248\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.00010859833337235613\n",
      "Training Loss: 0.00011038444058613095\n",
      "Training Loss: 0.00010560435992374551\n",
      "Validation Loss: 0.00012407792579079193\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.00010830847379111219\n",
      "Training Loss: 0.00011007125641299354\n",
      "Training Loss: 0.00010531541089221719\n",
      "Validation Loss: 0.00012368612906459348\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.0001080207954692014\n",
      "Training Loss: 0.00010976042331094505\n",
      "Training Loss: 0.00010502863353394786\n",
      "Validation Loss: 0.00012329739570049334\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.000107735569417855\n",
      "Training Loss: 0.00010945201911454206\n",
      "Training Loss: 0.00010474357210114248\n",
      "Validation Loss: 0.00012291092961471333\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.00010745268512437179\n",
      "Training Loss: 0.00010914581922406796\n",
      "Training Loss: 0.00010446027392390533\n",
      "Validation Loss: 0.00012252718545241397\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.00010717207346715441\n",
      "Training Loss: 0.00010884182773224894\n",
      "Training Loss: 0.0001041789167902607\n",
      "Validation Loss: 0.00012214429780930475\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.00010689368159546575\n",
      "Training Loss: 0.00010854003167878545\n",
      "Training Loss: 0.00010389936749561458\n",
      "Validation Loss: 0.00012176406356078322\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.00010661750695362571\n",
      "Training Loss: 0.00010824037619386217\n",
      "Training Loss: 0.00010362152132074698\n",
      "Validation Loss: 0.00012138499679688752\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.00010634338039380964\n",
      "Training Loss: 0.00010794300052111793\n",
      "Training Loss: 0.00010334545460864319\n",
      "Validation Loss: 0.00012100909395045941\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.00010607149213683443\n",
      "Training Loss: 0.00010764749007648789\n",
      "Training Loss: 0.00010307111042493489\n",
      "Validation Loss: 0.00012063510564147374\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.00010580179839053016\n",
      "Training Loss: 0.00010735425355960615\n",
      "Training Loss: 0.00010279844251272152\n",
      "Validation Loss: 0.00012026243297277892\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.00010553402416917379\n",
      "Training Loss: 0.0001070628382058203\n",
      "Training Loss: 0.00010252729824060225\n",
      "Validation Loss: 0.00011989279844558777\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.00010526837934776267\n",
      "Training Loss: 0.0001067735571632511\n",
      "Training Loss: 0.00010225805002846756\n",
      "Validation Loss: 0.00011952434035490287\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.00010500479924303363\n",
      "Training Loss: 0.00010648625821886526\n",
      "Training Loss: 0.000101990251805546\n",
      "Validation Loss: 0.00011915761659908098\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.00010474307753611357\n",
      "Training Loss: 0.00010620064965223719\n",
      "Training Loss: 0.00010172391495871124\n",
      "Validation Loss: 0.00011879281569900138\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.00010448343548887351\n",
      "Training Loss: 0.00010591698753160018\n",
      "Training Loss: 0.00010145930991711793\n",
      "Validation Loss: 0.0001184303941596901\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.00010422551855299389\n",
      "Training Loss: 0.00010563534430730215\n",
      "Training Loss: 0.00010119621721969451\n",
      "Validation Loss: 0.0001180693191221753\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.00010396965473773889\n",
      "Training Loss: 0.00010535547963627323\n",
      "Training Loss: 0.00010093455499372794\n",
      "Validation Loss: 0.00011770967023366712\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.00010371541683525721\n",
      "Training Loss: 0.00010507723865885055\n",
      "Training Loss: 0.00010067438844998833\n",
      "Validation Loss: 0.00011735157040879784\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.00010346306294195529\n",
      "Training Loss: 0.00010480076617568557\n",
      "Training Loss: 0.00010041572935733712\n",
      "Validation Loss: 0.00011699604929776031\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.00010321243763428356\n",
      "Training Loss: 0.00010452586840983713\n",
      "Training Loss: 0.00010015831181590328\n",
      "Validation Loss: 0.0001166420763715241\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.00010296350135831745\n",
      "Training Loss: 0.00010425275920169952\n",
      "Training Loss: 9.990234446377144e-05\n",
      "Validation Loss: 0.00011628933651572266\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.00010271632761032378\n",
      "Training Loss: 0.0001039811266491597\n",
      "Training Loss: 9.96476530781365e-05\n",
      "Validation Loss: 0.000115937550873149\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.00010247062072266999\n",
      "Training Loss: 0.00010371110332926037\n",
      "Training Loss: 9.939444609699421e-05\n",
      "Validation Loss: 0.00011558786860568739\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.00010222656289442966\n",
      "Training Loss: 0.00010344266851461726\n",
      "Training Loss: 9.914231097354786e-05\n",
      "Validation Loss: 0.00011523981215708609\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.00010198404305810982\n",
      "Training Loss: 0.00010317558449060015\n",
      "Training Loss: 9.88915671587165e-05\n",
      "Validation Loss: 0.00011489266200462654\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.00010174306935368804\n",
      "Training Loss: 0.00010290989458553667\n",
      "Training Loss: 9.864197798378882e-05\n",
      "Validation Loss: 0.00011454780940950738\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.00010150355105906784\n",
      "Training Loss: 0.00010264560795803845\n",
      "Training Loss: 9.839359008765314e-05\n",
      "Validation Loss: 0.00011420357486202376\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.00010126540243618365\n",
      "Training Loss: 0.00010238272280730598\n",
      "Training Loss: 9.814629795073415e-05\n",
      "Validation Loss: 0.00011386156212834424\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.00010102873307914706\n",
      "Training Loss: 0.00010212094806774985\n",
      "Training Loss: 9.790012445591856e-05\n",
      "Validation Loss: 0.00011352048655231208\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.00010079340750053234\n",
      "Training Loss: 0.0001018605629542435\n",
      "Training Loss: 9.765497514308664e-05\n",
      "Validation Loss: 0.00011317992003783689\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.00010055924124571902\n",
      "Training Loss: 0.00010160135142541549\n",
      "Training Loss: 9.741090727402479e-05\n",
      "Validation Loss: 0.00011284194118907385\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.00010032646283434587\n",
      "Training Loss: 0.00010134322388694273\n",
      "Training Loss: 9.716777449284564e-05\n",
      "Validation Loss: 0.00011250362369149147\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.00010009469621763856\n",
      "Training Loss: 0.00010108617097102979\n",
      "Training Loss: 9.692575482404208e-05\n",
      "Validation Loss: 0.00011216754114940067\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 9.986424372982584e-05\n",
      "Training Loss: 0.00010083018170007562\n",
      "Training Loss: 9.668455031714984e-05\n",
      "Validation Loss: 0.0001118312606849123\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 9.963485200387368e-05\n",
      "Training Loss: 0.00010057508139652782\n",
      "Training Loss: 9.644426820159423e-05\n",
      "Validation Loss: 0.00011149686209780943\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 9.940643054505927e-05\n",
      "Training Loss: 0.00010032097125986183\n",
      "Training Loss: 9.620475873816758e-05\n",
      "Validation Loss: 0.00011116436204512989\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 9.917923955981677e-05\n",
      "Training Loss: 0.00010006786419580749\n",
      "Training Loss: 9.596605723345419e-05\n",
      "Validation Loss: 0.00011083091562744648\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 9.895283913465392e-05\n",
      "Training Loss: 9.981545576920325e-05\n",
      "Training Loss: 9.572799626766937e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [14:03<21:07, 211.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.00011049935816894443\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.12568531858734786\n",
      "Training Loss: 0.08734140557702631\n",
      "Training Loss: 0.07911948510445654\n",
      "Validation Loss: 0.07065506354811486\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06639269927516579\n",
      "Training Loss: 0.06073510247282684\n",
      "Training Loss: 0.06288514274172485\n",
      "Validation Loss: 0.06189675026395348\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.058769625807181\n",
      "Training Loss: 0.054704523235559466\n",
      "Training Loss: 0.0562188568059355\n",
      "Validation Loss: 0.0549037792102507\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.05176056281663478\n",
      "Training Loss: 0.047342397794127467\n",
      "Training Loss: 0.047668782016262415\n",
      "Validation Loss: 0.04481573610074734\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.04134097854141146\n",
      "Training Loss: 0.03595398089382797\n",
      "Training Loss: 0.03464115606620908\n",
      "Validation Loss: 0.031361988366298964\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.02836581816431135\n",
      "Training Loss: 0.02478266018733848\n",
      "Training Loss: 0.024449684666469695\n",
      "Validation Loss: 0.02366506241999692\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.020794655963982223\n",
      "Training Loss: 0.018604340413585305\n",
      "Training Loss: 0.018513178955763578\n",
      "Validation Loss: 0.018601717556868628\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.015820476291119122\n",
      "Training Loss: 0.014333046462852508\n",
      "Training Loss: 0.014296073003206402\n",
      "Validation Loss: 0.01484286865439224\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.012241482845274732\n",
      "Training Loss: 0.011246010169852525\n",
      "Training Loss: 0.011222287963610142\n",
      "Validation Loss: 0.012007294723690812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.009621427938982378\n",
      "Training Loss: 0.008969233306124806\n",
      "Training Loss: 0.008940487373620272\n",
      "Validation Loss: 0.009819758269841668\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.007668359646922909\n",
      "Training Loss: 0.007259262627921998\n",
      "Training Loss: 0.007226534800138324\n",
      "Validation Loss: 0.008095711094243556\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.0062038110979483465\n",
      "Training Loss: 0.00597698382800445\n",
      "Training Loss: 0.0059498730581253765\n",
      "Validation Loss: 0.006716261128633378\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.005118060172098922\n",
      "Training Loss: 0.005032587100868113\n",
      "Training Loss: 0.005015045495238155\n",
      "Validation Loss: 0.005600091790011299\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.0043201573271653615\n",
      "Training Loss: 0.004340729181130883\n",
      "Training Loss: 0.004328233286505565\n",
      "Validation Loss: 0.004694479727578674\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.003726336125400849\n",
      "Training Loss: 0.00382180581393186\n",
      "Training Loss: 0.0038090224919142203\n",
      "Validation Loss: 0.0039720760456945624\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.003273691650101682\n",
      "Training Loss: 0.003418967857141979\n",
      "Training Loss: 0.003403126479825005\n",
      "Validation Loss: 0.003415028182031022\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.002920664945850149\n",
      "Training Loss: 0.003096512735646684\n",
      "Training Loss: 0.0030762077975668945\n",
      "Validation Loss: 0.0030024728614816004\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.0026390073310176374\n",
      "Training Loss: 0.0028315733233466745\n",
      "Training Loss: 0.0028058415051782503\n",
      "Validation Loss: 0.002709250724181177\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.0024096453622041735\n",
      "Training Loss: 0.00260988282301696\n",
      "Training Loss: 0.0025784106287756003\n",
      "Validation Loss: 0.002507026328867448\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.0022211814834736288\n",
      "Training Loss: 0.002423556665016804\n",
      "Training Loss: 0.0023868693807162344\n",
      "Validation Loss: 0.0023655879727873448\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.0020662084173818586\n",
      "Training Loss: 0.002266948048491031\n",
      "Training Loss: 0.002225944369856734\n",
      "Validation Loss: 0.002258929663685705\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0019374657405569452\n",
      "Training Loss: 0.002133838474546792\n",
      "Training Loss: 0.002089514987601433\n",
      "Validation Loss: 0.0021703894085936207\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.001828140871875803\n",
      "Training Loss: 0.0020184528511890676\n",
      "Training Loss: 0.0019717817526543513\n",
      "Validation Loss: 0.002091157192225683\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.001733112889705808\n",
      "Training Loss: 0.001916490487928968\n",
      "Training Loss: 0.0018682883042492903\n",
      "Validation Loss: 0.0020168144138153184\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0016488707565440563\n",
      "Training Loss: 0.001824958043580409\n",
      "Training Loss: 0.0017758558079367503\n",
      "Validation Loss: 0.0019452377043277277\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.0015730129375879187\n",
      "Training Loss: 0.0017417544974887278\n",
      "Training Loss: 0.0016922353344853037\n",
      "Validation Loss: 0.001875505207454409\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.001503869367406878\n",
      "Training Loss: 0.0016653826093534008\n",
      "Training Loss: 0.0016158256595372221\n",
      "Validation Loss: 0.001807377454098785\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.001440267668585875\n",
      "Training Loss: 0.0015947752652573398\n",
      "Training Loss: 0.0015454884250357282\n",
      "Validation Loss: 0.001740983683580606\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.0013813911181568984\n",
      "Training Loss: 0.0015291898170835339\n",
      "Training Loss: 0.001480413615645375\n",
      "Validation Loss: 0.0016766371237496018\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.0013266864374600119\n",
      "Training Loss: 0.0014681396637752186\n",
      "Training Loss: 0.0014200401736889036\n",
      "Validation Loss: 0.0016147439782389994\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.001275787515078264\n",
      "Training Loss: 0.0014113253904361044\n",
      "Training Loss: 0.0013639739394420759\n",
      "Validation Loss: 0.0015557244331105037\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.0012284500684108933\n",
      "Training Loss: 0.0013585647073341533\n",
      "Training Loss: 0.0013119132208521478\n",
      "Validation Loss: 0.0014999666621743732\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.0011844859820121202\n",
      "Training Loss: 0.0013097158887831028\n",
      "Training Loss: 0.0012635870368103497\n",
      "Validation Loss: 0.0014477224202677503\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.0011437042238685536\n",
      "Training Loss: 0.001264596500841435\n",
      "Training Loss: 0.0012187016969255637\n",
      "Validation Loss: 0.0013990747221429005\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.001105871015242883\n",
      "Training Loss: 0.001222930867370451\n",
      "Training Loss: 0.001176919045537943\n",
      "Validation Loss: 0.0013538858061263946\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.00107069545880222\n",
      "Training Loss: 0.0011843329113617074\n",
      "Training Loss: 0.0011378665591473692\n",
      "Validation Loss: 0.0013117956382149222\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0010378424842929234\n",
      "Training Loss: 0.0011483355648670113\n",
      "Training Loss: 0.001101158310775645\n",
      "Validation Loss: 0.0012723206167352605\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.0010069495295465458\n",
      "Training Loss: 0.0011144321711617521\n",
      "Training Loss: 0.0010664069684571586\n",
      "Validation Loss: 0.001234884270672226\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.0009776476014121726\n",
      "Training Loss: 0.001082129747082945\n",
      "Training Loss: 0.001033239227690501\n",
      "Validation Loss: 0.0011989263164630328\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.0009495785361104936\n",
      "Training Loss: 0.0010509800934232771\n",
      "Training Loss: 0.0010013043700018898\n",
      "Validation Loss: 0.0011639388353665873\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.0009224130023721955\n",
      "Training Loss: 0.001020605720259482\n",
      "Training Loss: 0.000970284081704449\n",
      "Validation Loss: 0.0011295061687653716\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0008958679732313612\n",
      "Training Loss: 0.0009907065231527668\n",
      "Training Loss: 0.0009399067380581982\n",
      "Validation Loss: 0.0010953113255862755\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0008697140228468925\n",
      "Training Loss: 0.0009610599739244208\n",
      "Training Loss: 0.0009099536521534901\n",
      "Validation Loss: 0.0010611275703125644\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0008437781446809823\n",
      "Training Loss: 0.0009315122374391649\n",
      "Training Loss: 0.0008802593799191527\n",
      "Validation Loss: 0.0010268042562856454\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.0008179413460493379\n",
      "Training Loss: 0.0009019662260834594\n",
      "Training Loss: 0.0008507081368588843\n",
      "Validation Loss: 0.0009922622325320026\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.0007921275281478302\n",
      "Training Loss: 0.0008723667544109048\n",
      "Training Loss: 0.0008212264819303527\n",
      "Validation Loss: 0.0009574559543365508\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0007662941932539979\n",
      "Training Loss: 0.0008426897900790209\n",
      "Training Loss: 0.000791777650010772\n",
      "Validation Loss: 0.0009223915760148986\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0007404336730724026\n",
      "Training Loss: 0.0008129460401687538\n",
      "Training Loss: 0.0007623624822008423\n",
      "Validation Loss: 0.0008871123950740092\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.0007145646764001867\n",
      "Training Loss: 0.0007831682778487448\n",
      "Training Loss: 0.0007330091299081687\n",
      "Validation Loss: 0.0008516914237047326\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0006887300674861763\n",
      "Training Loss: 0.0007534135203604819\n",
      "Training Loss: 0.0007037755557394121\n",
      "Validation Loss: 0.0008162309087321078\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.0006629970459471224\n",
      "Training Loss: 0.0007237651303148595\n",
      "Training Loss: 0.0006747498049662681\n",
      "Validation Loss: 0.0007808738683410554\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.000637458543133107\n",
      "Training Loss: 0.0006943316513206809\n",
      "Training Loss: 0.0006460443820833461\n",
      "Validation Loss: 0.0007457836073943612\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.0006122248882820714\n",
      "Training Loss: 0.0006652397370271501\n",
      "Training Loss: 0.0006177921481139492\n",
      "Validation Loss: 0.0007111580119098098\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.0005874249756379868\n",
      "Training Loss: 0.0006366364781570155\n",
      "Training Loss: 0.0005901456864376087\n",
      "Validation Loss: 0.0006772027632353691\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0005632035140661173\n",
      "Training Loss: 0.0006086865800534725\n",
      "Training Loss: 0.0005632703541778028\n",
      "Validation Loss: 0.0006441554051184284\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0005397148357951664\n",
      "Training Loss: 0.0005815624302340439\n",
      "Training Loss: 0.0005373401862743776\n",
      "Validation Loss: 0.000612257869771598\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0005171194668946554\n",
      "Training Loss: 0.0005554449579358334\n",
      "Training Loss: 0.0005125303002569126\n",
      "Validation Loss: 0.0005817383152839539\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.0004955750807857839\n",
      "Training Loss: 0.0005305102845886722\n",
      "Training Loss: 0.0004890070515102707\n",
      "Validation Loss: 0.0005528300620773755\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.0004752319350518519\n",
      "Training Loss: 0.0005069245358754415\n",
      "Training Loss: 0.00046692453368450513\n",
      "Validation Loss: 0.0005257328134422515\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.00045622333484061526\n",
      "Training Loss: 0.0004848336764189298\n",
      "Training Loss: 0.0004464112099958584\n",
      "Validation Loss: 0.0005006122662328717\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.00043865660656592807\n",
      "Training Loss: 0.00046435657299298327\n",
      "Training Loss: 0.0004275650445197243\n",
      "Validation Loss: 0.0004775920244989383\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.000422604531631805\n",
      "Training Loss: 0.00044557331904798047\n",
      "Training Loss: 0.0004104435928456951\n",
      "Validation Loss: 0.0004567418789806294\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.00040809749658365035\n",
      "Training Loss: 0.00042851878170040435\n",
      "Training Loss: 0.0003950546200212557\n",
      "Validation Loss: 0.0004380719820913869\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.00039511812134151113\n",
      "Training Loss: 0.00041317840048577637\n",
      "Training Loss: 0.00038135991992021445\n",
      "Validation Loss: 0.0004215310886077154\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.00038360574912076116\n",
      "Training Loss: 0.00039949074442120037\n",
      "Training Loss: 0.0003692730730836047\n",
      "Validation Loss: 0.0004070118019616315\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.0003734604147030041\n",
      "Training Loss: 0.00038735431749955753\n",
      "Training Loss: 0.0003586708662624005\n",
      "Validation Loss: 0.00039436289178288613\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.00036455336721701314\n",
      "Training Loss: 0.0003766354846447939\n",
      "Training Loss: 0.0003494017450429965\n",
      "Validation Loss: 0.00038339551500622915\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.00035673783710080895\n",
      "Training Loss: 0.00036718212631967615\n",
      "Training Loss: 0.00034130136264138856\n",
      "Validation Loss: 0.0003738996936672163\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0003498639062672737\n",
      "Training Loss: 0.00035883645879948746\n",
      "Training Loss: 0.000334204753562517\n",
      "Validation Loss: 0.00036566785127617777\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0003437867269531125\n",
      "Training Loss: 0.00035144513960403854\n",
      "Training Loss: 0.00032795612616610015\n",
      "Validation Loss: 0.0003584952949526003\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.00033837341630714945\n",
      "Training Loss: 0.00034486573858885097\n",
      "Training Loss: 0.00032241483189864083\n",
      "Validation Loss: 0.0003522012725778531\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.00033350832516589437\n",
      "Training Loss: 0.0003389720448103617\n",
      "Training Loss: 0.0003174593426956562\n",
      "Validation Loss: 0.00034662438007176255\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.00032909386933170024\n",
      "Training Loss: 0.00033365558036166473\n",
      "Training Loss: 0.00031298911424528343\n",
      "Validation Loss: 0.00034163117956002555\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.000325050838982861\n",
      "Training Loss: 0.0003288254472136032\n",
      "Training Loss: 0.0003089206009826739\n",
      "Validation Loss: 0.00033710994662879183\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.00032131453477632024\n",
      "Training Loss: 0.0003244070621440187\n",
      "Training Loss: 0.00030518683855916605\n",
      "Validation Loss: 0.0003329723818915022\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00031783507190993984\n",
      "Training Loss: 0.0003203386773384409\n",
      "Training Loss: 0.0003017348151479382\n",
      "Validation Loss: 0.0003291479310003146\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00031457235030757147\n",
      "Training Loss: 0.00031657034858653785\n",
      "Training Loss: 0.00029852091596694664\n",
      "Validation Loss: 0.0003255819762027509\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00031149526026638344\n",
      "Training Loss: 0.00031306049844715745\n",
      "Training Loss: 0.00029551160958362743\n",
      "Validation Loss: 0.0003222299216830517\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.0003085795138576941\n",
      "Training Loss: 0.00030977598504250637\n",
      "Training Loss: 0.00029267843819980044\n",
      "Validation Loss: 0.00031905892638462135\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.00030580495635149416\n",
      "Training Loss: 0.00030668907947983826\n",
      "Training Loss: 0.0002899988895660499\n",
      "Validation Loss: 0.00031604156342172847\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0003031561538409733\n",
      "Training Loss: 0.00030377582708752016\n",
      "Training Loss: 0.00028745459341735115\n",
      "Validation Loss: 0.0003131582335718735\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0003006198061120813\n",
      "Training Loss: 0.0003010169388653594\n",
      "Training Loss: 0.0002850298988778377\n",
      "Validation Loss: 0.0003103918539493585\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.0002981854736208334\n",
      "Training Loss: 0.0002983958981349133\n",
      "Training Loss: 0.0002827116727712564\n",
      "Validation Loss: 0.0003077291101648922\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.0002958440465226886\n",
      "Training Loss: 0.0002958977195157786\n",
      "Training Loss: 0.0002804890165134566\n",
      "Validation Loss: 0.00030515959604579193\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.0002935877816707944\n",
      "Training Loss: 0.00029351013825362315\n",
      "Training Loss: 0.00027835206674353685\n",
      "Validation Loss: 0.0003026755705862986\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.0002914094134393963\n",
      "Training Loss: 0.0002912223449311568\n",
      "Training Loss: 0.00027629289725155103\n",
      "Validation Loss: 0.0003002693598175662\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.00028930320429935817\n",
      "Training Loss: 0.0002890255438978784\n",
      "Training Loss: 0.0002743043201189721\n",
      "Validation Loss: 0.0002979347834267439\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.00028726430806273127\n",
      "Training Loss: 0.00028691087620245524\n",
      "Training Loss: 0.0002723800196690718\n",
      "Validation Loss: 0.0002956684841316591\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.0002852880169484706\n",
      "Training Loss: 0.0002848714117862983\n",
      "Training Loss: 0.0002705149860776146\n",
      "Validation Loss: 0.000293465832420337\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.00028337018400634404\n",
      "Training Loss: 0.000282900749516557\n",
      "Training Loss: 0.0002687039379088674\n",
      "Validation Loss: 0.00029132103810205876\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00028150691263363115\n",
      "Training Loss: 0.0002809936554331216\n",
      "Training Loss: 0.00026694355430663565\n",
      "Validation Loss: 0.0002892334531669589\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0002796948503146268\n",
      "Training Loss: 0.0002791448267089436\n",
      "Training Loss: 0.0002652291209597024\n",
      "Validation Loss: 0.0002872000348749649\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.0002779309085417481\n",
      "Training Loss: 0.0002773498907481553\n",
      "Training Loss: 0.0002635583660230623\n",
      "Validation Loss: 0.00028521742439060994\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.00027621256523161717\n",
      "Training Loss: 0.00027560561014979614\n",
      "Training Loss: 0.0002619278167185257\n",
      "Validation Loss: 0.0002832836199671506\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0002745368293926731\n",
      "Training Loss: 0.00027390737730456746\n",
      "Training Loss: 0.00026033520069177027\n",
      "Validation Loss: 0.00028139729345618916\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.00027290185542369724\n",
      "Training Loss: 0.0002722535733846598\n",
      "Training Loss: 0.0002587778737506596\n",
      "Validation Loss: 0.00027955602596970434\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.000271304987345502\n",
      "Training Loss: 0.00027064021429396235\n",
      "Training Loss: 0.00025725458814122246\n",
      "Validation Loss: 0.0002777588808712199\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.0002697443306624336\n",
      "Training Loss: 0.0002690657578932587\n",
      "Training Loss: 0.0002557627834903542\n",
      "Validation Loss: 0.0002760022687428144\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0002682188045127987\n",
      "Training Loss: 0.0002675272522537853\n",
      "Training Loss: 0.0002543009152577724\n",
      "Validation Loss: 0.0002742858575473885\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.0002667260887665179\n",
      "Training Loss: 0.00026602321322570787\n",
      "Training Loss: 0.0002528678704402409\n",
      "Validation Loss: 0.0002726085315291048\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0002652643155943224\n",
      "Training Loss: 0.00026455154486029644\n",
      "Training Loss: 0.00025146210871753285\n",
      "Validation Loss: 0.00027096886222945327\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.00026383275719126686\n",
      "Training Loss: 0.00026311086268833607\n",
      "Training Loss: 0.00025008209471707234\n",
      "Validation Loss: 0.0002693645386326158\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.00026242941883538153\n",
      "Training Loss: 0.0002616992492403369\n",
      "Training Loss: 0.0002487270118217566\n",
      "Validation Loss: 0.00026779573769151663\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0002610530784841103\n",
      "Training Loss: 0.000260315415871446\n",
      "Training Loss: 0.0002473957273105043\n",
      "Validation Loss: 0.00026625949988995807\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0002597029476874013\n",
      "Training Loss: 0.0002589583891676739\n",
      "Training Loss: 0.00024608708314190154\n",
      "Validation Loss: 0.0002647557186228982\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0002583772290654451\n",
      "Training Loss: 0.00025762628018128453\n",
      "Training Loss: 0.0002448004229881917\n",
      "Validation Loss: 0.00026328301812120544\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.00025707489651722427\n",
      "Training Loss: 0.0002563185164763126\n",
      "Training Loss: 0.00024353476939722896\n",
      "Validation Loss: 0.0002618394095550051\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0002557950585287472\n",
      "Training Loss: 0.0002550336975400569\n",
      "Training Loss: 0.00024228912996477448\n",
      "Validation Loss: 0.00026042459452828244\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.0002545365166133706\n",
      "Training Loss: 0.0002537710839169449\n",
      "Training Loss: 0.00024106283661240013\n",
      "Validation Loss: 0.0002590377793893914\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.0002532984346453304\n",
      "Training Loss: 0.0002525291255733464\n",
      "Training Loss: 0.00023985541836736956\n",
      "Validation Loss: 0.0002576767419872994\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.00025207968924405576\n",
      "Training Loss: 0.0002513076023024041\n",
      "Training Loss: 0.00023866571329563156\n",
      "Validation Loss: 0.00025634251281016066\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.00025087969543164946\n",
      "Training Loss: 0.0002501050630235113\n",
      "Training Loss: 0.00023749293133732864\n",
      "Validation Loss: 0.0002550315213248427\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.00024969712797428655\n",
      "Training Loss: 0.00024892107157938883\n",
      "Training Loss: 0.00023633671800780577\n",
      "Validation Loss: 0.00025374424999273704\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0002485313481065532\n",
      "Training Loss: 0.0002477543154964224\n",
      "Training Loss: 0.00023519606638728875\n",
      "Validation Loss: 0.00025247939735758277\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.00024738125900512385\n",
      "Training Loss: 0.00024660432660311926\n",
      "Training Loss: 0.000234071100858273\n",
      "Validation Loss: 0.000251235510476046\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.0002462467959867354\n",
      "Training Loss: 0.0002454702297472977\n",
      "Training Loss: 0.00023296024086448595\n",
      "Validation Loss: 0.00025001246913132166\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.00024512644457445274\n",
      "Training Loss: 0.00024435133040242365\n",
      "Training Loss: 0.00023186379057733576\n",
      "Validation Loss: 0.0002488102264384467\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.00024401963037234964\n",
      "Training Loss: 0.00024324663576408056\n",
      "Training Loss: 0.00023078053123754217\n",
      "Validation Loss: 0.000247626251713085\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.00024292589731885527\n",
      "Training Loss: 0.0002421560577568016\n",
      "Training Loss: 0.00022971011891058878\n",
      "Validation Loss: 0.0002464598796956288\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.00024184464680274687\n",
      "Training Loss: 0.00024107822733640205\n",
      "Training Loss: 0.0002286522291979054\n",
      "Validation Loss: 0.0002453110013693425\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.00024077480197775002\n",
      "Training Loss: 0.0002400129670786555\n",
      "Training Loss: 0.00022760588595701847\n",
      "Validation Loss: 0.00024417933697774326\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.0002397163071691466\n",
      "Training Loss: 0.00023895948801509802\n",
      "Training Loss: 0.00022657131881715032\n",
      "Validation Loss: 0.00024306410190281773\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.00023866822944000886\n",
      "Training Loss: 0.00023791743180481717\n",
      "Training Loss: 0.00022554720679181627\n",
      "Validation Loss: 0.00024196313101578545\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.00023763005583077757\n",
      "Training Loss: 0.00023688585239142412\n",
      "Training Loss: 0.00022453368466813118\n",
      "Validation Loss: 0.00024087737820286493\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.00023660148393446435\n",
      "Training Loss: 0.00023586426323163323\n",
      "Training Loss: 0.00022353018197463826\n",
      "Validation Loss: 0.0002398054059719133\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0002355814996644767\n",
      "Training Loss: 0.00023485262507165318\n",
      "Training Loss: 0.00022253623923461417\n",
      "Validation Loss: 0.0002387476370233671\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0002345701960484803\n",
      "Training Loss: 0.00023384990163322072\n",
      "Training Loss: 0.00022155136830406263\n",
      "Validation Loss: 0.00023770251671399064\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.0002335669531385065\n",
      "Training Loss: 0.00023285578285140219\n",
      "Training Loss: 0.00022057526854041498\n",
      "Validation Loss: 0.0002366703935948546\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00023257132136677682\n",
      "Training Loss: 0.00023186957154393894\n",
      "Training Loss: 0.00021960758953355254\n",
      "Validation Loss: 0.00023564898657149456\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.00023158288435752184\n",
      "Training Loss: 0.000230891272694862\n",
      "Training Loss: 0.0002186480594900786\n",
      "Validation Loss: 0.00023464054591774554\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.00023060110360802354\n",
      "Training Loss: 0.0002299204391965759\n",
      "Training Loss: 0.0002176959311691462\n",
      "Validation Loss: 0.00023364341034445377\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00022962568175444176\n",
      "Training Loss: 0.0002289564864258864\n",
      "Training Loss: 0.00021675138596037867\n",
      "Validation Loss: 0.00023265655762045069\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.00022865649023060542\n",
      "Training Loss: 0.00022799894108175068\n",
      "Training Loss: 0.00021581383058219216\n",
      "Validation Loss: 0.00023168069273697768\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.0002276927713137411\n",
      "Training Loss: 0.0002270474897159147\n",
      "Training Loss: 0.00021488296351890313\n",
      "Validation Loss: 0.00023071360560147133\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.00022673437942103192\n",
      "Training Loss: 0.00022610188152611955\n",
      "Training Loss: 0.0002139582509698812\n",
      "Validation Loss: 0.00022975739553208422\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.00022578133329716364\n",
      "Training Loss: 0.00022516166776767932\n",
      "Training Loss: 0.00021303981382516212\n",
      "Validation Loss: 0.0002288091872992737\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00022483301030661095\n",
      "Training Loss: 0.00022422679187002358\n",
      "Training Loss: 0.00021212702948105288\n",
      "Validation Loss: 0.00022787112800543081\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.00022388886660792195\n",
      "Training Loss: 0.00022329658728267532\n",
      "Training Loss: 0.00021122010941326152\n",
      "Validation Loss: 0.00022694163087865317\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.00022294914447684277\n",
      "Training Loss: 0.00022237107554246905\n",
      "Training Loss: 0.0002103179058758542\n",
      "Validation Loss: 0.00022602079488013955\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00022201311090157107\n",
      "Training Loss: 0.00022144982343888842\n",
      "Training Loss: 0.00020942116290825652\n",
      "Validation Loss: 0.00022510715966797014\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.00022108071923412353\n",
      "Training Loss: 0.00022053240452805768\n",
      "Training Loss: 0.00020852898491284576\n",
      "Validation Loss: 0.00022420264074017734\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.0002201519985055711\n",
      "Training Loss: 0.00021961899572488618\n",
      "Training Loss: 0.0002076412488167989\n",
      "Validation Loss: 0.00022330555485710607\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00021922670735420978\n",
      "Training Loss: 0.00021870912689337274\n",
      "Training Loss: 0.0002067579845606815\n",
      "Validation Loss: 0.00022241590263189884\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00021830427785062055\n",
      "Training Loss: 0.00021780262562970165\n",
      "Training Loss: 0.00020587882208928933\n",
      "Validation Loss: 0.00022153257574818066\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.00021738462693974726\n",
      "Training Loss: 0.00021689922075893264\n",
      "Training Loss: 0.0002050032600891427\n",
      "Validation Loss: 0.00022065790288027355\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0002164675774929492\n",
      "Training Loss: 0.0002159988058338058\n",
      "Training Loss: 0.00020413173475390068\n",
      "Validation Loss: 0.00021978928700182326\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.00021555313041062617\n",
      "Training Loss: 0.0002151010331181169\n",
      "Training Loss: 0.00020326355577708456\n",
      "Validation Loss: 0.00021892796890886\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.0002146408670614619\n",
      "Training Loss: 0.00021420602835860335\n",
      "Training Loss: 0.00020239879293512786\n",
      "Validation Loss: 0.00021807366900840575\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.0002137310708849327\n",
      "Training Loss: 0.00021331355270376662\n",
      "Training Loss: 0.00020153736470092552\n",
      "Validation Loss: 0.00021722531642665396\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.0002128234060455725\n",
      "Training Loss: 0.00021242301216261694\n",
      "Training Loss: 0.0002006789822189603\n",
      "Validation Loss: 0.00021638315650235516\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.00021191739016103385\n",
      "Training Loss: 0.00021153512418095488\n",
      "Training Loss: 0.00019982367139164124\n",
      "Validation Loss: 0.00021554764140766178\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.00021101373012697878\n",
      "Training Loss: 0.00021064933986053802\n",
      "Training Loss: 0.00019897091642633312\n",
      "Validation Loss: 0.0002147189037448575\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.00021011208124036785\n",
      "Training Loss: 0.0002097653647433617\n",
      "Training Loss: 0.00019812121190625475\n",
      "Validation Loss: 0.000213895968780879\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.0002092117930214954\n",
      "Training Loss: 0.0002088837037263147\n",
      "Training Loss: 0.00019727422662981552\n",
      "Validation Loss: 0.00021307890481980037\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.00020831335010370821\n",
      "Training Loss: 0.00020800350612262263\n",
      "Training Loss: 0.00019642964380182094\n",
      "Validation Loss: 0.00021226807353394157\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.0002074166897955365\n",
      "Training Loss: 0.0002071253788199101\n",
      "Training Loss: 0.00019558777748898137\n",
      "Validation Loss: 0.00021146355970047174\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.00020652155971220053\n",
      "Training Loss: 0.00020624904631404205\n",
      "Training Loss: 0.00019474826094665332\n",
      "Validation Loss: 0.00021066452065722106\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.00020562832903124218\n",
      "Training Loss: 0.00020537436917948072\n",
      "Training Loss: 0.00019391126563277793\n",
      "Validation Loss: 0.00020987178737577573\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.0002047367745581141\n",
      "Training Loss: 0.00020450129537493921\n",
      "Training Loss: 0.00019307673897856148\n",
      "Validation Loss: 0.00020908479744096266\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00020384672499858424\n",
      "Training Loss: 0.00020363013007226982\n",
      "Training Loss: 0.0001922447637480218\n",
      "Validation Loss: 0.00020830360852905255\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.00020295849068588723\n",
      "Training Loss: 0.00020276051949622343\n",
      "Training Loss: 0.00019141506083542481\n",
      "Validation Loss: 0.00020752868458453376\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.00020207243182994715\n",
      "Training Loss: 0.00020189322909573094\n",
      "Training Loss: 0.00019058777055761312\n",
      "Validation Loss: 0.00020675909690090634\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.00020118794338486623\n",
      "Training Loss: 0.00020102747696000732\n",
      "Training Loss: 0.00018976335264596854\n",
      "Validation Loss: 0.00020599583831937523\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.00020030568234687962\n",
      "Training Loss: 0.0002001634046609979\n",
      "Training Loss: 0.00018894092085247393\n",
      "Validation Loss: 0.00020523834143787507\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.00019942538004215748\n",
      "Training Loss: 0.0001993014954678074\n",
      "Training Loss: 0.00018812112823070491\n",
      "Validation Loss: 0.00020448733458793827\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.00019854690052852674\n",
      "Training Loss: 0.00019844132295474993\n",
      "Training Loss: 0.0001873043029627297\n",
      "Validation Loss: 0.00020374083351782063\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.000197670965794714\n",
      "Training Loss: 0.00019758360509513294\n",
      "Training Loss: 0.00018649001562152988\n",
      "Validation Loss: 0.0002030009682845518\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.0001967973117416477\n",
      "Training Loss: 0.0001967275050810713\n",
      "Training Loss: 0.00018567844588687876\n",
      "Validation Loss: 0.0002022673471718294\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.00019592637733694573\n",
      "Training Loss: 0.00019587381850215025\n",
      "Training Loss: 0.00018486961631424491\n",
      "Validation Loss: 0.0002015389076445848\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.00019505781577663583\n",
      "Training Loss: 0.00019502257780914077\n",
      "Training Loss: 0.0001840636338602053\n",
      "Validation Loss: 0.00020081631855952835\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.00019419181289777044\n",
      "Training Loss: 0.00019417370725932414\n",
      "Training Loss: 0.0001832609089979087\n",
      "Validation Loss: 0.00020009969637285803\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.00019332899239088875\n",
      "Training Loss: 0.0001933272755559301\n",
      "Training Loss: 0.00018246125982841478\n",
      "Validation Loss: 0.00019938944841557172\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.00019246924924118503\n",
      "Training Loss: 0.00019248355081799673\n",
      "Training Loss: 0.00018166434558224864\n",
      "Validation Loss: 0.00019868405671035505\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.00019161247766078305\n",
      "Training Loss: 0.00019164264011124033\n",
      "Training Loss: 0.00018087127937178594\n",
      "Validation Loss: 0.00019798440185246017\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.0001907593370287941\n",
      "Training Loss: 0.00019080453865171875\n",
      "Training Loss: 0.00018008150760579156\n",
      "Validation Loss: 0.00019729032082773556\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.00018990968408616026\n",
      "Training Loss: 0.00018996933478774737\n",
      "Training Loss: 0.00017929513320268597\n",
      "Validation Loss: 0.00019660316753656383\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.00018906351575424197\n",
      "Training Loss: 0.00018913736326794605\n",
      "Training Loss: 0.00017851239408628317\n",
      "Validation Loss: 0.00019591924979834127\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.00018822117343916033\n",
      "Training Loss: 0.00018830861197784542\n",
      "Training Loss: 0.0001777332058554748\n",
      "Validation Loss: 0.00019524227136162665\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.00018738287685664544\n",
      "Training Loss: 0.0001874833056353964\n",
      "Training Loss: 0.00017695795260806336\n",
      "Validation Loss: 0.00019456967754777406\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.00018654873020750528\n",
      "Training Loss: 0.00018666116206077275\n",
      "Training Loss: 0.00017618636702536604\n",
      "Validation Loss: 0.0001939023757668008\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.00018571885237179232\n",
      "Training Loss: 0.00018584288667625515\n",
      "Training Loss: 0.00017541882531077135\n",
      "Validation Loss: 0.00019323949032859467\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.00018489359133127436\n",
      "Training Loss: 0.00018502794608139084\n",
      "Training Loss: 0.00017465512384660543\n",
      "Validation Loss: 0.00019258170940385466\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.0001840724910289282\n",
      "Training Loss: 0.00018421677774313138\n",
      "Training Loss: 0.00017389561897289242\n",
      "Validation Loss: 0.00019192854899833283\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.00018325633971471688\n",
      "Training Loss: 0.00018340971335419453\n",
      "Training Loss: 0.0001731403610756388\n",
      "Validation Loss: 0.0001912798293999637\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.00018244494988721273\n",
      "Training Loss: 0.00018260677212310838\n",
      "Training Loss: 0.00017238916936548776\n",
      "Validation Loss: 0.0001906349009025209\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.00018163840249144414\n",
      "Training Loss: 0.00018180744933488313\n",
      "Training Loss: 0.00017164229739137228\n",
      "Validation Loss: 0.00018999446727096083\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.00018083684441990043\n",
      "Training Loss: 0.00018101251273037634\n",
      "Training Loss: 0.00017089986187784233\n",
      "Validation Loss: 0.0001893567080034618\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.0001800408451072144\n",
      "Training Loss: 0.0001802217679687601\n",
      "Training Loss: 0.00017016141151543707\n",
      "Validation Loss: 0.00018872342886598755\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.00017924967216458753\n",
      "Training Loss: 0.00017943531609489582\n",
      "Training Loss: 0.0001694274142937502\n",
      "Validation Loss: 0.00018809366168609946\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.00017846395461674548\n",
      "Training Loss: 0.00017865325326056337\n",
      "Training Loss: 0.0001686980474187294\n",
      "Validation Loss: 0.0001874663451141377\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.00017768352888197114\n",
      "Training Loss: 0.00017787541186407906\n",
      "Training Loss: 0.00016797284806671086\n",
      "Validation Loss: 0.00018684211532631759\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.00017690864880933076\n",
      "Training Loss: 0.00017710231304590707\n",
      "Training Loss: 0.00016725222681998275\n",
      "Validation Loss: 0.00018622000349618185\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.00017613956119930663\n",
      "Training Loss: 0.0001763333863709704\n",
      "Training Loss: 0.00016653586908432771\n",
      "Validation Loss: 0.00018560057788419542\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.00017537551455461652\n",
      "Training Loss: 0.00017556923463416752\n",
      "Training Loss: 0.0001658239235621295\n",
      "Validation Loss: 0.00018498282827678257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.00017461760939113446\n",
      "Training Loss: 0.00017480953300037073\n",
      "Training Loss: 0.00016511677084054098\n",
      "Validation Loss: 0.00018436693581667458\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.00017386513111887326\n",
      "Training Loss: 0.00017405460657755613\n",
      "Training Loss: 0.00016441319485238637\n",
      "Validation Loss: 0.00018375309773137417\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0001731182164621714\n",
      "Training Loss: 0.00017330412676528796\n",
      "Training Loss: 0.00016371465842894396\n",
      "Validation Loss: 0.00018314096487710628\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.00017237740512427991\n",
      "Training Loss: 0.00017255832248338265\n",
      "Training Loss: 0.00016302032885505467\n",
      "Validation Loss: 0.0001825296502001845\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.00017164223645522726\n",
      "Training Loss: 0.00017181728018840657\n",
      "Training Loss: 0.00016233025437031756\n",
      "Validation Loss: 0.00018191841719384315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.00017091267886826245\n",
      "Training Loss: 0.00017108082014601677\n",
      "Training Loss: 0.0001616445166291669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [18:07<18:36, 223.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0001813088319670825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.26908256351947785\n",
      "Training Loss: 0.17374100521206856\n",
      "Training Loss: 0.12392544124275445\n",
      "Validation Loss: 0.07903123397924257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.06587649870663881\n",
      "Training Loss: 0.05086699750740081\n",
      "Training Loss: 0.051114225662313405\n",
      "Validation Loss: 0.04866843951049815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.0463262801617384\n",
      "Training Loss: 0.04359148853458464\n",
      "Training Loss: 0.04524268184322864\n",
      "Validation Loss: 0.043827794900352365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.04143614975153469\n",
      "Training Loss: 0.038826160719618204\n",
      "Training Loss: 0.0399537554429844\n",
      "Validation Loss: 0.038454307023477685\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.03593956439639442\n",
      "Training Loss: 0.03353501744102687\n",
      "Training Loss: 0.034255981729365885\n",
      "Validation Loss: 0.03285707331398565\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.03029595878557302\n",
      "Training Loss: 0.028195446450263263\n",
      "Training Loss: 0.02859237710945308\n",
      "Validation Loss: 0.027466691598777522\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.024917695219628513\n",
      "Training Loss: 0.023145720745669677\n",
      "Training Loss: 0.023300903898198156\n",
      "Validation Loss: 0.02257702213560304\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.020067998659797012\n",
      "Training Loss: 0.018611545527819545\n",
      "Training Loss: 0.018618733438197522\n",
      "Validation Loss: 0.01843216130248365\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.01598056407412514\n",
      "Training Loss: 0.014845368136884645\n",
      "Training Loss: 0.014837553356774152\n",
      "Validation Loss: 0.01525923177950461\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.01288776105735451\n",
      "Training Loss: 0.012049888607580215\n",
      "Training Loss: 0.012100251442752778\n",
      "Validation Loss: 0.012998951282456851\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.010737819308415055\n",
      "Training Loss: 0.010130375166190787\n",
      "Training Loss: 0.010233750300249084\n",
      "Validation Loss: 0.01137675023951641\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.009267755688924809\n",
      "Training Loss: 0.008829567471984774\n",
      "Training Loss: 0.008954883688129484\n",
      "Validation Loss: 0.010141680595266182\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.008218251939688344\n",
      "Training Loss: 0.007907525263726711\n",
      "Training Loss: 0.008027997270692139\n",
      "Validation Loss: 0.009138688382305455\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.007413598343846388\n",
      "Training Loss: 0.007203046076465398\n",
      "Training Loss: 0.007302636825479567\n",
      "Validation Loss: 0.008277525766909625\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.006750419084128225\n",
      "Training Loss: 0.006620626706862822\n",
      "Training Loss: 0.006689650175394491\n",
      "Validation Loss: 0.007498943537285321\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.006165788865182549\n",
      "Training Loss: 0.006101715617114678\n",
      "Training Loss: 0.006133676108438521\n",
      "Validation Loss: 0.0067623497217140175\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.005618361190718133\n",
      "Training Loss: 0.0056078794633504\n",
      "Training Loss: 0.005599168293410912\n",
      "Validation Loss: 0.006042870036274028\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.005081107670848723\n",
      "Training Loss: 0.005114636583020911\n",
      "Training Loss: 0.005066112823551521\n",
      "Validation Loss: 0.005333156671552929\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.0045416269419365565\n",
      "Training Loss: 0.004613159030559473\n",
      "Training Loss: 0.0045335795113351195\n",
      "Validation Loss: 0.00465102332481968\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.0040096299312426704\n",
      "Training Loss: 0.00411869928764645\n",
      "Training Loss: 0.004027940711239353\n",
      "Validation Loss: 0.004045263363121684\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.003523864999297075\n",
      "Training Loss: 0.0036744874017313124\n",
      "Training Loss: 0.0035991146572632716\n",
      "Validation Loss: 0.003576022579355605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0031360171653795985\n",
      "Training Loss: 0.0033279429984395393\n",
      "Training Loss: 0.00328518005029764\n",
      "Validation Loss: 0.0032662499645002858\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.0028670831878844184\n",
      "Training Loss: 0.003088301275856793\n",
      "Training Loss: 0.003076624791719951\n",
      "Validation Loss: 0.0030842809316576616\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.0026913673587841912\n",
      "Training Loss: 0.00292546232114546\n",
      "Training Loss: 0.0029353601392358543\n",
      "Validation Loss: 0.002982411878588976\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.00257133367107599\n",
      "Training Loss: 0.0028070984859368763\n",
      "Training Loss: 0.0028316630670451558\n",
      "Validation Loss: 0.0029259612113420506\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.0024832186783896757\n",
      "Training Loss: 0.0027151321893325074\n",
      "Training Loss: 0.002750659190205624\n",
      "Validation Loss: 0.002893289050160583\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.0024151025380706416\n",
      "Training Loss: 0.002640785676194355\n",
      "Training Loss: 0.002684872977406485\n",
      "Validation Loss: 0.002870970198314302\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.0023603249403822703\n",
      "Training Loss: 0.0025789622371667066\n",
      "Training Loss: 0.0026296139002079146\n",
      "Validation Loss: 0.0028512858235659184\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.002314470684796106\n",
      "Training Loss: 0.0025260635878657923\n",
      "Training Loss: 0.002581518111983314\n",
      "Validation Loss: 0.0028304970235265583\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.00227443282630702\n",
      "Training Loss: 0.0024793929400038907\n",
      "Training Loss: 0.0025381397175078747\n",
      "Validation Loss: 0.0028072762550927404\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.002238047201462905\n",
      "Training Loss: 0.0024369462847243996\n",
      "Training Loss: 0.0024977374986337965\n",
      "Validation Loss: 0.002781445865703778\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.0022038330072246025\n",
      "Training Loss: 0.002397257917182287\n",
      "Training Loss: 0.002459083786525298\n",
      "Validation Loss: 0.0027532881442025787\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.002170781672466546\n",
      "Training Loss: 0.0023592492241004946\n",
      "Training Loss: 0.0024213002882606817\n",
      "Validation Loss: 0.002723096080321892\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.0021381815455242758\n",
      "Training Loss: 0.0023221052781445903\n",
      "Training Loss: 0.0023837300122249872\n",
      "Validation Loss: 0.002691077771191119\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.002105499121316825\n",
      "Training Loss: 0.0022851778924814427\n",
      "Training Loss: 0.0023458472147467545\n",
      "Validation Loss: 0.002657300949108452\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.0020722977794503093\n",
      "Training Loss: 0.0022479102680517827\n",
      "Training Loss: 0.002307189087005099\n",
      "Validation Loss: 0.0026216723917253586\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0020381816985172918\n",
      "Training Loss: 0.002209793531073956\n",
      "Training Loss: 0.002267314415657893\n",
      "Validation Loss: 0.0025839497586398314\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.0020027600802859525\n",
      "Training Loss: 0.0021703211311250924\n",
      "Training Loss: 0.0022257710032863543\n",
      "Validation Loss: 0.0025437374084565285\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.001965621592535172\n",
      "Training Loss: 0.002128967852477217\n",
      "Training Loss: 0.002182073908916209\n",
      "Validation Loss: 0.002500479512297276\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.0019263217740808614\n",
      "Training Loss: 0.002085174636886222\n",
      "Training Loss: 0.0021356962609570475\n",
      "Validation Loss: 0.0024534690181078557\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.0018843851718702354\n",
      "Training Loss: 0.0020383554327418098\n",
      "Training Loss: 0.002086082718160469\n",
      "Validation Loss: 0.0024018674984893477\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0018393316455330932\n",
      "Training Loss: 0.001987936096120393\n",
      "Training Loss: 0.0020326914814359043\n",
      "Validation Loss: 0.002344777921392593\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0017907367725274526\n",
      "Training Loss: 0.0019334314635489137\n",
      "Training Loss: 0.0019750827876850964\n",
      "Validation Loss: 0.002281360291245425\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0017383329781296198\n",
      "Training Loss: 0.001874595434492221\n",
      "Training Loss: 0.0019130682345712558\n",
      "Validation Loss: 0.002211107347249524\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.0016821603594871703\n",
      "Training Loss: 0.0018115853601193522\n",
      "Training Loss: 0.001846869413711829\n",
      "Validation Loss: 0.0021340995892918974\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.0016226810081570876\n",
      "Training Loss: 0.0017450940966955387\n",
      "Training Loss: 0.0017772230721311644\n",
      "Validation Loss: 0.00205123803010629\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.001560796951453085\n",
      "Training Loss: 0.0016763099055970088\n",
      "Training Loss: 0.0017052994805271738\n",
      "Validation Loss: 0.0019642115983290577\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0014976703123102198\n",
      "Training Loss: 0.0016066388931358234\n",
      "Training Loss: 0.0016323982771427837\n",
      "Validation Loss: 0.0018751339874429147\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.001434396440454293\n",
      "Training Loss: 0.0015373033359355759\n",
      "Training Loss: 0.0015595641994150355\n",
      "Validation Loss: 0.0017859964167798628\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.001371705133933574\n",
      "Training Loss: 0.0014690312903258018\n",
      "Training Loss: 0.001487334936100524\n",
      "Validation Loss: 0.0016983059698289385\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.0013098530831484822\n",
      "Training Loss: 0.001401996613785741\n",
      "Training Loss: 0.0014157497126143425\n",
      "Validation Loss: 0.0016128341629748034\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.0012487118582066613\n",
      "Training Loss: 0.001335962627781555\n",
      "Training Loss: 0.0013445497529755812\n",
      "Validation Loss: 0.0015296950480793445\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.0011879630391922547\n",
      "Training Loss: 0.0012705063774774317\n",
      "Training Loss: 0.0012734333392290865\n",
      "Validation Loss: 0.0014485013980243167\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.0011273064449051162\n",
      "Training Loss: 0.0012052389729069546\n",
      "Training Loss: 0.001202250195274246\n",
      "Validation Loss: 0.00136861590350706\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0010665813497325871\n",
      "Training Loss: 0.0011399331893335329\n",
      "Training Loss: 0.0011310576359392145\n",
      "Validation Loss: 0.0012894219696309858\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0010058006223698612\n",
      "Training Loss: 0.001074553933358402\n",
      "Training Loss: 0.0010600826255540597\n",
      "Validation Loss: 0.0012105228425150088\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0009451155485294294\n",
      "Training Loss: 0.0010092266408173601\n",
      "Training Loss: 0.0009896557412866968\n",
      "Validation Loss: 0.0011317961686748614\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.0008847634432459017\n",
      "Training Loss: 0.0009441748273820849\n",
      "Training Loss: 0.0009201402866892749\n",
      "Validation Loss: 0.001053346260408269\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.0008250116059207357\n",
      "Training Loss: 0.0008796518232702511\n",
      "Training Loss: 0.0008518851372355129\n",
      "Validation Loss: 0.0009753972947927129\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.0007661142651340924\n",
      "Training Loss: 0.0008158777780408854\n",
      "Training Loss: 0.0007851850552106043\n",
      "Validation Loss: 0.0008981818243504104\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.0007082737691234797\n",
      "Training Loss: 0.0007529811166750733\n",
      "Training Loss: 0.0007202510169736343\n",
      "Validation Loss: 0.0008218778956065405\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.0006516264837046037\n",
      "Training Loss: 0.0006909904193526017\n",
      "Training Loss: 0.0006572356006654445\n",
      "Validation Loss: 0.0007466558469858365\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.0005962974603789916\n",
      "Training Loss: 0.0006299312359624309\n",
      "Training Loss: 0.0005963444311055354\n",
      "Validation Loss: 0.0006729198378226929\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.0005425823157020204\n",
      "Training Loss: 0.0005701170804059075\n",
      "Training Loss: 0.0005380833788512973\n",
      "Validation Loss: 0.0006017758830004434\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0004912553563190159\n",
      "Training Loss: 0.000512595843829331\n",
      "Training Loss: 0.0004835882399856928\n",
      "Validation Loss: 0.0005354519468781883\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.00044387922165697093\n",
      "Training Loss: 0.00045949296580147345\n",
      "Training Loss: 0.00043486043370648985\n",
      "Validation Loss: 0.00047709391129613864\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.00040277435751704614\n",
      "Training Loss: 0.0004137595659449289\n",
      "Training Loss: 0.0003942624832779984\n",
      "Validation Loss: 0.00042924028056905226\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.00036982007809456264\n",
      "Training Loss: 0.0003773178840128821\n",
      "Training Loss: 0.00036236097472283293\n",
      "Validation Loss: 0.00039136556122766603\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.00034429167495545696\n",
      "Training Loss: 0.0003489544917283638\n",
      "Training Loss: 0.00033691939348500454\n",
      "Validation Loss: 0.00036050954195468037\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0003235396006607516\n",
      "Training Loss: 0.00032595907927316145\n",
      "Training Loss: 0.00031526864069746807\n",
      "Validation Loss: 0.00033425440173988553\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.00030544433221052714\n",
      "Training Loss: 0.00030625872812379385\n",
      "Training Loss: 0.0002957647059156443\n",
      "Validation Loss: 0.000311353671513439\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.00028892275326825257\n",
      "Training Loss: 0.00028864228997917964\n",
      "Training Loss: 0.0002777332863843185\n",
      "Validation Loss: 0.00029122442981716133\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.000273603588577771\n",
      "Training Loss: 0.0002725772622943623\n",
      "Training Loss: 0.0002611292085930472\n",
      "Validation Loss: 0.00027362825472010535\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.00025948479412932104\n",
      "Training Loss: 0.0002579412887644139\n",
      "Training Loss: 0.00024614730984467314\n",
      "Validation Loss: 0.00025846971349504234\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.0002466611874820046\n",
      "Training Loss: 0.0002447456597474229\n",
      "Training Loss: 0.00023293849370020325\n",
      "Validation Loss: 0.00024564432970600024\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00023517369912951836\n",
      "Training Loss: 0.00023296657336686623\n",
      "Training Loss: 0.00022148692771224886\n",
      "Validation Loss: 0.00023495245217970242\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00022496523154131866\n",
      "Training Loss: 0.00022250725940466508\n",
      "Training Loss: 0.00021161340806429506\n",
      "Validation Loss: 0.00022607937725287015\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00021590515135358146\n",
      "Training Loss: 0.00021322808492186597\n",
      "Training Loss: 0.00020304628915255308\n",
      "Validation Loss: 0.00021864099083389836\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.0002078425487388813\n",
      "Training Loss: 0.00020498739231697983\n",
      "Training Loss: 0.00019550892267943709\n",
      "Validation Loss: 0.00021226253546741686\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.00020065468865141156\n",
      "Training Loss: 0.00019766448245718494\n",
      "Training Loss: 0.0001887879399509984\n",
      "Validation Loss: 0.0002066366965316754\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.00019426225635015725\n",
      "Training Loss: 0.00019116552319246695\n",
      "Training Loss: 0.00018274901152835808\n",
      "Validation Loss: 0.00020154135181451493\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0001886093543816969\n",
      "Training Loss: 0.00018540766561272903\n",
      "Training Loss: 0.00017731538064253983\n",
      "Validation Loss: 0.0001968219784931136\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.00018363343605869886\n",
      "Training Loss: 0.00018030225599432014\n",
      "Training Loss: 0.0001724348201059911\n",
      "Validation Loss: 0.00019237959738317196\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.0001792477810249693\n",
      "Training Loss: 0.00017575138832398807\n",
      "Training Loss: 0.00016805768518679542\n",
      "Validation Loss: 0.0001881659842551466\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.00017535495914671628\n",
      "Training Loss: 0.0001716644465159334\n",
      "Training Loss: 0.00016413307132097542\n",
      "Validation Loss: 0.00018417646481722248\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.00017186828023113776\n",
      "Training Loss: 0.00016797148926343654\n",
      "Training Loss: 0.00016061379546954412\n",
      "Validation Loss: 0.00018043180134146098\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.00016872522299308913\n",
      "Training Loss: 0.00016462687226521667\n",
      "Training Loss: 0.00015746079145174008\n",
      "Validation Loss: 0.00017696010357039075\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.00016588520440564026\n",
      "Training Loss: 0.0001616016092521022\n",
      "Training Loss: 0.0001546406701163505\n",
      "Validation Loss: 0.0001737792287886815\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.00016332122842868557\n",
      "Training Loss: 0.00015887452233073418\n",
      "Training Loss: 0.0001521228571618849\n",
      "Validation Loss: 0.00017089757943977837\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.00016101111559692073\n",
      "Training Loss: 0.00015642458956790505\n",
      "Training Loss: 0.00014987928792834282\n",
      "Validation Loss: 0.00016830511269924162\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00015893402505753328\n",
      "Training Loss: 0.0001542310391232604\n",
      "Training Loss: 0.00014788154663619935\n",
      "Validation Loss: 0.00016598862934637856\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.00015706920756201727\n",
      "Training Loss: 0.00015227164431053097\n",
      "Training Loss: 0.00014610273488870008\n",
      "Validation Loss: 0.00016392508192704676\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.00015539470970907132\n",
      "Training Loss: 0.0001505230264410784\n",
      "Training Loss: 0.00014451587816438405\n",
      "Validation Loss: 0.00016208852642020236\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.00015388872929179343\n",
      "Training Loss: 0.00014896149088599486\n",
      "Training Loss: 0.00014309644957393174\n",
      "Validation Loss: 0.0001604547124270028\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.00015253097100867308\n",
      "Training Loss: 0.00014756485985344624\n",
      "Training Loss: 0.00014182102444465272\n",
      "Validation Loss: 0.0001589967441657464\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.0001513016768512898\n",
      "Training Loss: 0.00014631178824856762\n",
      "Training Loss: 0.00014066885461943456\n",
      "Validation Loss: 0.00015769205985246147\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.000150183352179738\n",
      "Training Loss: 0.00014518335818138438\n",
      "Training Loss: 0.00013962151268060553\n",
      "Validation Loss: 0.00015651834915291382\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.00014916016336428585\n",
      "Training Loss: 0.00014416203621294698\n",
      "Training Loss: 0.00013866344545022003\n",
      "Validation Loss: 0.0001554570967374694\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.00014821853863395518\n",
      "Training Loss: 0.0001432327191469085\n",
      "Training Loss: 0.00013778020102108712\n",
      "Validation Loss: 0.00015449065358552616\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.0001473462098147138\n",
      "Training Loss: 0.00014238184317946434\n",
      "Training Loss: 0.00013696101648747572\n",
      "Validation Loss: 0.00015360464342674266\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0001465330687824462\n",
      "Training Loss: 0.00014159855536490795\n",
      "Training Loss: 0.00013619520874271984\n",
      "Validation Loss: 0.0001527867679920092\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.0001457706514156598\n",
      "Training Loss: 0.0001408724731663824\n",
      "Training Loss: 0.00013547505821406958\n",
      "Validation Loss: 0.00015202638421082094\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.0001450516455770412\n",
      "Training Loss: 0.00014019554193509976\n",
      "Training Loss: 0.00013479337068929452\n",
      "Validation Loss: 0.00015131527055764515\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.00014436974915952306\n",
      "Training Loss: 0.00013956045057057054\n",
      "Training Loss: 0.00013414523406027\n",
      "Validation Loss: 0.00015064474229823166\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.00014372005367476958\n",
      "Training Loss: 0.00013896173413741053\n",
      "Training Loss: 0.00013352508067328018\n",
      "Validation Loss: 0.0001500103429289799\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.00014309801166746184\n",
      "Training Loss: 0.00013839413128152956\n",
      "Training Loss: 0.00013292973933857867\n",
      "Validation Loss: 0.00014940521142624053\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.0001425003959047899\n",
      "Training Loss: 0.00013785314204142196\n",
      "Training Loss: 0.00013235567046649522\n",
      "Validation Loss: 0.00014882611505751984\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.00014192372400430032\n",
      "Training Loss: 0.00013733585175941697\n",
      "Training Loss: 0.00013179991026845527\n",
      "Validation Loss: 0.000148268440497805\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.00014136554140350199\n",
      "Training Loss: 0.0001368384740089823\n",
      "Training Loss: 0.0001312610481181764\n",
      "Validation Loss: 0.0001477307655017632\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.00014082396606681868\n",
      "Training Loss: 0.00013635881505251745\n",
      "Training Loss: 0.00013073662226815942\n",
      "Validation Loss: 0.00014720912927106348\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.00014029705156644922\n",
      "Training Loss: 0.0001358949433779344\n",
      "Training Loss: 0.00013022514163822053\n",
      "Validation Loss: 0.00014670275707972456\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0001397833820010419\n",
      "Training Loss: 0.0001354445974357077\n",
      "Training Loss: 0.00012972564249139396\n",
      "Validation Loss: 0.00014620899328496962\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.0001392813818893046\n",
      "Training Loss: 0.00013500648601620923\n",
      "Training Loss: 0.00012923652930112438\n",
      "Validation Loss: 0.00014572650092958703\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.00013879022382752736\n",
      "Training Loss: 0.0001345791017593001\n",
      "Training Loss: 0.00012875753114713006\n",
      "Validation Loss: 0.0001452547658731562\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.00013830888141455944\n",
      "Training Loss: 0.00013416139709079288\n",
      "Training Loss: 0.00012828731640183834\n",
      "Validation Loss: 0.00014479233141173609\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.00013783639657049206\n",
      "Training Loss: 0.00013375277423619992\n",
      "Training Loss: 0.00012782543524735956\n",
      "Validation Loss: 0.00014433868865749348\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.000137372206809232\n",
      "Training Loss: 0.000133351537733688\n",
      "Training Loss: 0.0001273716381911072\n",
      "Validation Loss: 0.00014389141859977867\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.00013691555728655657\n",
      "Training Loss: 0.00013295761862536893\n",
      "Training Loss: 0.00012692472850176273\n",
      "Validation Loss: 0.00014345218673799046\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.00013646609111674478\n",
      "Training Loss: 0.00013257032389446977\n",
      "Training Loss: 0.00012648452322537197\n",
      "Validation Loss: 0.0001430191044092997\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.0001360231680155266\n",
      "Training Loss: 0.00013218908577982802\n",
      "Training Loss: 0.00012605095758772223\n",
      "Validation Loss: 0.0001425925163019794\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0001355865367622755\n",
      "Training Loss: 0.00013181308655475733\n",
      "Training Loss: 0.00012562323659949472\n",
      "Validation Loss: 0.0001421715244484066\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.00013515596290744725\n",
      "Training Loss: 0.00013144254058715887\n",
      "Training Loss: 0.0001252013310477196\n",
      "Validation Loss: 0.00014175586254035744\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0001347306436400686\n",
      "Training Loss: 0.00013107607703204848\n",
      "Training Loss: 0.00012478481585276313\n",
      "Validation Loss: 0.00014134473193929336\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.00013431068471618347\n",
      "Training Loss: 0.00013071426990791225\n",
      "Training Loss: 0.0001243732712464407\n",
      "Validation Loss: 0.000140938768412668\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.00013389598203502828\n",
      "Training Loss: 0.00013035656906140502\n",
      "Training Loss: 0.0001239669756796502\n",
      "Validation Loss: 0.00014053715928097967\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00013348573165785637\n",
      "Training Loss: 0.00013000264754737146\n",
      "Training Loss: 0.0001235654626179894\n",
      "Validation Loss: 0.00014013985933518833\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0001330799703828234\n",
      "Training Loss: 0.00012965212887138477\n",
      "Training Loss: 0.00012316857537371108\n",
      "Validation Loss: 0.00013974639735484805\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.0001326787793186668\n",
      "Training Loss: 0.00012930547583891894\n",
      "Training Loss: 0.00012277573097890126\n",
      "Validation Loss: 0.00013935744573245756\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00013228179182988241\n",
      "Training Loss: 0.0001289615431596758\n",
      "Training Loss: 0.0001223875298819621\n",
      "Validation Loss: 0.00013897173812438268\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.00013188874338084132\n",
      "Training Loss: 0.00012862063837019378\n",
      "Training Loss: 0.00012200323197248509\n",
      "Validation Loss: 0.00013858961088015839\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.00013149961441740742\n",
      "Training Loss: 0.00012828276991058373\n",
      "Training Loss: 0.00012162309933046345\n",
      "Validation Loss: 0.0001382111297490983\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00013111419071719864\n",
      "Training Loss: 0.00012794793568900786\n",
      "Training Loss: 0.00012124627477533067\n",
      "Validation Loss: 0.00013783580666977592\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.00013073261134195492\n",
      "Training Loss: 0.00012761544041495654\n",
      "Training Loss: 0.0001208738201057713\n",
      "Validation Loss: 0.00013746407586970666\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.00013035443744229268\n",
      "Training Loss: 0.00012728556745059905\n",
      "Training Loss: 0.00012050465786160202\n",
      "Validation Loss: 0.00013709507590412226\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.00012997972167795525\n",
      "Training Loss: 0.0001269580596817832\n",
      "Training Loss: 0.0001201390782262024\n",
      "Validation Loss: 0.00013672952535558947\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.00012960844414919848\n",
      "Training Loss: 0.00012663332752708813\n",
      "Training Loss: 0.00011977673751971452\n",
      "Validation Loss: 0.00013636697759067645\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.0001292404477680975\n",
      "Training Loss: 0.00012631043573492208\n",
      "Training Loss: 0.00011941806193135562\n",
      "Validation Loss: 0.000136007795691931\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.00012887562850664837\n",
      "Training Loss: 0.0001259901828598231\n",
      "Training Loss: 0.00011906260542673408\n",
      "Validation Loss: 0.00013565097659910648\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.0001285140171785315\n",
      "Training Loss: 0.00012567196115924163\n",
      "Training Loss: 0.00011871026303197141\n",
      "Validation Loss: 0.0001352971237261709\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00012815556445275433\n",
      "Training Loss: 0.00012535590074548962\n",
      "Training Loss: 0.0001183613565081032\n",
      "Validation Loss: 0.00013494609423136088\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.0001278001981700072\n",
      "Training Loss: 0.00012504196964073345\n",
      "Training Loss: 0.00011801529111835407\n",
      "Validation Loss: 0.00013459785592469538\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.00012744755880703451\n",
      "Training Loss: 0.00012473015600335203\n",
      "Training Loss: 0.00011767226642405149\n",
      "Validation Loss: 0.00013425255596037338\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00012709816075584968\n",
      "Training Loss: 0.0001244202731504629\n",
      "Training Loss: 0.00011733219565940089\n",
      "Validation Loss: 0.0001339097296376974\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.0001267516319603601\n",
      "Training Loss: 0.0001241126536660886\n",
      "Training Loss: 0.00011699518385285047\n",
      "Validation Loss: 0.0001335695440175595\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.00012640771250516992\n",
      "Training Loss: 0.00012380698268316337\n",
      "Training Loss: 0.00011666101587252342\n",
      "Validation Loss: 0.00013323194804562785\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.00012606672520632856\n",
      "Training Loss: 0.00012350312154012498\n",
      "Training Loss: 0.00011632966705292347\n",
      "Validation Loss: 0.00013289713404426517\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.00012572874302350102\n",
      "Training Loss: 0.00012320129249928868\n",
      "Training Loss: 0.00011600120849834638\n",
      "Validation Loss: 0.0001325649174371423\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.00012539350744191323\n",
      "Training Loss: 0.00012290155478694942\n",
      "Training Loss: 0.00011567546636797487\n",
      "Validation Loss: 0.0001322350258868007\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.00012506102744737292\n",
      "Training Loss: 0.00012260367069757193\n",
      "Training Loss: 0.00011535231373272836\n",
      "Validation Loss: 0.00013190812683972305\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.00012473121596485727\n",
      "Training Loss: 0.00012230777341756038\n",
      "Training Loss: 0.00011503234492920455\n",
      "Validation Loss: 0.0001315837007802728\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.0001244042593134509\n",
      "Training Loss: 0.00012201389779875171\n",
      "Training Loss: 0.00011471470994365517\n",
      "Validation Loss: 0.00013126138131493268\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.00012407974771122098\n",
      "Training Loss: 0.00012172182068752591\n",
      "Training Loss: 0.00011439982146839611\n",
      "Validation Loss: 0.00013094220782242314\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.00012375804097246147\n",
      "Training Loss: 0.00012143181600549725\n",
      "Training Loss: 0.00011408770973503124\n",
      "Validation Loss: 0.00013062487596471328\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.0001234389326600649\n",
      "Training Loss: 0.00012114380873754272\n",
      "Training Loss: 0.00011377801194612403\n",
      "Validation Loss: 0.00013031027028160536\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.0001231225618175813\n",
      "Training Loss: 0.00012085765158190043\n",
      "Training Loss: 0.00011347096380632138\n",
      "Validation Loss: 0.0001299985481259018\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.0001228087423442048\n",
      "Training Loss: 0.00012057358424499398\n",
      "Training Loss: 0.00011316656895360211\n",
      "Validation Loss: 0.0001296889047276391\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.00012249759854967123\n",
      "Training Loss: 0.00012029145085762138\n",
      "Training Loss: 0.00011286464765362325\n",
      "Validation Loss: 0.00012938203275389447\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.00012218898018545588\n",
      "Training Loss: 0.00012001130930002547\n",
      "Training Loss: 0.0001125654865973047\n",
      "Validation Loss: 0.0001290776448656564\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.00012188287912067607\n",
      "Training Loss: 0.00011973308341111988\n",
      "Training Loss: 0.00011226855311178951\n",
      "Validation Loss: 0.00012877561759646442\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00012157934414062765\n",
      "Training Loss: 0.00011945668013140675\n",
      "Training Loss: 0.0001119744967763836\n",
      "Validation Loss: 0.00012847586273555148\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.00012127826266805641\n",
      "Training Loss: 0.0001191823232875322\n",
      "Training Loss: 0.00011168259956320981\n",
      "Validation Loss: 0.00012817924279046606\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.00012097993476345437\n",
      "Training Loss: 0.00011890999956449378\n",
      "Training Loss: 0.0001113933428132441\n",
      "Validation Loss: 0.00012788397282165125\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.00012068390229615034\n",
      "Training Loss: 0.00011863945088407491\n",
      "Training Loss: 0.00011110638242826099\n",
      "Validation Loss: 0.00012759161292931085\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.00012039047462167218\n",
      "Training Loss: 0.000118371037660836\n",
      "Training Loss: 0.00011082192935646162\n",
      "Validation Loss: 0.000127301893862137\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.00012009961696094252\n",
      "Training Loss: 0.00011810483118097182\n",
      "Training Loss: 0.00011053983866077033\n",
      "Validation Loss: 0.00012701430297168913\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.00011981125238889945\n",
      "Training Loss: 0.00011784020169216092\n",
      "Training Loss: 0.00011026021995348856\n",
      "Validation Loss: 0.00012672938530076966\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.00011952503387874458\n",
      "Training Loss: 0.0001175780138328264\n",
      "Training Loss: 0.00010998296549587394\n",
      "Validation Loss: 0.00012644609803706016\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.00011924147591344081\n",
      "Training Loss: 0.00011731715319911018\n",
      "Training Loss: 0.00010970801153234789\n",
      "Validation Loss: 0.0001261657890312706\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.00011896022138898843\n",
      "Training Loss: 0.00011705879509463557\n",
      "Training Loss: 0.00010943549021249055\n",
      "Validation Loss: 0.0001258872766351835\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.00011868131981827901\n",
      "Training Loss: 0.00011680197654641233\n",
      "Training Loss: 0.00010916514462223859\n",
      "Validation Loss: 0.00012561107447826739\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0001184047381138953\n",
      "Training Loss: 0.00011654745858322713\n",
      "Training Loss: 0.00010889713348660734\n",
      "Validation Loss: 0.00012533715400852007\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.00011813042952780961\n",
      "Training Loss: 0.00011629458535026061\n",
      "Training Loss: 0.00010863145784242079\n",
      "Validation Loss: 0.00012506552491359298\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.00011785833872636431\n",
      "Training Loss: 0.0001160436836835288\n",
      "Training Loss: 0.00010836789972017868\n",
      "Validation Loss: 0.0001247963678252653\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.0001175886309465568\n",
      "Training Loss: 0.00011579477093619061\n",
      "Training Loss: 0.00010810672583829727\n",
      "Validation Loss: 0.00012452886229756373\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.00011732121340173762\n",
      "Training Loss: 0.00011554768067071563\n",
      "Training Loss: 0.00010784763040646794\n",
      "Validation Loss: 0.00012426340824246192\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.00011705589769917424\n",
      "Training Loss: 0.0001153025466373947\n",
      "Training Loss: 0.00010759056878669071\n",
      "Validation Loss: 0.0001239998489816142\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.00011679269247906632\n",
      "Training Loss: 0.0001150592880367185\n",
      "Training Loss: 0.00010733595312558464\n",
      "Validation Loss: 0.0001237389255794788\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.00011653188208583742\n",
      "Training Loss: 0.00011481781797556323\n",
      "Training Loss: 0.00010708327135944273\n",
      "Validation Loss: 0.00012347903994145527\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.00011627285473878146\n",
      "Training Loss: 0.0001145783005085832\n",
      "Training Loss: 0.00010683264032195439\n",
      "Validation Loss: 0.00012322108343366554\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.00011601616675761761\n",
      "Training Loss: 0.00011434052266849904\n",
      "Training Loss: 0.00010658415721991333\n",
      "Validation Loss: 0.00012296566895334825\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.0001157610961854516\n",
      "Training Loss: 0.0001141044583528128\n",
      "Training Loss: 0.00010633751744535403\n",
      "Validation Loss: 0.00012271119104819293\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.00011550844343219069\n",
      "Training Loss: 0.00011387029608158627\n",
      "Training Loss: 0.00010609298551571556\n",
      "Validation Loss: 0.00012245921451916258\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.00011525754369358765\n",
      "Training Loss: 0.00011363779747625814\n",
      "Training Loss: 0.0001058503964850388\n",
      "Validation Loss: 0.00012220891477135297\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.00011500854205223732\n",
      "Training Loss: 0.00011340690520228236\n",
      "Training Loss: 0.00010560973061728873\n",
      "Validation Loss: 0.00012195970247263544\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.00011476151207716612\n",
      "Training Loss: 0.00011317772648908431\n",
      "Training Loss: 0.00010537067500990816\n",
      "Validation Loss: 0.00012171232776197715\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.00011451612130258581\n",
      "Training Loss: 0.00011295030111796222\n",
      "Training Loss: 0.0001051339205878321\n",
      "Validation Loss: 0.00012146637913901123\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.00011427277314396634\n",
      "Training Loss: 0.00011272439709500759\n",
      "Training Loss: 0.0001048987698595738\n",
      "Validation Loss: 0.00012122143337191108\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.00011403093549233745\n",
      "Training Loss: 0.00011250015737459762\n",
      "Training Loss: 0.00010466552235811832\n",
      "Validation Loss: 0.00012097874102611069\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.00011379106052118005\n",
      "Training Loss: 0.0001122774530085735\n",
      "Training Loss: 0.00010443393202876904\n",
      "Validation Loss: 0.0001207369638363611\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.0001135527534825087\n",
      "Training Loss: 0.00011205639098989195\n",
      "Training Loss: 0.00010420390808576485\n",
      "Validation Loss: 0.00012049640144539992\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.00011331604097904347\n",
      "Training Loss: 0.00011183676693690358\n",
      "Training Loss: 0.00010397567884865567\n",
      "Validation Loss: 0.0001202574878354855\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.00011308096993161598\n",
      "Training Loss: 0.00011161869770148769\n",
      "Training Loss: 0.00010374892288382398\n",
      "Validation Loss: 0.00012001945768215489\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.00011284731381238089\n",
      "Training Loss: 0.00011140202999740723\n",
      "Training Loss: 0.0001035242196303443\n",
      "Validation Loss: 0.0001197824563206176\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.00011261526622547536\n",
      "Training Loss: 0.00011118661928776419\n",
      "Training Loss: 0.00010330077455364517\n",
      "Validation Loss: 0.00011954664451683599\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.00011238479124585866\n",
      "Training Loss: 0.00011097280908870743\n",
      "Training Loss: 0.000103078625688795\n",
      "Validation Loss: 0.0001193119330994531\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.00011215564574740711\n",
      "Training Loss: 0.00011076014936406864\n",
      "Training Loss: 0.00010285829643180477\n",
      "Validation Loss: 0.00011907805013977216\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0001119277687757858\n",
      "Training Loss: 0.00011054900602175622\n",
      "Training Loss: 0.00010263931004374172\n",
      "Validation Loss: 0.00011884567172958495\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.00011170152459271776\n",
      "Training Loss: 0.00011033884955395479\n",
      "Training Loss: 0.00010242169288176229\n",
      "Validation Loss: 0.000118613626541906\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.00011147628891194472\n",
      "Training Loss: 0.00011013029556124821\n",
      "Training Loss: 0.0001022052493681258\n",
      "Validation Loss: 0.00011838253074406719\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0001112523904430418\n",
      "Training Loss: 0.00010992271310897195\n",
      "Training Loss: 0.00010199038755672519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [22:18<15:30, 232.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.00011815246612927746\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.423232104703784\n",
      "Training Loss: 0.2679158175364137\n",
      "Training Loss: 0.1746002635359764\n",
      "Validation Loss: 0.09651752176244607\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.08018592119216919\n",
      "Training Loss: 0.05744230300188065\n",
      "Training Loss: 0.05821157587692141\n",
      "Validation Loss: 0.05550269796146771\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.05293436213396489\n",
      "Training Loss: 0.049725236780941484\n",
      "Training Loss: 0.05169403394684195\n",
      "Validation Loss: 0.050425681044881265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.04773036615923047\n",
      "Training Loss: 0.044319221135228876\n",
      "Training Loss: 0.04534483583644033\n",
      "Validation Loss: 0.043437524060352464\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.04069655813742429\n",
      "Training Loss: 0.03707318820524961\n",
      "Training Loss: 0.037343807984143496\n",
      "Validation Loss: 0.035115000127364744\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.032446577097289264\n",
      "Training Loss: 0.028956922837533056\n",
      "Training Loss: 0.028770283441990615\n",
      "Validation Loss: 0.02672342268038499\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.024209814148489387\n",
      "Training Loss: 0.02119978182716295\n",
      "Training Loss: 0.020861397359985857\n",
      "Validation Loss: 0.019450244648130926\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.017120997515739873\n",
      "Training Loss: 0.014850009836954996\n",
      "Training Loss: 0.014616015336941927\n",
      "Validation Loss: 0.014118494043451096\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.01198126186063746\n",
      "Training Loss: 0.010552221110556275\n",
      "Training Loss: 0.010547646830091253\n",
      "Validation Loss: 0.010890735333570897\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.008952506015775725\n",
      "Training Loss: 0.008235489068320022\n",
      "Training Loss: 0.008392419236479327\n",
      "Validation Loss: 0.009160510706453679\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.007444939339256962\n",
      "Training Loss: 0.007159191294340417\n",
      "Training Loss: 0.007331897427793593\n",
      "Validation Loss: 0.008101648458781956\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.006636528049129993\n",
      "Training Loss: 0.006553605649387464\n",
      "Training Loss: 0.00667691801325418\n",
      "Validation Loss: 0.007265680058848824\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.006052159765749821\n",
      "Training Loss: 0.00606736347079277\n",
      "Training Loss: 0.006150818857131526\n",
      "Validation Loss: 0.006548555337646035\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.00555744055120158\n",
      "Training Loss: 0.005631643171655014\n",
      "Training Loss: 0.005694183933082968\n",
      "Validation Loss: 0.005942393145314679\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.005129828719072975\n",
      "Training Loss: 0.005244826836278662\n",
      "Training Loss: 0.005296074902871623\n",
      "Validation Loss: 0.005440132968452121\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.004761563604552066\n",
      "Training Loss: 0.004906655073282309\n",
      "Training Loss: 0.0049514585791621355\n",
      "Validation Loss: 0.005028596477520265\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.004446378656648448\n",
      "Training Loss: 0.004614711424801498\n",
      "Training Loss: 0.004657103191129863\n",
      "Validation Loss: 0.004694854889687653\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.004180151898472104\n",
      "Training Loss: 0.004367088215076365\n",
      "Training Loss: 0.004410861034411937\n",
      "Validation Loss: 0.004428518272863094\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.0039599097758764405\n",
      "Training Loss: 0.0041619079932570455\n",
      "Training Loss: 0.004209570850944147\n",
      "Validation Loss: 0.004219579493386273\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.0037811389325361233\n",
      "Training Loss: 0.003995121822226793\n",
      "Training Loss: 0.0040470889204880225\n",
      "Validation Loss: 0.004056182649742624\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.0036363860571873375\n",
      "Training Loss: 0.003859706907533109\n",
      "Training Loss: 0.003914712113619316\n",
      "Validation Loss: 0.00392577535076475\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0035168090650404336\n",
      "Training Loss: 0.003747399827116169\n",
      "Training Loss: 0.0038036509495577775\n",
      "Validation Loss: 0.0038180469768775856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.003414739659638144\n",
      "Training Loss: 0.003650977384822909\n",
      "Training Loss: 0.003706975918612443\n",
      "Validation Loss: 0.0037260325298529495\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.003324758409435162\n",
      "Training Loss: 0.003565186437917873\n",
      "Training Loss: 0.0036199361670878716\n",
      "Validation Loss: 0.0036451574791029223\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0032432887230243066\n",
      "Training Loss: 0.0034865197562612593\n",
      "Training Loss: 0.003539398454595357\n",
      "Validation Loss: 0.0035721493117757182\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.0031678802719397937\n",
      "Training Loss: 0.0034126585751073435\n",
      "Training Loss: 0.00346324592246674\n",
      "Validation Loss: 0.0035045392310320074\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.0030967687256634236\n",
      "Training Loss: 0.0033420302873128096\n",
      "Training Loss: 0.003389988455164712\n",
      "Validation Loss: 0.0034404413053881\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.0030286363929189976\n",
      "Training Loss: 0.0032734993088524788\n",
      "Training Loss: 0.0033185322387726045\n",
      "Validation Loss: 0.0033783871733473634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.002962484751769807\n",
      "Training Loss: 0.0032062067452352495\n",
      "Training Loss: 0.0032480524739366954\n",
      "Validation Loss: 0.0033172517341268615\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.002897551129353815\n",
      "Training Loss: 0.0031394972061389126\n",
      "Training Loss: 0.003177935517160222\n",
      "Validation Loss: 0.003256186296485804\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.002833268021240656\n",
      "Training Loss: 0.0030728842518874446\n",
      "Training Loss: 0.003107743150321767\n",
      "Validation Loss: 0.003194603149444283\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.002769238365726778\n",
      "Training Loss: 0.0030060322475037537\n",
      "Training Loss: 0.0030371975695015862\n",
      "Validation Loss: 0.0031321239363391665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.002705219860581565\n",
      "Training Loss: 0.0029387558030430226\n",
      "Training Loss: 0.002966179768263828\n",
      "Validation Loss: 0.0030685830795928164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.0026411210891092197\n",
      "Training Loss: 0.002871018401638139\n",
      "Training Loss: 0.0028947191723273135\n",
      "Validation Loss: 0.003003981507257715\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.0025769826906253002\n",
      "Training Loss: 0.00280291588511318\n",
      "Training Loss: 0.002822979267803021\n",
      "Validation Loss: 0.002938477310686885\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.002512963154294994\n",
      "Training Loss: 0.0027346642926568167\n",
      "Training Loss: 0.002751241646474227\n",
      "Validation Loss: 0.0028723460645676494\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0024493118757891352\n",
      "Training Loss: 0.0026665757849696093\n",
      "Training Loss: 0.002679870963329449\n",
      "Validation Loss: 0.002805942462840944\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.002386338800606609\n",
      "Training Loss: 0.0025990214160992765\n",
      "Training Loss: 0.002609283275960479\n",
      "Validation Loss: 0.0027396677076957825\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.0023243851808365436\n",
      "Training Loss: 0.002532396574388258\n",
      "Training Loss: 0.0025399067852413283\n",
      "Validation Loss: 0.002673927082724639\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.0022637856379151343\n",
      "Training Loss: 0.002467091067810543\n",
      "Training Loss: 0.0024721460675937125\n",
      "Validation Loss: 0.0026090936516176156\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.0022048434155294673\n",
      "Training Loss: 0.0024034507812757512\n",
      "Training Loss: 0.002406353974074591\n",
      "Validation Loss: 0.002545489809604027\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.002147805851564044\n",
      "Training Loss: 0.002341757317626616\n",
      "Training Loss: 0.0023428017363767138\n",
      "Validation Loss: 0.0024833577813245774\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0020928493998508203\n",
      "Training Loss: 0.0022822094184812157\n",
      "Training Loss: 0.0022816725220764058\n",
      "Validation Loss: 0.0024228564923330933\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.002040069200375001\n",
      "Training Loss: 0.0022249183112580794\n",
      "Training Loss: 0.0022230495110852643\n",
      "Validation Loss: 0.002364055781685381\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.0019894830574048684\n",
      "Training Loss: 0.002169902031891979\n",
      "Training Loss: 0.002166921422467567\n",
      "Validation Loss: 0.002306953066988754\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.0019410300860181451\n",
      "Training Loss: 0.0021170957655704116\n",
      "Training Loss: 0.002113187308132183\n",
      "Validation Loss: 0.00225146547345094\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0018945833321777172\n",
      "Training Loss: 0.0020663542540569325\n",
      "Training Loss: 0.002061667506059166\n",
      "Validation Loss: 0.0021974517137474075\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0018499562292709016\n",
      "Training Loss: 0.0020174661579949317\n",
      "Training Loss: 0.0020121196207765024\n",
      "Validation Loss: 0.002144711134789166\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.0018069150192604865\n",
      "Training Loss: 0.001970166428218363\n",
      "Training Loss: 0.0019642449356615543\n",
      "Validation Loss: 0.002092999900924042\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.001765182766102953\n",
      "Training Loss: 0.0019241374034027104\n",
      "Training Loss: 0.001917698430333985\n",
      "Validation Loss: 0.002042022579823087\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.0017244445918186103\n",
      "Training Loss: 0.0018790140851342584\n",
      "Training Loss: 0.0018720878509338945\n",
      "Validation Loss: 0.0019914358867374198\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.0016843467860599049\n",
      "Training Loss: 0.0018343840424495284\n",
      "Training Loss: 0.0018269724860147107\n",
      "Validation Loss: 0.00194083205558323\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.0016444908994890284\n",
      "Training Loss: 0.0017897817453194876\n",
      "Training Loss: 0.0017818532054661774\n",
      "Validation Loss: 0.001889729472740522\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.0016044235959998333\n",
      "Training Loss: 0.001744671174819814\n",
      "Training Loss: 0.0017361477292433846\n",
      "Validation Loss: 0.0018375441696876764\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.001563615979248425\n",
      "Training Loss: 0.0016984299214527709\n",
      "Training Loss: 0.0016891655167273712\n",
      "Validation Loss: 0.0017835613965708238\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0015214478044072166\n",
      "Training Loss: 0.0016503285950602731\n",
      "Training Loss: 0.0016400733309274074\n",
      "Validation Loss: 0.001726899801111215\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0014771910131094045\n",
      "Training Loss: 0.0015995263820514083\n",
      "Training Loss: 0.0015878800541395321\n",
      "Validation Loss: 0.0016664928458094744\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.0014300196745898575\n",
      "Training Loss: 0.0015451077546458692\n",
      "Training Loss: 0.0015314731466060038\n",
      "Validation Loss: 0.001601137349701204\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.001379099298210349\n",
      "Training Loss: 0.0014862334976351122\n",
      "Training Loss: 0.0014698217539989855\n",
      "Validation Loss: 0.001529750404218892\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.0013238268582063029\n",
      "Training Loss: 0.0014224789776926627\n",
      "Training Loss: 0.0014024399782647378\n",
      "Validation Loss: 0.0014519354889661158\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.0012642383205820806\n",
      "Training Loss: 0.0013542828151548746\n",
      "Training Loss: 0.0013299484954040963\n",
      "Validation Loss: 0.0013686700582143255\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.0012012619376037038\n",
      "Training Loss: 0.0012830441278492799\n",
      "Training Loss: 0.0012540593855374026\n",
      "Validation Loss: 0.0012822558493006796\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.0011363130611425731\n",
      "Training Loss: 0.001210450110520469\n",
      "Training Loss: 0.0011766181366692763\n",
      "Validation Loss: 0.0011951868024032062\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.001070477956236573\n",
      "Training Loss: 0.0011376258554810193\n",
      "Training Loss: 0.0010987569549615727\n",
      "Validation Loss: 0.0011092218204324259\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.001004304513844545\n",
      "Training Loss: 0.0010650945638190023\n",
      "Training Loss: 0.0010211580773466266\n",
      "Validation Loss: 0.001025530258752798\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.0009382757984712953\n",
      "Training Loss: 0.0009933063586868229\n",
      "Training Loss: 0.000944686172369984\n",
      "Validation Loss: 0.000945132945417662\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0008731402366538532\n",
      "Training Loss: 0.0009228984957007924\n",
      "Training Loss: 0.0008704606849642005\n",
      "Validation Loss: 0.0008688999105799483\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0008097636104139383\n",
      "Training Loss: 0.0008544938550403458\n",
      "Training Loss: 0.0007994537084596232\n",
      "Validation Loss: 0.0007972549417368764\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0007487769557337743\n",
      "Training Loss: 0.0007883728442175198\n",
      "Training Loss: 0.0007320913349394687\n",
      "Validation Loss: 0.0007300251350755112\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0006903962851356482\n",
      "Training Loss: 0.0007244489852746483\n",
      "Training Loss: 0.0006682794335210929\n",
      "Validation Loss: 0.000666727990330932\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0006346487359769526\n",
      "Training Loss: 0.0006626699299522443\n",
      "Training Loss: 0.000607806848056498\n",
      "Validation Loss: 0.0006070544575654975\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.0005817281909185112\n",
      "Training Loss: 0.0006033573120657821\n",
      "Training Loss: 0.0005505858549076947\n",
      "Validation Loss: 0.0005509642898605409\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.0005319726218658616\n",
      "Training Loss: 0.0005469986033858731\n",
      "Training Loss: 0.0004965242577236495\n",
      "Validation Loss: 0.0004985383804544173\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.0004856609812850365\n",
      "Training Loss: 0.0004941021547710989\n",
      "Training Loss: 0.00044614035705308196\n",
      "Validation Loss: 0.0004508244245688515\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.0004433948643054464\n",
      "Training Loss: 0.00044580475318070964\n",
      "Training Loss: 0.0004011459410685347\n",
      "Validation Loss: 0.0004092431469023191\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00040568948825239206\n",
      "Training Loss: 0.0004031906531054119\n",
      "Training Loss: 0.0003623238633372239\n",
      "Validation Loss: 0.0003738296528466522\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.0003727684589466662\n",
      "Training Loss: 0.00036662802507635205\n",
      "Training Loss: 0.0003295464524126146\n",
      "Validation Loss: 0.0003441413047787566\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00034477988419894244\n",
      "Training Loss: 0.0003360442247139872\n",
      "Training Loss: 0.0003025114582487731\n",
      "Validation Loss: 0.0003196198832596197\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.0003215524988900143\n",
      "Training Loss: 0.0003110157132687164\n",
      "Training Loss: 0.00028067726980225417\n",
      "Validation Loss: 0.0002995715811348316\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.0003025821590608757\n",
      "Training Loss: 0.00029081764187139926\n",
      "Training Loss: 0.00026326532646635313\n",
      "Validation Loss: 0.0002832313304393568\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.00028717601556081716\n",
      "Training Loss: 0.0002745803681318648\n",
      "Training Loss: 0.00024940733957919294\n",
      "Validation Loss: 0.0002698554609002273\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0002746161246250267\n",
      "Training Loss: 0.0002614596143757808\n",
      "Training Loss: 0.00023828961853723742\n",
      "Validation Loss: 0.00025878938416436345\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.00026426286533023815\n",
      "Training Loss: 0.0002507308169151656\n",
      "Training Loss: 0.00022922809821466217\n",
      "Validation Loss: 0.0002494921248940505\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.00025559109928963153\n",
      "Training Loss: 0.00024181385935662548\n",
      "Training Loss: 0.00022168461578985442\n",
      "Validation Loss: 0.00024153844788088463\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.0002481911670065529\n",
      "Training Loss: 0.0002342612088250462\n",
      "Training Loss: 0.00021525390024180525\n",
      "Validation Loss: 0.00023460250651644377\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.00024175169204909252\n",
      "Training Loss: 0.00022773619093641173\n",
      "Training Loss: 0.0002096386881748913\n",
      "Validation Loss: 0.00022844390508233506\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.00023603945237937296\n",
      "Training Loss: 0.00022198850128916093\n",
      "Training Loss: 0.00020462349293666194\n",
      "Validation Loss: 0.00022288533298787922\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.00023088321263685428\n",
      "Training Loss: 0.00021683316095732152\n",
      "Training Loss: 0.00020005494650831678\n",
      "Validation Loss: 0.00021779772238884372\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.0002261545991973435\n",
      "Training Loss: 0.00021213368247117615\n",
      "Training Loss: 0.00019582213077228515\n",
      "Validation Loss: 0.00021308600962890593\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.00022175936778921822\n",
      "Training Loss: 0.0002077902353630634\n",
      "Training Loss: 0.00019184619195584675\n",
      "Validation Loss: 0.00020868177409610826\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.0002176279100103784\n",
      "Training Loss: 0.0002037284024481778\n",
      "Training Loss: 0.0001880700219408027\n",
      "Validation Loss: 0.00020453330282098353\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0002137069819741555\n",
      "Training Loss: 0.00019989247804915066\n",
      "Training Loss: 0.00018445231558871455\n",
      "Validation Loss: 0.00020060111425020417\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.00020995800321543356\n",
      "Training Loss: 0.00019624209435278316\n",
      "Training Loss: 0.00018096247085850337\n",
      "Validation Loss: 0.00019685523761070093\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.00020635106332292708\n",
      "Training Loss: 0.00019274619560746942\n",
      "Training Loss: 0.0001775788829945668\n",
      "Validation Loss: 0.00019327066134853718\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.00020286401512748854\n",
      "Training Loss: 0.0001893820270743163\n",
      "Training Loss: 0.0001742859407204378\n",
      "Validation Loss: 0.00018983050536467653\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.0001994810451878948\n",
      "Training Loss: 0.00018613304973769118\n",
      "Training Loss: 0.00017107269384723624\n",
      "Validation Loss: 0.00018651916485559717\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.00019619037951997597\n",
      "Training Loss: 0.0001829870545770973\n",
      "Training Loss: 0.00016793224054708844\n",
      "Validation Loss: 0.0001833258707908877\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.0001929836205272295\n",
      "Training Loss: 0.00017993598063185346\n",
      "Training Loss: 0.0001648603748617461\n",
      "Validation Loss: 0.00018024062052737986\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.00018985546746080217\n",
      "Training Loss: 0.0001769741258431168\n",
      "Training Loss: 0.00016185565256819246\n",
      "Validation Loss: 0.0001772569110144513\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.0001868027717637233\n",
      "Training Loss: 0.0001740988823075895\n",
      "Training Loss: 0.00015891786204520032\n",
      "Validation Loss: 0.00017436865975591745\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00018382376851150184\n",
      "Training Loss: 0.0001713080664012523\n",
      "Training Loss: 0.00015604875097778858\n",
      "Validation Loss: 0.00017157188506572628\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.00018091882219323452\n",
      "Training Loss: 0.00016860241998074344\n",
      "Training Loss: 0.00015325141457651626\n",
      "Validation Loss: 0.00016886216201711566\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.00017808838662858762\n",
      "Training Loss: 0.00016598239593804465\n",
      "Training Loss: 0.00015052881790325045\n",
      "Validation Loss: 0.00016623749536265269\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.00017533405534777557\n",
      "Training Loss: 0.00016344887524610385\n",
      "Training Loss: 0.00014788462563956274\n",
      "Validation Loss: 0.00016369468227075935\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.00017265702079384936\n",
      "Training Loss: 0.0001610037114005536\n",
      "Training Loss: 0.00014532295021126628\n",
      "Validation Loss: 0.00016123199650005363\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0001700593311124976\n",
      "Training Loss: 0.0001586482674247236\n",
      "Training Loss: 0.00014284710439824266\n",
      "Validation Loss: 0.00015884656842102773\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.00016754200242758088\n",
      "Training Loss: 0.00015638299550118972\n",
      "Training Loss: 0.00014045996798813575\n",
      "Validation Loss: 0.0001565367347211577\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.00016510623486283295\n",
      "Training Loss: 0.0001542083962885954\n",
      "Training Loss: 0.00013816358359690638\n",
      "Validation Loss: 0.0001542991431664775\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.000162752093265226\n",
      "Training Loss: 0.0001521233416860923\n",
      "Training Loss: 0.00013595894763057003\n",
      "Validation Loss: 0.00015213115953332143\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.00016047903627622873\n",
      "Training Loss: 0.00015012687541457126\n",
      "Training Loss: 0.00013384620902797905\n",
      "Validation Loss: 0.0001500299234506334\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.00015828613074518217\n",
      "Training Loss: 0.0001482163907166978\n",
      "Training Loss: 0.00013182443379264441\n",
      "Validation Loss: 0.00014799229113245774\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0001561711276644928\n",
      "Training Loss: 0.0001463885298653622\n",
      "Training Loss: 0.00012989167204068507\n",
      "Validation Loss: 0.0001460145037700396\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.00015413174664899998\n",
      "Training Loss: 0.00014463939845882123\n",
      "Training Loss: 0.00012804490039343365\n",
      "Validation Loss: 0.00014409376071240187\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.00015216532256090431\n",
      "Training Loss: 0.00014296449569883407\n",
      "Training Loss: 0.00012628066571778617\n",
      "Validation Loss: 0.00014222693783502496\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.0001502675482788618\n",
      "Training Loss: 0.0001413585408772633\n",
      "Training Loss: 0.0001245946970084333\n",
      "Validation Loss: 0.00014040917939679132\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.00014843437576018915\n",
      "Training Loss: 0.0001398160174358054\n",
      "Training Loss: 0.00012298188026761637\n",
      "Validation Loss: 0.0001386381055055424\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.0001466617608184606\n",
      "Training Loss: 0.0001383318248372234\n",
      "Training Loss: 0.00012143757830926916\n",
      "Validation Loss: 0.000136910263147124\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.00014494555838609813\n",
      "Training Loss: 0.0001368998471662053\n",
      "Training Loss: 0.00011995642109468462\n",
      "Validation Loss: 0.00013522184448181966\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.00014328089198897942\n",
      "Training Loss: 0.00013551505604482374\n",
      "Training Loss: 0.00011853310717924615\n",
      "Validation Loss: 0.00013357150010562507\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.0001416636909925728\n",
      "Training Loss: 0.00013417192863926175\n",
      "Training Loss: 0.00011716268540112651\n",
      "Validation Loss: 0.00013195586441806778\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.00014008963616106483\n",
      "Training Loss: 0.00013286548497490002\n",
      "Training Loss: 0.00011584023570321734\n",
      "Validation Loss: 0.00013037196247219735\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.00013855405960384816\n",
      "Training Loss: 0.00013159117148461518\n",
      "Training Loss: 0.00011456059097326943\n",
      "Validation Loss: 0.00012881725040111386\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0001370534882789798\n",
      "Training Loss: 0.00013034420328040142\n",
      "Training Loss: 0.00011331932213579421\n",
      "Validation Loss: 0.00012728913974201992\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.00013558374549575093\n",
      "Training Loss: 0.0001291205404959328\n",
      "Training Loss: 0.00011211229299078696\n",
      "Validation Loss: 0.00012578587431174158\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.00013414110528174207\n",
      "Training Loss: 0.00012791614271918662\n",
      "Training Loss: 0.00011093517008703202\n",
      "Validation Loss: 0.00012430436524145274\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00013272168273942953\n",
      "Training Loss: 0.0001267274510428251\n",
      "Training Loss: 0.00010978413271004683\n",
      "Validation Loss: 0.00012284450496552156\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0001313228084836737\n",
      "Training Loss: 0.00012555093544506236\n",
      "Training Loss: 0.00010865584265047801\n",
      "Validation Loss: 0.00012140247133757338\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.0001299408044815209\n",
      "Training Loss: 0.00012438334137186756\n",
      "Training Loss: 0.00010754671466202126\n",
      "Validation Loss: 0.00011997670364426188\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00012857227898166457\n",
      "Training Loss: 0.00012322190985287307\n",
      "Training Loss: 0.00010645336320521892\n",
      "Validation Loss: 0.00011856591405130837\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.00012721475561647822\n",
      "Training Loss: 0.00012206343841171474\n",
      "Training Loss: 0.00010537301252043107\n",
      "Validation Loss: 0.0001171676230002417\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.0001258652849446662\n",
      "Training Loss: 0.00012090541497855156\n",
      "Training Loss: 0.000104302562376688\n",
      "Validation Loss: 0.0001157810902032508\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00012452104069780035\n",
      "Training Loss: 0.00011974490046668507\n",
      "Training Loss: 0.00010323985370632726\n",
      "Validation Loss: 0.00011440492402103251\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.00012317984977926243\n",
      "Training Loss: 0.00011858005291287554\n",
      "Training Loss: 0.00010218176545095048\n",
      "Validation Loss: 0.00011303764289693666\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.00012183887766695988\n",
      "Training Loss: 0.00011740830736926\n",
      "Training Loss: 0.00010112626312547946\n",
      "Validation Loss: 0.00011167834523558065\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.00012049632761772954\n",
      "Training Loss: 0.00011622751601862546\n",
      "Training Loss: 0.00010007130935264285\n",
      "Validation Loss: 0.00011032687157903945\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.00011915049399704003\n",
      "Training Loss: 0.00011503587697916373\n",
      "Training Loss: 9.901517260004766e-05\n",
      "Validation Loss: 0.00010898308709727828\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00011779970051748023\n",
      "Training Loss: 0.0001138323141458386\n",
      "Training Loss: 9.795618725547684e-05\n",
      "Validation Loss: 0.00010764697809678142\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.00011644282523320726\n",
      "Training Loss: 0.00011261534239565663\n",
      "Training Loss: 9.689319859717216e-05\n",
      "Validation Loss: 0.00010631918374794337\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.0001150793253646043\n",
      "Training Loss: 0.00011138479448163707\n",
      "Training Loss: 9.582563683579792e-05\n",
      "Validation Loss: 0.00010500117943511316\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.0001137091336568119\n",
      "Training Loss: 0.00011014036532287719\n",
      "Training Loss: 9.475359951466089e-05\n",
      "Validation Loss: 0.00010369462864453842\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.00011233312564058906\n",
      "Training Loss: 0.00010888308343965037\n",
      "Training Loss: 9.367737131469766e-05\n",
      "Validation Loss: 0.00010240211994120696\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.00011095244277726124\n",
      "Training Loss: 0.00010761425242890255\n",
      "Training Loss: 9.259841282982962e-05\n",
      "Validation Loss: 0.00010112756878156228\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00010956992838146107\n",
      "Training Loss: 0.00010633640171363368\n",
      "Training Loss: 9.151873127848376e-05\n",
      "Validation Loss: 9.987385105938465e-05\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.0001081882150583624\n",
      "Training Loss: 0.00010505308974643412\n",
      "Training Loss: 9.044115719007095e-05\n",
      "Validation Loss: 9.864628638874667e-05\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.00010681217317369373\n",
      "Training Loss: 0.00010376861283475591\n",
      "Training Loss: 8.936919769439555e-05\n",
      "Validation Loss: 9.745063299520512e-05\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.00010544651962959506\n",
      "Training Loss: 0.00010248831412354776\n",
      "Training Loss: 8.830720207697595e-05\n",
      "Validation Loss: 9.62921219448372e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.00010409715781975137\n",
      "Training Loss: 0.00010121814918420568\n",
      "Training Loss: 8.726020450922078e-05\n",
      "Validation Loss: 9.51771462087492e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.00010277016546979212\n",
      "Training Loss: 9.996442116971593e-05\n",
      "Training Loss: 8.623274845376728e-05\n",
      "Validation Loss: 9.411093119166927e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.00010147190750330992\n",
      "Training Loss: 9.873354841147375e-05\n",
      "Training Loss: 8.522986471689365e-05\n",
      "Validation Loss: 9.30976506260981e-05\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.00010020798369623662\n",
      "Training Loss: 9.75317832944711e-05\n",
      "Training Loss: 8.425605746197107e-05\n",
      "Validation Loss: 9.214241704744943e-05\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 9.898425411847712e-05\n",
      "Training Loss: 9.636482131099911e-05\n",
      "Training Loss: 8.331508299306733e-05\n",
      "Validation Loss: 9.124574529103872e-05\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 9.780418653917877e-05\n",
      "Training Loss: 9.523722325866402e-05\n",
      "Training Loss: 8.240992145147174e-05\n",
      "Validation Loss: 9.040768090313434e-05\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 9.66714170499472e-05\n",
      "Training Loss: 9.415272648766404e-05\n",
      "Training Loss: 8.154275545166456e-05\n",
      "Validation Loss: 8.962784393976986e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 9.558754896715982e-05\n",
      "Training Loss: 9.311348030678346e-05\n",
      "Training Loss: 8.071455615208833e-05\n",
      "Validation Loss: 8.890345035591085e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 9.455325689486927e-05\n",
      "Training Loss: 9.212102554556623e-05\n",
      "Training Loss: 7.992571788236092e-05\n",
      "Validation Loss: 8.822819166126781e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 9.356809603559668e-05\n",
      "Training Loss: 9.117537797465047e-05\n",
      "Training Loss: 7.917547371107503e-05\n",
      "Validation Loss: 8.759818734851956e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 9.263064298465906e-05\n",
      "Training Loss: 9.027595207953709e-05\n",
      "Training Loss: 7.846276374948502e-05\n",
      "Validation Loss: 8.70073756661359e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 9.173916001600446e-05\n",
      "Training Loss: 8.942110066527676e-05\n",
      "Training Loss: 7.778606512147235e-05\n",
      "Validation Loss: 8.645058407249084e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 9.089096189200063e-05\n",
      "Training Loss: 8.860922110216052e-05\n",
      "Training Loss: 7.714314093391294e-05\n",
      "Validation Loss: 8.592235745923931e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 9.008357466882443e-05\n",
      "Training Loss: 8.783756369666662e-05\n",
      "Training Loss: 7.653226669390278e-05\n",
      "Validation Loss: 8.541712019465971e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 8.931424729780702e-05\n",
      "Training Loss: 8.710374017027789e-05\n",
      "Training Loss: 7.595072974254435e-05\n",
      "Validation Loss: 8.493122584879754e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 8.858003337081755e-05\n",
      "Training Loss: 8.640496767839067e-05\n",
      "Training Loss: 7.539674425970589e-05\n",
      "Validation Loss: 8.445944552201244e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 8.787812926129846e-05\n",
      "Training Loss: 8.573876438276784e-05\n",
      "Training Loss: 7.486790038456092e-05\n",
      "Validation Loss: 8.39999049296668e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 8.720627313778095e-05\n",
      "Training Loss: 8.510246839250612e-05\n",
      "Training Loss: 7.436209265506478e-05\n",
      "Validation Loss: 8.354945890623435e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 8.65617672116059e-05\n",
      "Training Loss: 8.44935231089039e-05\n",
      "Training Loss: 7.387737614408251e-05\n",
      "Validation Loss: 8.310534402870656e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 8.594254728450324e-05\n",
      "Training Loss: 8.39094356069836e-05\n",
      "Training Loss: 7.341185757468338e-05\n",
      "Validation Loss: 8.266701059439808e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 8.534636696822418e-05\n",
      "Training Loss: 8.334824308803946e-05\n",
      "Training Loss: 7.296397772734054e-05\n",
      "Validation Loss: 8.22332891872842e-05\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 8.477165905787843e-05\n",
      "Training Loss: 8.280799087515334e-05\n",
      "Training Loss: 7.253166704686009e-05\n",
      "Validation Loss: 8.180275902335514e-05\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 8.421655606070999e-05\n",
      "Training Loss: 8.228661020439177e-05\n",
      "Training Loss: 7.21139804500126e-05\n",
      "Validation Loss: 8.137438000855755e-05\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 8.367927805920772e-05\n",
      "Training Loss: 8.178231084457366e-05\n",
      "Training Loss: 7.170942624725285e-05\n",
      "Validation Loss: 8.094817214155585e-05\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 8.315869369653228e-05\n",
      "Training Loss: 8.12939875140728e-05\n",
      "Training Loss: 7.131651541385509e-05\n",
      "Validation Loss: 8.052371997743312e-05\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 8.265351829322753e-05\n",
      "Training Loss: 8.081994291387673e-05\n",
      "Training Loss: 7.093468992025009e-05\n",
      "Validation Loss: 8.010098313053696e-05\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 8.216249733777658e-05\n",
      "Training Loss: 8.035903871132177e-05\n",
      "Training Loss: 7.05625137561583e-05\n",
      "Validation Loss: 7.967944898308395e-05\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 8.168466618371895e-05\n",
      "Training Loss: 7.991012395905272e-05\n",
      "Training Loss: 7.019928469162551e-05\n",
      "Validation Loss: 7.925957772917417e-05\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 8.121901601953141e-05\n",
      "Training Loss: 7.947237192638567e-05\n",
      "Training Loss: 6.984444466070273e-05\n",
      "Validation Loss: 7.884106255230281e-05\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 8.076473563050967e-05\n",
      "Training Loss: 7.904490983491997e-05\n",
      "Training Loss: 6.949695212824735e-05\n",
      "Validation Loss: 7.842403171166092e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 8.032111485590576e-05\n",
      "Training Loss: 7.86269708260079e-05\n",
      "Training Loss: 6.915644869877724e-05\n",
      "Validation Loss: 7.800928314067318e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 7.988742375346192e-05\n",
      "Training Loss: 7.821768525900552e-05\n",
      "Training Loss: 6.882219264298328e-05\n",
      "Validation Loss: 7.759641229814008e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 7.946298483147984e-05\n",
      "Training Loss: 7.781659624015447e-05\n",
      "Training Loss: 6.849383146800391e-05\n",
      "Validation Loss: 7.718604209596093e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 7.904737727585597e-05\n",
      "Training Loss: 7.742320373836264e-05\n",
      "Training Loss: 6.817100374064467e-05\n",
      "Validation Loss: 7.677699424798295e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 7.86401207005838e-05\n",
      "Training Loss: 7.70368820940348e-05\n",
      "Training Loss: 6.785330802813405e-05\n",
      "Validation Loss: 7.637113315703052e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 7.824062487998162e-05\n",
      "Training Loss: 7.66575141915382e-05\n",
      "Training Loss: 6.754028433533676e-05\n",
      "Validation Loss: 7.596762679895619e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 7.78486302078818e-05\n",
      "Training Loss: 7.628446548551437e-05\n",
      "Training Loss: 6.723185561895662e-05\n",
      "Validation Loss: 7.5566601961409e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 7.746369827145827e-05\n",
      "Training Loss: 7.591770155158883e-05\n",
      "Training Loss: 6.692766153719276e-05\n",
      "Validation Loss: 7.51691774863768e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 7.70856572125922e-05\n",
      "Training Loss: 7.555682918791717e-05\n",
      "Training Loss: 6.662769683316583e-05\n",
      "Validation Loss: 7.477458059971529e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 7.6714180631825e-05\n",
      "Training Loss: 7.52016409933276e-05\n",
      "Training Loss: 6.633167253312421e-05\n",
      "Validation Loss: 7.438342571108121e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 7.634886108462524e-05\n",
      "Training Loss: 7.485210345294035e-05\n",
      "Training Loss: 6.603922633075853e-05\n",
      "Validation Loss: 7.399578759320188e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 7.598988841891696e-05\n",
      "Training Loss: 7.450778372913191e-05\n",
      "Training Loss: 6.575058861017168e-05\n",
      "Validation Loss: 7.361145008269049e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 7.563667547401565e-05\n",
      "Training Loss: 7.416870835186273e-05\n",
      "Training Loss: 6.546561263348849e-05\n",
      "Validation Loss: 7.323069103528795e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 7.528925559199706e-05\n",
      "Training Loss: 7.383478204246785e-05\n",
      "Training Loss: 6.518399536616925e-05\n",
      "Validation Loss: 7.28541585937073e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 7.49474905023817e-05\n",
      "Training Loss: 7.350597988988738e-05\n",
      "Training Loss: 6.490583418781171e-05\n",
      "Validation Loss: 7.248127597480296e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 7.461104443791555e-05\n",
      "Training Loss: 7.31821871795546e-05\n",
      "Training Loss: 6.463124948822951e-05\n",
      "Validation Loss: 7.211260797478471e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 7.428002645610831e-05\n",
      "Training Loss: 7.286312803444162e-05\n",
      "Training Loss: 6.435982829316344e-05\n",
      "Validation Loss: 7.174789221975965e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 7.395426626317204e-05\n",
      "Training Loss: 7.254905051922833e-05\n",
      "Training Loss: 6.409181630260718e-05\n",
      "Validation Loss: 7.138756855771604e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 7.363374726992333e-05\n",
      "Training Loss: 7.223978379442996e-05\n",
      "Training Loss: 6.382726324773102e-05\n",
      "Validation Loss: 7.103117084707162e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 7.331822703235958e-05\n",
      "Training Loss: 7.193516988991177e-05\n",
      "Training Loss: 6.356601464176493e-05\n",
      "Validation Loss: 7.06797645864171e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 7.300782949641871e-05\n",
      "Training Loss: 7.163555462284421e-05\n",
      "Training Loss: 6.33079914314294e-05\n",
      "Validation Loss: 7.033284506750727e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 7.270239747413143e-05\n",
      "Training Loss: 7.134052386390976e-05\n",
      "Training Loss: 6.305325147877739e-05\n",
      "Validation Loss: 6.999031677296035e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 7.240190107040689e-05\n",
      "Training Loss: 7.105046369360934e-05\n",
      "Training Loss: 6.280176199652488e-05\n",
      "Validation Loss: 6.965238106899091e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 7.210626177766244e-05\n",
      "Training Loss: 7.076479948409542e-05\n",
      "Training Loss: 6.255368502934288e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [26:29<11:55, 238.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6.931897906326228e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.22060230936855077\n",
      "Training Loss: 0.1715247732400894\n",
      "Training Loss: 0.1376078696642071\n",
      "Validation Loss: 0.09530612466113872\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.07505607400089502\n",
      "Training Loss: 0.051576787314843386\n",
      "Training Loss: 0.0476211242377758\n",
      "Validation Loss: 0.04424179236540634\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.04181766632013023\n",
      "Training Loss: 0.037409441547933964\n",
      "Training Loss: 0.03756043422035873\n",
      "Validation Loss: 0.03497564664968614\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.03277952701319009\n",
      "Training Loss: 0.029006605508038776\n",
      "Training Loss: 0.028700713869184255\n",
      "Validation Loss: 0.02628837743240377\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.024379447251849342\n",
      "Training Loss: 0.021300372702535243\n",
      "Training Loss: 0.02082989983027801\n",
      "Validation Loss: 0.0188451243546174\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.01729486898126197\n",
      "Training Loss: 0.015091920592822135\n",
      "Training Loss: 0.014753426490351558\n",
      "Validation Loss: 0.013439085848598082\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.012240360284340567\n",
      "Training Loss: 0.010933330447878689\n",
      "Training Loss: 0.01081822412321344\n",
      "Validation Loss: 0.010128937386959958\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.00917881777480943\n",
      "Training Loss: 0.008532143094344065\n",
      "Training Loss: 0.008548875474371016\n",
      "Validation Loss: 0.008233514995042109\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.0074272320105228575\n",
      "Training Loss: 0.007154060315806419\n",
      "Training Loss: 0.007202389388112351\n",
      "Validation Loss: 0.0070396130407501135\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.0063222606937051755\n",
      "Training Loss: 0.0062320600444218145\n",
      "Training Loss: 0.006265843951841816\n",
      "Validation Loss: 0.006138826413027775\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.005482061710354174\n",
      "Training Loss: 0.005473553722258657\n",
      "Training Loss: 0.005475102692143991\n",
      "Validation Loss: 0.0053513817923415575\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.004724332689074799\n",
      "Training Loss: 0.004751728737028315\n",
      "Training Loss: 0.004725877473247237\n",
      "Validation Loss: 0.004656543191967092\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.004025548757927027\n",
      "Training Loss: 0.004100204691058025\n",
      "Training Loss: 0.004087020584847778\n",
      "Validation Loss: 0.00413378834818605\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.0035001066757831723\n",
      "Training Loss: 0.003631271435879171\n",
      "Training Loss: 0.003634996137116104\n",
      "Validation Loss: 0.0037475224758310014\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.0031446899070579092\n",
      "Training Loss: 0.0033041989931371064\n",
      "Training Loss: 0.00330947061534971\n",
      "Validation Loss: 0.0034341343712543\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.002882386416240479\n",
      "Training Loss: 0.003055019142047968\n",
      "Training Loss: 0.0030571078922366724\n",
      "Validation Loss: 0.0031756629580056315\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.002675459787715226\n",
      "Training Loss: 0.002855417507234961\n",
      "Training Loss: 0.002852768266748171\n",
      "Validation Loss: 0.0029623944111550333\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.0025066820104257203\n",
      "Training Loss: 0.002690630258002784\n",
      "Training Loss: 0.002682032258308027\n",
      "Validation Loss: 0.002783780122565608\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.002364771583888796\n",
      "Training Loss: 0.002549885252083186\n",
      "Training Loss: 0.002533704541856423\n",
      "Validation Loss: 0.002628249075692775\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.0022402171736757735\n",
      "Training Loss: 0.0024236593401292337\n",
      "Training Loss: 0.002397913615568541\n",
      "Validation Loss: 0.0024841704560846635\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.0021249620673188473\n",
      "Training Loss: 0.0023038650830858385\n",
      "Training Loss: 0.00226710894407006\n",
      "Validation Loss: 0.0023427288478967637\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0020139471301808954\n",
      "Training Loss: 0.0021857007761718703\n",
      "Training Loss: 0.0021381485753227025\n",
      "Validation Loss: 0.002201513116499095\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.0019066654685593676\n",
      "Training Loss: 0.00206964252400212\n",
      "Training Loss: 0.002013654608745128\n",
      "Validation Loss: 0.0020656370629132674\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.0018073052418185397\n",
      "Training Loss: 0.0019614550653204786\n",
      "Training Loss: 0.0019007934193359687\n",
      "Validation Loss: 0.0019444491496123898\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0017218809816404246\n",
      "Training Loss: 0.0018679397653613705\n",
      "Training Loss: 0.0018060249820700846\n",
      "Validation Loss: 0.0018442100006498376\n",
      "Validation Accuracy: 0.21067415730337077\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.0016527635604143144\n",
      "Training Loss: 0.0017907499718421605\n",
      "Training Loss: 0.0017295882382313721\n",
      "Validation Loss: 0.001763243778880811\n",
      "Validation Accuracy: 0.22823033707865167\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.0015962743628188036\n",
      "Training Loss: 0.0017252847000781913\n",
      "Training Loss: 0.0016659414948662743\n",
      "Validation Loss: 0.0016947277431478306\n",
      "Validation Accuracy: 0.22823033707865167\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.001546073205245193\n",
      "Training Loss: 0.0016650072400807403\n",
      "Training Loss: 0.0016083818548941052\n",
      "Validation Loss: 0.0016318667603831927\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.0014969445826136507\n",
      "Training Loss: 0.0016046057904895862\n",
      "Training Loss: 0.0015517071477370336\n",
      "Validation Loss: 0.0015696729776996262\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.0014453039079671726\n",
      "Training Loss: 0.001539894869638374\n",
      "Training Loss: 0.001491968310874654\n",
      "Validation Loss: 0.0015041160726202965\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.0013878970107180067\n",
      "Training Loss: 0.0014663160119380335\n",
      "Training Loss: 0.0014248885076085572\n",
      "Validation Loss: 0.0014300448806914554\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.0013195802234986331\n",
      "Training Loss: 0.0013756599706539418\n",
      "Training Loss: 0.0013412364209943917\n",
      "Validation Loss: 0.0013339625217943987\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.0012256476334005129\n",
      "Training Loss: 0.0012432834631181321\n",
      "Training Loss: 0.0012113950519415085\n",
      "Validation Loss: 0.0011777594870968796\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.001067757049750071\n",
      "Training Loss: 0.0010364546076743863\n",
      "Training Loss: 0.0010256886937713716\n",
      "Validation Loss: 0.0010012667013000007\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.0009007941522577312\n",
      "Training Loss: 0.0008718428973224946\n",
      "Training Loss: 0.0008952748321462423\n",
      "Validation Loss: 0.0008879634324432445\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.0007974994479081943\n",
      "Training Loss: 0.0007717616387526504\n",
      "Training Loss: 0.0008073378147673793\n",
      "Validation Loss: 0.000810420612989025\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0007235954053066962\n",
      "Training Loss: 0.0007006989455476287\n",
      "Training Loss: 0.0007410398227511905\n",
      "Validation Loss: 0.0007571494533795487\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.0006675014483062114\n",
      "Training Loss: 0.0006467817056909552\n",
      "Training Loss: 0.0006887908451608382\n",
      "Validation Loss: 0.0007184847338802673\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.0006232144460227573\n",
      "Training Loss: 0.0006040194784145569\n",
      "Training Loss: 0.0006462382708559744\n",
      "Validation Loss: 0.0006882146284052203\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.0005870248052724492\n",
      "Training Loss: 0.0005688949350223993\n",
      "Training Loss: 0.0006105291596759343\n",
      "Validation Loss: 0.0006625772447467866\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.0005565081547410955\n",
      "Training Loss: 0.0005391577537011471\n",
      "Training Loss: 0.0005797287473251345\n",
      "Validation Loss: 0.0006393764212660836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0005300418346268998\n",
      "Training Loss: 0.000513317384502443\n",
      "Training Loss: 0.0005525233727530577\n",
      "Validation Loss: 0.0006173945576040168\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.000506535720123793\n",
      "Training Loss: 0.0004903751247911714\n",
      "Training Loss: 0.0005280260128347436\n",
      "Validation Loss: 0.0005960113074947044\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0004852613839739206\n",
      "Training Loss: 0.00046966190180683045\n",
      "Training Loss: 0.0005056416601291857\n",
      "Validation Loss: 0.000574970516190456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.000465738096277164\n",
      "Training Loss: 0.0004507291583649931\n",
      "Training Loss: 0.0004849779739743099\n",
      "Validation Loss: 0.0005542110726780132\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.00044765242821540594\n",
      "Training Loss: 0.00043327842715370937\n",
      "Training Loss: 0.00046577668930694924\n",
      "Validation Loss: 0.0005337646903631701\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0004308025635918966\n",
      "Training Loss: 0.00041710575409524607\n",
      "Training Loss: 0.00044786677564843556\n",
      "Validation Loss: 0.0005137103825076379\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0004150579573342839\n",
      "Training Loss: 0.00040206742356531323\n",
      "Training Loss: 0.00043112870895129163\n",
      "Validation Loss: 0.0004941328206155816\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.00040032853029515537\n",
      "Training Loss: 0.00038805598313047086\n",
      "Training Loss: 0.0004154736147029325\n",
      "Validation Loss: 0.0004751005160849243\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.00038654714656331633\n",
      "Training Loss: 0.00037498427598620766\n",
      "Training Loss: 0.00040082597399305087\n",
      "Validation Loss: 0.0004566848209414571\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.00037365947177022465\n",
      "Training Loss: 0.0003627798514935421\n",
      "Training Loss: 0.0003871204191091238\n",
      "Validation Loss: 0.0004389325418905878\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.0003616188809519372\n",
      "Training Loss: 0.0003513830332667567\n",
      "Training Loss: 0.0003742991576291388\n",
      "Validation Loss: 0.0004218990222980619\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.0003503848946456856\n",
      "Training Loss: 0.00034074354844051415\n",
      "Training Loss: 0.00036231166573998054\n",
      "Validation Loss: 0.00040562391872851316\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.00033992208271229176\n",
      "Training Loss: 0.0003308184305933537\n",
      "Training Loss: 0.0003511142876232043\n",
      "Validation Loss: 0.00039014419635759114\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0003301992858359881\n",
      "Training Loss: 0.00032157053050468676\n",
      "Training Loss: 0.00034067032545863184\n",
      "Validation Loss: 0.00037549550903309003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0003211878717229411\n",
      "Training Loss: 0.00031296686902351214\n",
      "Training Loss: 0.00033094656984758333\n",
      "Validation Loss: 0.0003617034238364856\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0003128581998316804\n",
      "Training Loss: 0.0003049738718982553\n",
      "Training Loss: 0.00032191061491175786\n",
      "Validation Loss: 0.00034877899848611427\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.0003051784190029139\n",
      "Training Loss: 0.000297556741606968\n",
      "Training Loss: 0.00031352800273452884\n",
      "Validation Loss: 0.00033672680855784934\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00029811154141498266\n",
      "Training Loss: 0.0002906772415008163\n",
      "Training Loss: 0.0003057604356945376\n",
      "Validation Loss: 0.0003255285759468882\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.00029161640428355896\n",
      "Training Loss: 0.0002842935943044722\n",
      "Training Loss: 0.0002985650909613469\n",
      "Validation Loss: 0.0003151590026799966\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.00028564710250066126\n",
      "Training Loss: 0.0002783607030141866\n",
      "Training Loss: 0.00029189524611865635\n",
      "Validation Loss: 0.00030557783697424405\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.0002801536871629651\n",
      "Training Loss: 0.0002728323672272381\n",
      "Training Loss: 0.00028570052010763904\n",
      "Validation Loss: 0.0002967406277563763\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.00027508587496413386\n",
      "Training Loss: 0.00026766268125356875\n",
      "Training Loss: 0.0002799309463443933\n",
      "Validation Loss: 0.0002885929161186979\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.00027039455340855057\n",
      "Training Loss: 0.00026280758193024666\n",
      "Training Loss: 0.00027453838749352143\n",
      "Validation Loss: 0.00028107756690914855\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0002660321392249898\n",
      "Training Loss: 0.00025822627474553884\n",
      "Training Loss: 0.0002694762308237841\n",
      "Validation Loss: 0.00027413827333117974\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.0002619551961106481\n",
      "Training Loss: 0.0002538821277630632\n",
      "Training Loss: 0.0002647022873497917\n",
      "Validation Loss: 0.0002677191445032354\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.00025812532687268685\n",
      "Training Loss: 0.0002497436665726127\n",
      "Training Loss: 0.00026017955475253983\n",
      "Validation Loss: 0.000261765431787353\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0002545082117285347\n",
      "Training Loss: 0.00024578311778896024\n",
      "Training Loss: 0.00025587554820958756\n",
      "Validation Loss: 0.0002562253520705638\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0002510754237664514\n",
      "Training Loss: 0.00024197841623390558\n",
      "Training Loss: 0.00025176301460305694\n",
      "Validation Loss: 0.0002510545444115604\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0002478022110517486\n",
      "Training Loss: 0.00023831108021113323\n",
      "Training Loss: 0.00024781855438050116\n",
      "Validation Loss: 0.0002462103270210739\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0002446688105192152\n",
      "Training Loss: 0.0002347661884050467\n",
      "Training Loss: 0.0002440234184905421\n",
      "Validation Loss: 0.00024165629816053205\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.0002416581862780731\n",
      "Training Loss: 0.0002313320020402898\n",
      "Training Loss: 0.0002403616814262932\n",
      "Validation Loss: 0.0002373597761903879\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.0002387569054189953\n",
      "Training Loss: 0.0002279983246990014\n",
      "Training Loss: 0.00023682040784478886\n",
      "Validation Loss: 0.00023329187200268666\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.00023595378683239688\n",
      "Training Loss: 0.00022475831043266225\n",
      "Training Loss: 0.0002333893845207058\n",
      "Validation Loss: 0.00022942840149972767\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.00023324010529904627\n",
      "Training Loss: 0.00022160541018820368\n",
      "Training Loss: 0.00023006028699455781\n",
      "Validation Loss: 0.00022574812464881688\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00023060783518303652\n",
      "Training Loss: 0.00021853495120012666\n",
      "Training Loss: 0.0002268260055279825\n",
      "Validation Loss: 0.00022223189963175644\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00022805159369454486\n",
      "Training Loss: 0.00021554288352490403\n",
      "Training Loss: 0.00022368116329744226\n",
      "Validation Loss: 0.00021886513418854173\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00022556597912625875\n",
      "Training Loss: 0.0002126263162426767\n",
      "Training Loss: 0.0002206211346492637\n",
      "Validation Loss: 0.00021563414661665027\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00022314724257739725\n",
      "Training Loss: 0.00020978211316105444\n",
      "Training Loss: 0.0002176419425086351\n",
      "Validation Loss: 0.00021252735522660698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.00022079145539464663\n",
      "Training Loss: 0.00020700811186543432\n",
      "Training Loss: 0.00021474033361300825\n",
      "Validation Loss: 0.00020953468392077268\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0002184959613805404\n",
      "Training Loss: 0.00020430289459909544\n",
      "Training Loss: 0.0002119136085457285\n",
      "Validation Loss: 0.00020664837644657831\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.00021625806355586973\n",
      "Training Loss: 0.00020166411755781155\n",
      "Training Loss: 0.00020915890629112256\n",
      "Validation Loss: 0.0002038606390271341\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.000214075760304695\n",
      "Training Loss: 0.000199090090682148\n",
      "Training Loss: 0.00020647452041885116\n",
      "Validation Loss: 0.0002011649378933787\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.00021194645265495637\n",
      "Training Loss: 0.00019657964941870886\n",
      "Training Loss: 0.00020385772237204946\n",
      "Validation Loss: 0.00019855557646101415\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.0002098683973963489\n",
      "Training Loss: 0.00019413128691667225\n",
      "Training Loss: 0.00020130667955527314\n",
      "Validation Loss: 0.00019602799184621688\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.00020784017746336758\n",
      "Training Loss: 0.00019174319040757837\n",
      "Training Loss: 0.00019881998188793658\n",
      "Validation Loss: 0.00019357789405445003\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.00020585978802046157\n",
      "Training Loss: 0.0001894141651064274\n",
      "Training Loss: 0.00019639524231024552\n",
      "Validation Loss: 0.0001912008974000712\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.00020392555476064444\n",
      "Training Loss: 0.00018714286823524163\n",
      "Training Loss: 0.00019403090485866414\n",
      "Validation Loss: 0.00018889391238067373\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.00020203629246680067\n",
      "Training Loss: 0.00018492790739401243\n",
      "Training Loss: 0.0001917248319659848\n",
      "Validation Loss: 0.00018665319555694122\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.0002001900171126181\n",
      "Training Loss: 0.00018276764330948936\n",
      "Training Loss: 0.00018947563137771794\n",
      "Validation Loss: 0.0001844764804680769\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00019838564863675857\n",
      "Training Loss: 0.0001806607593971421\n",
      "Training Loss: 0.00018728089282376458\n",
      "Validation Loss: 0.0001823602724062368\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.00019662143053210456\n",
      "Training Loss: 0.00017860607131297\n",
      "Training Loss: 0.00018513931001507445\n",
      "Validation Loss: 0.00018030262995789905\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.00019489572749080254\n",
      "Training Loss: 0.00017660131219599863\n",
      "Training Loss: 0.000183048902763403\n",
      "Validation Loss: 0.00017830089551823088\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.00019320723466080382\n",
      "Training Loss: 0.0001746456723776646\n",
      "Training Loss: 0.00018100772111210973\n",
      "Validation Loss: 0.00017635241303569376\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.00019155412977852392\n",
      "Training Loss: 0.00017273684883548413\n",
      "Training Loss: 0.00017901393985084724\n",
      "Validation Loss: 0.00017445525789687024\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.00018993499761563726\n",
      "Training Loss: 0.00017087383494072129\n",
      "Training Loss: 0.00017706554088363192\n",
      "Validation Loss: 0.00017260702603134938\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.00018834837479516863\n",
      "Training Loss: 0.00016905471406062134\n",
      "Training Loss: 0.00017516111671284307\n",
      "Validation Loss: 0.00017080621060057218\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.00018679282464290737\n",
      "Training Loss: 0.00016727797188650584\n",
      "Training Loss: 0.00017329829974187304\n",
      "Validation Loss: 0.0001690501263878554\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0001852665667865949\n",
      "Training Loss: 0.00016554187528527108\n",
      "Training Loss: 0.00017147575901617528\n",
      "Validation Loss: 0.00016733722385948424\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.00018376835510935051\n",
      "Training Loss: 0.00016384531330913886\n",
      "Training Loss: 0.00016969113930827008\n",
      "Validation Loss: 0.00016566552992290792\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00018229674291433184\n",
      "Training Loss: 0.00016218612330703763\n",
      "Training Loss: 0.00016794321731140372\n",
      "Validation Loss: 0.0001640332730868282\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.0001808498030550254\n",
      "Training Loss: 0.00016056259173637954\n",
      "Training Loss: 0.0001662302883778466\n",
      "Validation Loss: 0.00016243826516830687\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.00017942675463928026\n",
      "Training Loss: 0.00015897394427156542\n",
      "Training Loss: 0.00016455025141112856\n",
      "Validation Loss: 0.00016087937849610417\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0001780257996870205\n",
      "Training Loss: 0.0001574180950774462\n",
      "Training Loss: 0.00016290176805341616\n",
      "Validation Loss: 0.0001593549779009367\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.00017664576020251843\n",
      "Training Loss: 0.00015589372549584368\n",
      "Training Loss: 0.00016128360337461345\n",
      "Validation Loss: 0.00015786266156058272\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.00017528529249830172\n",
      "Training Loss: 0.0001543993959967338\n",
      "Training Loss: 0.0001596938092916389\n",
      "Validation Loss: 0.00015640166852432904\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.00017394332850017235\n",
      "Training Loss: 0.00015293385158656747\n",
      "Training Loss: 0.00015813116366189206\n",
      "Validation Loss: 0.00015497036697473797\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0001726186439373123\n",
      "Training Loss: 0.00015149569971981692\n",
      "Training Loss: 0.00015659439204682713\n",
      "Validation Loss: 0.00015356734038913999\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.0001713100306551496\n",
      "Training Loss: 0.00015008366919573744\n",
      "Training Loss: 0.00015508192376728402\n",
      "Validation Loss: 0.00015219141419089445\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.00017001634429107072\n",
      "Training Loss: 0.0001486964541254565\n",
      "Training Loss: 0.00015359290617197985\n",
      "Validation Loss: 0.0001508406697549423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.000168736816940509\n",
      "Training Loss: 0.00014733317255377187\n",
      "Training Loss: 0.0001521260373374389\n",
      "Validation Loss: 0.00014951472736213597\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.00016747063422371865\n",
      "Training Loss: 0.0001459923823858844\n",
      "Training Loss: 0.00015068045702719247\n",
      "Validation Loss: 0.00014821195380524812\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.00016621641230813111\n",
      "Training Loss: 0.0001446734572709829\n",
      "Training Loss: 0.00014925522340490714\n",
      "Validation Loss: 0.00014693133200350704\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.00016497379116117372\n",
      "Training Loss: 0.00014337533475554664\n",
      "Training Loss: 0.00014784917528231744\n",
      "Validation Loss: 0.0001456718503535474\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.00016374207078115433\n",
      "Training Loss: 0.00014209686691174283\n",
      "Training Loss: 0.00014646180443378398\n",
      "Validation Loss: 0.0001444329274736549\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.00016252029927272816\n",
      "Training Loss: 0.00014083749581914162\n",
      "Training Loss: 0.00014509200267639243\n",
      "Validation Loss: 0.00014321334171828323\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.000161308334536443\n",
      "Training Loss: 0.0001395966283598682\n",
      "Training Loss: 0.0001437392491970968\n",
      "Validation Loss: 0.00014201235592664805\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.00016010513245419135\n",
      "Training Loss: 0.00013837343018167304\n",
      "Training Loss: 0.00014240282976970775\n",
      "Validation Loss: 0.00014082963466892815\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.0001589106063511281\n",
      "Training Loss: 0.0001371671968990995\n",
      "Training Loss: 0.00014108268345808028\n",
      "Validation Loss: 0.0001396642013726327\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.00015772438349813454\n",
      "Training Loss: 0.00013597759447293357\n",
      "Training Loss: 0.0001397777693637181\n",
      "Validation Loss: 0.00013851527961541955\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.00015654589466066683\n",
      "Training Loss: 0.00013480395537044388\n",
      "Training Loss: 0.00013848797882019427\n",
      "Validation Loss: 0.00013738257040856982\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.00015537495868557017\n",
      "Training Loss: 0.00013364598322368693\n",
      "Training Loss: 0.00013721296434596297\n",
      "Validation Loss: 0.0001362655546860848\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.00015421136080476572\n",
      "Training Loss: 0.00013250297888589557\n",
      "Training Loss: 0.00013595233378509874\n",
      "Validation Loss: 0.00013516402710654271\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.00015305510632970253\n",
      "Training Loss: 0.00013137522895704024\n",
      "Training Loss: 0.00013470605792463176\n",
      "Validation Loss: 0.0001340771101404039\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.00015190577283647144\n",
      "Training Loss: 0.00013026194614212727\n",
      "Training Loss: 0.0001334739425146836\n",
      "Validation Loss: 0.00013300479311898896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00015076378833327907\n",
      "Training Loss: 0.00012916331801534397\n",
      "Training Loss: 0.00013225580889411503\n",
      "Validation Loss: 0.0001319464915539949\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.00014962881276005645\n",
      "Training Loss: 0.00012807875211365172\n",
      "Training Loss: 0.00013105145066219848\n",
      "Validation Loss: 0.00013090199270541388\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.000148500653613155\n",
      "Training Loss: 0.00012700865177976085\n",
      "Training Loss: 0.00012986094106963718\n",
      "Validation Loss: 0.00012987126387231277\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00014737988743945607\n",
      "Training Loss: 0.0001259525816931273\n",
      "Training Loss: 0.00012868406431152836\n",
      "Validation Loss: 0.00012885382171446232\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.0001462663674283249\n",
      "Training Loss: 0.00012491044159105514\n",
      "Training Loss: 0.00012752131135130185\n",
      "Validation Loss: 0.00012784896630926574\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.00014516013485263101\n",
      "Training Loss: 0.00012388242677843664\n",
      "Training Loss: 0.00012637252290005564\n",
      "Validation Loss: 0.00012685717760423053\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00014406162921659415\n",
      "Training Loss: 0.00012286847924769973\n",
      "Training Loss: 0.0001252377897799306\n",
      "Validation Loss: 0.0001258782266110726\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.00014297089584943022\n",
      "Training Loss: 0.00012186857589767897\n",
      "Training Loss: 0.0001241170414868975\n",
      "Validation Loss: 0.0001249114070713894\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.00014188830251441687\n",
      "Training Loss: 0.00012088310499166255\n",
      "Training Loss: 0.00012301084176215228\n",
      "Validation Loss: 0.00012395692743319138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.00014081395112953033\n",
      "Training Loss: 0.0001199116541465628\n",
      "Training Loss: 0.00012191894886200316\n",
      "Validation Loss: 0.0001230142486215964\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.00013974825980767492\n",
      "Training Loss: 0.0001189543063446763\n",
      "Training Loss: 0.0001208416485133057\n",
      "Validation Loss: 0.00012208289318254317\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00013869135711502167\n",
      "Training Loss: 0.00011801166790974093\n",
      "Training Loss: 0.00011977907095570117\n",
      "Validation Loss: 0.00012116360408123164\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.00013764362194706336\n",
      "Training Loss: 0.00011708311909387703\n",
      "Training Loss: 0.00011873124529302004\n",
      "Validation Loss: 0.00012025453909015769\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.00013660507116583176\n",
      "Training Loss: 0.00011616911806413554\n",
      "Training Loss: 0.00011769853579608026\n",
      "Validation Loss: 0.00011935747818242408\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00013557651720475406\n",
      "Training Loss: 0.00011526956088346196\n",
      "Training Loss: 0.00011668079729133751\n",
      "Validation Loss: 0.0001184702550234035\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.000134557769906678\n",
      "Training Loss: 0.00011438471025030594\n",
      "Training Loss: 0.00011567816460228642\n",
      "Validation Loss: 0.00011759328722444969\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.00013354930149944266\n",
      "Training Loss: 0.00011351437571647693\n",
      "Training Loss: 0.00011469090743048582\n",
      "Validation Loss: 0.00011672665815250399\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.0001325514521340665\n",
      "Training Loss: 0.00011265839631960262\n",
      "Training Loss: 0.00011371882195817307\n",
      "Validation Loss: 0.00011586943758864824\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00013156452258044737\n",
      "Training Loss: 0.00011181719464730122\n",
      "Training Loss: 0.00011276244133114233\n",
      "Validation Loss: 0.00011502172872476447\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.00013058859893135376\n",
      "Training Loss: 0.00011099069373813109\n",
      "Training Loss: 0.00011182126872881782\n",
      "Validation Loss: 0.00011418379200334671\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0001296243758588389\n",
      "Training Loss: 0.00011017853121302324\n",
      "Training Loss: 0.00011089555864600698\n",
      "Validation Loss: 0.00011335380514446002\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.00012867139719674014\n",
      "Training Loss: 0.00010938072442513658\n",
      "Training Loss: 0.0001099853736559453\n",
      "Validation Loss: 0.00011253338800926347\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.00012773046746588078\n",
      "Training Loss: 0.0001085971395514207\n",
      "Training Loss: 0.00010909042783168843\n",
      "Validation Loss: 0.00011172103003946688\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.00012680151265158203\n",
      "Training Loss: 0.00010782813296827953\n",
      "Training Loss: 0.00010821065543495934\n",
      "Validation Loss: 0.00011091700436961392\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.000125885052339072\n",
      "Training Loss: 0.00010707295814427198\n",
      "Training Loss: 0.000107346359791336\n",
      "Validation Loss: 0.00011012178585812936\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.0001249808141437825\n",
      "Training Loss: 0.0001063315420105937\n",
      "Training Loss: 0.0001064968279661116\n",
      "Validation Loss: 0.00010933272033377833\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.00012408927213073185\n",
      "Training Loss: 0.00010560394472122425\n",
      "Training Loss: 0.00010566246633970877\n",
      "Validation Loss: 0.00010855202896656531\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.0001232101436198718\n",
      "Training Loss: 0.00010488994725164957\n",
      "Training Loss: 0.00010484269031621807\n",
      "Validation Loss: 0.00010777881160199255\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.00012234405761773814\n",
      "Training Loss: 0.00010418912990644457\n",
      "Training Loss: 0.00010403771122582839\n",
      "Validation Loss: 0.00010701304146617107\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.00012149045612204645\n",
      "Training Loss: 0.00010350130636652466\n",
      "Training Loss: 0.00010324702778234496\n",
      "Validation Loss: 0.00010625446467723154\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.00012065011026606953\n",
      "Training Loss: 0.00010282636088959407\n",
      "Training Loss: 0.0001024704208066396\n",
      "Validation Loss: 0.00010550388659784188\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.00011982261914454284\n",
      "Training Loss: 0.00010216409156782902\n",
      "Training Loss: 0.00010170796099373547\n",
      "Validation Loss: 0.00010476001795283859\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.00011900805536242842\n",
      "Training Loss: 0.00010151384525670437\n",
      "Training Loss: 0.00010095899123371055\n",
      "Validation Loss: 0.0001040229572537826\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.00011820631536465953\n",
      "Training Loss: 0.00010087571345138713\n",
      "Training Loss: 0.00010022388270044757\n",
      "Validation Loss: 0.00010329394096383051\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00011741768216779746\n",
      "Training Loss: 0.00010024928020357038\n",
      "Training Loss: 9.95019028596289e-05\n",
      "Validation Loss: 0.00010257172671321968\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.00011664148005365859\n",
      "Training Loss: 9.963420105123077e-05\n",
      "Training Loss: 9.879262362119334e-05\n",
      "Validation Loss: 0.00010185747650916966\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.00011587829475502076\n",
      "Training Loss: 9.903042959194864e-05\n",
      "Training Loss: 9.809624853915011e-05\n",
      "Validation Loss: 0.00010115079007182123\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.000115127591543569\n",
      "Training Loss: 9.843745530815795e-05\n",
      "Training Loss: 9.741237290654681e-05\n",
      "Validation Loss: 0.00010045167958227201\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.00011438954537879908\n",
      "Training Loss: 9.78551697517105e-05\n",
      "Training Loss: 9.674060157522036e-05\n",
      "Validation Loss: 9.976025119592603e-05\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.00011366384344000835\n",
      "Training Loss: 9.728303310112096e-05\n",
      "Training Loss: 9.608084374121973e-05\n",
      "Validation Loss: 9.907618983929291e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.00011295041911580484\n",
      "Training Loss: 9.672101485193707e-05\n",
      "Training Loss: 9.543281033984385e-05\n",
      "Validation Loss: 9.840074448363615e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.00011224897127249279\n",
      "Training Loss: 9.616885621653637e-05\n",
      "Training Loss: 9.479603189902264e-05\n",
      "Validation Loss: 9.773368998303136e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.00011155959087773226\n",
      "Training Loss: 9.56260627208394e-05\n",
      "Training Loss: 9.417034972102556e-05\n",
      "Validation Loss: 9.707481466025099e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.00011088188210123918\n",
      "Training Loss: 9.509249501206796e-05\n",
      "Training Loss: 9.355567216516647e-05\n",
      "Validation Loss: 9.642355822568257e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.00011021561665984336\n",
      "Training Loss: 9.456790441618068e-05\n",
      "Training Loss: 9.295165175899456e-05\n",
      "Validation Loss: 9.578197437967697e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.00010956094052744447\n",
      "Training Loss: 9.405202015841496e-05\n",
      "Training Loss: 9.235794715095835e-05\n",
      "Validation Loss: 9.514828560170142e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.00010891732325035263\n",
      "Training Loss: 9.354465765682108e-05\n",
      "Training Loss: 9.177449197522947e-05\n",
      "Validation Loss: 9.452352374926172e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.00010828471437889674\n",
      "Training Loss: 9.304559220254305e-05\n",
      "Training Loss: 9.120089666794228e-05\n",
      "Validation Loss: 9.390747722688267e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.00010766277676339087\n",
      "Training Loss: 9.255435636077891e-05\n",
      "Training Loss: 9.063722111022798e-05\n",
      "Validation Loss: 9.33003821557393e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.00010705155757023021\n",
      "Training Loss: 9.207106466419646e-05\n",
      "Training Loss: 9.008279893350846e-05\n",
      "Validation Loss: 9.270237912940583e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.00010645078099514648\n",
      "Training Loss: 9.159560233456431e-05\n",
      "Training Loss: 8.953777489296045e-05\n",
      "Validation Loss: 9.211270335480947e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.00010586010792394518\n",
      "Training Loss: 9.112715855735587e-05\n",
      "Training Loss: 8.900187231120071e-05\n",
      "Validation Loss: 9.15320987577401e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.0001052792957489146\n",
      "Training Loss: 9.066616065865674e-05\n",
      "Training Loss: 8.847479437463335e-05\n",
      "Validation Loss: 9.096068177395761e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.00010470836650711135\n",
      "Training Loss: 9.021200454299105e-05\n",
      "Training Loss: 8.795644560450456e-05\n",
      "Validation Loss: 9.03984201112906e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.00010414690506877378\n",
      "Training Loss: 8.976469774097496e-05\n",
      "Training Loss: 8.744658860450727e-05\n",
      "Validation Loss: 8.984491412346041e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.00010359478320424386\n",
      "Training Loss: 8.932440755415882e-05\n",
      "Training Loss: 8.694484785337408e-05\n",
      "Validation Loss: 8.929986678380414e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.00010305193918611622\n",
      "Training Loss: 8.889022622497578e-05\n",
      "Training Loss: 8.645145556329225e-05\n",
      "Validation Loss: 8.876419950242605e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.0001025178593226883\n",
      "Training Loss: 8.846252373587049e-05\n",
      "Training Loss: 8.596579470577126e-05\n",
      "Validation Loss: 8.823727076877107e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.00010199288493822678\n",
      "Training Loss: 8.804088056422188e-05\n",
      "Training Loss: 8.54881981285871e-05\n",
      "Validation Loss: 8.771880800012379e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.00010147618625524047\n",
      "Training Loss: 8.762546652178571e-05\n",
      "Training Loss: 8.501809708832297e-05\n",
      "Validation Loss: 8.720854088239597e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.0001009679261551355\n",
      "Training Loss: 8.721579909433786e-05\n",
      "Training Loss: 8.455557467641483e-05\n",
      "Validation Loss: 8.670750104827451e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.00010046797452559985\n",
      "Training Loss: 8.681213489580841e-05\n",
      "Training Loss: 8.410013887441892e-05\n",
      "Validation Loss: 8.621372607388806e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 9.997595578170148e-05\n",
      "Training Loss: 8.641378661195632e-05\n",
      "Training Loss: 8.365196350496263e-05\n",
      "Validation Loss: 8.572914545735595e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 9.949182806849421e-05\n",
      "Training Loss: 8.602077579780598e-05\n",
      "Training Loss: 8.321092213918746e-05\n",
      "Validation Loss: 8.525173409120572e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 9.901525925670284e-05\n",
      "Training Loss: 8.563347633753437e-05\n",
      "Training Loss: 8.277674362943798e-05\n",
      "Validation Loss: 8.478276334684002e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 9.854643816652242e-05\n",
      "Training Loss: 8.525134895535302e-05\n",
      "Training Loss: 8.234926874138182e-05\n",
      "Validation Loss: 8.432246184465736e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 9.808499067730736e-05\n",
      "Training Loss: 8.487415862873604e-05\n",
      "Training Loss: 8.192829309336958e-05\n",
      "Validation Loss: 8.386872433946112e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 9.763067655512713e-05\n",
      "Training Loss: 8.450219774204015e-05\n",
      "Training Loss: 8.15138077359734e-05\n",
      "Validation Loss: 8.342252080224119e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 9.718336049445497e-05\n",
      "Training Loss: 8.413497296714922e-05\n",
      "Training Loss: 8.110564066555525e-05\n",
      "Validation Loss: 8.298368298669681e-05\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 9.674292579802567e-05\n",
      "Training Loss: 8.377256699532154e-05\n",
      "Training Loss: 8.070369294728152e-05\n",
      "Validation Loss: 8.255234651994797e-05\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 9.630919586925302e-05\n",
      "Training Loss: 8.341488093719817e-05\n",
      "Training Loss: 8.030788094401942e-05\n",
      "Validation Loss: 8.212752471265273e-05\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 9.588208993591252e-05\n",
      "Training Loss: 8.306175172037911e-05\n",
      "Training Loss: 7.991802332071529e-05\n",
      "Validation Loss: 8.171010750026725e-05\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 9.546164254061296e-05\n",
      "Training Loss: 8.271310221971362e-05\n",
      "Training Loss: 7.953408400680928e-05\n",
      "Validation Loss: 8.129953103695258e-05\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 9.504737276984087e-05\n",
      "Training Loss: 8.23687988849997e-05\n",
      "Training Loss: 7.915583918475022e-05\n",
      "Validation Loss: 8.089565405467776e-05\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 9.463936841711984e-05\n",
      "Training Loss: 8.202890464417578e-05\n",
      "Training Loss: 7.87832428522961e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [30:44<08:07, 243.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.049825441191664e-05\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.3548054592311382\n",
      "Training Loss: 0.2673355854675174\n",
      "Training Loss: 0.2297498022764921\n",
      "Validation Loss: 0.16912432693028717\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.14447290692478418\n",
      "Training Loss: 0.0906189585966058\n",
      "Training Loss: 0.07702909500338137\n",
      "Validation Loss: 0.060084365418159896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.05702523129293695\n",
      "Training Loss: 0.051249965438619254\n",
      "Training Loss: 0.05511751252692193\n",
      "Validation Loss: 0.051707999472077305\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.049491969366790727\n",
      "Training Loss: 0.045617568865418436\n",
      "Training Loss: 0.04831907212268561\n",
      "Validation Loss: 0.045430090958566476\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.043216055185184815\n",
      "Training Loss: 0.039752987059764565\n",
      "Training Loss: 0.04177470926661044\n",
      "Validation Loss: 0.039109208736191975\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.036911117649287915\n",
      "Training Loss: 0.03370724345557392\n",
      "Training Loss: 0.03513123610522598\n",
      "Validation Loss: 0.03260810257280978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.03044188667117851\n",
      "Training Loss: 0.027424411801621317\n",
      "Training Loss: 0.02832427334971726\n",
      "Validation Loss: 0.025970480797205413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.023866210196283644\n",
      "Training Loss: 0.02106324179330841\n",
      "Training Loss: 0.0215898315818049\n",
      "Validation Loss: 0.019634452331915832\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.01768042988027446\n",
      "Training Loss: 0.015367686512181535\n",
      "Training Loss: 0.015796271418221295\n",
      "Validation Loss: 0.014562021022704378\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.012894001963431947\n",
      "Training Loss: 0.011326265269890428\n",
      "Training Loss: 0.011762928604148327\n",
      "Validation Loss: 0.0111656480552524\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.009852201911853627\n",
      "Training Loss: 0.008918762829853221\n",
      "Training Loss: 0.00927751982701011\n",
      "Validation Loss: 0.008959779972367491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.007998795399180381\n",
      "Training Loss: 0.007471113604260608\n",
      "Training Loss: 0.00771641285973601\n",
      "Validation Loss: 0.007446792029011785\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.006782984000165015\n",
      "Training Loss: 0.006496244536247104\n",
      "Training Loss: 0.006651657331967726\n",
      "Validation Loss: 0.00638352465564699\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.005919219647767022\n",
      "Training Loss: 0.005769642398227006\n",
      "Training Loss: 0.00586308813537471\n",
      "Validation Loss: 0.0056409106493059006\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.005265709127415903\n",
      "Training Loss: 0.005189141283626669\n",
      "Training Loss: 0.005237028733827174\n",
      "Validation Loss: 0.005115482175272753\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.004738654606044292\n",
      "Training Loss: 0.004697815406834707\n",
      "Training Loss: 0.004708113761735149\n",
      "Validation Loss: 0.004716776906554535\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.004283948449010495\n",
      "Training Loss: 0.004259620530647225\n",
      "Training Loss: 0.004237035683472641\n",
      "Validation Loss: 0.00437505720209414\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.0038682572104153223\n",
      "Training Loss: 0.0038529791752807798\n",
      "Training Loss: 0.0038021746987942605\n",
      "Validation Loss: 0.004046250347262562\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.0034745220321929083\n",
      "Training Loss: 0.0034682111931033433\n",
      "Training Loss: 0.0033953501761425286\n",
      "Validation Loss: 0.003711053269674604\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.003098815194534836\n",
      "Training Loss: 0.0031057077451259827\n",
      "Training Loss: 0.0030190364661393685\n",
      "Validation Loss: 0.003370543083782946\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.0027479945935192517\n",
      "Training Loss: 0.002774396794848144\n",
      "Training Loss: 0.0026840010791784152\n",
      "Validation Loss: 0.0030405875870937044\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.002436953407523106\n",
      "Training Loss: 0.002488870665838476\n",
      "Training Loss: 0.002404675240686629\n",
      "Validation Loss: 0.002742643993258937\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.0021817143627413317\n",
      "Training Loss: 0.0022614593215985225\n",
      "Training Loss: 0.002189038517244626\n",
      "Validation Loss: 0.0024885258508527005\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.0019873622692284697\n",
      "Training Loss: 0.00209085236347164\n",
      "Training Loss: 0.0020288708718726413\n",
      "Validation Loss: 0.0022736680644873026\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0018420761110974127\n",
      "Training Loss: 0.001960720546921948\n",
      "Training Loss: 0.0019050739565864205\n",
      "Validation Loss: 0.0020907317243092642\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.001728046907173848\n",
      "Training Loss: 0.0018538433796493337\n",
      "Training Loss: 0.0018021558536565863\n",
      "Validation Loss: 0.001936513379875445\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.0016328944607812445\n",
      "Training Loss: 0.001760175308882026\n",
      "Training Loss: 0.001711623420997057\n",
      "Validation Loss: 0.0018074068765998347\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.0015497001196172278\n",
      "Training Loss: 0.0016745116359379608\n",
      "Training Loss: 0.0016288895288016647\n",
      "Validation Loss: 0.0016981931766259662\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.0014743302373199185\n",
      "Training Loss: 0.0015938362522865646\n",
      "Training Loss: 0.001551143505203072\n",
      "Validation Loss: 0.0016033860009158386\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.001404060760974062\n",
      "Training Loss: 0.0015162349994352552\n",
      "Training Loss: 0.0014765023218933492\n",
      "Validation Loss: 0.0015182510738423242\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.0013370155202210299\n",
      "Training Loss: 0.0014404730882961302\n",
      "Training Loss: 0.0014036649918125477\n",
      "Validation Loss: 0.0014391559845862093\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.001271921968591414\n",
      "Training Loss: 0.001365823890955653\n",
      "Training Loss: 0.001331794242723845\n",
      "Validation Loss: 0.0013635937076093357\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.0012080038898818657\n",
      "Training Loss: 0.0012919860339025036\n",
      "Training Loss: 0.001260499971540412\n",
      "Validation Loss: 0.0012900907437714717\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.0011449277149836235\n",
      "Training Loss: 0.0012190468920744025\n",
      "Training Loss: 0.0011898434757313225\n",
      "Validation Loss: 0.0012180719737505039\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.0010827763905604116\n",
      "Training Loss: 0.0011474454756535124\n",
      "Training Loss: 0.0011202986600983421\n",
      "Validation Loss: 0.0011476784213014428\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.001021968832604898\n",
      "Training Loss: 0.0010778976515575778\n",
      "Training Loss: 0.0010526613346883095\n",
      "Validation Loss: 0.0010795528777113729\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0009631723168013195\n",
      "Training Loss: 0.0010113008416374213\n",
      "Training Loss: 0.0009879183070734144\n",
      "Validation Loss: 0.0010146192628811793\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.0009071899275272699\n",
      "Training Loss: 0.000948598143804702\n",
      "Training Loss: 0.0009270651187398471\n",
      "Validation Loss: 0.0009538000205916279\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.000854753989395931\n",
      "Training Loss: 0.0008905346293613547\n",
      "Training Loss: 0.0008708226071030367\n",
      "Validation Loss: 0.0008976803791677852\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.0008062505565453648\n",
      "Training Loss: 0.0008373620916245272\n",
      "Training Loss: 0.0008193596155615523\n",
      "Validation Loss: 0.0008462836826612566\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.0007615644559513157\n",
      "Training Loss: 0.0007887863132782513\n",
      "Training Loss: 0.000772323842102196\n",
      "Validation Loss: 0.0007992708353126903\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0007203580273966281\n",
      "Training Loss: 0.0007443575788056478\n",
      "Training Loss: 0.0007292588397103827\n",
      "Validation Loss: 0.0007563959486688373\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0006825152332777406\n",
      "Training Loss: 0.0007038298969564493\n",
      "Training Loss: 0.0006898938350786921\n",
      "Validation Loss: 0.000717535637285733\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0006480927740403785\n",
      "Training Loss: 0.0006669984561449383\n",
      "Training Loss: 0.0006539912527659908\n",
      "Validation Loss: 0.0006823635810863206\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.0006169904369153301\n",
      "Training Loss: 0.000633521657800884\n",
      "Training Loss: 0.0006212243325717282\n",
      "Validation Loss: 0.0006502879778769526\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.0005888986931915952\n",
      "Training Loss: 0.000602969168976415\n",
      "Training Loss: 0.000591233042141539\n",
      "Validation Loss: 0.0006206517069034142\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0005634283999415856\n",
      "Training Loss: 0.0005749424451641971\n",
      "Training Loss: 0.0005637306582502787\n",
      "Validation Loss: 0.0005930177004107933\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0005402493247129314\n",
      "Training Loss: 0.0005491671279014554\n",
      "Training Loss: 0.0005385612067766488\n",
      "Validation Loss: 0.0005672896956813029\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.0005191361262131977\n",
      "Training Loss: 0.0005255031131673604\n",
      "Training Loss: 0.000515672295296099\n",
      "Validation Loss: 0.0005436276266837932\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0004999522293792324\n",
      "Training Loss: 0.0005038993828929961\n",
      "Training Loss: 0.0004950446448492584\n",
      "Validation Loss: 0.0005222409170610161\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.00048259254160711864\n",
      "Training Loss: 0.0004843177708244184\n",
      "Training Loss: 0.00047662014432717117\n",
      "Validation Loss: 0.000503224651316139\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.00046694051819031303\n",
      "Training Loss: 0.0004666858749988023\n",
      "Training Loss: 0.00046027342119487\n",
      "Validation Loss: 0.00048650854124603895\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.00045285753289135754\n",
      "Training Loss: 0.00045088258266332557\n",
      "Training Loss: 0.0004458160750073148\n",
      "Validation Loss: 0.00047189104993725615\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.0004401833918927878\n",
      "Training Loss: 0.00043674483087670525\n",
      "Training Loss: 0.00043302574835252017\n",
      "Validation Loss: 0.00045910915683593933\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0004287568120253127\n",
      "Training Loss: 0.00042409197201777717\n",
      "Training Loss: 0.00042167552615865136\n",
      "Validation Loss: 0.0004478787639200572\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0004184240213180601\n",
      "Training Loss: 0.0004127413406968117\n",
      "Training Loss: 0.00041154938182444314\n",
      "Validation Loss: 0.00043793463817237764\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.00040904525806581657\n",
      "Training Loss: 0.0004025219831964932\n",
      "Training Loss: 0.0004024594419752248\n",
      "Validation Loss: 0.00042903290870547126\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.0004004978953889804\n",
      "Training Loss: 0.00039328113416559063\n",
      "Training Loss: 0.00039424322021659465\n",
      "Validation Loss: 0.0004209694335677954\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.00039267643304810915\n",
      "Training Loss: 0.0003848846139590023\n",
      "Training Loss: 0.00038676482807204594\n",
      "Validation Loss: 0.0004135769968649655\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.0003854894613232318\n",
      "Training Loss: 0.00037721752021752764\n",
      "Training Loss: 0.0003799145494122058\n",
      "Validation Loss: 0.00040671796527471434\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.0003788602306121902\n",
      "Training Loss: 0.00037018188028014264\n",
      "Training Loss: 0.00037359864181780723\n",
      "Validation Loss: 0.0004002871994109657\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.0003727224212161673\n",
      "Training Loss: 0.0003636938239651499\n",
      "Training Loss: 0.00036774247775611\n",
      "Validation Loss: 0.0003942038949410264\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.0003670207785853563\n",
      "Training Loss: 0.00035768224428466054\n",
      "Training Loss: 0.00036228329539881085\n",
      "Validation Loss: 0.00038840624587505723\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.00036170616986055395\n",
      "Training Loss: 0.0003520870540523902\n",
      "Training Loss: 0.0003571688352531055\n",
      "Validation Loss: 0.0003828485942709919\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0003567374132489931\n",
      "Training Loss: 0.0003468569899268914\n",
      "Training Loss: 0.00035235661765909755\n",
      "Validation Loss: 0.00037749381018135425\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.00035207778935728126\n",
      "Training Loss: 0.00034194715481135064\n",
      "Training Loss: 0.0003478093298326712\n",
      "Validation Loss: 0.0003723170001843285\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0003476959752106268\n",
      "Training Loss: 0.00033732032490661366\n",
      "Training Loss: 0.00034349669760558755\n",
      "Validation Loss: 0.00036730106193716167\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0003435637987422524\n",
      "Training Loss: 0.0003329455262428382\n",
      "Training Loss: 0.0003393929440062493\n",
      "Validation Loss: 0.00036243383306355917\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.00033965740949952304\n",
      "Training Loss: 0.0003287944053590763\n",
      "Training Loss: 0.00033547547369380484\n",
      "Validation Loss: 0.00035770450402482863\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.00033595518833863026\n",
      "Training Loss: 0.00032484426177688876\n",
      "Training Loss: 0.00033172688854392617\n",
      "Validation Loss: 0.0003531074587627199\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.00033243799951378607\n",
      "Training Loss: 0.0003210763145034434\n",
      "Training Loss: 0.0003281309469457483\n",
      "Validation Loss: 0.00034863892170924985\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.00032909005079091003\n",
      "Training Loss: 0.0003174735558422981\n",
      "Training Loss: 0.0003246747322555166\n",
      "Validation Loss: 0.00034429715546384305\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.0003258966872635938\n",
      "Training Loss: 0.0003140219100896502\n",
      "Training Loss: 0.0003213470132322982\n",
      "Validation Loss: 0.00034007966306500055\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.00032284542667184726\n",
      "Training Loss: 0.00031071014294866474\n",
      "Training Loss: 0.0003181389820383629\n",
      "Validation Loss: 0.0003359866788149079\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.0003199253605953345\n",
      "Training Loss: 0.0003075282581994543\n",
      "Training Loss: 0.00031504275546467397\n",
      "Validation Loss: 0.0003320190099360986\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.0003171265807714008\n",
      "Training Loss: 0.0003044671058523818\n",
      "Training Loss: 0.00031205147781292907\n",
      "Validation Loss: 0.00032817619992354675\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.0003144409156675465\n",
      "Training Loss: 0.0003015199222136289\n",
      "Training Loss: 0.00030915911644115114\n",
      "Validation Loss: 0.000324458608405764\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0003118607284477548\n",
      "Training Loss: 0.0002986800269354717\n",
      "Training Loss: 0.0003063608733646106\n",
      "Validation Loss: 0.00032086675753489956\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00030937980337967017\n",
      "Training Loss: 0.00029594175968668425\n",
      "Training Loss: 0.00030365292288479393\n",
      "Validation Loss: 0.0003174001970599808\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.00030699103560436923\n",
      "Training Loss: 0.0002932998404503451\n",
      "Training Loss: 0.00030103052966296673\n",
      "Validation Loss: 0.0003140579710509811\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0003046895254465198\n",
      "Training Loss: 0.0002907503887399798\n",
      "Training Loss: 0.00029849020531401036\n",
      "Validation Loss: 0.0003108384583182659\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.00030247011649407794\n",
      "Training Loss: 0.0002882877937008743\n",
      "Training Loss: 0.00029602866976347286\n",
      "Validation Loss: 0.00030774102507687563\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.0003003282417898845\n",
      "Training Loss: 0.00028590908252226657\n",
      "Training Loss: 0.00029364221802097746\n",
      "Validation Loss: 0.00030476129013754617\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.0002982589447833561\n",
      "Training Loss: 0.0002836091573408339\n",
      "Training Loss: 0.00029132759002095557\n",
      "Validation Loss: 0.00030189791967009266\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.0002962582975328587\n",
      "Training Loss: 0.00028138491823483493\n",
      "Training Loss: 0.0002890814614511328\n",
      "Validation Loss: 0.00029914723694510236\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.0002943223060492528\n",
      "Training Loss: 0.00027923220841330476\n",
      "Training Loss: 0.0002869010480935685\n",
      "Validation Loss: 0.0002965033963437747\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.0002924468987066575\n",
      "Training Loss: 0.0002771479645525687\n",
      "Training Loss: 0.00028478300584538373\n",
      "Validation Loss: 0.0002939638798125088\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.000290628892425957\n",
      "Training Loss: 0.00027512861339346273\n",
      "Training Loss: 0.0002827242347848369\n",
      "Validation Loss: 0.00029152327465198233\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.00028886483322821733\n",
      "Training Loss: 0.00027317061514622767\n",
      "Training Loss: 0.0002807217665395001\n",
      "Validation Loss: 0.0002891761667322069\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.00028715077556171306\n",
      "Training Loss: 0.00027127082517836243\n",
      "Training Loss: 0.0002787729192641564\n",
      "Validation Loss: 0.0002869178825175766\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00028548456442649693\n",
      "Training Loss: 0.00026942582782794487\n",
      "Training Loss: 0.0002768744295462966\n",
      "Validation Loss: 0.00028474339681482623\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.00028386259473336397\n",
      "Training Loss: 0.0002676331203474547\n",
      "Training Loss: 0.0002750244302296778\n",
      "Validation Loss: 0.0002826471501383198\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.0002822823508040528\n",
      "Training Loss: 0.000265889459697064\n",
      "Training Loss: 0.000273219595401315\n",
      "Validation Loss: 0.0002806246228103416\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.000280741094120458\n",
      "Training Loss: 0.0002641923732153373\n",
      "Training Loss: 0.0002714577029837528\n",
      "Validation Loss: 0.0002786704775480962\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.00027923666790229617\n",
      "Training Loss: 0.00026253946660290237\n",
      "Training Loss: 0.0002697367752989521\n",
      "Validation Loss: 0.0002767806026588843\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.0002777671841317897\n",
      "Training Loss: 0.0002609279696480371\n",
      "Training Loss: 0.0002680540964138345\n",
      "Validation Loss: 0.0002749500460360262\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.00027632993345378054\n",
      "Training Loss: 0.00025935583529644644\n",
      "Training Loss: 0.00026640826145012396\n",
      "Validation Loss: 0.00027317456800354583\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.0002749231319126011\n",
      "Training Loss: 0.00025782122094824444\n",
      "Training Loss: 0.0002647966289805481\n",
      "Validation Loss: 0.0002714507631680227\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0002735453993068404\n",
      "Training Loss: 0.00025632152955950004\n",
      "Training Loss: 0.00026321810142690085\n",
      "Validation Loss: 0.00026977444123188975\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.0002721945887151378\n",
      "Training Loss: 0.0002548549449056736\n",
      "Training Loss: 0.00026167015901592096\n",
      "Validation Loss: 0.00026814231006958353\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00027086940458048047\n",
      "Training Loss: 0.00025341995649796444\n",
      "Training Loss: 0.00026015206982265226\n",
      "Validation Loss: 0.00026655191408628547\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.00026956882254580705\n",
      "Training Loss: 0.0002520146889946773\n",
      "Training Loss: 0.00025866222073091195\n",
      "Validation Loss: 0.00026499925220020254\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.00026829080235529545\n",
      "Training Loss: 0.00025063817731279413\n",
      "Training Loss: 0.0002571986829207162\n",
      "Validation Loss: 0.000263482044951917\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0002670346901322773\n",
      "Training Loss: 0.0002492879314377205\n",
      "Training Loss: 0.0002557605999390944\n",
      "Validation Loss: 0.00026199878286635713\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0002657991062062592\n",
      "Training Loss: 0.00024796362969937036\n",
      "Training Loss: 0.00025434644070628567\n",
      "Validation Loss: 0.00026054584966996014\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.00026458305320375074\n",
      "Training Loss: 0.00024666321973199957\n",
      "Training Loss: 0.0002529556867375504\n",
      "Validation Loss: 0.00025912202009963905\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.0002633855964438681\n",
      "Training Loss: 0.0002453857846921892\n",
      "Training Loss: 0.00025158708354865666\n",
      "Validation Loss: 0.00025772551224067326\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.00026220575640763857\n",
      "Training Loss: 0.00024413031635049266\n",
      "Training Loss: 0.0002502390650624875\n",
      "Validation Loss: 0.00025635459854364334\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.00026104293120397413\n",
      "Training Loss: 0.00024289551587571622\n",
      "Training Loss: 0.0002489115795833641\n",
      "Validation Loss: 0.0002550081646833778\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.0002598962925151227\n",
      "Training Loss: 0.00024168077481590445\n",
      "Training Loss: 0.00024760337131738195\n",
      "Validation Loss: 0.00025368390728790164\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.00025876498247839663\n",
      "Training Loss: 0.00024048477949691006\n",
      "Training Loss: 0.0002463138009261456\n",
      "Validation Loss: 0.00025238113174851974\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0002576481502558181\n",
      "Training Loss: 0.00023930640971229879\n",
      "Training Loss: 0.00024504201566742266\n",
      "Validation Loss: 0.0002510985397973119\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.0002565453068245915\n",
      "Training Loss: 0.0002381457478259108\n",
      "Training Loss: 0.00024378710240853253\n",
      "Validation Loss: 0.00024983593481991783\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.00025545606805735585\n",
      "Training Loss: 0.00023700109079072718\n",
      "Training Loss: 0.0002425487654545577\n",
      "Validation Loss: 0.00024859136444800324\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.0002543795016356398\n",
      "Training Loss: 0.00023587210300320293\n",
      "Training Loss: 0.00024132610797096278\n",
      "Validation Loss: 0.0002473648776920915\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.0002533150981810195\n",
      "Training Loss: 0.00023475806825445033\n",
      "Training Loss: 0.0002401187315263087\n",
      "Validation Loss: 0.000246154509839246\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.0002522627047142123\n",
      "Training Loss: 0.00023365828750684158\n",
      "Training Loss: 0.00023892585290013812\n",
      "Validation Loss: 0.00024496099670133763\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.00025122136600316483\n",
      "Training Loss: 0.0002325722410387243\n",
      "Training Loss: 0.00023774721314111957\n",
      "Validation Loss: 0.00024378228461889164\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.00025019095830202786\n",
      "Training Loss: 0.00023149919630668591\n",
      "Training Loss: 0.0002365823722357163\n",
      "Validation Loss: 0.00024261909575113932\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.00024917084066714777\n",
      "Training Loss: 0.00023043855049763805\n",
      "Training Loss: 0.00023543022474768803\n",
      "Validation Loss: 0.00024147033751902976\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.00024816117207365095\n",
      "Training Loss: 0.0002293899152573431\n",
      "Training Loss: 0.0002342909336584853\n",
      "Validation Loss: 0.0002403348581139469\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.0002471605764935703\n",
      "Training Loss: 0.0002283528588304762\n",
      "Training Loss: 0.00023316372113185935\n",
      "Validation Loss: 0.0002392131770310892\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0002461694174837703\n",
      "Training Loss: 0.00022732669101969806\n",
      "Training Loss: 0.00023204874229122653\n",
      "Validation Loss: 0.00023810464335838333\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.0002451873052348219\n",
      "Training Loss: 0.00022631141626334284\n",
      "Training Loss: 0.00023094516986020607\n",
      "Validation Loss: 0.00023700832618201632\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.0002442138471656108\n",
      "Training Loss: 0.00022530605710926465\n",
      "Training Loss: 0.0002298526715821936\n",
      "Validation Loss: 0.00023592450063946406\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0002432488168517466\n",
      "Training Loss: 0.0002243103666478419\n",
      "Training Loss: 0.0002287710242671892\n",
      "Validation Loss: 0.00023485270989869115\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.00024229173372077638\n",
      "Training Loss: 0.00022332429671223508\n",
      "Training Loss: 0.00022769960884033935\n",
      "Validation Loss: 0.00023379334330518229\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.00024134227591730451\n",
      "Training Loss: 0.00022234728498006008\n",
      "Training Loss: 0.00022663896230369575\n",
      "Validation Loss: 0.00023274426601939635\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.0002404007039285716\n",
      "Training Loss: 0.00022137883446703198\n",
      "Training Loss: 0.0002255879582662601\n",
      "Validation Loss: 0.00023170714836411984\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.0002394665204951707\n",
      "Training Loss: 0.00022041910084226403\n",
      "Training Loss: 0.00022454634079622338\n",
      "Validation Loss: 0.00023068097358559038\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.00023853934756516538\n",
      "Training Loss: 0.00021946794378891355\n",
      "Training Loss: 0.00022351446066750213\n",
      "Validation Loss: 0.00022966529441679557\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00023761915594150196\n",
      "Training Loss: 0.00021852404217497678\n",
      "Training Loss: 0.00022249184799875366\n",
      "Validation Loss: 0.00022866006102968854\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.0002367057692697472\n",
      "Training Loss: 0.00021758785493148025\n",
      "Training Loss: 0.00022147806852444772\n",
      "Validation Loss: 0.0002276651588892512\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.00023579904041753252\n",
      "Training Loss: 0.00021665924145054306\n",
      "Training Loss: 0.0002204729086952284\n",
      "Validation Loss: 0.00022668078211639757\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.00023489879247676982\n",
      "Training Loss: 0.0002157380379139795\n",
      "Training Loss: 0.0002194769245761563\n",
      "Validation Loss: 0.00022570604124574743\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.0002340051374471841\n",
      "Training Loss: 0.00021482390464370839\n",
      "Training Loss: 0.00021848890217370353\n",
      "Validation Loss: 0.00022474151941669028\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00023311731975127258\n",
      "Training Loss: 0.0002139165603512083\n",
      "Training Loss: 0.00021750942309154198\n",
      "Validation Loss: 0.0002237865477725157\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.00023223542092296157\n",
      "Training Loss: 0.00021301583263266366\n",
      "Training Loss: 0.00021653773339494364\n",
      "Validation Loss: 0.00022284150838275084\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.00023135989244508438\n",
      "Training Loss: 0.0002121216330851894\n",
      "Training Loss: 0.00021557437190494966\n",
      "Validation Loss: 0.00022190691604183496\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00023049008776524716\n",
      "Training Loss: 0.00021123416165210073\n",
      "Training Loss: 0.0002146184158846154\n",
      "Validation Loss: 0.0002209808903643626\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.0002296259023347602\n",
      "Training Loss: 0.000210352379908727\n",
      "Training Loss: 0.00021367027999076528\n",
      "Validation Loss: 0.00022006425598611418\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.00022876732562963299\n",
      "Training Loss: 0.00020947683606209466\n",
      "Training Loss: 0.00021272964895615586\n",
      "Validation Loss: 0.00021915675526461303\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00022791438576746258\n",
      "Training Loss: 0.0002086075779152452\n",
      "Training Loss: 0.00021179674149607308\n",
      "Validation Loss: 0.00021825823332006478\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00022706660099174768\n",
      "Training Loss: 0.0002077437482876121\n",
      "Training Loss: 0.00021087091237859568\n",
      "Validation Loss: 0.00021736934118717123\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.0002262244831206317\n",
      "Training Loss: 0.00020688579701527487\n",
      "Training Loss: 0.00020995232545828913\n",
      "Validation Loss: 0.0002164902112638353\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.00022538761017301567\n",
      "Training Loss: 0.0002060334237467032\n",
      "Training Loss: 0.00020904080338368658\n",
      "Validation Loss: 0.00021561899124657776\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.00022455562265463413\n",
      "Training Loss: 0.00020518678171356443\n",
      "Training Loss: 0.0002081365841513616\n",
      "Validation Loss: 0.00021475811592424864\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.00022372897416175874\n",
      "Training Loss: 0.00020434542650036748\n",
      "Training Loss: 0.000207239046212635\n",
      "Validation Loss: 0.00021390479553936917\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.00022290723813000568\n",
      "Training Loss: 0.00020350927428808063\n",
      "Training Loss: 0.0002063481281948043\n",
      "Validation Loss: 0.00021306054696559727\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.00022209064944604508\n",
      "Training Loss: 0.00020267827894713265\n",
      "Training Loss: 0.00020546413707052126\n",
      "Validation Loss: 0.00021222441901454147\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.00022127830357248967\n",
      "Training Loss: 0.00020185267723718425\n",
      "Training Loss: 0.000204587091175199\n",
      "Validation Loss: 0.0002113980668030543\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.00022047127138648647\n",
      "Training Loss: 0.00020103199261939153\n",
      "Training Loss: 0.00020371597369376105\n",
      "Validation Loss: 0.00021058038953597542\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.0002196690500989007\n",
      "Training Loss: 0.00020021656222525054\n",
      "Training Loss: 0.00020285181402869058\n",
      "Validation Loss: 0.00020977136297373812\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.00021887105686460017\n",
      "Training Loss: 0.00019940573740313993\n",
      "Training Loss: 0.00020199387086904607\n",
      "Validation Loss: 0.00020897056870790344\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.00021807760169565427\n",
      "Training Loss: 0.0001985996720759431\n",
      "Training Loss: 0.00020114201674005018\n",
      "Validation Loss: 0.0002081779056108358\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.00021728842529967097\n",
      "Training Loss: 0.00019779844162258088\n",
      "Training Loss: 0.0002002970129615278\n",
      "Validation Loss: 0.00020739473363259032\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.00021650427113172554\n",
      "Training Loss: 0.00019700186498084805\n",
      "Training Loss: 0.00019945765103329905\n",
      "Validation Loss: 0.00020661881333151622\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.00021572388723086532\n",
      "Training Loss: 0.00019620990380644798\n",
      "Training Loss: 0.00019862459663272603\n",
      "Validation Loss: 0.00020585193013768444\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.00021494814085485815\n",
      "Training Loss: 0.00019542236354027408\n",
      "Training Loss: 0.00019779733858740656\n",
      "Validation Loss: 0.00020509300500205508\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00021417614361894267\n",
      "Training Loss: 0.0001946395874983864\n",
      "Training Loss: 0.00019697601252119057\n",
      "Validation Loss: 0.00020434272693328854\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.00021340864000251258\n",
      "Training Loss: 0.0001938609894205001\n",
      "Training Loss: 0.0001961606729128107\n",
      "Validation Loss: 0.00020360125804626058\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.00021264490481598842\n",
      "Training Loss: 0.00019308674753119705\n",
      "Training Loss: 0.00019535096018444165\n",
      "Validation Loss: 0.00020286752722190767\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.00021188507215697428\n",
      "Training Loss: 0.00019231650130677736\n",
      "Training Loss: 0.0001945467320365424\n",
      "Validation Loss: 0.00020214182762409496\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.00021112914840159646\n",
      "Training Loss: 0.00019155051613779507\n",
      "Training Loss: 0.00019374830126253073\n",
      "Validation Loss: 0.00020142456531665925\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.00021037681808479646\n",
      "Training Loss: 0.00019078879715380025\n",
      "Training Loss: 0.0001929552710498683\n",
      "Validation Loss: 0.00020071550105139874\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.00020962824577736683\n",
      "Training Loss: 0.00019003072160558076\n",
      "Training Loss: 0.0001921675131416123\n",
      "Validation Loss: 0.00020001447124189562\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.0002088830626649951\n",
      "Training Loss: 0.00018927661472844192\n",
      "Training Loss: 0.0001913852118741488\n",
      "Validation Loss: 0.00019932084161267626\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.0002081413077075922\n",
      "Training Loss: 0.0001885265281816828\n",
      "Training Loss: 0.0001906080011758604\n",
      "Validation Loss: 0.00019863571336766275\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.00020740311368967922\n",
      "Training Loss: 0.00018777976982164545\n",
      "Training Loss: 0.00018983602656589939\n",
      "Validation Loss: 0.00019795843526777246\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.0002066681274015991\n",
      "Training Loss: 0.00018703696845477678\n",
      "Training Loss: 0.00018906880237409495\n",
      "Validation Loss: 0.00019728887896184298\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0002059363600085362\n",
      "Training Loss: 0.00018629782243806403\n",
      "Training Loss: 0.00018830704992069513\n",
      "Validation Loss: 0.0001966276512156879\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.0002052077388611906\n",
      "Training Loss: 0.00018556223272753414\n",
      "Training Loss: 0.000187549990714615\n",
      "Validation Loss: 0.00019597411579177766\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.00020448184826705074\n",
      "Training Loss: 0.00018482995430531445\n",
      "Training Loss: 0.00018679730343137635\n",
      "Validation Loss: 0.00019532853106835993\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.00020375922665209602\n",
      "Training Loss: 0.00018410106335068122\n",
      "Training Loss: 0.0001860499156282458\n",
      "Validation Loss: 0.00019469067014183514\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.0002030392051221952\n",
      "Training Loss: 0.00018337518769840245\n",
      "Training Loss: 0.0001853067731644842\n",
      "Validation Loss: 0.00019406021405547711\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.00020232148416369\n",
      "Training Loss: 0.0001826530396101589\n",
      "Training Loss: 0.00018456810461429994\n",
      "Validation Loss: 0.00019343802946576524\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.00020160686644715042\n",
      "Training Loss: 0.00018193341777077875\n",
      "Training Loss: 0.00018383369751973078\n",
      "Validation Loss: 0.0001928238282366028\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.00020089464023385518\n",
      "Training Loss: 0.00018121671348126256\n",
      "Training Loss: 0.00018310377456145943\n",
      "Validation Loss: 0.0001922174727379376\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.00020018426747242301\n",
      "Training Loss: 0.00018050348895485514\n",
      "Training Loss: 0.00018237761360069272\n",
      "Validation Loss: 0.0001916183409980221\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.00019947653881047245\n",
      "Training Loss: 0.00017979249572817933\n",
      "Training Loss: 0.00018165595949540147\n",
      "Validation Loss: 0.00019102663134801557\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.0001987704108205435\n",
      "Training Loss: 0.00017908427251313696\n",
      "Training Loss: 0.00018093803226292948\n",
      "Validation Loss: 0.0001904443396893696\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.00019806656045034287\n",
      "Training Loss: 0.00017837858917118865\n",
      "Training Loss: 0.00018022419000772062\n",
      "Validation Loss: 0.00018986818068196442\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.0001973646587981648\n",
      "Training Loss: 0.00017767533327059937\n",
      "Training Loss: 0.00017951370389710064\n",
      "Validation Loss: 0.00018930025458410433\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.0001966644408372531\n",
      "Training Loss: 0.00017697472427244065\n",
      "Training Loss: 0.00017880674566185916\n",
      "Validation Loss: 0.00018873904672944448\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0001959656222754802\n",
      "Training Loss: 0.00017627622768486616\n",
      "Training Loss: 0.0001781033647239383\n",
      "Validation Loss: 0.0001881863678095266\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.0001952681251282229\n",
      "Training Loss: 0.0001755795678400318\n",
      "Training Loss: 0.00017740352514010737\n",
      "Validation Loss: 0.00018764273932248124\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.0001945721574207937\n",
      "Training Loss: 0.00017488533643700066\n",
      "Training Loss: 0.00017670671611995205\n",
      "Validation Loss: 0.0001871029824215338\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.0001938769627702186\n",
      "Training Loss: 0.00017419266907381826\n",
      "Training Loss: 0.0001760130732509424\n",
      "Validation Loss: 0.00018657411448167\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0001931828920055523\n",
      "Training Loss: 0.00017350188976706705\n",
      "Training Loss: 0.00017532203904920606\n",
      "Validation Loss: 0.00018605360400103022\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.00019249000833497122\n",
      "Training Loss: 0.00017281274651395506\n",
      "Training Loss: 0.00017463411126300343\n",
      "Validation Loss: 0.00018553732743282708\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.00019179743066501941\n",
      "Training Loss: 0.00017212492786711664\n",
      "Training Loss: 0.00017394920039805583\n",
      "Validation Loss: 0.0001850312798800015\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.00019110562333366944\n",
      "Training Loss: 0.00017143862260127207\n",
      "Training Loss: 0.00017326628212686047\n",
      "Validation Loss: 0.00018453300137398504\n",
      "Validation Accuracy: 0.05266853932584269\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.0001904138648467324\n",
      "Training Loss: 0.00017075361449315097\n",
      "Training Loss: 0.00017258589676202973\n",
      "Validation Loss: 0.00018404076191278835\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.00018972246569319397\n",
      "Training Loss: 0.00017006963586027268\n",
      "Training Loss: 0.00017190795068017906\n",
      "Validation Loss: 0.00018355784086861092\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.00018903096406802433\n",
      "Training Loss: 0.00016938662960455986\n",
      "Training Loss: 0.0001712318276076985\n",
      "Validation Loss: 0.00018308249360579588\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.00018833927237665193\n",
      "Training Loss: 0.00016870466681211837\n",
      "Training Loss: 0.0001705580871202983\n",
      "Validation Loss: 0.0001826139661634545\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0001876474301093367\n",
      "Training Loss: 0.0001680230312740605\n",
      "Training Loss: 0.0001698854737514921\n",
      "Validation Loss: 0.00018215354515979947\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.00018695476491984665\n",
      "Training Loss: 0.00016734206557885044\n",
      "Training Loss: 0.00016921487098443323\n",
      "Validation Loss: 0.00018170288252334236\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.00018626143399842477\n",
      "Training Loss: 0.0001666610331994889\n",
      "Training Loss: 0.0001685454607832071\n",
      "Validation Loss: 0.0001812592013725969\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.00018556727541408692\n",
      "Training Loss: 0.00016598059482930694\n",
      "Training Loss: 0.00016787743092208985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [34:55<04:06, 246.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.00018082282814462774\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.683462620228529\n",
      "Training Loss: 0.5143584702908993\n",
      "Training Loss: 0.4062305615097284\n",
      "Validation Loss: 0.29426853220616833\n",
      "Validation Accuracy: 0.2633426966292135\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.24912343584001065\n",
      "Training Loss: 0.15952511331997812\n",
      "Training Loss: 0.11662242755293846\n",
      "Validation Loss: 0.07696480117738247\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.06650474824011327\n",
      "Training Loss: 0.05334077273961157\n",
      "Training Loss: 0.05498817914165557\n",
      "Validation Loss: 0.053123342333717294\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.05084321698639542\n",
      "Training Loss: 0.048397832335904244\n",
      "Training Loss: 0.050419600903987886\n",
      "Validation Loss: 0.04946747977872578\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.046962256648112086\n",
      "Training Loss: 0.04456062888726592\n",
      "Training Loss: 0.04591282916720957\n",
      "Validation Loss: 0.044831184816829274\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.04204986227210611\n",
      "Training Loss: 0.03957277008332312\n",
      "Training Loss: 0.0401293140091002\n",
      "Validation Loss: 0.03880994626729006\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.035688743842765686\n",
      "Training Loss: 0.032879936476238075\n",
      "Training Loss: 0.032316421223804355\n",
      "Validation Loss: 0.030523321740433908\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.027050991124124266\n",
      "Training Loss: 0.024020493251737208\n",
      "Training Loss: 0.022543067522346975\n",
      "Validation Loss: 0.020972721960154896\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.01751849236286944\n",
      "Training Loss: 0.014983102878322824\n",
      "Training Loss: 0.013483287529088556\n",
      "Validation Loss: 0.01356241663491479\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.010797061428893357\n",
      "Training Loss: 0.009944838678638917\n",
      "Training Loss: 0.009780903781065718\n",
      "Validation Loss: 0.010672536492870931\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.008459124058717862\n",
      "Training Loss: 0.008194149700575509\n",
      "Training Loss: 0.008227055059396662\n",
      "Validation Loss: 0.008941188281313045\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.007097683454339858\n",
      "Training Loss: 0.006986307858023792\n",
      "Training Loss: 0.00703182821162045\n",
      "Validation Loss: 0.007620579593166123\n",
      "Validation Accuracy: 0.0175561797752809\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.006019243667251431\n",
      "Training Loss: 0.005985603401204571\n",
      "Training Loss: 0.006022512825729791\n",
      "Validation Loss: 0.006560455092746931\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.005134746494877618\n",
      "Training Loss: 0.005153648724663071\n",
      "Training Loss: 0.0051854232378536836\n",
      "Validation Loss: 0.005715564162773865\n",
      "Validation Accuracy: 0.0877808988764045\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.004427037710847799\n",
      "Training Loss: 0.004481262844055891\n",
      "Training Loss: 0.004514028533012606\n",
      "Validation Loss: 0.0050557791837491095\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.0038748364564526127\n",
      "Training Loss: 0.003949622035142966\n",
      "Training Loss: 0.003986508245579898\n",
      "Validation Loss: 0.004546097925801375\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.003448736179670959\n",
      "Training Loss: 0.0035351060735411013\n",
      "Training Loss: 0.0035768419355736115\n",
      "Validation Loss: 0.00415390951714604\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.0031216366408989417\n",
      "Training Loss: 0.0032154700136743488\n",
      "Training Loss: 0.0032605271597276443\n",
      "Validation Loss: 0.0038513662257582326\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.0028703059774488793\n",
      "Training Loss: 0.0029693493689410388\n",
      "Training Loss: 0.003015102269419003\n",
      "Validation Loss: 0.0036149978918269318\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.002674912387265067\n",
      "Training Loss: 0.002777469397697132\n",
      "Training Loss: 0.002821586027566809\n",
      "Validation Loss: 0.0034257817229915283\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.002519591823220253\n",
      "Training Loss: 0.002624384549271781\n",
      "Training Loss: 0.0026653983606956897\n",
      "Validation Loss: 0.0032692342153567235\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0023926166138699047\n",
      "Training Loss: 0.002498796853615204\n",
      "Training Loss: 0.002536091764632147\n",
      "Validation Loss: 0.0031348722522523713\n",
      "Validation Accuracy: 0.1931179775280899\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.0022858035564422606\n",
      "Training Loss: 0.0023929039934591856\n",
      "Training Loss: 0.0024264666481758467\n",
      "Validation Loss: 0.0030154429913728676\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.002193638747485238\n",
      "Training Loss: 0.0023014597877045163\n",
      "Training Loss: 0.0023316386525402775\n",
      "Validation Loss: 0.0029060719142343554\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0021124735947159936\n",
      "Training Loss: 0.0022209672410099302\n",
      "Training Loss: 0.0022482844296609983\n",
      "Validation Loss: 0.0028036289905286867\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.0020399027343955822\n",
      "Training Loss: 0.002149059169314569\n",
      "Training Loss: 0.0021740996594598982\n",
      "Validation Loss: 0.0027062169346811897\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.0019743155511605438\n",
      "Training Loss: 0.002084078145562671\n",
      "Training Loss: 0.0021074118580145297\n",
      "Validation Loss: 0.0026127934561168587\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.0019145790734910405\n",
      "Training Loss: 0.0020247751173155847\n",
      "Training Loss: 0.002046918886480853\n",
      "Validation Loss: 0.0025229902772541586\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.0018598163015485624\n",
      "Training Loss: 0.0019701248468481937\n",
      "Training Loss: 0.0019915233421488663\n",
      "Validation Loss: 0.0024368695403302735\n",
      "Validation Accuracy: 0.12289325842696629\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.0018092695903033018\n",
      "Training Loss: 0.0019192211655899882\n",
      "Training Loss: 0.0019402469336637295\n",
      "Validation Loss: 0.002354759287698655\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.0017622338615183252\n",
      "Training Loss: 0.0018712393419991714\n",
      "Training Loss: 0.001892197136185132\n",
      "Validation Loss: 0.002277036212852978\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.0017180353069852572\n",
      "Training Loss: 0.0018254292232450099\n",
      "Training Loss: 0.001846565848827595\n",
      "Validation Loss: 0.002203957342511315\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.0016760338431777199\n",
      "Training Loss: 0.001781117004866246\n",
      "Training Loss: 0.0018026265465596225\n",
      "Validation Loss: 0.0021355476376229083\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.0016356286808149889\n",
      "Training Loss: 0.001737697553326143\n",
      "Training Loss: 0.0017597186524653807\n",
      "Validation Loss: 0.00207147486030459\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.0015962387155013858\n",
      "Training Loss: 0.001694590632978361\n",
      "Training Loss: 0.001717206167668337\n",
      "Validation Loss: 0.002011072420020682\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.001557271283800219\n",
      "Training Loss: 0.001651177342137089\n",
      "Training Loss: 0.0016743995207070838\n",
      "Validation Loss: 0.001953320139875175\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0015180455041809181\n",
      "Training Loss: 0.0016066852056246716\n",
      "Training Loss: 0.0016304417021456175\n",
      "Validation Loss: 0.0018968003340264301\n",
      "Validation Accuracy: 0.1580056179775281\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.001477683690654885\n",
      "Training Loss: 0.0015600000687118155\n",
      "Training Loss: 0.0015841076371725649\n",
      "Validation Loss: 0.0018395203428298464\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.0014349279933594517\n",
      "Training Loss: 0.0015093763401091565\n",
      "Training Loss: 0.0015335087846324313\n",
      "Validation Loss: 0.0017786342971121076\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.0013879536575768724\n",
      "Training Loss: 0.0014521441263786984\n",
      "Training Loss: 0.001475857594050467\n",
      "Validation Loss: 0.0017106413605473831\n",
      "Validation Accuracy: 0.22823033707865167\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.0013344448224597728\n",
      "Training Loss: 0.0013850475469371305\n",
      "Training Loss: 0.0014078975131269545\n",
      "Validation Loss: 0.0016333088494596606\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0012721350349966087\n",
      "Training Loss: 0.0013056013931054622\n",
      "Training Loss: 0.0013270761370949913\n",
      "Validation Loss: 0.0015476425081881265\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0011996513463236624\n",
      "Training Loss: 0.0012134352918656078\n",
      "Training Loss: 0.001233280088781612\n",
      "Validation Loss: 0.0014586916462821739\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0011187515575511496\n",
      "Training Loss: 0.0011130612208216917\n",
      "Training Loss: 0.001132491535099689\n",
      "Validation Loss: 0.0013761248539615255\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.0010366756307485048\n",
      "Training Loss: 0.001016164154134458\n",
      "Training Loss: 0.001036839728913037\n",
      "Validation Loss: 0.0013057899934741971\n",
      "Validation Accuracy: 0.2633426966292135\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.0009618657136161346\n",
      "Training Loss: 0.0009332552930573001\n",
      "Training Loss: 0.000954370265972102\n",
      "Validation Loss: 0.0012422345434459387\n",
      "Validation Accuracy: 0.2633426966292135\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0008961007521429565\n",
      "Training Loss: 0.0008648162261670222\n",
      "Training Loss: 0.0008842203758831601\n",
      "Validation Loss: 0.0011795168617292926\n",
      "Validation Accuracy: 0.2633426966292135\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0008371148050355259\n",
      "Training Loss: 0.0008066573581163539\n",
      "Training Loss: 0.0008232377713284222\n",
      "Validation Loss: 0.0011173158153509617\n",
      "Validation Accuracy: 0.2633426966292135\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.0007835502747184364\n",
      "Training Loss: 0.0007558730556047521\n",
      "Training Loss: 0.0007696102836780483\n",
      "Validation Loss: 0.001056746362572473\n",
      "Validation Accuracy: 0.2633426966292135\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.0007351081624437938\n",
      "Training Loss: 0.0007112739332660567\n",
      "Training Loss: 0.0007225629442837089\n",
      "Validation Loss: 0.0009983652544383148\n",
      "Validation Accuracy: 0.2633426966292135\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.0006918081023468403\n",
      "Training Loss: 0.0006724329554708675\n",
      "Training Loss: 0.0006816767554846592\n",
      "Validation Loss: 0.0009425060239521096\n",
      "Validation Accuracy: 0.2633426966292135\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.0006536789042729652\n",
      "Training Loss: 0.0006390681821358158\n",
      "Training Loss: 0.0006465437990118517\n",
      "Validation Loss: 0.0008896859138720052\n",
      "Validation Accuracy: 0.2633426966292135\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.0006206240683241049\n",
      "Training Loss: 0.0006107762212923262\n",
      "Training Loss: 0.0006166397096239962\n",
      "Validation Loss: 0.0008405150196206327\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.0005923567473291769\n",
      "Training Loss: 0.0005869850531598786\n",
      "Training Loss: 0.0005913189561397303\n",
      "Validation Loss: 0.0007955582483457087\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.0005684039476909675\n",
      "Training Loss: 0.0005670040171389701\n",
      "Training Loss: 0.0005698621631017886\n",
      "Validation Loss: 0.0007550946864060106\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0005481595981109422\n",
      "Training Loss: 0.0005501098545937566\n",
      "Training Loss: 0.0005515432382526341\n",
      "Validation Loss: 0.0007191187009840597\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0005309667163601262\n",
      "Training Loss: 0.0005356213566119549\n",
      "Training Loss: 0.0005356905879307306\n",
      "Validation Loss: 0.0006873300966964982\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.0005161895067067235\n",
      "Training Loss: 0.0005229477642569691\n",
      "Training Loss: 0.0005217212211573497\n",
      "Validation Loss: 0.0006592664308453586\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.0005032658547861502\n",
      "Training Loss: 0.0005116065643233015\n",
      "Training Loss: 0.0005091561289009406\n",
      "Validation Loss: 0.0006343999572962867\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.0004917281675807317\n",
      "Training Loss: 0.0005012220761273056\n",
      "Training Loss: 0.0004976185505802277\n",
      "Validation Loss: 0.0006121923577032585\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.00048120624262082857\n",
      "Training Loss: 0.000491510835418012\n",
      "Training Loss: 0.0004868196041934425\n",
      "Validation Loss: 0.0005921671067932202\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.00047141493632807396\n",
      "Training Loss: 0.00048226338505628517\n",
      "Training Loss: 0.0004765398743620608\n",
      "Validation Loss: 0.0005739121121489129\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.00046213941619498654\n",
      "Training Loss: 0.0004733247777039651\n",
      "Training Loss: 0.00046661371132358906\n",
      "Validation Loss: 0.0005570919845799989\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.000453215249144705\n",
      "Training Loss: 0.0004645784808235476\n",
      "Training Loss: 0.0004569125847046962\n",
      "Validation Loss: 0.0005414329590687915\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0004445170103281271\n",
      "Training Loss: 0.00045593099755933506\n",
      "Training Loss: 0.0004473328878520988\n",
      "Validation Loss: 0.0005267177631763291\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.0004359432134151575\n",
      "Training Loss: 0.00044730156019795686\n",
      "Training Loss: 0.00043778690313047266\n",
      "Validation Loss: 0.0005127851071530755\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.00042740943314129254\n",
      "Training Loss: 0.00043861306337930725\n",
      "Training Loss: 0.0004281973223260138\n",
      "Validation Loss: 0.0004994917865822961\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.00041884356702212247\n",
      "Training Loss: 0.0004297885880805552\n",
      "Training Loss: 0.0004184981603611959\n",
      "Validation Loss: 0.00048673460206077066\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.00041018849893589506\n",
      "Training Loss: 0.00042075576006027403\n",
      "Training Loss: 0.00040864253438485323\n",
      "Validation Loss: 0.00047442810948391373\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.00040141014487744543\n",
      "Training Loss: 0.00041146191375446506\n",
      "Training Loss: 0.00039861626173660626\n",
      "Validation Loss: 0.000462501904396785\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.00039250859790627144\n",
      "Training Loss: 0.0004018947027361719\n",
      "Training Loss: 0.00038844982780574357\n",
      "Validation Loss: 0.00045089291716373807\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.0003835238161991583\n",
      "Training Loss: 0.00039209766473504717\n",
      "Training Loss: 0.00037822115089511496\n",
      "Validation Loss: 0.0004395482747905804\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.0003745286216872046\n",
      "Training Loss: 0.0003821669102762826\n",
      "Training Loss: 0.00036804007104365157\n",
      "Validation Loss: 0.0004284187756986305\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.0003656120402229135\n",
      "Training Loss: 0.0003722277183260303\n",
      "Training Loss: 0.0003580265709024388\n",
      "Validation Loss: 0.000417451238115135\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.0003568590410941397\n",
      "Training Loss: 0.0003624082732130773\n",
      "Training Loss: 0.0003482874580367934\n",
      "Validation Loss: 0.00040661892036153945\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00034834051250072663\n",
      "Training Loss: 0.0003528196601109812\n",
      "Training Loss: 0.00033890850543684794\n",
      "Validation Loss: 0.0003959216080014751\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00034010997875157047\n",
      "Training Loss: 0.00034354678391537166\n",
      "Training Loss: 0.0003299483336013509\n",
      "Validation Loss: 0.0003853825239295904\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00033220446170162176\n",
      "Training Loss: 0.0003346477836021222\n",
      "Training Loss: 0.0003214430288790027\n",
      "Validation Loss: 0.00037504949268827404\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.0003246474780826247\n",
      "Training Loss: 0.00032615959986287635\n",
      "Training Loss: 0.00031340836772869807\n",
      "Validation Loss: 0.0003649825655352952\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.0003174515097271069\n",
      "Training Loss: 0.00031809924654226053\n",
      "Training Loss: 0.0003058426325878827\n",
      "Validation Loss: 0.0003552354425531969\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0003106186632430763\n",
      "Training Loss: 0.0003104676207294688\n",
      "Training Loss: 0.00029873166677134576\n",
      "Validation Loss: 0.0003458613455733755\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.00030414275697694394\n",
      "Training Loss: 0.00030325511917908446\n",
      "Training Loss: 0.0002920509924297221\n",
      "Validation Loss: 0.00033689793233636746\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.0002980120545544196\n",
      "Training Loss: 0.0002964440313371597\n",
      "Training Loss: 0.0002857701712855487\n",
      "Validation Loss: 0.0003283738339806415\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.0002922092017979594\n",
      "Training Loss: 0.0002900100408078288\n",
      "Training Loss: 0.0002798557762071141\n",
      "Validation Loss: 0.00032029777564653825\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.0002867143647381454\n",
      "Training Loss: 0.0002839273720019264\n",
      "Training Loss: 0.0002742740024405066\n",
      "Validation Loss: 0.00031267523061185865\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.0002815073300007498\n",
      "Training Loss: 0.00027816920592158565\n",
      "Training Loss: 0.00026899291631707453\n",
      "Validation Loss: 0.0003054926516566677\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.0002765670055669034\n",
      "Training Loss: 0.00027270910159131744\n",
      "Training Loss: 0.00026398336376587395\n",
      "Validation Loss: 0.0002987382549179713\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.0002718727441242663\n",
      "Training Loss: 0.0002675224516133312\n",
      "Training Loss: 0.00025921938566170866\n",
      "Validation Loss: 0.0002923869282864636\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.0002674061990910559\n",
      "Training Loss: 0.00026258744652295717\n",
      "Training Loss: 0.00025467925628618103\n",
      "Validation Loss: 0.00028641398852557635\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.00026315015220461645\n",
      "Training Loss: 0.0002578842217189958\n",
      "Training Loss: 0.00025034314385266045\n",
      "Validation Loss: 0.00028079217294913855\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00025908929983415875\n",
      "Training Loss: 0.00025339514981169485\n",
      "Training Loss: 0.0002461955888065859\n",
      "Validation Loss: 0.0002754894624793839\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.00025520960334688423\n",
      "Training Loss: 0.00024910541924327845\n",
      "Training Loss: 0.00024222373562224675\n",
      "Validation Loss: 0.00027048230369698277\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.0002514991437419667\n",
      "Training Loss: 0.0002450020649484941\n",
      "Training Loss: 0.00023841586265916704\n",
      "Validation Loss: 0.0002657441685013499\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.00024794700970232953\n",
      "Training Loss: 0.0002410739887636737\n",
      "Training Loss: 0.00023476228896470274\n",
      "Validation Loss: 0.0002612507928818794\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0002445440767405671\n",
      "Training Loss: 0.00023731082688755123\n",
      "Training Loss: 0.0002312552881630836\n",
      "Validation Loss: 0.00025697907187166265\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.00024128135108185235\n",
      "Training Loss: 0.00023370520892058267\n",
      "Training Loss: 0.0002278875605770736\n",
      "Validation Loss: 0.0002529116583205864\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.00023815180713427254\n",
      "Training Loss: 0.00023024891183013097\n",
      "Training Loss: 0.00022465282469056547\n",
      "Validation Loss: 0.00024903120503766104\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.00023514886968769132\n",
      "Training Loss: 0.00022693603663356043\n",
      "Training Loss: 0.00022154557544126874\n",
      "Validation Loss: 0.00024532510401462624\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.00023226655855978606\n",
      "Training Loss: 0.00022376035842171403\n",
      "Training Loss: 0.00021856064475286985\n",
      "Validation Loss: 0.00024177965225247676\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.000229499106608273\n",
      "Training Loss: 0.0002207155436190078\n",
      "Training Loss: 0.0002156929545890307\n",
      "Validation Loss: 0.00023838151222601103\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.00022684114432195202\n",
      "Training Loss: 0.00021779725393571424\n",
      "Training Loss: 0.00021293783211149276\n",
      "Validation Loss: 0.00023512352561922168\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.00022428799504268683\n",
      "Training Loss: 0.0002150002103007864\n",
      "Training Loss: 0.00021029087496572175\n",
      "Validation Loss: 0.00023199867922649197\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.0002218349037502776\n",
      "Training Loss: 0.0002123192575891153\n",
      "Training Loss: 0.0002077476689737523\n",
      "Validation Loss: 0.0002289964510729599\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.00021947720040770946\n",
      "Training Loss: 0.00020974941144231708\n",
      "Training Loss: 0.00020530387584585696\n",
      "Validation Loss: 0.0002261124342173767\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.0002172107811566093\n",
      "Training Loss: 0.00020728678406157998\n",
      "Training Loss: 0.00020295493299272494\n",
      "Validation Loss: 0.0002233403878501653\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.00021503112377104115\n",
      "Training Loss: 0.0002049255856400123\n",
      "Training Loss: 0.00020069711210453532\n",
      "Validation Loss: 0.00022067373507790884\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.00021293406560289442\n",
      "Training Loss: 0.00020266173440177225\n",
      "Training Loss: 0.0001985259395587491\n",
      "Validation Loss: 0.00021810900968345133\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0002109156893493491\n",
      "Training Loss: 0.00020049020888109225\n",
      "Training Loss: 0.0001964370460882492\n",
      "Validation Loss: 0.00021564034176754944\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.00020897189617244294\n",
      "Training Loss: 0.00019840652275888716\n",
      "Training Loss: 0.00019442656206592802\n",
      "Validation Loss: 0.00021326217119054859\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.00020709806954982924\n",
      "Training Loss: 0.00019640600748971337\n",
      "Training Loss: 0.0001924904523002624\n",
      "Validation Loss: 0.00021096923461463824\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.00020529135064862203\n",
      "Training Loss: 0.00019448432365607005\n",
      "Training Loss: 0.00019062484134337864\n",
      "Validation Loss: 0.0002087597875676662\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0002035474943841109\n",
      "Training Loss: 0.00019263686390331715\n",
      "Training Loss: 0.0001888255173980724\n",
      "Validation Loss: 0.00020662740627833476\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.00020186303368973312\n",
      "Training Loss: 0.00019085903419181706\n",
      "Training Loss: 0.000187089270020806\n",
      "Validation Loss: 0.00020456751784815954\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0002002342537707591\n",
      "Training Loss: 0.00018914693966507913\n",
      "Training Loss: 0.0001854121346696047\n",
      "Validation Loss: 0.00020257948726589602\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.00019865817897880334\n",
      "Training Loss: 0.0001874965933347994\n",
      "Training Loss: 0.00018379099670710276\n",
      "Validation Loss: 0.00020065742161901918\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.0001971315442096966\n",
      "Training Loss: 0.0001859041461830202\n",
      "Training Loss: 0.00018222217753645963\n",
      "Validation Loss: 0.00019879650651265376\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.00019565130429327837\n",
      "Training Loss: 0.000184365742643422\n",
      "Training Loss: 0.0001807027557697438\n",
      "Validation Loss: 0.00019699749190182415\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.00019421485001657856\n",
      "Training Loss: 0.00018287790357135236\n",
      "Training Loss: 0.00017923000727023463\n",
      "Validation Loss: 0.0001952525950462055\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.0001928190467697277\n",
      "Training Loss: 0.0001814369401654403\n",
      "Training Loss: 0.00017780064739781666\n",
      "Validation Loss: 0.0001935598148530731\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.00019146134496622836\n",
      "Training Loss: 0.0001800401966102072\n",
      "Training Loss: 0.00017641218430071602\n",
      "Validation Loss: 0.00019191918663943468\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.00019014004603377544\n",
      "Training Loss: 0.00017868430470116436\n",
      "Training Loss: 0.0001750622767576715\n",
      "Validation Loss: 0.0001903238681029251\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.0001888524967216654\n",
      "Training Loss: 0.0001773665678956604\n",
      "Training Loss: 0.00017374832761561266\n",
      "Validation Loss: 0.00018877316631077203\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.00018759646192847867\n",
      "Training Loss: 0.000176084439499391\n",
      "Training Loss: 0.0001724679739345447\n",
      "Validation Loss: 0.00018726440223903145\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.0001863703229901148\n",
      "Training Loss: 0.0001748354607479996\n",
      "Training Loss: 0.00017121919847340906\n",
      "Validation Loss: 0.0001857952522002736\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.00018517215172323632\n",
      "Training Loss: 0.00017361730449920289\n",
      "Training Loss: 0.0001700004235317465\n",
      "Validation Loss: 0.00018436252405203805\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.0001840002189601364\n",
      "Training Loss: 0.0001724282199938898\n",
      "Training Loss: 0.0001688093138000113\n",
      "Validation Loss: 0.000182965079066082\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.00018285315058164998\n",
      "Training Loss: 0.00017126567634477398\n",
      "Training Loss: 0.0001676447785939672\n",
      "Validation Loss: 0.00018160268690520793\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.00018172929925640345\n",
      "Training Loss: 0.00017012814510962925\n",
      "Training Loss: 0.0001665045062509307\n",
      "Validation Loss: 0.0001802709548121967\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00018062795026708045\n",
      "Training Loss: 0.00016901396917091917\n",
      "Training Loss: 0.00016538753421627917\n",
      "Validation Loss: 0.00017896885601862523\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.00017954694341824507\n",
      "Training Loss: 0.00016792159216493018\n",
      "Training Loss: 0.0001642925784472027\n",
      "Validation Loss: 0.0001776936933515316\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.00017848551833594685\n",
      "Training Loss: 0.00016684938596881694\n",
      "Training Loss: 0.0001632178390173067\n",
      "Validation Loss: 0.00017644641861244985\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00017744300161211867\n",
      "Training Loss: 0.00016579622630160884\n",
      "Training Loss: 0.00016216254414757713\n",
      "Validation Loss: 0.0001752243490168286\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.00017641828580963192\n",
      "Training Loss: 0.0001647608150051383\n",
      "Training Loss: 0.00016112512506879283\n",
      "Validation Loss: 0.00017402494917957248\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.00017540990236739162\n",
      "Training Loss: 0.00016374214568713795\n",
      "Training Loss: 0.0001601050307363039\n",
      "Validation Loss: 0.00017284909458641288\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.00017441749670979334\n",
      "Training Loss: 0.00016273880029984866\n",
      "Training Loss: 0.00015910111851553667\n",
      "Validation Loss: 0.00017169357952400205\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.0001734401861358492\n",
      "Training Loss: 0.00016175024360563838\n",
      "Training Loss: 0.00015811255831067684\n",
      "Validation Loss: 0.0001705584286815445\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00017247721863896003\n",
      "Training Loss: 0.00016077555184892844\n",
      "Training Loss: 0.00015713847509687184\n",
      "Validation Loss: 0.00016944298836738762\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.00017152796739537733\n",
      "Training Loss: 0.00015981379207005375\n",
      "Training Loss: 0.00015617784361893428\n",
      "Validation Loss: 0.00016834425949720466\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.00017059191084626947\n",
      "Training Loss: 0.00015886426701399613\n",
      "Training Loss: 0.00015523029289397528\n",
      "Validation Loss: 0.0001672635129110082\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00016966818286164197\n",
      "Training Loss: 0.00015792608703122824\n",
      "Training Loss: 0.00015429477190991747\n",
      "Validation Loss: 0.00016619955968462307\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.00016875633935342193\n",
      "Training Loss: 0.00015699850540841\n",
      "Training Loss: 0.00015337111508415547\n",
      "Validation Loss: 0.0001651509469155918\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.00016785586027253884\n",
      "Training Loss: 0.0001560812534262368\n",
      "Training Loss: 0.000152458493685117\n",
      "Validation Loss: 0.00016411723265080357\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00016696636157575995\n",
      "Training Loss: 0.00015517387953877914\n",
      "Training Loss: 0.00015155595780015575\n",
      "Validation Loss: 0.00016309609512400844\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.00016608728050414355\n",
      "Training Loss: 0.0001542755045738886\n",
      "Training Loss: 0.00015066376548929838\n",
      "Validation Loss: 0.00016208943994120654\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.00016521823310540639\n",
      "Training Loss: 0.00015338598466769327\n",
      "Training Loss: 0.00014978059487475549\n",
      "Validation Loss: 0.0001610942940561891\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.00016435900126452908\n",
      "Training Loss: 0.00015250470487444546\n",
      "Training Loss: 0.00014890679409290897\n",
      "Validation Loss: 0.0001601122679694731\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.00016350887539374527\n",
      "Training Loss: 0.0001516309118414938\n",
      "Training Loss: 0.00014804138232648255\n",
      "Validation Loss: 0.00015914187094770907\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.0001626675992702076\n",
      "Training Loss: 0.000150764627251192\n",
      "Training Loss: 0.00014718424366947147\n",
      "Validation Loss: 0.00015818201231258776\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.00016183511796043603\n",
      "Training Loss: 0.00014990537478297482\n",
      "Training Loss: 0.00014633505628808052\n",
      "Validation Loss: 0.00015723206050690064\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.0001610105636063963\n",
      "Training Loss: 0.00014905308666129713\n",
      "Training Loss: 0.0001454931274201954\n",
      "Validation Loss: 0.0001562927162094536\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.00016019449292798527\n",
      "Training Loss: 0.00014820738912021625\n",
      "Training Loss: 0.0001446581323943974\n",
      "Validation Loss: 0.00015536265484851345\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.0001593857457919512\n",
      "Training Loss: 0.00014736755432750214\n",
      "Training Loss: 0.00014383024305061553\n",
      "Validation Loss: 0.00015444135220768452\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.00015858450622545207\n",
      "Training Loss: 0.00014653376332717015\n",
      "Training Loss: 0.00014300881884992122\n",
      "Validation Loss: 0.0001535287362835998\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.00015779050518176517\n",
      "Training Loss: 0.00014570534493032027\n",
      "Training Loss: 0.00014219340999261477\n",
      "Validation Loss: 0.00015262357265691094\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.00015700357036621426\n",
      "Training Loss: 0.00014488280139630661\n",
      "Training Loss: 0.00014138432637992082\n",
      "Validation Loss: 0.00015172881452844798\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.00015622351897036423\n",
      "Training Loss: 0.00014406542464712402\n",
      "Training Loss: 0.00014058101121918297\n",
      "Validation Loss: 0.00015083888710445064\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.00015544995818345342\n",
      "Training Loss: 0.00014325278943942975\n",
      "Training Loss: 0.00013978315275380736\n",
      "Validation Loss: 0.00014995667934885348\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.00015468257792235817\n",
      "Training Loss: 0.00014244532392694963\n",
      "Training Loss: 0.00013899077081077848\n",
      "Validation Loss: 0.00014908153879628883\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.00015392138258903286\n",
      "Training Loss: 0.00014164238276862307\n",
      "Training Loss: 0.00013820356829455705\n",
      "Validation Loss: 0.00014821201315744169\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00015316615616029595\n",
      "Training Loss: 0.00014084384662055527\n",
      "Training Loss: 0.0001374212611335679\n",
      "Validation Loss: 0.00014735122255096734\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.00015241714691001108\n",
      "Training Loss: 0.00014004988202941603\n",
      "Training Loss: 0.00013664380836416967\n",
      "Validation Loss: 0.00014649519138136262\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.00015167362100328318\n",
      "Training Loss: 0.00013926036772318184\n",
      "Training Loss: 0.00013587091783847426\n",
      "Validation Loss: 0.00014564670916084917\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.000150936118188838\n",
      "Training Loss: 0.00013847460058968863\n",
      "Training Loss: 0.00013510275770386215\n",
      "Validation Loss: 0.00014480161859936426\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.00015020375947642607\n",
      "Training Loss: 0.00013769321141808178\n",
      "Training Loss: 0.000134338890111394\n",
      "Validation Loss: 0.00014396479356660523\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.00014947701700293692\n",
      "Training Loss: 0.00013691564132386702\n",
      "Training Loss: 0.00013357960242501576\n",
      "Validation Loss: 0.00014313040499861897\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.0001487554820414516\n",
      "Training Loss: 0.00013614249690363067\n",
      "Training Loss: 0.00013282450277984025\n",
      "Validation Loss: 0.00014230272854240795\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.0001480392135999864\n",
      "Training Loss: 0.00013537299459130735\n",
      "Training Loss: 0.00013207327720010653\n",
      "Validation Loss: 0.00014147990151194676\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.0001473283061022812\n",
      "Training Loss: 0.00013460717967973324\n",
      "Training Loss: 0.00013132655494700883\n",
      "Validation Loss: 0.0001406620422437371\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.0001466221210102958\n",
      "Training Loss: 0.00013384509155002887\n",
      "Training Loss: 0.0001305833781589172\n",
      "Validation Loss: 0.00013984823740071753\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.00014592101179005111\n",
      "Training Loss: 0.00013308675135704107\n",
      "Training Loss: 0.0001298443117957504\n",
      "Validation Loss: 0.00013904015759180766\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.00014522504465276143\n",
      "Training Loss: 0.0001323322204189026\n",
      "Training Loss: 0.000129109263416467\n",
      "Validation Loss: 0.00013823599582475876\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.00014453385965680355\n",
      "Training Loss: 0.0001315811999302241\n",
      "Training Loss: 0.00012837783486247645\n",
      "Validation Loss: 0.00013743584131180398\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.00014384751511897775\n",
      "Training Loss: 0.00013083395344438032\n",
      "Training Loss: 0.00012765035096890643\n",
      "Validation Loss: 0.000136641070353465\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.00014316610613605008\n",
      "Training Loss: 0.00013009031034016516\n",
      "Training Loss: 0.00012692670252363315\n",
      "Validation Loss: 0.0001358502890929067\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.00014248943822167348\n",
      "Training Loss: 0.00012935021800331015\n",
      "Training Loss: 0.0001262067666721123\n",
      "Validation Loss: 0.00013506368872839722\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.0001418176287188544\n",
      "Training Loss: 0.00012861434937804005\n",
      "Training Loss: 0.00012549031862363336\n",
      "Validation Loss: 0.00013428241550939398\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.00014115061832853826\n",
      "Training Loss: 0.00012788178249138583\n",
      "Training Loss: 0.00012477770904297357\n",
      "Validation Loss: 0.00013350321949453725\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.00014048846409423277\n",
      "Training Loss: 0.00012715293795736215\n",
      "Training Loss: 0.00012406914220264298\n",
      "Validation Loss: 0.0001327319383522132\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.00013983101212943438\n",
      "Training Loss: 0.00012642807025258662\n",
      "Training Loss: 0.00012336410360148876\n",
      "Validation Loss: 0.00013196165152263613\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.0001391786775093351\n",
      "Training Loss: 0.00012570697758746975\n",
      "Training Loss: 0.0001226631308963988\n",
      "Validation Loss: 0.00013119685432132664\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.000138530778385757\n",
      "Training Loss: 0.00012499001256401242\n",
      "Training Loss: 0.00012196572022730834\n",
      "Validation Loss: 0.00013043674257102558\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.00013788807458695374\n",
      "Training Loss: 0.00012427665368704765\n",
      "Training Loss: 0.0001212723271601135\n",
      "Validation Loss: 0.000129680854125796\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.00013725018361583353\n",
      "Training Loss: 0.00012356749307400604\n",
      "Training Loss: 0.00012058281445206375\n",
      "Validation Loss: 0.00012892929667389816\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.0001366171397967264\n",
      "Training Loss: 0.00012286232335100066\n",
      "Training Loss: 0.00011989753986199503\n",
      "Validation Loss: 0.00012818067656287773\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.00013598923223980818\n",
      "Training Loss: 0.00012216162615004576\n",
      "Training Loss: 0.00011921620131033706\n",
      "Validation Loss: 0.00012743877087313592\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.000135366139402322\n",
      "Training Loss: 0.0001214649364737852\n",
      "Training Loss: 0.00011853882273499039\n",
      "Validation Loss: 0.00012669941319922492\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.00013474831588609958\n",
      "Training Loss: 0.00012077287151441851\n",
      "Training Loss: 0.00011786554407080985\n",
      "Validation Loss: 0.00012596574140005625\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.00013413552125712158\n",
      "Training Loss: 0.00012008512934698957\n",
      "Training Loss: 0.00011719667582838156\n",
      "Validation Loss: 0.00012523668216969137\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.00013352779330489284\n",
      "Training Loss: 0.00011940195678107557\n",
      "Training Loss: 0.00011653200022919919\n",
      "Validation Loss: 0.00012451161212615508\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.00013292531890328973\n",
      "Training Loss: 0.00011872351841702766\n",
      "Training Loss: 0.00011587199021050764\n",
      "Validation Loss: 0.00012379218323883506\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.00013232818013420911\n",
      "Training Loss: 0.00011804993020632537\n",
      "Training Loss: 0.00011521620635903673\n",
      "Validation Loss: 0.00012307715242263585\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.00013173652227123966\n",
      "Training Loss: 0.00011738136917301745\n",
      "Training Loss: 0.00011456502914370504\n",
      "Validation Loss: 0.00012236801811606424\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.00013114984734784228\n",
      "Training Loss: 0.00011671766184008447\n",
      "Training Loss: 0.00011391863798053237\n",
      "Validation Loss: 0.00012166384960968816\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.0001305688630327495\n",
      "Training Loss: 0.00011605925736148493\n",
      "Training Loss: 0.00011327702830385533\n",
      "Validation Loss: 0.00012096481018335418\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.00012999341975046265\n",
      "Training Loss: 0.00011540596116901724\n",
      "Training Loss: 0.0001126402207864885\n",
      "Validation Loss: 0.00012027165847827205\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.00012942339593791986\n",
      "Training Loss: 0.00011475831887764798\n",
      "Training Loss: 0.000112008409896589\n",
      "Validation Loss: 0.00011958279934292529\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0001288590979675064\n",
      "Training Loss: 0.00011411596616198948\n",
      "Training Loss: 0.00011138160833070288\n",
      "Validation Loss: 0.00011890131310302126\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.00012830036482228024\n",
      "Training Loss: 0.00011347932913849946\n",
      "Training Loss: 0.00011076006810981198\n",
      "Validation Loss: 0.00011822360817420171\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.00012774703675859201\n",
      "Training Loss: 0.00011284856462225435\n",
      "Training Loss: 0.0001101436831686442\n",
      "Validation Loss: 0.00011755334947266736\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.00012719981958071003\n",
      "Training Loss: 0.00011222350201933295\n",
      "Training Loss: 0.00010953268142657179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [39:11<00:00, 235.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.00011688769931363445\n",
      "Validation Accuracy: 0.24578651685393257\n",
      "**************************************************\n",
      "Shape of the data after splitting into sequences: (22797, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2841, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2840, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Loss: 0.19412238586694003\n",
      "Training Loss: 0.14261591851711272\n",
      "Training Loss: 0.10310214936733246\n",
      "Training Loss: 0.07291305492632091\n",
      "Training Loss: 0.058897466389462354\n",
      "Training Loss: 0.052454485204070804\n",
      "Training Loss: 0.05528048830106855\n",
      "Validation Loss: 0.05390913003169642\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.05481846421957016\n",
      "Training Loss: 0.0528746566362679\n",
      "Training Loss: 0.05037756336852908\n",
      "Training Loss: 0.04670293960953131\n",
      "Training Loss: 0.04300972523633391\n",
      "Training Loss: 0.03772631625644863\n",
      "Training Loss: 0.03872961881570518\n",
      "Validation Loss: 0.03796613875999768\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.03930989015847444\n",
      "Training Loss: 0.037530330969020725\n",
      "Training Loss: 0.034363174280151725\n",
      "Training Loss: 0.03019074190349784\n",
      "Training Loss: 0.025133619108237325\n",
      "Training Loss: 0.021676753619685768\n",
      "Training Loss: 0.021575515307486057\n",
      "Validation Loss: 0.023109640140428246\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.025297817806713283\n",
      "Training Loss: 0.024391024676151573\n",
      "Training Loss: 0.02199651511386037\n",
      "Training Loss: 0.01815976471407339\n",
      "Training Loss: 0.013535990926902742\n",
      "Training Loss: 0.011992973443120718\n",
      "Training Loss: 0.011828673197887838\n",
      "Validation Loss: 0.014881977584729257\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.01740201875800267\n",
      "Training Loss: 0.017172097528818996\n",
      "Training Loss: 0.015732206562533976\n",
      "Training Loss: 0.012318836892955005\n",
      "Training Loss: 0.008178022959036753\n",
      "Training Loss: 0.00752813398023136\n",
      "Training Loss: 0.007434469904983416\n",
      "Validation Loss: 0.010476197586970383\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.01346149435499683\n",
      "Training Loss: 0.013481542699737474\n",
      "Training Loss: 0.012547626886516809\n",
      "Training Loss: 0.00941706454352243\n",
      "Training Loss: 0.005517984930193052\n",
      "Training Loss: 0.0052213513077003885\n",
      "Training Loss: 0.005138189599383622\n",
      "Validation Loss: 0.0074966293777084525\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.01107894923305139\n",
      "Training Loss: 0.011174417128786444\n",
      "Training Loss: 0.010539209726266563\n",
      "Training Loss: 0.007638392425869825\n",
      "Training Loss: 0.0039046560856513678\n",
      "Training Loss: 0.003792403948609717\n",
      "Training Loss: 0.0037405406188918276\n",
      "Validation Loss: 0.005508456257527715\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.009555005019064993\n",
      "Training Loss: 0.009720970389898866\n",
      "Training Loss: 0.009334704857319593\n",
      "Training Loss: 0.006611516309640138\n",
      "Training Loss: 0.0030028470954857767\n",
      "Training Loss: 0.0030024959731963464\n",
      "Training Loss: 0.002978419434512034\n",
      "Validation Loss: 0.004451964882879138\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.008737801408860832\n",
      "Training Loss: 0.008953514252789318\n",
      "Training Loss: 0.008702621411066503\n",
      "Training Loss: 0.006067312129162019\n",
      "Training Loss: 0.0025145329802762715\n",
      "Training Loss: 0.002557313686411362\n",
      "Training Loss: 0.002528950221312698\n",
      "Validation Loss: 0.0038405438326560796\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.008273018290055915\n",
      "Training Loss: 0.00852038426673971\n",
      "Training Loss: 0.008332290801918133\n",
      "Training Loss: 0.0057379051542375235\n",
      "Training Loss: 0.0022087477915920316\n",
      "Training Loss: 0.0022672536585014312\n",
      "Training Loss: 0.002230383532587439\n",
      "Validation Loss: 0.0034414291670040163\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.007985188874881715\n",
      "Training Loss: 0.008254067864036187\n",
      "Training Loss: 0.008096840154612437\n",
      "Training Loss: 0.005521589632116956\n",
      "Training Loss: 0.002004890668904409\n",
      "Training Loss: 0.002069031050777994\n",
      "Training Loss: 0.002025384056614712\n",
      "Validation Loss: 0.0031691264338906478\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.0077925960847642275\n",
      "Training Loss: 0.008070971636334434\n",
      "Training Loss: 0.007928798731882125\n",
      "Training Loss: 0.005366757423907984\n",
      "Training Loss: 0.0018646500328031834\n",
      "Training Loss: 0.0019303077350195962\n",
      "Training Loss: 0.0018812755137332716\n",
      "Validation Loss: 0.0029764049573128256\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.00764557565911673\n",
      "Training Loss: 0.00792385875247419\n",
      "Training Loss: 0.007790270686382428\n",
      "Training Loss: 0.005245013699750416\n",
      "Training Loss: 0.0017660628506564536\n",
      "Training Loss: 0.001831257100275252\n",
      "Training Loss: 0.0017778203281341122\n",
      "Validation Loss: 0.0028346824736163465\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.007518509529763833\n",
      "Training Loss: 0.007791074641281739\n",
      "Training Loss: 0.007664489309536293\n",
      "Training Loss: 0.005142715467372909\n",
      "Training Loss: 0.0016962814245198388\n",
      "Training Loss: 0.0017597767984261737\n",
      "Training Loss: 0.0017028749015298673\n",
      "Validation Loss: 0.002727179846696709\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.007401289012050256\n",
      "Training Loss: 0.007665495879482478\n",
      "Training Loss: 0.007546287794830278\n",
      "Training Loss: 0.005054261147306533\n",
      "Training Loss: 0.0016477631378802472\n",
      "Training Loss: 0.001708544770226581\n",
      "Training Loss: 0.0016491978193516844\n",
      "Validation Loss: 0.0026442325370532745\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.00729153812630102\n",
      "Training Loss: 0.007546579611953348\n",
      "Training Loss: 0.00743562109186314\n",
      "Training Loss: 0.0049777716532116755\n",
      "Training Loss: 0.0016157894919160754\n",
      "Training Loss: 0.0016728781511483248\n",
      "Training Loss: 0.001612155628099572\n",
      "Validation Loss: 0.0025800572074287003\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.00719000322627835\n",
      "Training Loss: 0.007436132509028539\n",
      "Training Loss: 0.007334156963042915\n",
      "Training Loss: 0.004912683152506361\n",
      "Training Loss: 0.0015968812530627475\n",
      "Training Loss: 0.0016493417463789228\n",
      "Training Loss: 0.0015882564125058707\n",
      "Validation Loss: 0.0025307415030357753\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.00709816541406326\n",
      "Training Loss: 0.007336207403568551\n",
      "Training Loss: 0.00724355028825812\n",
      "Training Loss: 0.004858488659665454\n",
      "Training Loss: 0.0015879658817721065\n",
      "Training Loss: 0.001635021600377513\n",
      "Training Loss: 0.0015744263245142066\n",
      "Validation Loss: 0.0024931686471455316\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.007017081604572013\n",
      "Training Loss: 0.007248095026006922\n",
      "Training Loss: 0.007164652079809457\n",
      "Training Loss: 0.004814247123140376\n",
      "Training Loss: 0.0015861606957332697\n",
      "Training Loss: 0.0016273013083264233\n",
      "Training Loss: 0.0015678264400048646\n",
      "Validation Loss: 0.0024646373938948737\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.0069469615910202264\n",
      "Training Loss: 0.007172015596879646\n",
      "Training Loss: 0.007097330800024793\n",
      "Training Loss: 0.004778572780778632\n",
      "Training Loss: 0.0015888671115681064\n",
      "Training Loss: 0.0016239208326442166\n",
      "Training Loss: 0.0015659575711470097\n",
      "Validation Loss: 0.002442829169407975\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.006887212105793878\n",
      "Training Loss: 0.007107260109623894\n",
      "Training Loss: 0.007040662675863132\n",
      "Training Loss: 0.00474987193767447\n",
      "Training Loss: 0.0015939664181496482\n",
      "Training Loss: 0.001623079766286537\n",
      "Training Loss: 0.0015668115811422467\n",
      "Validation Loss: 0.002425850463323888\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.006836705947062\n",
      "Training Loss: 0.007052529966458678\n",
      "Training Loss: 0.0069932734849862755\n",
      "Training Loss: 0.004726593517843867\n",
      "Training Loss: 0.0015999088545504492\n",
      "Training Loss: 0.0016234820356476121\n",
      "Training Loss: 0.0015689250305877067\n",
      "Validation Loss: 0.002412263320127593\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.006794079272076488\n",
      "Training Loss: 0.007006271646823734\n",
      "Training Loss: 0.0069536279502790425\n",
      "Training Loss: 0.0047073899555834945\n",
      "Training Loss: 0.001605704044486629\n",
      "Training Loss: 0.0016242894089373294\n",
      "Training Loss: 0.0015713489441259298\n",
      "Validation Loss: 0.0024010375204665257\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.006757959075039253\n",
      "Training Loss: 0.006966946485918015\n",
      "Training Loss: 0.006920262380735949\n",
      "Training Loss: 0.004691187694115797\n",
      "Training Loss: 0.0016108072217321024\n",
      "Training Loss: 0.0016250240316730924\n",
      "Training Loss: 0.0015735400887206196\n",
      "Validation Loss: 0.0023914714608069382\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.006727108822669834\n",
      "Training Loss: 0.0069331754965242\n",
      "Training Loss: 0.00689189565135166\n",
      "Training Loss: 0.004677181434963131\n",
      "Training Loss: 0.0016149897206923925\n",
      "Training Loss: 0.0016254563671827783\n",
      "Training Loss: 0.001575235648342641\n",
      "Validation Loss: 0.002383101766637774\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.006700482156593353\n",
      "Training Loss: 0.006903799824649468\n",
      "Training Loss: 0.006867459664354101\n",
      "Training Loss: 0.004664781770115951\n",
      "Training Loss: 0.001618218962685205\n",
      "Training Loss: 0.0016255153487145435\n",
      "Training Loss: 0.0015763575283926912\n",
      "Validation Loss: 0.002375634121603929\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.006677217700053006\n",
      "Training Loss: 0.0068778747378382835\n",
      "Training Loss: 0.006846101302653551\n",
      "Training Loss: 0.004653574181575095\n",
      "Training Loss: 0.0016205684395390564\n",
      "Training Loss: 0.0016252092750073644\n",
      "Training Loss: 0.0015769183525117115\n",
      "Validation Loss: 0.002368866431213783\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.006656641580630094\n",
      "Training Loss: 0.006854664919665084\n",
      "Training Loss: 0.006827152050100267\n",
      "Training Loss: 0.00464326340254047\n",
      "Training Loss: 0.0016221588740881999\n",
      "Training Loss: 0.0016245898722991115\n",
      "Training Loss: 0.0015769842253939715\n",
      "Validation Loss: 0.0023626760174808082\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.006638222433393821\n",
      "Training Loss: 0.00683360303286463\n",
      "Training Loss: 0.006810100815491751\n",
      "Training Loss: 0.004633646813599626\n",
      "Training Loss: 0.0016231155628338456\n",
      "Training Loss: 0.0016237155919952783\n",
      "Training Loss: 0.0015766318599344231\n",
      "Validation Loss: 0.0023569709747595442\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006621552882716059\n",
      "Training Loss: 0.006814256705110893\n",
      "Training Loss: 0.00679455807665363\n",
      "Training Loss: 0.004624579778173939\n",
      "Training Loss: 0.0016235610863077454\n",
      "Training Loss: 0.0016226452777482337\n",
      "Training Loss: 0.001575938286841847\n",
      "Validation Loss: 0.0023516825676506755\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006606320802820847\n",
      "Training Loss: 0.006796300205169245\n",
      "Training Loss: 0.006780229288851842\n",
      "Training Loss: 0.00461596033142996\n",
      "Training Loss: 0.0016235983757360373\n",
      "Training Loss: 0.0016214326347835594\n",
      "Training Loss: 0.0015749727947695646\n",
      "Validation Loss: 0.0023467608778528198\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006592280082404613\n",
      "Training Loss: 0.006779484031721949\n",
      "Training Loss: 0.006766890120925382\n",
      "Training Loss: 0.004607717236212921\n",
      "Training Loss: 0.001623317580524599\n",
      "Training Loss: 0.0016201189654384508\n",
      "Training Loss: 0.0015737958809768316\n",
      "Validation Loss: 0.0023421655724711076\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.006579242806183174\n",
      "Training Loss: 0.00676361836027354\n",
      "Training Loss: 0.006754369067493826\n",
      "Training Loss: 0.004599794847308658\n",
      "Training Loss: 0.0016227859117498155\n",
      "Training Loss: 0.0016187345265643671\n",
      "Training Loss: 0.0015724521128868219\n",
      "Validation Loss: 0.002337857275984986\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006567064194241539\n",
      "Training Loss: 0.006748563919682055\n",
      "Training Loss: 0.006742539684055373\n",
      "Training Loss: 0.00459215575712733\n",
      "Training Loss: 0.0016220577077183406\n",
      "Training Loss: 0.001617302845334052\n",
      "Training Loss: 0.0015709760655590798\n",
      "Validation Loss: 0.002333803808060213\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.0065556297439616175\n",
      "Training Loss: 0.006734214851167053\n",
      "Training Loss: 0.0067313059093430634\n",
      "Training Loss: 0.004584768146305578\n",
      "Training Loss: 0.0016211729840142652\n",
      "Training Loss: 0.0016158406191971154\n",
      "Training Loss: 0.001569397398707224\n",
      "Validation Loss: 0.0023299767055866144\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.006544846448814496\n",
      "Training Loss: 0.006720483747776598\n",
      "Training Loss: 0.006720588636817411\n",
      "Training Loss: 0.004577610242413357\n",
      "Training Loss: 0.0016201657291094306\n",
      "Training Loss: 0.0016143596139590955\n",
      "Training Loss: 0.0015677356082596816\n",
      "Validation Loss: 0.00232634965423233\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.0065346406772732735\n",
      "Training Loss: 0.00670730690122582\n",
      "Training Loss: 0.006710331111680717\n",
      "Training Loss: 0.004570661824545823\n",
      "Training Loss: 0.0016190566837030928\n",
      "Training Loss: 0.0016128661100810859\n",
      "Training Loss: 0.0015660064293479081\n",
      "Validation Loss: 0.0023229023674284964\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006524951851461082\n",
      "Training Loss: 0.006694630535785109\n",
      "Training Loss: 0.006700483727036044\n",
      "Training Loss: 0.004563908868440194\n",
      "Training Loss: 0.0016178660238801968\n",
      "Training Loss: 0.001611364285927266\n",
      "Training Loss: 0.0015642212756210938\n",
      "Validation Loss: 0.0023196122445792223\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006515728860395029\n",
      "Training Loss: 0.006682413802482187\n",
      "Training Loss: 0.006691009203204885\n",
      "Training Loss: 0.0045573365649033805\n",
      "Training Loss: 0.0016166050029278267\n",
      "Training Loss: 0.0016098585343570448\n",
      "Training Loss: 0.0015623879111080895\n",
      "Validation Loss: 0.002316460034427188\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.00650693066068925\n",
      "Training Loss: 0.006670621308730915\n",
      "Training Loss: 0.006681877868250013\n",
      "Training Loss: 0.004550933971331688\n",
      "Training Loss: 0.0016152814069937448\n",
      "Training Loss: 0.0016083464733674191\n",
      "Training Loss: 0.0015605105389840902\n",
      "Validation Loss: 0.0023134325176201874\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006498521375469864\n",
      "Training Loss: 0.006659225873881951\n",
      "Training Loss: 0.0066730628616642204\n",
      "Training Loss: 0.004544690364855342\n",
      "Training Loss: 0.001613903406105237\n",
      "Training Loss: 0.0016068286275549325\n",
      "Training Loss: 0.0015585943337646313\n",
      "Validation Loss: 0.0023105128360641877\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006490466264076531\n",
      "Training Loss: 0.00664819611934945\n",
      "Training Loss: 0.006664539666380733\n",
      "Training Loss: 0.004538595262129092\n",
      "Training Loss: 0.0016124748601578176\n",
      "Training Loss: 0.0016053060009289767\n",
      "Training Loss: 0.0015566427867452148\n",
      "Validation Loss: 0.0023076903093074298\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006482736689504236\n",
      "Training Loss: 0.0066375119239091875\n",
      "Training Loss: 0.006656289749080315\n",
      "Training Loss: 0.004532640185789205\n",
      "Training Loss: 0.001610999020631425\n",
      "Training Loss: 0.001603773494498455\n",
      "Training Loss: 0.001554654840438161\n",
      "Validation Loss: 0.002304952357082945\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006475310656242073\n",
      "Training Loss: 0.006627154391026124\n",
      "Training Loss: 0.006648295866325498\n",
      "Training Loss: 0.004526818714657566\n",
      "Training Loss: 0.001609477413003333\n",
      "Training Loss: 0.0016022328408871545\n",
      "Training Loss: 0.0015526333419256843\n",
      "Validation Loss: 0.002302288835779293\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006468164663529024\n",
      "Training Loss: 0.006617104395991192\n",
      "Training Loss: 0.0066405422729440035\n",
      "Training Loss: 0.004521120296994923\n",
      "Training Loss: 0.0016079091584833805\n",
      "Training Loss: 0.001600679167895578\n",
      "Training Loss: 0.0015505755553022027\n",
      "Validation Loss: 0.002299691410255636\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.0064612796518486\n",
      "Training Loss: 0.006607346092350781\n",
      "Training Loss: 0.006633014911785722\n",
      "Training Loss: 0.004515540043794317\n",
      "Training Loss: 0.0016062981865252368\n",
      "Training Loss: 0.0015991127922461601\n",
      "Training Loss: 0.0015484851930523292\n",
      "Validation Loss: 0.0022971534366633965\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006454632442910224\n",
      "Training Loss: 0.006597858805907891\n",
      "Training Loss: 0.006625697222771123\n",
      "Training Loss: 0.004510069752068375\n",
      "Training Loss: 0.0016046452951559331\n",
      "Training Loss: 0.0015975329712091479\n",
      "Training Loss: 0.0015463628669385799\n",
      "Validation Loss: 0.002294669707596986\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0064482086931820955\n",
      "Training Loss: 0.006588629964971915\n",
      "Training Loss: 0.00661857784842141\n",
      "Training Loss: 0.004504702534832177\n",
      "Training Loss: 0.0016029495748807676\n",
      "Training Loss: 0.0015959365759772482\n",
      "Training Loss: 0.0015442051488207653\n",
      "Validation Loss: 0.0022922305469593026\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006441993456101045\n",
      "Training Loss: 0.006579648826736957\n",
      "Training Loss: 0.006611648856196552\n",
      "Training Loss: 0.0044994334337388866\n",
      "Training Loss: 0.001601207828207407\n",
      "Training Loss: 0.00159432059401297\n",
      "Training Loss: 0.0015420112552237697\n",
      "Validation Loss: 0.0022898324873069065\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006435971301980317\n",
      "Training Loss: 0.006570898171048611\n",
      "Training Loss: 0.006604892521863803\n",
      "Training Loss: 0.004494256203834084\n",
      "Training Loss: 0.0015994237925042397\n",
      "Training Loss: 0.0015926855040743247\n",
      "Training Loss: 0.0015397844469407574\n",
      "Validation Loss: 0.0022874703304994718\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006430126695195213\n",
      "Training Loss: 0.00656236506300047\n",
      "Training Loss: 0.00659830147982575\n",
      "Training Loss: 0.004489165505146957\n",
      "Training Loss: 0.0015975956617330666\n",
      "Training Loss: 0.0015910279625677504\n",
      "Training Loss: 0.001537521290301811\n",
      "Validation Loss: 0.0022851405373695115\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006424448303878307\n",
      "Training Loss: 0.006554039180045947\n",
      "Training Loss: 0.006591866388916969\n",
      "Training Loss: 0.00448415423023107\n",
      "Training Loss: 0.0015957194885413627\n",
      "Training Loss: 0.001589346319087781\n",
      "Training Loss: 0.001535219391225837\n",
      "Validation Loss: 0.0022828373212144056\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006418926833430305\n",
      "Training Loss: 0.006545911030843854\n",
      "Training Loss: 0.006585576990619302\n",
      "Training Loss: 0.004479218813648913\n",
      "Training Loss: 0.0015937961166491731\n",
      "Training Loss: 0.001587638775454252\n",
      "Training Loss: 0.0015328801368013957\n",
      "Validation Loss: 0.0022805575498842812\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006413548226701096\n",
      "Training Loss: 0.006537967300973832\n",
      "Training Loss: 0.006579424041556195\n",
      "Training Loss: 0.004474354201665847\n",
      "Training Loss: 0.0015918282691563944\n",
      "Training Loss: 0.0015859053311578465\n",
      "Training Loss: 0.0015305044180422556\n",
      "Validation Loss: 0.0022782987411603096\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006408302207710222\n",
      "Training Loss: 0.006530197899555787\n",
      "Training Loss: 0.006573397510219365\n",
      "Training Loss: 0.004469555177347501\n",
      "Training Loss: 0.0015898125963576604\n",
      "Training Loss: 0.0015841427598206792\n",
      "Training Loss: 0.001528089813364204\n",
      "Validation Loss: 0.0022760556755375555\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0064031792560126635\n",
      "Training Loss: 0.006522591871907934\n",
      "Training Loss: 0.006567488316213712\n",
      "Training Loss: 0.004464817027401296\n",
      "Training Loss: 0.0015877483763324562\n",
      "Training Loss: 0.0015823510344489477\n",
      "Training Loss: 0.0015256374340970069\n",
      "Validation Loss: 0.0022738281807911055\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006398169154999777\n",
      "Training Loss: 0.006515139032853767\n",
      "Training Loss: 0.006561688035726547\n",
      "Training Loss: 0.00446013618260622\n",
      "Training Loss: 0.0015856373599672225\n",
      "Training Loss: 0.0015805283350346145\n",
      "Training Loss: 0.0015231449106067885\n",
      "Validation Loss: 0.002271610396804745\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006393264696234837\n",
      "Training Loss: 0.006507832651259378\n",
      "Training Loss: 0.006555990681517869\n",
      "Training Loss: 0.004455508667961112\n",
      "Training Loss: 0.001583477337117074\n",
      "Training Loss: 0.001578672684263438\n",
      "Training Loss: 0.0015206125803524629\n",
      "Validation Loss: 0.002269400538325533\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006388458136934787\n",
      "Training Loss: 0.006500664260238409\n",
      "Training Loss: 0.006550386857707053\n",
      "Training Loss: 0.004450929406666546\n",
      "Training Loss: 0.0015812674703192897\n",
      "Training Loss: 0.0015767824699287304\n",
      "Training Loss: 0.0015180401415273081\n",
      "Validation Loss: 0.002267195503070448\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006383740170858801\n",
      "Training Loss: 0.006493624167051166\n",
      "Training Loss: 0.0065448714257217945\n",
      "Training Loss: 0.0044463960854045585\n",
      "Training Loss: 0.001579009421984665\n",
      "Training Loss: 0.0015748585111577994\n",
      "Training Loss: 0.0015154292988881935\n",
      "Validation Loss: 0.002264993185698916\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006379103732760996\n",
      "Training Loss: 0.0064867034927010535\n",
      "Training Loss: 0.006539435456506908\n",
      "Training Loss: 0.004441904236227856\n",
      "Training Loss: 0.0015767004268127493\n",
      "Training Loss: 0.00157289688650053\n",
      "Training Loss: 0.0015127754924469628\n",
      "Validation Loss: 0.002262790235291821\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006374544503632933\n",
      "Training Loss: 0.006479898325633258\n",
      "Training Loss: 0.006534074406372384\n",
      "Training Loss: 0.0044374509367480644\n",
      "Training Loss: 0.001574344618129544\n",
      "Training Loss: 0.0015709011498256586\n",
      "Training Loss: 0.0015100847257417628\n",
      "Validation Loss: 0.002260585705593162\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006370052044512704\n",
      "Training Loss: 0.0064731967425905165\n",
      "Training Loss: 0.0065287803113460545\n",
      "Training Loss: 0.004433032674714923\n",
      "Training Loss: 0.0015719378358335235\n",
      "Training Loss: 0.0015688647050410508\n",
      "Training Loss: 0.0015073516799020581\n",
      "Validation Loss: 0.002258376453225745\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006365626058541238\n",
      "Training Loss: 0.006466599883278832\n",
      "Training Loss: 0.00652355239726603\n",
      "Training Loss: 0.004428646516098525\n",
      "Training Loss: 0.0015694802642974538\n",
      "Training Loss: 0.0015667900365951937\n",
      "Training Loss: 0.001504577403247822\n",
      "Validation Loss: 0.00225615975899798\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006361257701646536\n",
      "Training Loss: 0.006460098153911531\n",
      "Training Loss: 0.006518381384667009\n",
      "Training Loss: 0.0044242898435186365\n",
      "Training Loss: 0.001566974443703657\n",
      "Training Loss: 0.0015646744160039815\n",
      "Training Loss: 0.0015017630736110731\n",
      "Validation Loss: 0.0022539338982626553\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006356942335842177\n",
      "Training Loss: 0.006453682774445042\n",
      "Training Loss: 0.006513262197840959\n",
      "Training Loss: 0.0044199599698913515\n",
      "Training Loss: 0.0015644216151849833\n",
      "Training Loss: 0.0015625197443296201\n",
      "Training Loss: 0.001498909118818119\n",
      "Validation Loss: 0.0022516975238503303\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.006352676126407459\n",
      "Training Loss: 0.006447353232651949\n",
      "Training Loss: 0.006508193219779059\n",
      "Training Loss: 0.004415654303666088\n",
      "Training Loss: 0.0015618197568983306\n",
      "Training Loss: 0.001560323478479404\n",
      "Training Loss: 0.001496015509183053\n",
      "Validation Loss: 0.0022494495170075823\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.00634845465188846\n",
      "Training Loss: 0.006441101997625083\n",
      "Training Loss: 0.006503169655334204\n",
      "Training Loss: 0.004411371310052346\n",
      "Training Loss: 0.0015591706226405222\n",
      "Training Loss: 0.0015580854545987678\n",
      "Training Loss: 0.0014930821879534052\n",
      "Validation Loss: 0.002247187946526306\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0063442752894479785\n",
      "Training Loss: 0.006434927807422355\n",
      "Training Loss: 0.006498188149416819\n",
      "Training Loss: 0.004407107594306581\n",
      "Training Loss: 0.0015564750551129692\n",
      "Training Loss: 0.0015558071609120816\n",
      "Training Loss: 0.0014901108953927179\n",
      "Validation Loss: 0.0022449113093626124\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006340131649048999\n",
      "Training Loss: 0.006428824993781746\n",
      "Training Loss: 0.0064932459336705505\n",
      "Training Loss: 0.004402863012364833\n",
      "Training Loss: 0.0015537344661424868\n",
      "Training Loss: 0.0015534861548803748\n",
      "Training Loss: 0.0014870998221158515\n",
      "Validation Loss: 0.0022426171855929513\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006336025458294898\n",
      "Training Loss: 0.00642279107007198\n",
      "Training Loss: 0.00648834042425733\n",
      "Training Loss: 0.004398634706230951\n",
      "Training Loss: 0.0015509482797642705\n",
      "Training Loss: 0.0015511222096392884\n",
      "Training Loss: 0.0014840500280843116\n",
      "Validation Loss: 0.002240305794586082\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006331951268948615\n",
      "Training Loss: 0.006416821321472525\n",
      "Training Loss: 0.006483468046062626\n",
      "Training Loss: 0.004394420876124059\n",
      "Training Loss: 0.0015481197176268325\n",
      "Training Loss: 0.0015487176705210005\n",
      "Training Loss: 0.0014809655293356626\n",
      "Validation Loss: 0.002237977223817001\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006327904958743602\n",
      "Training Loss: 0.006410915100714192\n",
      "Training Loss: 0.0064786292612552645\n",
      "Training Loss: 0.004390221245776047\n",
      "Training Loss: 0.001545245422021253\n",
      "Training Loss: 0.0015462684248632285\n",
      "Training Loss: 0.0014778404938988387\n",
      "Validation Loss: 0.0022356278580885507\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006323889831546694\n",
      "Training Loss: 0.006405072251800448\n",
      "Training Loss: 0.00647382230614312\n",
      "Training Loss: 0.004386033846603823\n",
      "Training Loss: 0.00154233178938739\n",
      "Training Loss: 0.0015437789328279904\n",
      "Training Loss: 0.0014746808825293555\n",
      "Validation Loss: 0.0022332603383825897\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006319900763919577\n",
      "Training Loss: 0.006399288057582453\n",
      "Training Loss: 0.0064690446620807055\n",
      "Training Loss: 0.004381857492189738\n",
      "Training Loss: 0.0015393760544247924\n",
      "Training Loss: 0.0015412468599970452\n",
      "Training Loss: 0.001471486448426731\n",
      "Validation Loss: 0.0022308741341264785\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006315934960730374\n",
      "Training Loss: 0.006393558221170679\n",
      "Training Loss: 0.0064642946573439984\n",
      "Training Loss: 0.004377692710841074\n",
      "Training Loss: 0.0015363832225557417\n",
      "Training Loss: 0.0015386743292037863\n",
      "Training Loss: 0.0014682565179828088\n",
      "Validation Loss: 0.0022284669720603516\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00631199233001098\n",
      "Training Loss: 0.006387884517898783\n",
      "Training Loss: 0.006459572001476772\n",
      "Training Loss: 0.0043735374794050584\n",
      "Training Loss: 0.0015333516919054091\n",
      "Training Loss: 0.001536059566278709\n",
      "Training Loss: 0.0014649929625738878\n",
      "Validation Loss: 0.0022260389248347817\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.006308071461971849\n",
      "Training Loss: 0.006382264447165653\n",
      "Training Loss: 0.006454875069321134\n",
      "Training Loss: 0.00436939049504872\n",
      "Training Loss: 0.0015302834782050922\n",
      "Training Loss: 0.0015334026388882194\n",
      "Training Loss: 0.001461694357712986\n",
      "Validation Loss: 0.002223588820066816\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006304173541720957\n",
      "Training Loss: 0.006376699802931398\n",
      "Training Loss: 0.006450206295121461\n",
      "Training Loss: 0.0043652521826152224\n",
      "Training Loss: 0.0015271774285065475\n",
      "Training Loss: 0.001530703987227753\n",
      "Training Loss: 0.001458362645789748\n",
      "Validation Loss: 0.0022211201056338364\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006300295407418162\n",
      "Training Loss: 0.006371186197502539\n",
      "Training Loss: 0.0064455622853711245\n",
      "Training Loss: 0.004361122019618051\n",
      "Training Loss: 0.0015240386303048581\n",
      "Training Loss: 0.001527965562272584\n",
      "Training Loss: 0.0014549982963944785\n",
      "Validation Loss: 0.0022186271331581715\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006296436155680567\n",
      "Training Loss: 0.006365724480710924\n",
      "Training Loss: 0.006440942413755693\n",
      "Training Loss: 0.004356999876181362\n",
      "Training Loss: 0.0015208660648204386\n",
      "Training Loss: 0.0015251851521315984\n",
      "Training Loss: 0.0014516011728846934\n",
      "Validation Loss: 0.002216113082914568\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006292597211431712\n",
      "Training Loss: 0.006360314566409215\n",
      "Training Loss: 0.006436350223375484\n",
      "Training Loss: 0.004352885002881521\n",
      "Training Loss: 0.00151765989328851\n",
      "Training Loss: 0.0015223625212092884\n",
      "Training Loss: 0.0014481717826856765\n",
      "Validation Loss: 0.0022135781656060375\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006288778284797445\n",
      "Training Loss: 0.006354957395233214\n",
      "Training Loss: 0.00643178352562245\n",
      "Training Loss: 0.004348777883715229\n",
      "Training Loss: 0.0015144236039486714\n",
      "Training Loss: 0.0015195019514067098\n",
      "Training Loss: 0.0014447109367756639\n",
      "Validation Loss: 0.002211021125843759\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006284976026508957\n",
      "Training Loss: 0.0063496489892713726\n",
      "Training Loss: 0.006427241517812945\n",
      "Training Loss: 0.004344677355475141\n",
      "Training Loss: 0.0015111579922086093\n",
      "Training Loss: 0.0015166018283343874\n",
      "Training Loss: 0.001441221779823536\n",
      "Validation Loss: 0.002208445295089793\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006281190400477499\n",
      "Training Loss: 0.006344386598793789\n",
      "Training Loss: 0.0064227203041082245\n",
      "Training Loss: 0.004340583218217944\n",
      "Training Loss: 0.001507865601888625\n",
      "Training Loss: 0.0015136633774091024\n",
      "Training Loss: 0.0014377033612981904\n",
      "Validation Loss: 0.0022058488490135837\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006277420344995335\n",
      "Training Loss: 0.006339172546286136\n",
      "Training Loss: 0.006418223708751611\n",
      "Training Loss: 0.0043364959301106865\n",
      "Training Loss: 0.001504546189244138\n",
      "Training Loss: 0.0015106859778461512\n",
      "Training Loss: 0.0014341558300657198\n",
      "Validation Loss: 0.0022032323472370996\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006273668328067288\n",
      "Training Loss: 0.00633400846272707\n",
      "Training Loss: 0.0064137529477011415\n",
      "Training Loss: 0.004332415925455279\n",
      "Training Loss: 0.0015012012705847156\n",
      "Training Loss: 0.0015076698576740454\n",
      "Training Loss: 0.0014305780324502847\n",
      "Validation Loss: 0.0022005918634138053\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006269931998103857\n",
      "Training Loss: 0.006328891143202782\n",
      "Training Loss: 0.006409304793924093\n",
      "Training Loss: 0.004328341938598896\n",
      "Training Loss: 0.0014978316643100697\n",
      "Training Loss: 0.0015046153069124557\n",
      "Training Loss: 0.001426973130000988\n",
      "Validation Loss: 0.0021979320275053132\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006266209847526624\n",
      "Training Loss: 0.0063238213933072985\n",
      "Training Loss: 0.0064048810576787215\n",
      "Training Loss: 0.004324274374739616\n",
      "Training Loss: 0.0014944388656294905\n",
      "Training Loss: 0.0015015235071768984\n",
      "Training Loss: 0.0014233396346389781\n",
      "Validation Loss: 0.0021952522372694384\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006262503690086305\n",
      "Training Loss: 0.006318798150168732\n",
      "Training Loss: 0.006400478841387666\n",
      "Training Loss: 0.004320213983155554\n",
      "Training Loss: 0.0014910248573869467\n",
      "Training Loss: 0.001498394358495716\n",
      "Training Loss: 0.001419679880636977\n",
      "Validation Loss: 0.002192551131039328\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006258812133455649\n",
      "Training Loss: 0.006313821015646681\n",
      "Training Loss: 0.0063961003260919825\n",
      "Training Loss: 0.0043161602120380844\n",
      "Training Loss: 0.0014875904221844393\n",
      "Training Loss: 0.001495227441046154\n",
      "Training Loss: 0.0014159912407922092\n",
      "Validation Loss: 0.002189827305617941\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006255136032123119\n",
      "Training Loss: 0.006308891349472105\n",
      "Training Loss: 0.00639174664625898\n",
      "Training Loss: 0.004312113290579873\n",
      "Training Loss: 0.0014841365937900263\n",
      "Training Loss: 0.0014920242826337927\n",
      "Training Loss: 0.0014122774761926849\n",
      "Validation Loss: 0.0021870847295905197\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006251471645664424\n",
      "Training Loss: 0.0063040042761713265\n",
      "Training Loss: 0.006387412988115102\n",
      "Training Loss: 0.004308071200794075\n",
      "Training Loss: 0.0014806646785291378\n",
      "Training Loss: 0.001488785107358126\n",
      "Training Loss: 0.0014085369603708386\n",
      "Validation Loss: 0.002184322359688294\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.00624782114638947\n",
      "Training Loss: 0.006299162082141266\n",
      "Training Loss: 0.006383102932013571\n",
      "Training Loss: 0.004304036533721955\n",
      "Training Loss: 0.0014771759178256617\n",
      "Training Loss: 0.0014855108507617842\n",
      "Training Loss: 0.0014047706554993055\n",
      "Validation Loss: 0.002181538090734195\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006244182862574234\n",
      "Training Loss: 0.006294362581102177\n",
      "Training Loss: 0.006378813533810899\n",
      "Training Loss: 0.004300008670543321\n",
      "Training Loss: 0.0014736732965684495\n",
      "Training Loss: 0.0014822022733278573\n",
      "Training Loss: 0.0014009785436792299\n",
      "Validation Loss: 0.0021787348217749444\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.0062405583134386685\n",
      "Training Loss: 0.006289608048973605\n",
      "Training Loss: 0.006374547184095718\n",
      "Training Loss: 0.004295986436918611\n",
      "Training Loss: 0.0014701564010465517\n",
      "Training Loss: 0.0014788596281141509\n",
      "Training Loss: 0.0013971630694868508\n",
      "Validation Loss: 0.002175909424966292\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006236942862160504\n",
      "Training Loss: 0.006284894314594567\n",
      "Training Loss: 0.00637030167854391\n",
      "Training Loss: 0.0042919717701443\n",
      "Training Loss: 0.0014666276135540102\n",
      "Training Loss: 0.001475483277317835\n",
      "Training Loss: 0.001393322891817661\n",
      "Validation Loss: 0.002173064823662653\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.00623333940980956\n",
      "Training Loss: 0.006280220567714423\n",
      "Training Loss: 0.006366075684782118\n",
      "Training Loss: 0.004287962312519085\n",
      "Training Loss: 0.001463086244330043\n",
      "Training Loss: 0.0014720719070464839\n",
      "Training Loss: 0.001389455512544373\n",
      "Validation Loss: 0.002170197712126701\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006229748609475791\n",
      "Training Loss: 0.006275590974837542\n",
      "Training Loss: 0.006361872050911188\n",
      "Training Loss: 0.00428395963186631\n",
      "Training Loss: 0.0014595351778552868\n",
      "Training Loss: 0.0014686276753491257\n",
      "Training Loss: 0.00138556488105678\n",
      "Validation Loss: 0.002167310055139499\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006226168485591188\n",
      "Training Loss: 0.006271000661654398\n",
      "Training Loss: 0.006357687920681201\n",
      "Training Loss: 0.0042799635931442025\n",
      "Training Loss: 0.0014559762965654954\n",
      "Training Loss: 0.0014651513840362896\n",
      "Training Loss: 0.0013816491958277765\n",
      "Validation Loss: 0.0021643992138447496\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006222599701723084\n",
      "Training Loss: 0.0062664506875444205\n",
      "Training Loss: 0.0063535247318213806\n",
      "Training Loss: 0.004275972765462939\n",
      "Training Loss: 0.0014524074262590148\n",
      "Training Loss: 0.001461642436479451\n",
      "Training Loss: 0.0013777081391890534\n",
      "Validation Loss: 0.0021614677989759486\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006219040366122499\n",
      "Training Loss: 0.0062619394576177\n",
      "Training Loss: 0.006349380847532302\n",
      "Training Loss: 0.004271988301552483\n",
      "Training Loss: 0.0014488344911660533\n",
      "Training Loss: 0.0014581038735923357\n",
      "Training Loss: 0.0013737457786919549\n",
      "Validation Loss: 0.00215851449508356\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006215490425238386\n",
      "Training Loss: 0.006257465971866622\n",
      "Training Loss: 0.006345256611239165\n",
      "Training Loss: 0.004268009891966358\n",
      "Training Loss: 0.0014452566833642777\n",
      "Training Loss: 0.0014545352318964433\n",
      "Training Loss: 0.001369759017106844\n",
      "Validation Loss: 0.002155540281451527\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.0062119498441461475\n",
      "Training Loss: 0.006253027701750398\n",
      "Training Loss: 0.006341149879153818\n",
      "Training Loss: 0.004264037401226233\n",
      "Training Loss: 0.0014416742614412215\n",
      "Training Loss: 0.0014509370134328493\n",
      "Training Loss: 0.001365750670520356\n",
      "Validation Loss: 0.0021525453614796536\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006208418607711792\n",
      "Training Loss: 0.00624862470314838\n",
      "Training Loss: 0.006337059721117839\n",
      "Training Loss: 0.00426007045643928\n",
      "Training Loss: 0.0014380945595621596\n",
      "Training Loss: 0.0014473148254910485\n",
      "Training Loss: 0.0013617240145686082\n",
      "Validation Loss: 0.002149531472241506\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0062048936018254605\n",
      "Training Loss: 0.00624425325775519\n",
      "Training Loss: 0.0063329848420107734\n",
      "Training Loss: 0.00425610874000995\n",
      "Training Loss: 0.0014345143300306517\n",
      "Training Loss: 0.0014436653281154578\n",
      "Training Loss: 0.0013576740022108425\n",
      "Validation Loss: 0.002146496061647262\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006201378047699109\n",
      "Training Loss: 0.006239916934864595\n",
      "Training Loss: 0.006328927337890491\n",
      "Training Loss: 0.004252152520566596\n",
      "Training Loss: 0.0014309354667784646\n",
      "Training Loss: 0.001439991043880582\n",
      "Training Loss: 0.0013536046376975718\n",
      "Validation Loss: 0.0021434410906628763\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006197869146708399\n",
      "Training Loss: 0.00623560976004228\n",
      "Training Loss: 0.006324884097557515\n",
      "Training Loss: 0.004248202434973791\n",
      "Training Loss: 0.0014273612141550984\n",
      "Training Loss: 0.0014362943521700798\n",
      "Training Loss: 0.0013495167762448545\n",
      "Validation Loss: 0.0021403656851924076\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.0061943680932745335\n",
      "Training Loss: 0.006231333944015205\n",
      "Training Loss: 0.006320855977246537\n",
      "Training Loss: 0.004244256411184324\n",
      "Training Loss: 0.0014237892431265208\n",
      "Training Loss: 0.001432574004138587\n",
      "Training Loss: 0.0013454073981847615\n",
      "Validation Loss: 0.002137269487160638\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006190875124884769\n",
      "Training Loss: 0.006227087333099917\n",
      "Training Loss: 0.006316840641084127\n",
      "Training Loss: 0.004240315225324594\n",
      "Training Loss: 0.0014202258391014767\n",
      "Training Loss: 0.0014288347195542883\n",
      "Training Loss: 0.0013412830859306267\n",
      "Validation Loss: 0.0021341530335168895\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006187386917881668\n",
      "Training Loss: 0.00622286592843011\n",
      "Training Loss: 0.006312836321303621\n",
      "Training Loss: 0.004236377940687816\n",
      "Training Loss: 0.001416671032056911\n",
      "Training Loss: 0.0014250797378190328\n",
      "Training Loss: 0.001337144462595461\n",
      "Validation Loss: 0.0021310212977460764\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006183902218472212\n",
      "Training Loss: 0.006218666629865766\n",
      "Training Loss: 0.006308839854318649\n",
      "Training Loss: 0.0042324446348357015\n",
      "Training Loss: 0.0014131283249298576\n",
      "Training Loss: 0.0014213110020500608\n",
      "Training Loss: 0.001332993998657912\n",
      "Validation Loss: 0.0021278730106159822\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.0061804218997713175\n",
      "Training Loss: 0.006214489535195753\n",
      "Training Loss: 0.006304850927554071\n",
      "Training Loss: 0.004228515387949301\n",
      "Training Loss: 0.001409599729522597\n",
      "Training Loss: 0.0014175307171535678\n",
      "Training Loss: 0.0013288308771734591\n",
      "Validation Loss: 0.0021247096893408997\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006176943725440651\n",
      "Training Loss: 0.006210331147303805\n",
      "Training Loss: 0.006300868156831712\n",
      "Training Loss: 0.004224588912911713\n",
      "Training Loss: 0.0014060862887708937\n",
      "Training Loss: 0.0014137416517769452\n",
      "Training Loss: 0.0013246578788675834\n",
      "Validation Loss: 0.002121531151885634\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006173468113411218\n",
      "Training Loss: 0.006206189056392759\n",
      "Training Loss: 0.006296888659708202\n",
      "Training Loss: 0.00422066547034774\n",
      "Training Loss: 0.0014025886481977068\n",
      "Training Loss: 0.0014099453108792658\n",
      "Training Loss: 0.001320476601104019\n",
      "Validation Loss: 0.00211833837230198\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006169991800561547\n",
      "Training Loss: 0.0062020591553300615\n",
      "Training Loss: 0.0062929082935443146\n",
      "Training Loss: 0.004216742852440803\n",
      "Training Loss: 0.0013991124882886652\n",
      "Training Loss: 0.0014061477614450268\n",
      "Training Loss: 0.001316293015552219\n",
      "Validation Loss: 0.0021151358720389534\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006166511095361784\n",
      "Training Loss: 0.006197937784017995\n",
      "Training Loss: 0.006288924203836359\n",
      "Training Loss: 0.004212820679531433\n",
      "Training Loss: 0.0013956584405968896\n",
      "Training Loss: 0.0014023514019208961\n",
      "Training Loss: 0.0013121057438547723\n",
      "Validation Loss: 0.002111924327896866\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.0061630277056247\n",
      "Training Loss: 0.006193822581553832\n",
      "Training Loss: 0.006284935042494908\n",
      "Training Loss: 0.004208899953227956\n",
      "Training Loss: 0.0013922285399166868\n",
      "Training Loss: 0.0013985584405600093\n",
      "Training Loss: 0.001307918128586607\n",
      "Validation Loss: 0.0021087017094747825\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006159537802450359\n",
      "Training Loss: 0.006189710028702393\n",
      "Training Loss: 0.00628093664883636\n",
      "Training Loss: 0.004204976993787568\n",
      "Training Loss: 0.0013888243662950118\n",
      "Training Loss: 0.001394773703214014\n",
      "Training Loss: 0.001303734903340228\n",
      "Validation Loss: 0.0021054736520154994\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006156035321764648\n",
      "Training Loss: 0.0061855931335594505\n",
      "Training Loss: 0.006276925758575089\n",
      "Training Loss: 0.004201053405267885\n",
      "Training Loss: 0.0013854491485108156\n",
      "Training Loss: 0.0013910001621115953\n",
      "Training Loss: 0.0012995554077497218\n",
      "Validation Loss: 0.002102238202728296\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006152523065684363\n",
      "Training Loss: 0.0061814723047427834\n",
      "Training Loss: 0.006272897490416654\n",
      "Training Loss: 0.00419712624992826\n",
      "Training Loss: 0.0013821063347859309\n",
      "Training Loss: 0.0013872435453231447\n",
      "Training Loss: 0.0012953860066772904\n",
      "Validation Loss: 0.0020990009192457556\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.00614899457897991\n",
      "Training Loss: 0.006177338586421683\n",
      "Training Loss: 0.006268848183681257\n",
      "Training Loss: 0.004193195583939087\n",
      "Training Loss: 0.001378795702912612\n",
      "Training Loss: 0.0013835048807959538\n",
      "Training Loss: 0.0012912280522868968\n",
      "Validation Loss: 0.002095764449035421\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006145447955932468\n",
      "Training Loss: 0.006173190355766565\n",
      "Training Loss: 0.006264774938463233\n",
      "Training Loss: 0.0041892589487542865\n",
      "Training Loss: 0.0013755206089990678\n",
      "Training Loss: 0.001379789036582224\n",
      "Training Loss: 0.001287084735085955\n",
      "Validation Loss: 0.0020925259959495065\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006141878793714568\n",
      "Training Loss: 0.006169024052796885\n",
      "Training Loss: 0.006260672319913283\n",
      "Training Loss: 0.004185316667571897\n",
      "Training Loss: 0.0013722830297774634\n",
      "Training Loss: 0.0013761003606487065\n",
      "Training Loss: 0.0012829584011342376\n",
      "Validation Loss: 0.0020892897408023086\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006138283684849739\n",
      "Training Loss: 0.006164831686764956\n",
      "Training Loss: 0.00625653697818052\n",
      "Training Loss: 0.00418136576670804\n",
      "Training Loss: 0.0013690838706679643\n",
      "Training Loss: 0.0013724402703519446\n",
      "Training Loss: 0.0012788505367643665\n",
      "Validation Loss: 0.0020860554349577324\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006134660267271101\n",
      "Training Loss: 0.006160612293169834\n",
      "Training Loss: 0.006252364505780861\n",
      "Training Loss: 0.004177405980008189\n",
      "Training Loss: 0.001365923848352395\n",
      "Training Loss: 0.0013688131983508355\n",
      "Training Loss: 0.0012747649213997647\n",
      "Validation Loss: 0.002082829413785554\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006131002600304782\n",
      "Training Loss: 0.006156358459847979\n",
      "Training Loss: 0.0062481485074386\n",
      "Training Loss: 0.004173436821147334\n",
      "Training Loss: 0.0013628089886333328\n",
      "Training Loss: 0.001365224855398992\n",
      "Training Loss: 0.0012707077496452257\n",
      "Validation Loss: 0.0020796078355801518\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006127307341666892\n",
      "Training Loss: 0.006152068071532995\n",
      "Training Loss: 0.006243887862656266\n",
      "Training Loss: 0.004169454475049861\n",
      "Training Loss: 0.0013597347709583118\n",
      "Training Loss: 0.0013616735715186222\n",
      "Training Loss: 0.001266675103106536\n",
      "Validation Loss: 0.0020763924716420694\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006123571677599102\n",
      "Training Loss: 0.006147736820275895\n",
      "Training Loss: 0.0062395772896707054\n",
      "Training Loss: 0.004165459458599799\n",
      "Training Loss: 0.0013567077476182021\n",
      "Training Loss: 0.0013581670293933712\n",
      "Training Loss: 0.001262675344623858\n",
      "Validation Loss: 0.002073186303206374\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006119788738433271\n",
      "Training Loss: 0.006143357920227572\n",
      "Training Loss: 0.0062352109770290555\n",
      "Training Loss: 0.004161450159008382\n",
      "Training Loss: 0.0013537252566311509\n",
      "Training Loss: 0.001354703500692267\n",
      "Training Loss: 0.0012587059516226873\n",
      "Validation Loss: 0.0020699918338565036\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006115958284353838\n",
      "Training Loss: 0.006138929768349044\n",
      "Training Loss: 0.006230786537053064\n",
      "Training Loss: 0.004157425670273369\n",
      "Training Loss: 0.0013507920320262202\n",
      "Training Loss: 0.001351289054582594\n",
      "Training Loss: 0.0012547714721586089\n",
      "Validation Loss: 0.002066805227882009\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.006112073953263461\n",
      "Training Loss: 0.006134447352960706\n",
      "Training Loss: 0.006226299034897238\n",
      "Training Loss: 0.004153383747616317\n",
      "Training Loss: 0.0013479063982958905\n",
      "Training Loss: 0.0013479245379858186\n",
      "Training Loss: 0.0012508748432446736\n",
      "Validation Loss: 0.0020636316295756787\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006108133635716512\n",
      "Training Loss: 0.006129909171140753\n",
      "Training Loss: 0.006221747051458806\n",
      "Training Loss: 0.00414932470957865\n",
      "Training Loss: 0.0013450687268050387\n",
      "Training Loss: 0.001344609409716213\n",
      "Training Loss: 0.001247012881358387\n",
      "Validation Loss: 0.0020604647852110673\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006104131435276941\n",
      "Training Loss: 0.006125308947521262\n",
      "Training Loss: 0.0062171241524629295\n",
      "Training Loss: 0.0041452464446774685\n",
      "Training Loss: 0.0013422815653029829\n",
      "Training Loss: 0.0013413487332582007\n",
      "Training Loss: 0.0012431932226172648\n",
      "Validation Loss: 0.0020573090742061384\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006100065339123831\n",
      "Training Loss: 0.006120646338094957\n",
      "Training Loss: 0.006212429888546467\n",
      "Training Loss: 0.004141148476628586\n",
      "Training Loss: 0.0013395428728836124\n",
      "Training Loss: 0.0013381410397414583\n",
      "Training Loss: 0.0012394121031684335\n",
      "Validation Loss: 0.002054165275497118\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006095932660391554\n",
      "Training Loss: 0.006115918809082359\n",
      "Training Loss: 0.006207659604260698\n",
      "Training Loss: 0.004137029675621307\n",
      "Training Loss: 0.0013368552457541226\n",
      "Training Loss: 0.0013349906461371575\n",
      "Training Loss: 0.0012356738942617086\n",
      "Validation Loss: 0.0020510312834709243\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006091727869352326\n",
      "Training Loss: 0.006111121661961079\n",
      "Training Loss: 0.00620281035429798\n",
      "Training Loss: 0.004132889517495642\n",
      "Training Loss: 0.0013342176913283764\n",
      "Training Loss: 0.0013318965837243013\n",
      "Training Loss: 0.00123198032961227\n",
      "Validation Loss: 0.002047907527163799\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006087447786703706\n",
      "Training Loss: 0.006106251640594564\n",
      "Training Loss: 0.00619788002804853\n",
      "Training Loss: 0.004128726892522536\n",
      "Training Loss: 0.0013316293875686824\n",
      "Training Loss: 0.0013288585472037084\n",
      "Training Loss: 0.0012283276052039583\n",
      "Validation Loss: 0.0020447953661715383\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006083094412460923\n",
      "Training Loss: 0.006101312544778921\n",
      "Training Loss: 0.006192867243662477\n",
      "Training Loss: 0.004124540947377682\n",
      "Training Loss: 0.001329091757943388\n",
      "Training Loss: 0.0013258790329564363\n",
      "Training Loss: 0.0012247201889113058\n",
      "Validation Loss: 0.002041686726524756\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006078660488128662\n",
      "Training Loss: 0.006096298097399994\n",
      "Training Loss: 0.006187769763637334\n",
      "Training Loss: 0.004120332573511405\n",
      "Training Loss: 0.0013266021668096072\n",
      "Training Loss: 0.001322954561765073\n",
      "Training Loss: 0.0012211551958171184\n",
      "Validation Loss: 0.0020385837148175633\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006074147243052721\n",
      "Training Loss: 0.0060912108916090805\n",
      "Training Loss: 0.006182587095536291\n",
      "Training Loss: 0.004116099988605129\n",
      "Training Loss: 0.0013241621809720527\n",
      "Training Loss: 0.00132008946602582\n",
      "Training Loss: 0.0012176365610503126\n",
      "Validation Loss: 0.0020354896878007423\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006069548587547615\n",
      "Training Loss: 0.006086045536212623\n",
      "Training Loss: 0.006177315234672278\n",
      "Training Loss: 0.004111842797574355\n",
      "Training Loss: 0.0013217710093886125\n",
      "Training Loss: 0.0013172808251692913\n",
      "Training Loss: 0.001214162542601116\n",
      "Validation Loss: 0.0020323995473988227\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006064865622902289\n",
      "Training Loss: 0.006080803158693015\n",
      "Training Loss: 0.0061719552031718194\n",
      "Training Loss: 0.004107562401186442\n",
      "Training Loss: 0.0013194275929708965\n",
      "Training Loss: 0.0013145299795723985\n",
      "Training Loss: 0.0012107346950506325\n",
      "Validation Loss: 0.0020293140160844206\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006060094275744632\n",
      "Training Loss: 0.006075483755557798\n",
      "Training Loss: 0.006166505509754643\n",
      "Training Loss: 0.004103257594542811\n",
      "Training Loss: 0.0013171310356119648\n",
      "Training Loss: 0.0013118340038636234\n",
      "Training Loss: 0.0012073495394724887\n",
      "Validation Loss: 0.0020262229909042543\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.0060552357893902805\n",
      "Training Loss: 0.006070089557906613\n",
      "Training Loss: 0.0061609670333564286\n",
      "Training Loss: 0.0040989282898954114\n",
      "Training Loss: 0.0013148793553409632\n",
      "Training Loss: 0.0013091929076472298\n",
      "Training Loss: 0.001204009347129613\n",
      "Validation Loss: 0.002023137458499821\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006050288192927838\n",
      "Training Loss: 0.006064615870709531\n",
      "Training Loss: 0.006155336578376591\n",
      "Training Loss: 0.004094574864284368\n",
      "Training Loss: 0.0013126747150090524\n",
      "Training Loss: 0.001306608536397107\n",
      "Training Loss: 0.0012007157714106142\n",
      "Validation Loss: 0.0020200483425091326\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.00604524806374684\n",
      "Training Loss: 0.006059061957057565\n",
      "Training Loss: 0.006149613573215902\n",
      "Training Loss: 0.004090195742464857\n",
      "Training Loss: 0.001310515272343764\n",
      "Training Loss: 0.0013040782704774757\n",
      "Training Loss: 0.0011974656276288443\n",
      "Validation Loss: 0.0020169586438968192\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006040118584642187\n",
      "Training Loss: 0.006053434520144947\n",
      "Training Loss: 0.006143802751321346\n",
      "Training Loss: 0.004085793836566154\n",
      "Training Loss: 0.0013083993930194993\n",
      "Training Loss: 0.0013015995276509783\n",
      "Training Loss: 0.0011942591842671391\n",
      "Validation Loss: 0.002013863052903371\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006034898223588243\n",
      "Training Loss: 0.006047729186248034\n",
      "Training Loss: 0.006137900075991638\n",
      "Training Loss: 0.004081368835031753\n",
      "Training Loss: 0.0013063270749989898\n",
      "Training Loss: 0.0012991742976009846\n",
      "Training Loss: 0.0011910963090485892\n",
      "Validation Loss: 0.002010759044035725\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006029585226206109\n",
      "Training Loss: 0.006041948821512051\n",
      "Training Loss: 0.006131908599636518\n",
      "Training Loss: 0.004076919803774217\n",
      "Training Loss: 0.0013042965531349182\n",
      "Training Loss: 0.0012967997633677442\n",
      "Training Loss: 0.0011879775015404448\n",
      "Validation Loss: 0.002007647622084552\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006024180548265577\n",
      "Training Loss: 0.006036092528956942\n",
      "Training Loss: 0.006125827194191516\n",
      "Training Loss: 0.004072447293583537\n",
      "Training Loss: 0.0013023065619927365\n",
      "Training Loss: 0.001294474421738414\n",
      "Training Loss: 0.0011849011786398478\n",
      "Validation Loss: 0.002004527787055318\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.00601868411526084\n",
      "Training Loss: 0.00603016150300391\n",
      "Training Loss: 0.006119655325310305\n",
      "Training Loss: 0.004067952686309582\n",
      "Training Loss: 0.001300357685686322\n",
      "Training Loss: 0.0012921984498098027\n",
      "Training Loss: 0.0011818665456667077\n",
      "Validation Loss: 0.00200139519321202\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006013096937676892\n",
      "Training Loss: 0.006024158373475075\n",
      "Training Loss: 0.006113398255547508\n",
      "Training Loss: 0.004063435955467867\n",
      "Training Loss: 0.0012984479061560706\n",
      "Training Loss: 0.0012899695959640667\n",
      "Training Loss: 0.0011788740918564145\n",
      "Validation Loss: 0.001998248712263464\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006007419503293932\n",
      "Training Loss: 0.006018083771341481\n",
      "Training Loss: 0.00610705534403678\n",
      "Training Loss: 0.0040588986811053475\n",
      "Training Loss: 0.0012965764600085094\n",
      "Training Loss: 0.001287786839675391\n",
      "Training Loss: 0.0011759230514871888\n",
      "Validation Loss: 0.001995087845847429\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006001653977436945\n",
      "Training Loss: 0.00601193824200891\n",
      "Training Loss: 0.006100625492399559\n",
      "Training Loss: 0.004054340073780622\n",
      "Training Loss: 0.0012947446084581316\n",
      "Training Loss: 0.0012856503807415721\n",
      "Training Loss: 0.0011730137177801225\n",
      "Validation Loss: 0.0019919117101666966\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.00599579829024151\n",
      "Training Loss: 0.006005723066045902\n",
      "Training Loss: 0.0060941113444278015\n",
      "Training Loss: 0.004049761359638069\n",
      "Training Loss: 0.001292949433263857\n",
      "Training Loss: 0.0012835573285701685\n",
      "Training Loss: 0.00117014471499715\n",
      "Validation Loss: 0.0019887168625760334\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005989856357919052\n",
      "Training Loss: 0.005999439352890477\n",
      "Training Loss: 0.006087513857637532\n",
      "Training Loss: 0.004045162463007727\n",
      "Training Loss: 0.0012911921639170033\n",
      "Training Loss: 0.0012815082429733593\n",
      "Training Loss: 0.0011673155955213587\n",
      "Validation Loss: 0.0019855037851961963\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005983827166492119\n",
      "Training Loss: 0.005993089535040781\n",
      "Training Loss: 0.006080836227629334\n",
      "Training Loss: 0.004040544148301706\n",
      "Training Loss: 0.0012894725966907572\n",
      "Training Loss: 0.0012795015797019006\n",
      "Training Loss: 0.0011645261105149983\n",
      "Validation Loss: 0.0019822719339926617\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005977716132765636\n",
      "Training Loss: 0.00598667585873045\n",
      "Training Loss: 0.006074078406672925\n",
      "Training Loss: 0.004035909343074308\n",
      "Training Loss: 0.0012877896220015827\n",
      "Training Loss: 0.0012775359161605593\n",
      "Training Loss: 0.0011617765703704208\n",
      "Validation Loss: 0.0019790215388529897\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005971523101907223\n",
      "Training Loss: 0.005980199839104899\n",
      "Training Loss: 0.006067244316218421\n",
      "Training Loss: 0.0040312575486314015\n",
      "Training Loss: 0.001286140629381407\n",
      "Training Loss: 0.001275608459545765\n",
      "Training Loss: 0.0011590631073340773\n",
      "Validation Loss: 0.0019757445996029\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005965252137975767\n",
      "Training Loss: 0.005973663121112622\n",
      "Training Loss: 0.006060334590729326\n",
      "Training Loss: 0.004026587571279379\n",
      "Training Loss: 0.0012845297543390187\n",
      "Training Loss: 0.0012737223839212675\n",
      "Training Loss: 0.0011563911350094714\n",
      "Validation Loss: 0.0019724485820834265\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005958903259597719\n",
      "Training Loss: 0.005967067355522886\n",
      "Training Loss: 0.006053350755828433\n",
      "Training Loss: 0.004021902604290517\n",
      "Training Loss: 0.0012829537596553565\n",
      "Training Loss: 0.0012718751358625014\n",
      "Training Loss: 0.0011537570641667117\n",
      "Validation Loss: 0.0019691283173670763\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0059524801443330945\n",
      "Training Loss: 0.005960416597663425\n",
      "Training Loss: 0.006046298490837217\n",
      "Training Loss: 0.004017203285184223\n",
      "Training Loss: 0.001281413190299645\n",
      "Training Loss: 0.0012700646431767382\n",
      "Training Loss: 0.0011511594589683227\n",
      "Validation Loss: 0.001965785380548794\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005945988581515849\n",
      "Training Loss: 0.005953713284106925\n",
      "Training Loss: 0.006039177062339149\n",
      "Training Loss: 0.004012489895976614\n",
      "Training Loss: 0.0012799117471149657\n",
      "Training Loss: 0.001268294066831004\n",
      "Training Loss: 0.001148602612374816\n",
      "Validation Loss: 0.001962420330923539\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0059394265303853895\n",
      "Training Loss: 0.0059469559916760775\n",
      "Training Loss: 0.006031988617032766\n",
      "Training Loss: 0.004007764927810058\n",
      "Training Loss: 0.0012784457512316294\n",
      "Training Loss: 0.0012665583228226752\n",
      "Training Loss: 0.0011460809558047913\n",
      "Validation Loss: 0.0019590314301097074\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005932801285525784\n",
      "Training Loss: 0.0059401513397460805\n",
      "Training Loss: 0.006024737396510318\n",
      "Training Loss: 0.004003027408616617\n",
      "Training Loss: 0.0012770178672508337\n",
      "Training Loss: 0.0012648623512359336\n",
      "Training Loss: 0.0011436017473170068\n",
      "Validation Loss: 0.001955619607140809\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005926112566376105\n",
      "Training Loss: 0.0059332979953614995\n",
      "Training Loss: 0.0060174240660853685\n",
      "Training Loss: 0.00399827972825733\n",
      "Training Loss: 0.0012756292168342044\n",
      "Training Loss: 0.0012632014146947768\n",
      "Training Loss: 0.0011411567762843334\n",
      "Validation Loss: 0.0019521861484715513\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.0059193708980456\n",
      "Training Loss: 0.0059264080837601795\n",
      "Training Loss: 0.006010057702078484\n",
      "Training Loss: 0.0039935226543457245\n",
      "Training Loss: 0.0012742786209855694\n",
      "Training Loss: 0.0012615785664820578\n",
      "Training Loss: 0.001138754670828348\n",
      "Validation Loss: 0.001948729756310903\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.0059125711955130104\n",
      "Training Loss: 0.005919471669476479\n",
      "Training Loss: 0.006002634783508256\n",
      "Training Loss: 0.003988757972547319\n",
      "Training Loss: 0.0012729673167632427\n",
      "Training Loss: 0.0012599886977113783\n",
      "Training Loss: 0.001136385719437385\n",
      "Validation Loss: 0.0019452516087644807\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.0059057282807771115\n",
      "Training Loss: 0.005912504093139433\n",
      "Training Loss: 0.005995162357576192\n",
      "Training Loss: 0.003983988159743604\n",
      "Training Loss: 0.0012716983174323104\n",
      "Training Loss: 0.0012584359655738809\n",
      "Training Loss: 0.0011340574218775146\n",
      "Validation Loss: 0.0019417566681988816\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005898838969878853\n",
      "Training Loss: 0.0059054999198997395\n",
      "Training Loss: 0.005987641551182605\n",
      "Training Loss: 0.0039792130378191355\n",
      "Training Loss: 0.0012704716272128281\n",
      "Training Loss: 0.0012569216011615936\n",
      "Training Loss: 0.0011317696624610107\n",
      "Validation Loss: 0.0019382383760059609\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005891908481717109\n",
      "Training Loss: 0.005898468309314921\n",
      "Training Loss: 0.0059800785832339895\n",
      "Training Loss: 0.003974434980627848\n",
      "Training Loss: 0.0012692877087101806\n",
      "Training Loss: 0.0012554443246335723\n",
      "Training Loss: 0.0011295224454079288\n",
      "Validation Loss: 0.0019347049178832913\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005884942712727934\n",
      "Training Loss: 0.005891411203774624\n",
      "Training Loss: 0.005972477067261934\n",
      "Training Loss: 0.0039696548780193555\n",
      "Training Loss: 0.001268147328228224\n",
      "Training Loss: 0.0012540012063982431\n",
      "Training Loss: 0.0011273119115503506\n",
      "Validation Loss: 0.001931151916156959\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005877947411499917\n",
      "Training Loss: 0.005884331652196124\n",
      "Training Loss: 0.0059648385405307635\n",
      "Training Loss: 0.003964874908851925\n",
      "Training Loss: 0.001267051102477126\n",
      "Training Loss: 0.001252597181155579\n",
      "Training Loss: 0.0011251446828828193\n",
      "Validation Loss: 0.0019275850554481707\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005870923947077245\n",
      "Training Loss: 0.005877233105129562\n",
      "Training Loss: 0.005957167989108712\n",
      "Training Loss: 0.0039600984545541\n",
      "Training Loss: 0.0012660041060007642\n",
      "Training Loss: 0.0012512302793038542\n",
      "Training Loss: 0.0011230175737000536\n",
      "Validation Loss: 0.0019240056854334384\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005863879262469709\n",
      "Training Loss: 0.005870121909538284\n",
      "Training Loss: 0.005949471009662375\n",
      "Training Loss: 0.0039553253108169885\n",
      "Training Loss: 0.001265002561849542\n",
      "Training Loss: 0.0012498998022056184\n",
      "Training Loss: 0.0011209306678210851\n",
      "Validation Loss: 0.0019204149297168233\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.00585682341363281\n",
      "Training Loss: 0.005863003385602496\n",
      "Training Loss: 0.005941753103397787\n",
      "Training Loss: 0.0039505592352361415\n",
      "Training Loss: 0.0012640491523779928\n",
      "Training Loss: 0.001248606843437301\n",
      "Training Loss: 0.0011188866235897876\n",
      "Validation Loss: 0.0019168138989310167\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005849751991918311\n",
      "Training Loss: 0.005855876360437833\n",
      "Training Loss: 0.005934014564845711\n",
      "Training Loss: 0.003945800605288241\n",
      "Training Loss: 0.0012631449711625463\n",
      "Training Loss: 0.0012473523647349794\n",
      "Training Loss: 0.0011168844024359713\n",
      "Validation Loss: 0.0019132068901966336\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005842676315223798\n",
      "Training Loss: 0.005848749084980227\n",
      "Training Loss: 0.005926260694395751\n",
      "Training Loss: 0.003941052738809958\n",
      "Training Loss: 0.0012622923124581576\n",
      "Training Loss: 0.0012461367706418968\n",
      "Training Loss: 0.001114924069843255\n",
      "Validation Loss: 0.001909592849235591\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005835601412691176\n",
      "Training Loss: 0.005841627328190952\n",
      "Training Loss: 0.005918500496773049\n",
      "Training Loss: 0.003936318787746132\n",
      "Training Loss: 0.0012614909895637537\n",
      "Training Loss: 0.0012449599902902265\n",
      "Training Loss: 0.0011130091440281831\n",
      "Validation Loss: 0.0019059749537065612\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005828528564888984\n",
      "Training Loss: 0.0058345134078990665\n",
      "Training Loss: 0.005910733925993554\n",
      "Training Loss: 0.003931600391952088\n",
      "Training Loss: 0.0012607415662205313\n",
      "Training Loss: 0.0012438212578126694\n",
      "Training Loss: 0.001111135631799698\n",
      "Validation Loss: 0.0019023614641922536\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005821465738117695\n",
      "Training Loss: 0.005827410925994627\n",
      "Training Loss: 0.00590296650712844\n",
      "Training Loss: 0.0039268979607732036\n",
      "Training Loss: 0.0012600456883956213\n",
      "Training Loss: 0.001242721804737812\n",
      "Training Loss: 0.0011093081992294173\n",
      "Validation Loss: 0.001898747289811404\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005814417296787724\n",
      "Training Loss: 0.005820325379027054\n",
      "Training Loss: 0.005895204837433994\n",
      "Training Loss: 0.003922216136124916\n",
      "Training Loss: 0.0012594011925102678\n",
      "Training Loss: 0.0012416597649280447\n",
      "Training Loss: 0.0011075220599013847\n",
      "Validation Loss: 0.0018951373864994263\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005807389132678508\n",
      "Training Loss: 0.005813263424788602\n",
      "Training Loss: 0.005887451660819352\n",
      "Training Loss: 0.003917557244640193\n",
      "Training Loss: 0.0012588121094449889\n",
      "Training Loss: 0.0012406384464702569\n",
      "Training Loss: 0.0011057832432561554\n",
      "Validation Loss: 0.0018915365579143893\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005800382072338834\n",
      "Training Loss: 0.005806224893894978\n",
      "Training Loss: 0.005879710705485195\n",
      "Training Loss: 0.00391292218293529\n",
      "Training Loss: 0.0012582789537555073\n",
      "Training Loss: 0.0012396595273457933\n",
      "Training Loss: 0.0011040938113001176\n",
      "Validation Loss: 0.0018879438769848396\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005793400566326454\n",
      "Training Loss: 0.0057992142683360726\n",
      "Training Loss: 0.005871984829427674\n",
      "Training Loss: 0.003908315020380542\n",
      "Training Loss: 0.0012578009135904723\n",
      "Training Loss: 0.0012387163979292381\n",
      "Training Loss: 0.0011024448643729555\n",
      "Validation Loss: 0.0018843640834930238\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005786454256158322\n",
      "Training Loss: 0.005792240082519129\n",
      "Training Loss: 0.005864282709662803\n",
      "Training Loss: 0.003903736847132677\n",
      "Training Loss: 0.001257377548608929\n",
      "Training Loss: 0.0012378163653193042\n",
      "Training Loss: 0.0011008459262666292\n",
      "Validation Loss: 0.0018808034801077837\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005779540424700826\n",
      "Training Loss: 0.005785300141433254\n",
      "Training Loss: 0.00585660393931903\n",
      "Training Loss: 0.0038991917717066827\n",
      "Training Loss: 0.0012570088668144308\n",
      "Training Loss: 0.0012369533728633542\n",
      "Training Loss: 0.0010992917271505575\n",
      "Validation Loss: 0.001877259303818279\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005772670164005831\n",
      "Training Loss: 0.005778406028985046\n",
      "Training Loss: 0.005848958231508732\n",
      "Training Loss: 0.0038946803464205005\n",
      "Training Loss: 0.0012566938846430276\n",
      "Training Loss: 0.0012361314050212968\n",
      "Training Loss: 0.0010977858552359977\n",
      "Validation Loss: 0.0018737359080997326\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.00576584066147916\n",
      "Training Loss: 0.0057715561828808855\n",
      "Training Loss: 0.005841346419765614\n",
      "Training Loss: 0.0038902039445383708\n",
      "Training Loss: 0.0012564331894100178\n",
      "Training Loss: 0.0012353471756796353\n",
      "Training Loss: 0.0010963246747269296\n",
      "Validation Loss: 0.0018702386626633658\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005759061676217243\n",
      "Training Loss: 0.005764757110155188\n",
      "Training Loss: 0.0058337738213595\n",
      "Training Loss: 0.0038857663540693465\n",
      "Training Loss: 0.0012562258787511382\n",
      "Training Loss: 0.0012346009236352984\n",
      "Training Loss: 0.0010949103401799221\n",
      "Validation Loss: 0.001866768364430918\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005752332674455829\n",
      "Training Loss: 0.005758009694400244\n",
      "Training Loss: 0.005826241981703788\n",
      "Training Loss: 0.0038813690616370878\n",
      "Training Loss: 0.0012560729258984792\n",
      "Training Loss: 0.0012338949953846169\n",
      "Training Loss: 0.001093543623574078\n",
      "Validation Loss: 0.001863324905707316\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005745658326195553\n",
      "Training Loss: 0.005751318520051427\n",
      "Training Loss: 0.005818755503278226\n",
      "Training Loss: 0.0038770150403433946\n",
      "Training Loss: 0.0012559748375497292\n",
      "Training Loss: 0.0012332265390432439\n",
      "Training Loss: 0.0010922244534594938\n",
      "Validation Loss: 0.0018599118940594993\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005739039169857279\n",
      "Training Loss: 0.005744684208766557\n",
      "Training Loss: 0.0058113168639829385\n",
      "Training Loss: 0.0038727053064940265\n",
      "Training Loss: 0.0012559270064230077\n",
      "Training Loss: 0.001232594612374669\n",
      "Training Loss: 0.0010909500214620494\n",
      "Validation Loss: 0.001856536196247406\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005732483303290792\n",
      "Training Loss: 0.005738114744308405\n",
      "Training Loss: 0.005803932248381898\n",
      "Training Loss: 0.0038684407760592875\n",
      "Training Loss: 0.0012559279659762979\n",
      "Training Loss: 0.0012319965344795491\n",
      "Training Loss: 0.0010897200390172657\n",
      "Validation Loss: 0.0018531940966820142\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005725989895872772\n",
      "Training Loss: 0.005731610920629464\n",
      "Training Loss: 0.0057966022973414515\n",
      "Training Loss: 0.0038642233694554306\n",
      "Training Loss: 0.0012559785805933642\n",
      "Training Loss: 0.0012314366943610366\n",
      "Training Loss: 0.0010885383139248007\n",
      "Validation Loss: 0.0018498944235252294\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005719558392302133\n",
      "Training Loss: 0.005725170226069168\n",
      "Training Loss: 0.005789329094113782\n",
      "Training Loss: 0.0038600558859616285\n",
      "Training Loss: 0.0012560799010680058\n",
      "Training Loss: 0.0012309124754392542\n",
      "Training Loss: 0.001087401196709834\n",
      "Validation Loss: 0.0018466328610169042\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005713195740827359\n",
      "Training Loss: 0.0057188001560280095\n",
      "Training Loss: 0.005782116536283866\n",
      "Training Loss: 0.0038559385888947873\n",
      "Training Loss: 0.001256226398400031\n",
      "Training Loss: 0.0012304208874411415\n",
      "Training Loss: 0.0010863080342824105\n",
      "Validation Loss: 0.0018434163177741712\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005706902055535466\n",
      "Training Loss: 0.005712503484683111\n",
      "Training Loss: 0.005774968132609502\n",
      "Training Loss: 0.003851872311061015\n",
      "Training Loss: 0.0012564182335336226\n",
      "Training Loss: 0.0012299615444499067\n",
      "Training Loss: 0.001085256920341635\n",
      "Validation Loss: 0.001840241677071097\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005700678917346522\n",
      "Training Loss: 0.005706277915160172\n",
      "Training Loss: 0.00576788337493781\n",
      "Training Loss: 0.003847857961227419\n",
      "Training Loss: 0.0012566548107133713\n",
      "Training Loss: 0.0012295350692875218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [08:08<1:13:14, 488.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0010842521357699298\n",
      "Validation Loss: 0.0018371143897172161\n",
      "Validation Accuracy: 0.11704119850187265\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.32630624793469903\n",
      "Training Loss: 0.2590935175865889\n",
      "Training Loss: 0.20894528672099114\n",
      "Training Loss: 0.15822062402963638\n",
      "Training Loss: 0.11961612882092595\n",
      "Training Loss: 0.08670199638232588\n",
      "Training Loss: 0.07145557120908051\n",
      "Validation Loss: 0.05992891248106287\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.05793328261002898\n",
      "Training Loss: 0.04661825004033744\n",
      "Training Loss: 0.03954632599838078\n",
      "Training Loss: 0.03329122568713501\n",
      "Training Loss: 0.030068998946808277\n",
      "Training Loss: 0.02631930774077773\n",
      "Training Loss: 0.02619848961941898\n",
      "Validation Loss: 0.025651116984502207\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.02762579240370542\n",
      "Training Loss: 0.025994624486193062\n",
      "Training Loss: 0.023866250379942357\n",
      "Training Loss: 0.02008333454024978\n",
      "Training Loss: 0.01601879051188007\n",
      "Training Loss: 0.013661122738849372\n",
      "Training Loss: 0.013134975167922676\n",
      "Validation Loss: 0.013886434534452995\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.016920984480530022\n",
      "Training Loss: 0.01645158090395853\n",
      "Training Loss: 0.015202872541267424\n",
      "Training Loss: 0.012132156092848163\n",
      "Training Loss: 0.00830797123373486\n",
      "Training Loss: 0.0075516138225793835\n",
      "Training Loss: 0.0076385826361365615\n",
      "Validation Loss: 0.009363339485754067\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.013090753101278097\n",
      "Training Loss: 0.01322700731921941\n",
      "Training Loss: 0.012456584447063506\n",
      "Training Loss: 0.009630590722954366\n",
      "Training Loss: 0.005878685272764414\n",
      "Training Loss: 0.005508192360866815\n",
      "Training Loss: 0.0055033912928774955\n",
      "Validation Loss: 0.0070331829324476285\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.011034794982988387\n",
      "Training Loss: 0.011007978698471562\n",
      "Training Loss: 0.01018925774260424\n",
      "Training Loss: 0.007198970947938505\n",
      "Training Loss: 0.003447952762362547\n",
      "Training Loss: 0.003208995920140296\n",
      "Training Loss: 0.0031527641290449537\n",
      "Validation Loss: 0.004703536837218741\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.0086712446808815\n",
      "Training Loss: 0.008929645076859742\n",
      "Training Loss: 0.008649851112859324\n",
      "Training Loss: 0.00606571547454223\n",
      "Training Loss: 0.0027092903916491194\n",
      "Training Loss: 0.002700689008634072\n",
      "Training Loss: 0.002709856920700986\n",
      "Validation Loss: 0.004136304741669516\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.008129520607180894\n",
      "Training Loss: 0.008416074957931414\n",
      "Training Loss: 0.008230947139672935\n",
      "Training Loss: 0.005722751152643468\n",
      "Training Loss: 0.002432510051003192\n",
      "Training Loss: 0.002449385766813066\n",
      "Training Loss: 0.0024332100729225203\n",
      "Validation Loss: 0.0036898034956996863\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.00778090835432522\n",
      "Training Loss: 0.008081461897818372\n",
      "Training Loss: 0.007946082028793171\n",
      "Training Loss: 0.005483992088993545\n",
      "Training Loss: 0.0022308583377161992\n",
      "Training Loss: 0.002262465641106246\n",
      "Training Loss: 0.0022297998418798672\n",
      "Validation Loss: 0.003411227495371179\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.007541683538584039\n",
      "Training Loss: 0.007856972770532593\n",
      "Training Loss: 0.007749698286643252\n",
      "Training Loss: 0.00531610924663255\n",
      "Training Loss: 0.002082810653373599\n",
      "Training Loss: 0.002121932207082864\n",
      "Training Loss: 0.002080409621994477\n",
      "Validation Loss: 0.0032258697657685464\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.007372707421891392\n",
      "Training Loss: 0.007699161302298307\n",
      "Training Loss: 0.0076069824048317965\n",
      "Training Loss: 0.005193212302983739\n",
      "Training Loss: 0.0019716998626245186\n",
      "Training Loss: 0.002013878735306207\n",
      "Training Loss: 0.001968072358140489\n",
      "Validation Loss: 0.0030867865421174423\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.007243894287385046\n",
      "Training Loss: 0.007575518439989537\n",
      "Training Loss: 0.00749209743575193\n",
      "Training Loss: 0.005095976044685813\n",
      "Training Loss: 0.0018848300189711154\n",
      "Training Loss: 0.0019272970921883825\n",
      "Training Loss: 0.0018797174899373202\n",
      "Validation Loss: 0.0029708124576868973\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.007136046824743971\n",
      "Training Loss: 0.007467326193582266\n",
      "Training Loss: 0.007390544862719253\n",
      "Training Loss: 0.005013134089385858\n",
      "Training Loss: 0.001814059937169077\n",
      "Training Loss: 0.0018548411346273497\n",
      "Training Loss: 0.0018070460422313771\n",
      "Validation Loss: 0.0028676432716548583\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.007040141356410459\n",
      "Training Loss: 0.007367075203219429\n",
      "Training Loss: 0.007296533858170733\n",
      "Training Loss: 0.0049395238066790624\n",
      "Training Loss: 0.001754888064606348\n",
      "Training Loss: 0.0017923978451290167\n",
      "Training Loss: 0.001745497472875286\n",
      "Validation Loss: 0.002773217622200306\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.006953316917642951\n",
      "Training Loss: 0.007273134116549045\n",
      "Training Loss: 0.007208792244782671\n",
      "Training Loss: 0.004873213630780811\n",
      "Training Loss: 0.0017049554007826373\n",
      "Training Loss: 0.0017379297214210965\n",
      "Training Loss: 0.001692656411396456\n",
      "Validation Loss: 0.002686215190073887\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.006875318760285154\n",
      "Training Loss: 0.007186056955251843\n",
      "Training Loss: 0.007127794281113892\n",
      "Training Loss: 0.004813622844812926\n",
      "Training Loss: 0.0016628879624477123\n",
      "Training Loss: 0.0016904296244319994\n",
      "Training Loss: 0.0016471115591411944\n",
      "Validation Loss: 0.0026064729701713676\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.006806621886789798\n",
      "Training Loss: 0.007106869004201144\n",
      "Training Loss: 0.007054439806379378\n",
      "Training Loss: 0.004760587276541628\n",
      "Training Loss: 0.0016276224199100397\n",
      "Training Loss: 0.001649254568910692\n",
      "Training Loss: 0.001607828462656471\n",
      "Validation Loss: 0.002534263078511382\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.006747577019268647\n",
      "Training Loss: 0.007036339169135317\n",
      "Training Loss: 0.0069894093764014545\n",
      "Training Loss: 0.004713920362846693\n",
      "Training Loss: 0.0015981195589120033\n",
      "Training Loss: 0.0016137895617430332\n",
      "Training Loss: 0.0015739165184641025\n",
      "Validation Loss: 0.002469946523149034\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.006698033683933318\n",
      "Training Loss: 0.006974646026501432\n",
      "Training Loss: 0.006932859536027536\n",
      "Training Loss: 0.004673249745101202\n",
      "Training Loss: 0.0015733399191231\n",
      "Training Loss: 0.0015833538272272563\n",
      "Training Loss: 0.0015445608820300548\n",
      "Validation Loss: 0.0024137269706188\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.006657284188549966\n",
      "Training Loss: 0.0069213486020453276\n",
      "Training Loss: 0.006884387340396643\n",
      "Training Loss: 0.004638006285968004\n",
      "Training Loss: 0.0015523314381425735\n",
      "Training Loss: 0.0015572479373076932\n",
      "Training Loss: 0.001519056257311604\n",
      "Validation Loss: 0.002365510143585545\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.006624201467493549\n",
      "Training Loss: 0.0068755442136898635\n",
      "Training Loss: 0.006843168179038912\n",
      "Training Loss: 0.004607513586524874\n",
      "Training Loss: 0.0015343084424966945\n",
      "Training Loss: 0.0015348178295243996\n",
      "Training Loss: 0.0014968109536130213\n",
      "Validation Loss: 0.002324872119905957\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.006597464807564393\n",
      "Training Loss: 0.006836108911084011\n",
      "Training Loss: 0.006808164748363197\n",
      "Training Loss: 0.004581077248731163\n",
      "Training Loss: 0.001518669112410862\n",
      "Training Loss: 0.001515485865456867\n",
      "Training Loss: 0.0014773403519939166\n",
      "Validation Loss: 0.0022911064900991538\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.0065757585701067\n",
      "Training Loss: 0.006801903410814702\n",
      "Training Loss: 0.0067783133243210615\n",
      "Training Loss: 0.004558050972118508\n",
      "Training Loss: 0.001504959814628819\n",
      "Training Loss: 0.0014987496446701699\n",
      "Training Loss: 0.0014602251462201821\n",
      "Validation Loss: 0.0022633272413494004\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.006557914754375815\n",
      "Training Loss: 0.006771925661014393\n",
      "Training Loss: 0.0067526469298172746\n",
      "Training Loss: 0.004537872100481764\n",
      "Training Loss: 0.0014928297146980184\n",
      "Training Loss: 0.0014841701956174803\n",
      "Training Loss: 0.001445091288042022\n",
      "Validation Loss: 0.002240574336283725\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.006542977434583009\n",
      "Training Loss: 0.006745361823122948\n",
      "Training Loss: 0.006730353832244873\n",
      "Training Loss: 0.004520065323013114\n",
      "Training Loss: 0.0014819878571142907\n",
      "Training Loss: 0.001471360550785903\n",
      "Training Loss: 0.0014316077114199287\n",
      "Validation Loss: 0.002221921255113557\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.00653019489836879\n",
      "Training Loss: 0.00672157587017864\n",
      "Training Loss: 0.006710777138359845\n",
      "Training Loss: 0.004504238601366523\n",
      "Training Loss: 0.0014721855995594524\n",
      "Training Loss: 0.0014599767603067448\n",
      "Training Loss: 0.0014194667449919506\n",
      "Validation Loss: 0.0022065130652980146\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.006519014541991055\n",
      "Training Loss: 0.006700103565817699\n",
      "Training Loss: 0.006693415585905313\n",
      "Training Loss: 0.004490073893684894\n",
      "Training Loss: 0.0014631995155650656\n",
      "Training Loss: 0.0014497144319466316\n",
      "Training Loss: 0.001408394071768271\n",
      "Validation Loss: 0.0021936118964425004\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.006509040950331837\n",
      "Training Loss: 0.006680601114640012\n",
      "Training Loss: 0.006677885425742716\n",
      "Training Loss: 0.004477309323265217\n",
      "Training Loss: 0.0014548352341807913\n",
      "Training Loss: 0.0014403146677068434\n",
      "Training Loss: 0.0013981538620282663\n",
      "Validation Loss: 0.0021826038757305983\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.006499996355269104\n",
      "Training Loss: 0.006662813621805981\n",
      "Training Loss: 0.006663896322716027\n",
      "Training Loss: 0.004465732942335308\n",
      "Training Loss: 0.0014469247628585435\n",
      "Training Loss: 0.0014315577622619458\n",
      "Training Loss: 0.0013885424727050123\n",
      "Validation Loss: 0.0021729884025630322\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006491691468982026\n",
      "Training Loss: 0.0066465445118956265\n",
      "Training Loss: 0.006651225935202092\n",
      "Training Loss: 0.0044551692930690475\n",
      "Training Loss: 0.0014393316140922253\n",
      "Training Loss: 0.0014232673800142947\n",
      "Training Loss: 0.0013793965033255518\n",
      "Validation Loss: 0.0021643789144135204\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.0064839942276012155\n",
      "Training Loss: 0.006631634257500991\n",
      "Training Loss: 0.006639695090707391\n",
      "Training Loss: 0.004445471399376402\n",
      "Training Loss: 0.0014319509283814114\n",
      "Training Loss: 0.0014153068552695913\n",
      "Training Loss: 0.0013705894996382994\n",
      "Validation Loss: 0.0021564784448433404\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006476807618746534\n",
      "Training Loss: 0.006617938432609663\n",
      "Training Loss: 0.006629154934780672\n",
      "Training Loss: 0.00443651629058877\n",
      "Training Loss: 0.0014247007407539058\n",
      "Training Loss: 0.0014075736573431641\n",
      "Training Loss: 0.00136202187903109\n",
      "Validation Loss: 0.0021490636256598426\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.0064700614451430735\n",
      "Training Loss: 0.006605335515923798\n",
      "Training Loss: 0.006619483119575307\n",
      "Training Loss: 0.004428199498070171\n",
      "Training Loss: 0.0014175260094634722\n",
      "Training Loss: 0.001399996650070534\n",
      "Training Loss: 0.0013536271055636462\n",
      "Validation Loss: 0.002141979633205246\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006463694251142443\n",
      "Training Loss: 0.006593698116485029\n",
      "Training Loss: 0.006610560590634123\n",
      "Training Loss: 0.004420432182378135\n",
      "Training Loss: 0.0014103982696542516\n",
      "Training Loss: 0.001392530846860609\n",
      "Training Loss: 0.0013453590494464152\n",
      "Validation Loss: 0.002135113405965422\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006457656994462014\n",
      "Training Loss: 0.006582919871434569\n",
      "Training Loss: 0.006602291034068912\n",
      "Training Loss: 0.004413137949450174\n",
      "Training Loss: 0.0014032980259798932\n",
      "Training Loss: 0.0013851499141310341\n",
      "Training Loss: 0.001337191113634617\n",
      "Validation Loss: 0.002128391796367658\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.00645190219511278\n",
      "Training Loss: 0.006572888824157417\n",
      "Training Loss: 0.006594578516669571\n",
      "Training Loss: 0.004406250472966349\n",
      "Training Loss: 0.0013962238578824327\n",
      "Training Loss: 0.0013778444726631279\n",
      "Training Loss: 0.001329109207217698\n",
      "Validation Loss: 0.0021217690153663964\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006446387292817235\n",
      "Training Loss: 0.006563506931997836\n",
      "Training Loss: 0.006587339219404385\n",
      "Training Loss: 0.004399713306629565\n",
      "Training Loss: 0.0013891842536395416\n",
      "Training Loss: 0.0013706139749410796\n",
      "Training Loss: 0.0013211104513175087\n",
      "Validation Loss: 0.0021152171756058918\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006441072771558538\n",
      "Training Loss: 0.0065546805143821985\n",
      "Training Loss: 0.006580495877424255\n",
      "Training Loss: 0.004393477382400306\n",
      "Training Loss: 0.0013821887428639456\n",
      "Training Loss: 0.0013634650354651967\n",
      "Training Loss: 0.0013131980444450165\n",
      "Validation Loss: 0.002108726250980056\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.00643592047970742\n",
      "Training Loss: 0.0065463239885866645\n",
      "Training Loss: 0.0065739774750545625\n",
      "Training Loss: 0.004387500928569352\n",
      "Training Loss: 0.0013752592459786684\n",
      "Training Loss: 0.0013564127315476072\n",
      "Training Loss: 0.0013053824200324016\n",
      "Validation Loss: 0.002102293545242455\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006430895497323945\n",
      "Training Loss: 0.006538357520475983\n",
      "Training Loss: 0.0065677188558038325\n",
      "Training Loss: 0.004381746552389813\n",
      "Training Loss: 0.0013684141903650016\n",
      "Training Loss: 0.0013494699115835828\n",
      "Training Loss: 0.001297673757726443\n",
      "Validation Loss: 0.002095920623241739\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006425967656541616\n",
      "Training Loss: 0.006530714423861355\n",
      "Training Loss: 0.006561663800384849\n",
      "Training Loss: 0.004376182846754091\n",
      "Training Loss: 0.0013616736282710917\n",
      "Training Loss: 0.001342652846724377\n",
      "Training Loss: 0.0012900835498294327\n",
      "Validation Loss: 0.002089613410822998\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.0064211059815716\n",
      "Training Loss: 0.006523327347822488\n",
      "Training Loss: 0.006555757649475708\n",
      "Training Loss: 0.004370780781609938\n",
      "Training Loss: 0.0013550580004812217\n",
      "Training Loss: 0.0013359741716703866\n",
      "Training Loss: 0.0012826235142711084\n",
      "Validation Loss: 0.002083382023147862\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006416285909945145\n",
      "Training Loss: 0.006516143352491781\n",
      "Training Loss: 0.006549957224633545\n",
      "Training Loss: 0.004365516365796793\n",
      "Training Loss: 0.0013485816490720027\n",
      "Training Loss: 0.0013294478415627963\n",
      "Training Loss: 0.0012753029918530956\n",
      "Validation Loss: 0.002077231756132445\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006411482375115156\n",
      "Training Loss: 0.0065091109881177545\n",
      "Training Loss: 0.006544218685012311\n",
      "Training Loss: 0.0043603667497518475\n",
      "Training Loss: 0.0013422611223359126\n",
      "Training Loss: 0.0013230828002997442\n",
      "Training Loss: 0.0012681317290116566\n",
      "Validation Loss: 0.0020711702547643912\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006406672009034082\n",
      "Training Loss: 0.006502184356795624\n",
      "Training Loss: 0.0065385036903899164\n",
      "Training Loss: 0.0043553118726413235\n",
      "Training Loss: 0.0013361066361539998\n",
      "Training Loss: 0.0013168857951677637\n",
      "Training Loss: 0.001261111596977571\n",
      "Validation Loss: 0.0020651992889272035\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006401835838332772\n",
      "Training Loss: 0.006495324524585158\n",
      "Training Loss: 0.006532778386026621\n",
      "Training Loss: 0.004350332816102309\n",
      "Training Loss: 0.0013301240965665783\n",
      "Training Loss: 0.0013108608480979457\n",
      "Training Loss: 0.0012542492384818615\n",
      "Validation Loss: 0.002059321965809043\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006396954344818368\n",
      "Training Loss: 0.006488494517980143\n",
      "Training Loss: 0.006527011032449082\n",
      "Training Loss: 0.004345411958784098\n",
      "Training Loss: 0.0013243203071760946\n",
      "Training Loss: 0.001305011152435327\n",
      "Training Loss: 0.0012475439286936307\n",
      "Validation Loss: 0.0020535392195120635\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006392009819392115\n",
      "Training Loss: 0.006481663915328681\n",
      "Training Loss: 0.0065211760555393995\n",
      "Training Loss: 0.004340534477378242\n",
      "Training Loss: 0.0013186935598787385\n",
      "Training Loss: 0.0012993344561255072\n",
      "Training Loss: 0.0012409959000797245\n",
      "Validation Loss: 0.0020478483895973515\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006386982976691797\n",
      "Training Loss: 0.006474799995776266\n",
      "Training Loss: 0.0065152428776491435\n",
      "Training Loss: 0.004335682772798464\n",
      "Training Loss: 0.0013132440314802807\n",
      "Training Loss: 0.00129383017141663\n",
      "Training Loss: 0.0012346029274340254\n",
      "Validation Loss: 0.0020422475728160993\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006381854673381895\n",
      "Training Loss: 0.006467877135146409\n",
      "Training Loss: 0.006509188879281282\n",
      "Training Loss: 0.004330843748903135\n",
      "Training Loss: 0.0013079648184066172\n",
      "Training Loss: 0.0012884904151724186\n",
      "Training Loss: 0.0012283582468808163\n",
      "Validation Loss: 0.0020367288495473255\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006376610923325643\n",
      "Training Loss: 0.006460869008442387\n",
      "Training Loss: 0.006502991211600601\n",
      "Training Loss: 0.00432600255109719\n",
      "Training Loss: 0.0013028477011539509\n",
      "Training Loss: 0.0012833086277532857\n",
      "Training Loss: 0.001222254585591145\n",
      "Validation Loss: 0.0020312843161935075\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006371233757818118\n",
      "Training Loss: 0.006453754818066954\n",
      "Training Loss: 0.006496627500746399\n",
      "Training Loss: 0.0043211435718694706\n",
      "Training Loss: 0.001297883196530165\n",
      "Training Loss: 0.0012782773912476842\n",
      "Training Loss: 0.0012162866977450905\n",
      "Validation Loss: 0.0020259078658553895\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.0063657009904272855\n",
      "Training Loss: 0.006446506619686261\n",
      "Training Loss: 0.006490073752356693\n",
      "Training Loss: 0.004316252927819733\n",
      "Training Loss: 0.0012930628651520238\n",
      "Training Loss: 0.001273388533736579\n",
      "Training Loss: 0.0012104453262872994\n",
      "Validation Loss: 0.00202059060605923\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006359995026141405\n",
      "Training Loss: 0.006439103198936209\n",
      "Training Loss: 0.006483309987233951\n",
      "Training Loss: 0.004311316220700974\n",
      "Training Loss: 0.0012883708972367459\n",
      "Training Loss: 0.0012686293550359551\n",
      "Training Loss: 0.0012047187109419611\n",
      "Validation Loss: 0.0020153215889686417\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006354097516741604\n",
      "Training Loss: 0.006431524294894189\n",
      "Training Loss: 0.006476312975864857\n",
      "Training Loss: 0.0043063182511832564\n",
      "Training Loss: 0.001283796265051933\n",
      "Training Loss: 0.0012639904533716618\n",
      "Training Loss: 0.00119909733235545\n",
      "Validation Loss: 0.002010090841943562\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.006347987345652655\n",
      "Training Loss: 0.006423746302025393\n",
      "Training Loss: 0.006469061261741444\n",
      "Training Loss: 0.004301243353838799\n",
      "Training Loss: 0.0012793234029959421\n",
      "Training Loss: 0.0012594585006445414\n",
      "Training Loss: 0.0011935668130900012\n",
      "Validation Loss: 0.002004885037064688\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0063416475732810795\n",
      "Training Loss: 0.006415750371525064\n",
      "Training Loss: 0.006461533184628934\n",
      "Training Loss: 0.0042960764687450135\n",
      "Training Loss: 0.0012749389333475847\n",
      "Training Loss: 0.001255024153069826\n",
      "Training Loss: 0.0011881197548063937\n",
      "Validation Loss: 0.0019996941634863683\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006335051242494956\n",
      "Training Loss: 0.006407509712735191\n",
      "Training Loss: 0.006453703469596803\n",
      "Training Loss: 0.004290801056631608\n",
      "Training Loss: 0.0012706302106380463\n",
      "Training Loss: 0.0012506756793300156\n",
      "Training Loss: 0.0011827437045576516\n",
      "Validation Loss: 0.001994509898221612\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006328177608083934\n",
      "Training Loss: 0.006399002600228414\n",
      "Training Loss: 0.00644554543774575\n",
      "Training Loss: 0.004285401623928919\n",
      "Training Loss: 0.0012663845556380694\n",
      "Training Loss: 0.0012464010458643315\n",
      "Training Loss: 0.0011774258186051157\n",
      "Validation Loss: 0.0019893170871675076\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006321003099437803\n",
      "Training Loss: 0.006390203982591629\n",
      "Training Loss: 0.006437033036490902\n",
      "Training Loss: 0.004279858891677577\n",
      "Training Loss: 0.0012621899652003777\n",
      "Training Loss: 0.0012421897957392502\n",
      "Training Loss: 0.0011721574421972036\n",
      "Validation Loss: 0.0019841074180193697\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006313501391559839\n",
      "Training Loss: 0.006381087809568271\n",
      "Training Loss: 0.006428136654431\n",
      "Training Loss: 0.004274155791936209\n",
      "Training Loss: 0.0012580355128739029\n",
      "Training Loss: 0.001238032615146949\n",
      "Training Loss: 0.001166926506048185\n",
      "Validation Loss: 0.001978869733545835\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006305648434208706\n",
      "Training Loss: 0.006371629404602572\n",
      "Training Loss: 0.0064188285323325545\n",
      "Training Loss: 0.0042682735275593586\n",
      "Training Loss: 0.001253910023951903\n",
      "Training Loss: 0.0012339179409173084\n",
      "Training Loss: 0.0011617239022598369\n",
      "Validation Loss: 0.0019735929493841188\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006297416629968211\n",
      "Training Loss: 0.0063618013367522505\n",
      "Training Loss: 0.006409074069233611\n",
      "Training Loss: 0.004262193355971249\n",
      "Training Loss: 0.0012498090181907173\n",
      "Training Loss: 0.0012298418015416245\n",
      "Training Loss: 0.00115654438144702\n",
      "Validation Loss: 0.001968272793593325\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006288776331348345\n",
      "Training Loss: 0.006351570825790987\n",
      "Training Loss: 0.006398836978478357\n",
      "Training Loss: 0.004255894508096389\n",
      "Training Loss: 0.0012457246458507142\n",
      "Training Loss: 0.001225792819459457\n",
      "Training Loss: 0.0011513783047848846\n",
      "Validation Loss: 0.001962899104303991\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006279700888553634\n",
      "Training Loss: 0.006340912550804205\n",
      "Training Loss: 0.0063880840258207176\n",
      "Training Loss: 0.004249357376247645\n",
      "Training Loss: 0.0012416536849923431\n",
      "Training Loss: 0.0012217691429395926\n",
      "Training Loss: 0.001146224919211818\n",
      "Validation Loss: 0.0019574690923445303\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006270157732069492\n",
      "Training Loss: 0.0063297907239757475\n",
      "Training Loss: 0.006376773467054591\n",
      "Training Loss: 0.004242561896826373\n",
      "Training Loss: 0.0012375982268713415\n",
      "Training Loss: 0.0012177679057640489\n",
      "Training Loss: 0.0011410829189844663\n",
      "Validation Loss: 0.0019519759830081318\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.006260116552002728\n",
      "Training Loss: 0.006318177500506863\n",
      "Training Loss: 0.006364867432275787\n",
      "Training Loss: 0.004235488313570386\n",
      "Training Loss: 0.0012335611866728868\n",
      "Training Loss: 0.0012137887330754894\n",
      "Training Loss: 0.0011359541743149748\n",
      "Validation Loss: 0.0019464202313827811\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0062495476554613565\n",
      "Training Loss: 0.006306039114715532\n",
      "Training Loss: 0.006352322674356401\n",
      "Training Loss: 0.004228115682344651\n",
      "Training Loss: 0.0012295494751015212\n",
      "Training Loss: 0.0012098342561512254\n",
      "Training Loss: 0.0011308440628636162\n",
      "Validation Loss: 0.0019408068625663656\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0062384195590857415\n",
      "Training Loss: 0.006293342426070012\n",
      "Training Loss: 0.0063390974939102306\n",
      "Training Loss: 0.004220425973617239\n",
      "Training Loss: 0.0012255760938569438\n",
      "Training Loss: 0.0012059112033603015\n",
      "Training Loss: 0.0011257633233617526\n",
      "Validation Loss: 0.0019351357740959613\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006226704983273521\n",
      "Training Loss: 0.006280061271390878\n",
      "Training Loss: 0.006325155139202252\n",
      "Training Loss: 0.004212403539713705\n",
      "Training Loss: 0.0012216538279608358\n",
      "Training Loss: 0.0012020279857097194\n",
      "Training Loss: 0.0011207249220024096\n",
      "Validation Loss: 0.0019294196295169138\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006214376722928136\n",
      "Training Loss: 0.006266164016560652\n",
      "Training Loss: 0.006310452509205788\n",
      "Training Loss: 0.004204031346453121\n",
      "Training Loss: 0.0012178093264810742\n",
      "Training Loss: 0.0011982012039516122\n",
      "Training Loss: 0.001115750554818078\n",
      "Validation Loss: 0.0019236705819675036\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006201408518245443\n",
      "Training Loss: 0.00625162260606885\n",
      "Training Loss: 0.006294954925542697\n",
      "Training Loss: 0.004195299237180734\n",
      "Training Loss: 0.0012140671139059124\n",
      "Training Loss: 0.0011944499086530413\n",
      "Training Loss: 0.0011108645381318638\n",
      "Validation Loss: 0.0019179090721469917\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006187782495981082\n",
      "Training Loss: 0.0062364183436147864\n",
      "Training Loss: 0.006278634738991968\n",
      "Training Loss: 0.004186197241215268\n",
      "Training Loss: 0.0012104561620799358\n",
      "Training Loss: 0.0011907965165300993\n",
      "Training Loss: 0.0011060935931891435\n",
      "Validation Loss: 0.0019121534439163067\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006173486585030332\n",
      "Training Loss: 0.006220536279142834\n",
      "Training Loss: 0.00626146950875409\n",
      "Training Loss: 0.004176723426935496\n",
      "Training Loss: 0.0012070138710259925\n",
      "Training Loss: 0.0011872668433352373\n",
      "Training Loss: 0.001101471014553681\n",
      "Validation Loss: 0.0019064319178537022\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006158517637522891\n",
      "Training Loss: 0.0062039715482387695\n",
      "Training Loss: 0.006243452551425435\n",
      "Training Loss: 0.004166878887772328\n",
      "Training Loss: 0.0012037731024611275\n",
      "Training Loss: 0.0011838896667177323\n",
      "Training Loss: 0.0010970305139926496\n",
      "Validation Loss: 0.0019007689666289886\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006142878582468256\n",
      "Training Loss: 0.006186724546132609\n",
      "Training Loss: 0.006224583468283526\n",
      "Training Loss: 0.004156671706005\n",
      "Training Loss: 0.0012007750078919343\n",
      "Training Loss: 0.0011806996660743607\n",
      "Training Loss: 0.0010928094374685316\n",
      "Validation Loss: 0.0018952019741123871\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006126582500291989\n",
      "Training Loss: 0.006168808611109852\n",
      "Training Loss: 0.006204881953308359\n",
      "Training Loss: 0.004146114643663168\n",
      "Training Loss: 0.001198051733954344\n",
      "Training Loss: 0.00117772800847888\n",
      "Training Loss: 0.0010888431243802187\n",
      "Validation Loss: 0.0018897593137590137\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0061096572934184225\n",
      "Training Loss: 0.006150249837082811\n",
      "Training Loss: 0.006184381132479757\n",
      "Training Loss: 0.004135228893283056\n",
      "Training Loss: 0.0011956416217435617\n",
      "Training Loss: 0.0011750087591644843\n",
      "Training Loss: 0.001085166932462016\n",
      "Validation Loss: 0.0018844780886470784\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006092138112289831\n",
      "Training Loss: 0.006131086380337365\n",
      "Training Loss: 0.0061631331505486745\n",
      "Training Loss: 0.004124041533796117\n",
      "Training Loss: 0.0011935681346221826\n",
      "Training Loss: 0.0011725684021803317\n",
      "Training Loss: 0.0010818057612050325\n",
      "Validation Loss: 0.0018793833846978893\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.0060740816139150415\n",
      "Training Loss: 0.00611137296713423\n",
      "Training Loss: 0.006141208584303968\n",
      "Training Loss: 0.004112585316761397\n",
      "Training Loss: 0.0011918547471577766\n",
      "Training Loss: 0.0011704325097525725\n",
      "Training Loss: 0.0010787829214677913\n",
      "Validation Loss: 0.0018745046340592714\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0060555482748895885\n",
      "Training Loss: 0.006091173632303253\n",
      "Training Loss: 0.006118692763266154\n",
      "Training Loss: 0.0041008986921224275\n",
      "Training Loss: 0.0011905167360964698\n",
      "Training Loss: 0.0011686229058977915\n",
      "Training Loss: 0.0010761163332063007\n",
      "Validation Loss: 0.001869865326792497\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006036607037531212\n",
      "Training Loss: 0.006070560930529609\n",
      "Training Loss: 0.006095681237056851\n",
      "Training Loss: 0.0040890284330816935\n",
      "Training Loss: 0.0011895627087506\n",
      "Training Loss: 0.0011671565367578295\n",
      "Training Loss: 0.0010738147173833567\n",
      "Validation Loss: 0.0018654857181825359\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006017343637067824\n",
      "Training Loss: 0.006049626852036454\n",
      "Training Loss: 0.0060722895769868045\n",
      "Training Loss: 0.004077019685501\n",
      "Training Loss: 0.0011889890288875903\n",
      "Training Loss: 0.001166038162264158\n",
      "Training Loss: 0.0010718756599817426\n",
      "Validation Loss: 0.0018613725115936633\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.005997844389639795\n",
      "Training Loss: 0.006028465438284911\n",
      "Training Loss: 0.00604863501386717\n",
      "Training Loss: 0.004064923897531116\n",
      "Training Loss: 0.0011887877453409601\n",
      "Training Loss: 0.0011652705772576156\n",
      "Training Loss: 0.0010702925943041918\n",
      "Validation Loss: 0.0018575378279537416\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.005978205656865612\n",
      "Training Loss: 0.006007173738908023\n",
      "Training Loss: 0.006024840638274327\n",
      "Training Loss: 0.004052793766313698\n",
      "Training Loss: 0.0011889461596729233\n",
      "Training Loss: 0.0011648501080344432\n",
      "Training Loss: 0.0010690508109109942\n",
      "Validation Loss: 0.0018539791347390068\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.005958519655978307\n",
      "Training Loss: 0.005985856525367126\n",
      "Training Loss: 0.006001032012864016\n",
      "Training Loss: 0.004040680703765247\n",
      "Training Loss: 0.0011894438874151092\n",
      "Training Loss: 0.0011647660744347377\n",
      "Training Loss: 0.0010681309090432478\n",
      "Validation Loss: 0.0018506916794683505\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.0059388826100621375\n",
      "Training Loss: 0.005964616189012304\n",
      "Training Loss: 0.005977333323680795\n",
      "Training Loss: 0.004028639037132961\n",
      "Training Loss: 0.0011902569438097998\n",
      "Training Loss: 0.0011649981360824313\n",
      "Training Loss: 0.0010675033161533065\n",
      "Validation Loss: 0.0018476669437122054\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.00591938852914609\n",
      "Training Loss: 0.005943555357516743\n",
      "Training Loss: 0.005953862013993785\n",
      "Training Loss: 0.004016714932222385\n",
      "Training Loss: 0.0011913581979752052\n",
      "Training Loss: 0.0011655290314956802\n",
      "Training Loss: 0.001067141888925107\n",
      "Validation Loss: 0.0018448929983881836\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.005900120296282694\n",
      "Training Loss: 0.005922764107817784\n",
      "Training Loss: 0.005930724595091305\n",
      "Training Loss: 0.004004954421689035\n",
      "Training Loss: 0.0011927188609843142\n",
      "Training Loss: 0.0011663323504762957\n",
      "Training Loss: 0.0010670115117682145\n",
      "Validation Loss: 0.0018423521689144538\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.0058811606245581065\n",
      "Training Loss: 0.005902332622790709\n",
      "Training Loss: 0.005908021935028956\n",
      "Training Loss: 0.003993397963204188\n",
      "Training Loss: 0.0011943075103044976\n",
      "Training Loss: 0.001167380607657833\n",
      "Training Loss: 0.0010670785587717545\n",
      "Validation Loss: 0.001840026736562925\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.005862580032553524\n",
      "Training Loss: 0.005882336114300415\n",
      "Training Loss: 0.005885838076937944\n",
      "Training Loss: 0.003982082138390979\n",
      "Training Loss: 0.0011960923485457896\n",
      "Training Loss: 0.0011686402359919158\n",
      "Training Loss: 0.0010673076182138176\n",
      "Validation Loss: 0.0018379001131857665\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.005844441562658176\n",
      "Training Loss: 0.005862842524074949\n",
      "Training Loss: 0.005864246886922046\n",
      "Training Loss: 0.003971036069560796\n",
      "Training Loss: 0.0011980397473962511\n",
      "Training Loss: 0.0011700786073924974\n",
      "Training Loss: 0.0010676619224250317\n",
      "Validation Loss: 0.0018359473680820985\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.005826794982422143\n",
      "Training Loss: 0.005843909155228175\n",
      "Training Loss: 0.005843306600581855\n",
      "Training Loss: 0.003960280452156439\n",
      "Training Loss: 0.0012001155494363047\n",
      "Training Loss: 0.0011716622204403393\n",
      "Training Loss: 0.001068106827660813\n",
      "Validation Loss: 0.0018341476102063103\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.005809679622761905\n",
      "Training Loss: 0.005825577218201943\n",
      "Training Loss: 0.005823059925460256\n",
      "Training Loss: 0.003949833774240688\n",
      "Training Loss: 0.001202285950712394\n",
      "Training Loss: 0.001173355692299083\n",
      "Training Loss: 0.0010686084286135155\n",
      "Validation Loss: 0.0018324846991953484\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0057931233383715156\n",
      "Training Loss: 0.005807876628823578\n",
      "Training Loss: 0.005803536525927484\n",
      "Training Loss: 0.003939704926306149\n",
      "Training Loss: 0.001204518721642671\n",
      "Training Loss: 0.0011751235649717273\n",
      "Training Loss: 0.0010691327493987045\n",
      "Validation Loss: 0.0018309335318509113\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.005777144356397912\n",
      "Training Loss: 0.005790827967575751\n",
      "Training Loss: 0.0057847517286427315\n",
      "Training Loss: 0.003929897526686546\n",
      "Training Loss: 0.0012067797478812281\n",
      "Training Loss: 0.001176931816880824\n",
      "Training Loss: 0.001069649841520004\n",
      "Validation Loss: 0.00182947583020445\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.005761751112295314\n",
      "Training Loss: 0.0057744411879684775\n",
      "Training Loss: 0.0057667102589039135\n",
      "Training Loss: 0.00392041093451553\n",
      "Training Loss: 0.001209037051157793\n",
      "Training Loss: 0.0011787445562367793\n",
      "Training Loss: 0.0010701288233394735\n",
      "Validation Loss: 0.0018280840056777447\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.005746941000688821\n",
      "Training Loss: 0.005758714567054995\n",
      "Training Loss: 0.005749406602699309\n",
      "Training Loss: 0.003911237623979105\n",
      "Training Loss: 0.001211262899741996\n",
      "Training Loss: 0.001180539080669405\n",
      "Training Loss: 0.001070549087235122\n",
      "Validation Loss: 0.0018267460690186907\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.005732704593101517\n",
      "Training Loss: 0.00574363833933603\n",
      "Training Loss: 0.0057328245829558\n",
      "Training Loss: 0.003902368988347007\n",
      "Training Loss: 0.0012134292369592003\n",
      "Training Loss: 0.0011822823494003387\n",
      "Training Loss: 0.0010708853677351725\n",
      "Validation Loss: 0.0018254409829540047\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.0057190279848873616\n",
      "Training Loss: 0.005729193688021042\n",
      "Training Loss: 0.005716939258854836\n",
      "Training Loss: 0.0038937920000171287\n",
      "Training Loss: 0.0012155116449866909\n",
      "Training Loss: 0.0011839510160643841\n",
      "Training Loss: 0.0010711208364955383\n",
      "Validation Loss: 0.0018241509559960192\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.005705888647353277\n",
      "Training Loss: 0.005715361449983902\n",
      "Training Loss: 0.005701723703532479\n",
      "Training Loss: 0.0038854904907930175\n",
      "Training Loss: 0.0012174862666870468\n",
      "Training Loss: 0.001185522450323333\n",
      "Training Loss: 0.0010712352121481671\n",
      "Validation Loss: 0.0018228590445922624\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.00569326586322859\n",
      "Training Loss: 0.0057021151663502675\n",
      "Training Loss: 0.005687147096614354\n",
      "Training Loss: 0.0038774503252352586\n",
      "Training Loss: 0.0012193362954712938\n",
      "Training Loss: 0.0011869782251596917\n",
      "Training Loss: 0.0010712190183403436\n",
      "Validation Loss: 0.0018215483300429716\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.00568112819455564\n",
      "Training Loss: 0.005689423191943206\n",
      "Training Loss: 0.005673171114176511\n",
      "Training Loss: 0.0038696527469437567\n",
      "Training Loss: 0.0012210471312573646\n",
      "Training Loss: 0.0011883027941803448\n",
      "Training Loss: 0.001071061469556298\n",
      "Validation Loss: 0.0018202128297124364\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.005669450700515881\n",
      "Training Loss: 0.005677253963076509\n",
      "Training Loss: 0.00565975817036815\n",
      "Training Loss: 0.003862079183163587\n",
      "Training Loss: 0.0012226045898569282\n",
      "Training Loss: 0.001189485283830436\n",
      "Training Loss: 0.0010707545760669747\n",
      "Validation Loss: 0.0018188376167147292\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.005658204073552042\n",
      "Training Loss: 0.005665573128499091\n",
      "Training Loss: 0.005646868926123716\n",
      "Training Loss: 0.003854712450702209\n",
      "Training Loss: 0.001223999592330074\n",
      "Training Loss: 0.0011905146784556564\n",
      "Training Loss: 0.0010702943585056345\n",
      "Validation Loss: 0.0018174158153516564\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0056473588082008065\n",
      "Training Loss: 0.005654347866075113\n",
      "Training Loss: 0.0056344682065537196\n",
      "Training Loss: 0.003847536231187405\n",
      "Training Loss: 0.0012252262882248032\n",
      "Training Loss: 0.001191386802820489\n",
      "Training Loss: 0.0010696786796324886\n",
      "Validation Loss: 0.001815937989441964\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.005636888981098309\n",
      "Training Loss: 0.005643546213395894\n",
      "Training Loss: 0.005622516213334166\n",
      "Training Loss: 0.0038405336209689266\n",
      "Training Loss: 0.0012262777313299012\n",
      "Training Loss: 0.001192096132726874\n",
      "Training Loss: 0.0010689051884401125\n",
      "Validation Loss: 0.0018143998319457534\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.005626764944754541\n",
      "Training Loss: 0.005633131787762977\n",
      "Training Loss: 0.005610976930474862\n",
      "Training Loss: 0.00383369004848646\n",
      "Training Loss: 0.0012271555842016824\n",
      "Training Loss: 0.0011926403726101853\n",
      "Training Loss: 0.001067975680634845\n",
      "Validation Loss: 0.0018127932136290626\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.005616962137864902\n",
      "Training Loss: 0.005623077116324566\n",
      "Training Loss: 0.005599815476452932\n",
      "Training Loss: 0.0038269884146575352\n",
      "Training Loss: 0.0012278553066425958\n",
      "Training Loss: 0.001193016088946024\n",
      "Training Loss: 0.0010668911032553296\n",
      "Validation Loss: 0.0018111197984243712\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.0056074577732943\n",
      "Training Loss: 0.00561335027392488\n",
      "Training Loss: 0.005588999823085032\n",
      "Training Loss: 0.0038204161294561347\n",
      "Training Loss: 0.0012283792158996221\n",
      "Training Loss: 0.001193227833864512\n",
      "Training Loss: 0.0010656549091800115\n",
      "Validation Loss: 0.001809374236350021\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.00559822624665685\n",
      "Training Loss: 0.005603921635192819\n",
      "Training Loss: 0.0055784964293707166\n",
      "Training Loss: 0.0038139614219835495\n",
      "Training Loss: 0.0012287277590075974\n",
      "Training Loss: 0.0011932726671511774\n",
      "Training Loss: 0.0010642681262106634\n",
      "Validation Loss: 0.0018075538078874127\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0055892499606125055\n",
      "Training Loss: 0.00559476570924744\n",
      "Training Loss: 0.005568277513957582\n",
      "Training Loss: 0.0038076118257595226\n",
      "Training Loss: 0.0012289032664557454\n",
      "Training Loss: 0.001193152904015733\n",
      "Training Loss: 0.0010627341848157812\n",
      "Validation Loss: 0.001805657150279661\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.005580507189151831\n",
      "Training Loss: 0.005585857126861811\n",
      "Training Loss: 0.005558315374655649\n",
      "Training Loss: 0.003801356864423724\n",
      "Training Loss: 0.0012289106294338125\n",
      "Training Loss: 0.001192873180116294\n",
      "Training Loss: 0.0010610585509857628\n",
      "Validation Loss: 0.0018036911482722166\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.005571981036337092\n",
      "Training Loss: 0.005577172066550702\n",
      "Training Loss: 0.005548582831979729\n",
      "Training Loss: 0.0037951850386161823\n",
      "Training Loss: 0.0012287529149034525\n",
      "Training Loss: 0.0011924349782930221\n",
      "Training Loss: 0.0010592430813994725\n",
      "Validation Loss: 0.0018016477121992477\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.005563652904820629\n",
      "Training Loss: 0.00556868651707191\n",
      "Training Loss: 0.005539053974789567\n",
      "Training Loss: 0.0037890881301427728\n",
      "Training Loss: 0.0012284316976729315\n",
      "Training Loss: 0.001191837556398241\n",
      "Training Loss: 0.0010572880624386016\n",
      "Validation Loss: 0.0017995309745866721\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.005555509133846499\n",
      "Training Loss: 0.005560379776288755\n",
      "Training Loss: 0.0055297084979247305\n",
      "Training Loss: 0.003783056873799069\n",
      "Training Loss: 0.0012279486159968655\n",
      "Training Loss: 0.0011910818204341922\n",
      "Training Loss: 0.001055195278604515\n",
      "Validation Loss: 0.0017973390076054811\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.005547540155239403\n",
      "Training Loss: 0.005552239719545469\n",
      "Training Loss: 0.005520525448955596\n",
      "Training Loss: 0.0037770833502872845\n",
      "Training Loss: 0.0012273042983724737\n",
      "Training Loss: 0.0011901674440014177\n",
      "Training Loss: 0.0010529649832460564\n",
      "Validation Loss: 0.0017950723429728467\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.005539729698211886\n",
      "Training Loss: 0.005544240429881029\n",
      "Training Loss: 0.005511482473812066\n",
      "Training Loss: 0.003771157750597922\n",
      "Training Loss: 0.0012265011304407381\n",
      "Training Loss: 0.001189097260212293\n",
      "Training Loss: 0.0010505958869180176\n",
      "Validation Loss: 0.0017927288701705467\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.005532070267945528\n",
      "Training Loss: 0.005536370670306497\n",
      "Training Loss: 0.005502558076987043\n",
      "Training Loss: 0.003765274158358807\n",
      "Training Loss: 0.0012255415313120466\n",
      "Training Loss: 0.0011878689398872667\n",
      "Training Loss: 0.001048088303359691\n",
      "Validation Loss: 0.0017903105197526682\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.005524548572138883\n",
      "Training Loss: 0.0055286103050457315\n",
      "Training Loss: 0.005493736215867102\n",
      "Training Loss: 0.0037594245026411955\n",
      "Training Loss: 0.0012244206060131545\n",
      "Training Loss: 0.0011864765487553087\n",
      "Training Loss: 0.0010454353776731295\n",
      "Validation Loss: 0.0017878125084447238\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.005517160259769298\n",
      "Training Loss: 0.005520948411431164\n",
      "Training Loss: 0.005484997780877165\n",
      "Training Loss: 0.0037536013759381604\n",
      "Training Loss: 0.0012231366710329895\n",
      "Training Loss: 0.0011849183595040813\n",
      "Training Loss: 0.0010426298547099578\n",
      "Validation Loss: 0.0017852321219602634\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.005509899250464514\n",
      "Training Loss: 0.005513371652341448\n",
      "Training Loss: 0.0054763254045974465\n",
      "Training Loss: 0.0037477964769641403\n",
      "Training Loss: 0.0012216829613316805\n",
      "Training Loss: 0.0011831851184251718\n",
      "Training Loss: 0.0010396642212435836\n",
      "Validation Loss: 0.0017825653877899343\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.005502757536596618\n",
      "Training Loss: 0.005505865321028977\n",
      "Training Loss: 0.005467699135188013\n",
      "Training Loss: 0.0037420017702970653\n",
      "Training Loss: 0.0012200541810307186\n",
      "Training Loss: 0.0011812718926375964\n",
      "Training Loss: 0.0010365273420757147\n",
      "Validation Loss: 0.0017798058251538122\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.005495732553536073\n",
      "Training Loss: 0.0054984166240319605\n",
      "Training Loss: 0.0054591002117376775\n",
      "Training Loss: 0.003736208302725572\n",
      "Training Loss: 0.001218243728944799\n",
      "Training Loss: 0.0011791660839662655\n",
      "Training Loss: 0.0010332039855711628\n",
      "Validation Loss: 0.001776949959250746\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.00548881919006817\n",
      "Training Loss: 0.005491011630510911\n",
      "Training Loss: 0.00545051074528601\n",
      "Training Loss: 0.003730406170361675\n",
      "Training Loss: 0.0012162355294276495\n",
      "Training Loss: 0.0011768549284897744\n",
      "Training Loss: 0.0010296740639023482\n",
      "Validation Loss: 0.0017739863975116572\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.00548201781057287\n",
      "Training Loss: 0.005483641212922521\n",
      "Training Loss: 0.00544191129913088\n",
      "Training Loss: 0.0037245858406822663\n",
      "Training Loss: 0.0012140152954088989\n",
      "Training Loss: 0.0011743232617300236\n",
      "Training Loss: 0.0010259188811323839\n",
      "Validation Loss: 0.0017709010815874008\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0054753254854585974\n",
      "Training Loss: 0.0054762906284304335\n",
      "Training Loss: 0.005433278889977373\n",
      "Training Loss: 0.0037187342181277928\n",
      "Training Loss: 0.0012115701416041702\n",
      "Training Loss: 0.0011715534980612575\n",
      "Training Loss: 0.0010219117312226444\n",
      "Validation Loss: 0.0017676849699969847\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.005468742588418536\n",
      "Training Loss: 0.005468945268075913\n",
      "Training Loss: 0.00542459123651497\n",
      "Training Loss: 0.0037128414581820836\n",
      "Training Loss: 0.001208873821015004\n",
      "Training Loss: 0.001168520910650841\n",
      "Training Loss: 0.0010176198834960814\n",
      "Validation Loss: 0.0017643189093678813\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.005462272676522844\n",
      "Training Loss: 0.00546159470512066\n",
      "Training Loss: 0.005415827401448041\n",
      "Training Loss: 0.0037068918294244214\n",
      "Training Loss: 0.0012059015571139753\n",
      "Training Loss: 0.0011652018048334866\n",
      "Training Loss: 0.001013007487708819\n",
      "Validation Loss: 0.0017607853304697654\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.005455916161299684\n",
      "Training Loss: 0.0054542252264218405\n",
      "Training Loss: 0.00540696109412238\n",
      "Training Loss: 0.0037008702794264535\n",
      "Training Loss: 0.0012026235485973302\n",
      "Training Loss: 0.0011615688582242\n",
      "Training Loss: 0.0010080357519473182\n",
      "Validation Loss: 0.0017570643794761224\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.005449679502053186\n",
      "Training Loss: 0.00544682159088552\n",
      "Training Loss: 0.005397965742158704\n",
      "Training Loss: 0.00369476312866027\n",
      "Training Loss: 0.0011990086836158298\n",
      "Training Loss: 0.0011575884369085542\n",
      "Training Loss: 0.0010026569639740046\n",
      "Validation Loss: 0.0017531267919949141\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00544356565515045\n",
      "Training Loss: 0.005439370307140052\n",
      "Training Loss: 0.0053888153308071195\n",
      "Training Loss: 0.0036885506847465876\n",
      "Training Loss: 0.0011950114530918654\n",
      "Training Loss: 0.001153220493288245\n",
      "Training Loss: 0.0009968161778670037\n",
      "Validation Loss: 0.0017489434950435638\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.005437582359882072\n",
      "Training Loss: 0.005431855412898585\n",
      "Training Loss: 0.005379480274859816\n",
      "Training Loss: 0.0036822138954448747\n",
      "Training Loss: 0.0011905925495375414\n",
      "Training Loss: 0.0011484298561845207\n",
      "Training Loss: 0.00099045967523125\n",
      "Validation Loss: 0.0017444877327186116\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.0054317409748909995\n",
      "Training Loss: 0.005424266543705016\n",
      "Training Loss: 0.005369935589260422\n",
      "Training Loss: 0.0036757361765921816\n",
      "Training Loss: 0.0011857026819780004\n",
      "Training Loss: 0.0011431728512980043\n",
      "Training Loss: 0.0009835263749118895\n",
      "Validation Loss: 0.0017397222742447037\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005426049636444077\n",
      "Training Loss: 0.005416588158113882\n",
      "Training Loss: 0.005360154613736085\n",
      "Training Loss: 0.003669097039528424\n",
      "Training Loss: 0.0011802925592928658\n",
      "Training Loss: 0.0011374069934390718\n",
      "Training Loss: 0.0009759567503351719\n",
      "Validation Loss: 0.0017346151526066848\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.005420521719497628\n",
      "Training Loss: 0.005408812172245234\n",
      "Training Loss: 0.005350115465116687\n",
      "Training Loss: 0.003662281720608007\n",
      "Training Loss: 0.001174318304983899\n",
      "Training Loss: 0.0011310956416855334\n",
      "Training Loss: 0.00096769449475687\n",
      "Validation Loss: 0.0017291462566216967\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.005415168779436499\n",
      "Training Loss: 0.005400927055743523\n",
      "Training Loss: 0.005339798383647576\n",
      "Training Loss: 0.003655273679032689\n",
      "Training Loss: 0.0011677335717831738\n",
      "Training Loss: 0.0011242054973263293\n",
      "Training Loss: 0.0009586931158992229\n",
      "Validation Loss: 0.0017232852686006465\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005410002407734283\n",
      "Training Loss: 0.0053929301438620314\n",
      "Training Loss: 0.005329194392543286\n",
      "Training Loss: 0.003648063479267876\n",
      "Training Loss: 0.001160507002932718\n",
      "Training Loss: 0.0011167172107525403\n",
      "Training Loss: 0.0009489208913146286\n",
      "Validation Loss: 0.0017170289222653327\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.005405037328600883\n",
      "Training Loss: 0.005384821906336584\n",
      "Training Loss: 0.005318303871899843\n",
      "Training Loss: 0.0036406443138548637\n",
      "Training Loss: 0.0011526162603695412\n",
      "Training Loss: 0.0011086213736416538\n",
      "Training Loss: 0.000938366570553626\n",
      "Validation Loss: 0.0017103833323632357\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005400283195194788\n",
      "Training Loss: 0.005376609717495739\n",
      "Training Loss: 0.0053071358246961605\n",
      "Training Loss: 0.003633019562548725\n",
      "Training Loss: 0.0011440728139132262\n",
      "Training Loss: 0.0010999414471007185\n",
      "Training Loss: 0.0009270540304714814\n",
      "Validation Loss: 0.0017033792013644278\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.005395744369598106\n",
      "Training Loss: 0.005368303522700444\n",
      "Training Loss: 0.005295712283696048\n",
      "Training Loss: 0.0036251994605117945\n",
      "Training Loss: 0.0011349034459271934\n",
      "Training Loss: 0.0010907143080112292\n",
      "Training Loss: 0.0009150354936718941\n",
      "Validation Loss: 0.0016960705966646836\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.0053914177592378106\n",
      "Training Loss: 0.005359920343034901\n",
      "Training Loss: 0.005284064087318257\n",
      "Training Loss: 0.0036171986423141787\n",
      "Training Loss: 0.0011251659311528783\n",
      "Training Loss: 0.0010810095960914624\n",
      "Training Loss: 0.0009024013516318518\n",
      "Validation Loss: 0.0016885319548529966\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005387291787192225\n",
      "Training Loss: 0.005351473301998339\n",
      "Training Loss: 0.005272229426191188\n",
      "Training Loss: 0.003609043753167498\n",
      "Training Loss: 0.001114949770271778\n",
      "Training Loss: 0.0010709211123321439\n",
      "Training Loss: 0.0008892735463450663\n",
      "Validation Loss: 0.0016808693738887958\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005383344058645889\n",
      "Training Loss: 0.005342980296700261\n",
      "Training Loss: 0.005260248440899886\n",
      "Training Loss: 0.0036007606824568937\n",
      "Training Loss: 0.0011043569036701228\n",
      "Training Loss: 0.0010605570884217742\n",
      "Training Loss: 0.0008757926870021038\n",
      "Validation Loss: 0.0016731838124528638\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005379541901638731\n",
      "Training Loss: 0.00533445363980718\n",
      "Training Loss: 0.005248162471107207\n",
      "Training Loss: 0.0035923820366588186\n",
      "Training Loss: 0.00109351122751832\n",
      "Training Loss: 0.0010500348935602232\n",
      "Training Loss: 0.0008621104741905583\n",
      "Validation Loss: 0.0016655899683050183\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005375844608643092\n",
      "Training Loss: 0.00532589835813269\n",
      "Training Loss: 0.0052360053226584565\n",
      "Training Loss: 0.0035839391489571426\n",
      "Training Loss: 0.0010825334451510572\n",
      "Training Loss: 0.0010394734956207686\n",
      "Training Loss: 0.0008483733292814577\n",
      "Validation Loss: 0.0016581804202939296\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005372206101892516\n",
      "Training Loss: 0.005317319259629585\n",
      "Training Loss: 0.005223808042937889\n",
      "Training Loss: 0.003575460551510332\n",
      "Training Loss: 0.0010715387607342563\n",
      "Training Loss: 0.0010289804318745154\n",
      "Training Loss: 0.0008347164685255848\n",
      "Validation Loss: 0.0016510409301047455\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005368576089967974\n",
      "Training Loss: 0.00530870836111717\n",
      "Training Loss: 0.005211591181578114\n",
      "Training Loss: 0.003566970246320125\n",
      "Training Loss: 0.0010606359243683983\n",
      "Training Loss: 0.0010186546081240521\n",
      "Training Loss: 0.0008212534341146238\n",
      "Validation Loss: 0.001644233294870802\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.0053649126848904416\n",
      "Training Loss: 0.005300060277804732\n",
      "Training Loss: 0.005199376699165441\n",
      "Training Loss: 0.0035584938658575994\n",
      "Training Loss: 0.001049903623497812\n",
      "Training Loss: 0.001008564084986574\n",
      "Training Loss: 0.0008080700951541075\n",
      "Validation Loss: 0.0016377878139686704\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005361174246645533\n",
      "Training Loss: 0.005291364695876837\n",
      "Training Loss: 0.0051871726376703006\n",
      "Training Loss: 0.0035500471427076263\n",
      "Training Loss: 0.0010394049377646297\n",
      "Training Loss: 0.0009987667459790829\n",
      "Training Loss: 0.0007952304755599471\n",
      "Validation Loss: 0.0016317310233335343\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005357338764006272\n",
      "Training Loss: 0.00528261873696465\n",
      "Training Loss: 0.005174995695706457\n",
      "Training Loss: 0.003541646349840448\n",
      "Training Loss: 0.0010291902336030034\n",
      "Training Loss: 0.0009892993316316279\n",
      "Training Loss: 0.0007827771824668162\n",
      "Validation Loss: 0.0016260557164427605\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005353375537088141\n",
      "Training Loss: 0.005273806211771443\n",
      "Training Loss: 0.005162850102642551\n",
      "Training Loss: 0.0035333058395190164\n",
      "Training Loss: 0.0010192903231654783\n",
      "Training Loss: 0.0009801813872763888\n",
      "Training Loss: 0.0007707347351970384\n",
      "Validation Loss: 0.0016207611835225963\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.0053492765664123\n",
      "Training Loss: 0.005264927410753444\n",
      "Training Loss: 0.0051507499068975445\n",
      "Training Loss: 0.0035250372412701835\n",
      "Training Loss: 0.0010097170664812437\n",
      "Training Loss: 0.0009714182456082199\n",
      "Training Loss: 0.0007591119298740522\n",
      "Validation Loss: 0.0016158327651262451\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005345037814695388\n",
      "Training Loss: 0.005255979415378534\n",
      "Training Loss: 0.005138703648117371\n",
      "Training Loss: 0.003516851448075613\n",
      "Training Loss: 0.001000478127389215\n",
      "Training Loss: 0.0009630096761247841\n",
      "Training Loss: 0.0007479088137915823\n",
      "Validation Loss: 0.0016112494667641442\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005340660506626591\n",
      "Training Loss: 0.0052469654660671945\n",
      "Training Loss: 0.005126726311864331\n",
      "Training Loss: 0.003508757531744777\n",
      "Training Loss: 0.0009915680420817807\n",
      "Training Loss: 0.0009549461060669273\n",
      "Training Loss: 0.0007371193140716059\n",
      "Validation Loss: 0.0016069864212842275\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005336143220192753\n",
      "Training Loss: 0.00523788214486558\n",
      "Training Loss: 0.005114824878983199\n",
      "Training Loss: 0.003500764359268942\n",
      "Training Loss: 0.0009829808957874774\n",
      "Training Loss: 0.000947211596503621\n",
      "Training Loss: 0.0007267316918296274\n",
      "Validation Loss: 0.00160302799398887\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005331500124302693\n",
      "Training Loss: 0.005228738203295506\n",
      "Training Loss: 0.005103014561464079\n",
      "Training Loss: 0.003492877768148901\n",
      "Training Loss: 0.0009747000003699213\n",
      "Training Loss: 0.0009397911894484423\n",
      "Training Loss: 0.0007167326912167482\n",
      "Validation Loss: 0.0015993539888907943\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005326740699238144\n",
      "Training Loss: 0.005219540304387919\n",
      "Training Loss: 0.005091309513663873\n",
      "Training Loss: 0.0034851064016402232\n",
      "Training Loss: 0.0009667117273056647\n",
      "Training Loss: 0.0009326632664306089\n",
      "Training Loss: 0.0007071036108391126\n",
      "Validation Loss: 0.0015959394310883592\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005321875425288454\n",
      "Training Loss: 0.005210296084405854\n",
      "Training Loss: 0.005079720840440132\n",
      "Training Loss: 0.0034774556603952077\n",
      "Training Loss: 0.0009590007686347235\n",
      "Training Loss: 0.0009258103367756121\n",
      "Training Loss: 0.0006978293695283355\n",
      "Validation Loss: 0.0015927668588283064\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005316917365998961\n",
      "Training Loss: 0.005201015361235477\n",
      "Training Loss: 0.005068261857377365\n",
      "Training Loss: 0.003469930217252113\n",
      "Training Loss: 0.000951547748336452\n",
      "Training Loss: 0.0009192105406691553\n",
      "Training Loss: 0.0006888930563582107\n",
      "Validation Loss: 0.0015898159531678496\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005311873991158791\n",
      "Training Loss: 0.005191706340992823\n",
      "Training Loss: 0.005056943577947095\n",
      "Training Loss: 0.0034625339660851752\n",
      "Training Loss: 0.0009443357423151611\n",
      "Training Loss: 0.0009128440362837864\n",
      "Training Loss: 0.000680277338688029\n",
      "Validation Loss: 0.0015870668476361132\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005306759357335977\n",
      "Training Loss: 0.00518238146789372\n",
      "Training Loss: 0.005045779288047924\n",
      "Training Loss: 0.003455267448698578\n",
      "Training Loss: 0.0009373458292975556\n",
      "Training Loss: 0.0009066941687342478\n",
      "Training Loss: 0.0006719676459761104\n",
      "Validation Loss: 0.001584500223066839\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0053015789686469365\n",
      "Training Loss: 0.0051730464969296005\n",
      "Training Loss: 0.005034777077962644\n",
      "Training Loss: 0.003448135190192261\n",
      "Training Loss: 0.0009305654076160863\n",
      "Training Loss: 0.0009007443316659192\n",
      "Training Loss: 0.000663948774963501\n",
      "Validation Loss: 0.0015820985647857586\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.0052963423711480575\n",
      "Training Loss: 0.005163712973590009\n",
      "Training Loss: 0.0050239449564833194\n",
      "Training Loss: 0.003441138076232164\n",
      "Training Loss: 0.0009239814297325211\n",
      "Training Loss: 0.0008949793253123061\n",
      "Training Loss: 0.0006562102284806315\n",
      "Validation Loss: 0.00157984267336653\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.00529105101712048\n",
      "Training Loss: 0.005154386950889603\n",
      "Training Loss: 0.005013288942864165\n",
      "Training Loss: 0.0034342735201789766\n",
      "Training Loss: 0.00091757891656016\n",
      "Training Loss: 0.0008893856077338569\n",
      "Training Loss: 0.0006487373297568411\n",
      "Validation Loss: 0.0015777166763212434\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005285711959004402\n",
      "Training Loss: 0.005145079130888916\n",
      "Training Loss: 0.00500281614891719\n",
      "Training Loss: 0.0034275429532499404\n",
      "Training Loss: 0.0009113450266886502\n",
      "Training Loss: 0.0008839514787541702\n",
      "Training Loss: 0.0006415192873828346\n",
      "Validation Loss: 0.00157570207734812\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005280327105429024\n",
      "Training Loss: 0.005135794502566568\n",
      "Training Loss: 0.004992528567672707\n",
      "Training Loss: 0.003420946139121952\n",
      "Training Loss: 0.0009052751012495719\n",
      "Training Loss: 0.0008786672765563708\n",
      "Training Loss: 0.0006345470055384795\n",
      "Validation Loss: 0.001573787014074676\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005274900366785005\n",
      "Training Loss: 0.005126543139340356\n",
      "Training Loss: 0.004982429910451174\n",
      "Training Loss: 0.0034144789050333203\n",
      "Training Loss: 0.0008993579762318404\n",
      "Training Loss: 0.0008735233726474689\n",
      "Training Loss: 0.0006278102047508582\n",
      "Validation Loss: 0.0015719534487207158\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.0052694377058651294\n",
      "Training Loss: 0.0051173361402470615\n",
      "Training Loss: 0.004972527195932343\n",
      "Training Loss: 0.0034081400379363915\n",
      "Training Loss: 0.0008935824484797195\n",
      "Training Loss: 0.000868507562117884\n",
      "Training Loss: 0.0006212990453059319\n",
      "Validation Loss: 0.0015701876347404922\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.00526393570471555\n",
      "Training Loss: 0.00510817639355082\n",
      "Training Loss: 0.004962819506181404\n",
      "Training Loss: 0.003401928796338325\n",
      "Training Loss: 0.0008879493148560868\n",
      "Training Loss: 0.0008636189458047738\n",
      "Training Loss: 0.0006150081707892241\n",
      "Validation Loss: 0.0015684740501455963\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0052583977137692275\n",
      "Training Loss: 0.005099076075712219\n",
      "Training Loss: 0.004953309491975233\n",
      "Training Loss: 0.0033958404078657622\n",
      "Training Loss: 0.0008824495915177976\n",
      "Training Loss: 0.0008588493040588219\n",
      "Training Loss: 0.000608932787363301\n",
      "Validation Loss: 0.001566804252318124\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.00525281714333687\n",
      "Training Loss: 0.0050900344602996484\n",
      "Training Loss: 0.004943995185894892\n",
      "Training Loss: 0.0033898735143156953\n",
      "Training Loss: 0.0008770799079502467\n",
      "Training Loss: 0.0008541955379769206\n",
      "Training Loss: 0.0006030658105009934\n",
      "Validation Loss: 0.0015651672024585915\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005247202313621528\n",
      "Training Loss: 0.005081066263373941\n",
      "Training Loss: 0.0049348818743601445\n",
      "Training Loss: 0.00338402605146257\n",
      "Training Loss: 0.0008718420826335205\n",
      "Training Loss: 0.0008496520158223575\n",
      "Training Loss: 0.0005974032382800943\n",
      "Validation Loss: 0.001563553445582707\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005241554615786299\n",
      "Training Loss: 0.0050721766916103665\n",
      "Training Loss: 0.0049259673437336456\n",
      "Training Loss: 0.003378293927435152\n",
      "Training Loss: 0.0008667303375841585\n",
      "Training Loss: 0.0008452170325472252\n",
      "Training Loss: 0.000591941052043694\n",
      "Validation Loss: 0.0015619510576767282\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005235870834439993\n",
      "Training Loss: 0.005063370104762726\n",
      "Training Loss: 0.004917252424638719\n",
      "Training Loss: 0.0033726730129455974\n",
      "Training Loss: 0.000861741930275457\n",
      "Training Loss: 0.0008408867080288474\n",
      "Training Loss: 0.0005866761969809887\n",
      "Validation Loss: 0.0015603549383761193\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005230153371812775\n",
      "Training Loss: 0.005054656440624967\n",
      "Training Loss: 0.004908737353980541\n",
      "Training Loss: 0.0033671636525104986\n",
      "Training Loss: 0.0008568801767978584\n",
      "Training Loss: 0.0008366583781025839\n",
      "Training Loss: 0.000581605871557258\n",
      "Validation Loss: 0.0015587557805048073\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005224403828615323\n",
      "Training Loss: 0.005046042125904933\n",
      "Training Loss: 0.004900423523504287\n",
      "Training Loss: 0.00336176387301748\n",
      "Training Loss: 0.0008521450972330058\n",
      "Training Loss: 0.0008325336701091146\n",
      "Training Loss: 0.0005767307616042672\n",
      "Validation Loss: 0.0015571514290713003\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005218623984837904\n",
      "Training Loss: 0.005037530268309638\n",
      "Training Loss: 0.004892308058915659\n",
      "Training Loss: 0.0033564662559365387\n",
      "Training Loss: 0.0008475358108262299\n",
      "Training Loss: 0.0008285083304508589\n",
      "Training Loss: 0.0005720494483830407\n",
      "Validation Loss: 0.0015555329841896474\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005212812777608633\n",
      "Training Loss: 0.005029127555899322\n",
      "Training Loss: 0.0048843905032845214\n",
      "Training Loss: 0.00335127365495282\n",
      "Training Loss: 0.0008430559060070664\n",
      "Training Loss: 0.0008245859635644592\n",
      "Training Loss: 0.0005675609315949259\n",
      "Validation Loss: 0.00155389792756099\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005206973580643534\n",
      "Training Loss: 0.005020836067851633\n",
      "Training Loss: 0.004876667132484727\n",
      "Training Loss: 0.003346181095330394\n",
      "Training Loss: 0.0008387092536577257\n",
      "Training Loss: 0.0008207635977305472\n",
      "Training Loss: 0.0005632649551989743\n",
      "Validation Loss: 0.0015522426733100081\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005201108267647215\n",
      "Training Loss: 0.005012664453824982\n",
      "Training Loss: 0.004869138320209458\n",
      "Training Loss: 0.003341187443365925\n",
      "Training Loss: 0.0008344931302417535\n",
      "Training Loss: 0.0008170432647602865\n",
      "Training Loss: 0.0005591630893468391\n",
      "Validation Loss: 0.001550564423653869\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005195220389286988\n",
      "Training Loss: 0.005004610472824424\n",
      "Training Loss: 0.0048617989855119954\n",
      "Training Loss: 0.003336289427606971\n",
      "Training Loss: 0.0008304153335484443\n",
      "Training Loss: 0.0008134261814120691\n",
      "Training Loss: 0.0005552548534615198\n",
      "Validation Loss: 0.0015488652083939917\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005189312804141082\n",
      "Training Loss: 0.004996683375211432\n",
      "Training Loss: 0.004854647297761403\n",
      "Training Loss: 0.0033314864711792326\n",
      "Training Loss: 0.0008264733430405613\n",
      "Training Loss: 0.000809910179596045\n",
      "Training Loss: 0.0005515404530160594\n",
      "Validation Loss: 0.0015471408480786184\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005183386274147779\n",
      "Training Loss: 0.004988879369921051\n",
      "Training Loss: 0.004847678446094505\n",
      "Training Loss: 0.003326776112480729\n",
      "Training Loss: 0.0008226721362007084\n",
      "Training Loss: 0.0008064989229023922\n",
      "Training Loss: 0.0005480207138316473\n",
      "Validation Loss: 0.0015453930459358495\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0051774467522045595\n",
      "Training Loss: 0.004981201938353479\n",
      "Training Loss: 0.004840886414749548\n",
      "Training Loss: 0.00332215590900887\n",
      "Training Loss: 0.0008190141914383275\n",
      "Training Loss: 0.0008031931745790643\n",
      "Training Loss: 0.0005446942093840334\n",
      "Validation Loss: 0.0015436167836804919\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005171490898937918\n",
      "Training Loss: 0.004973648630548269\n",
      "Training Loss: 0.00483426728809718\n",
      "Training Loss: 0.0033176253433339297\n",
      "Training Loss: 0.0008155011549388292\n",
      "Training Loss: 0.000799993226537481\n",
      "Training Loss: 0.0005415602750144899\n",
      "Validation Loss: 0.0015418231715635686\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.0051655302161816505\n",
      "Training Loss: 0.00496622079052031\n",
      "Training Loss: 0.00482781340891961\n",
      "Training Loss: 0.003313181126395648\n",
      "Training Loss: 0.000812131765196682\n",
      "Training Loss: 0.0007969005052291322\n",
      "Training Loss: 0.0005386175612511579\n",
      "Validation Loss: 0.001540006874373343\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005159562615444884\n",
      "Training Loss: 0.004958917309995741\n",
      "Training Loss: 0.00482152053504251\n",
      "Training Loss: 0.0033088214985400556\n",
      "Training Loss: 0.0008089075035968563\n",
      "Training Loss: 0.0007939152781909798\n",
      "Training Loss: 0.0005358646398235578\n",
      "Validation Loss: 0.0015381689245872764\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005153591965790838\n",
      "Training Loss: 0.004951735130744055\n",
      "Training Loss: 0.004815379549982026\n",
      "Training Loss: 0.0033045455231331287\n",
      "Training Loss: 0.0008058327429898782\n",
      "Training Loss: 0.0007910397012892645\n",
      "Training Loss: 0.0005332983004336711\n",
      "Validation Loss: 0.001536312325622569\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005147620668867603\n",
      "Training Loss: 0.004944673393620178\n",
      "Training Loss: 0.004809385223779827\n",
      "Training Loss: 0.003300350286044704\n",
      "Training Loss: 0.0008029011928738328\n",
      "Training Loss: 0.0007882727567266556\n",
      "Training Loss: 0.0005309168763051275\n",
      "Validation Loss: 0.0015344413215153212\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005141650321311317\n",
      "Training Loss: 0.0049377262796042485\n",
      "Training Loss: 0.004803530378849246\n",
      "Training Loss: 0.0032962353261973476\n",
      "Training Loss: 0.00080011562386062\n",
      "Training Loss: 0.0007856164136683219\n",
      "Training Loss: 0.0005287146929185837\n",
      "Validation Loss: 0.0015325568918219841\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.00513568953901995\n",
      "Training Loss: 0.004930893385317176\n",
      "Training Loss: 0.00479780905705411\n",
      "Training Loss: 0.0032921977383011834\n",
      "Training Loss: 0.0007974718286277493\n",
      "Training Loss: 0.0007830677536549047\n",
      "Training Loss: 0.0005266876464156667\n",
      "Validation Loss: 0.0015306612015078332\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005129740073462017\n",
      "Training Loss: 0.0049241707799956205\n",
      "Training Loss: 0.004792212513857521\n",
      "Training Loss: 0.0032882350268482697\n",
      "Training Loss: 0.0007949706056388095\n",
      "Training Loss: 0.000780627797357738\n",
      "Training Loss: 0.0005248323699197499\n",
      "Validation Loss: 0.0015287602347346225\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005123801758163609\n",
      "Training Loss: 0.004917556085274555\n",
      "Training Loss: 0.004786735743400641\n",
      "Training Loss: 0.0032843453246823627\n",
      "Training Loss: 0.000792604512462276\n",
      "Training Loss: 0.0007782932408008492\n",
      "Training Loss: 0.0005231417635513935\n",
      "Validation Loss: 0.0015268520903791046\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.005117879614117555\n",
      "Training Loss: 0.004911042249295861\n",
      "Training Loss: 0.004781371001154184\n",
      "Training Loss: 0.003280526786511473\n",
      "Training Loss: 0.0007903767948300811\n",
      "Training Loss: 0.0007760676821271773\n",
      "Training Loss: 0.0005216115376242669\n",
      "Validation Loss: 0.001524941824441549\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.0051119735464453695\n",
      "Training Loss: 0.004904628799413331\n",
      "Training Loss: 0.004776114077540115\n",
      "Training Loss: 0.0032767771917860955\n",
      "Training Loss: 0.0007882803936081473\n",
      "Training Loss: 0.0007739458934884169\n",
      "Training Loss: 0.0005202351586194709\n",
      "Validation Loss: 0.0015230288651461403\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.0051060867233900355\n",
      "Training Loss: 0.00489831006329041\n",
      "Training Loss: 0.004770956552238204\n",
      "Training Loss: 0.003273095386866771\n",
      "Training Loss: 0.0007863102572446223\n",
      "Training Loss: 0.0007719274937699084\n",
      "Training Loss: 0.0005190061953908298\n",
      "Validation Loss: 0.001521121254033838\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005100223706103861\n",
      "Training Loss: 0.004892081615980715\n",
      "Training Loss: 0.004765893465955742\n",
      "Training Loss: 0.00326947877791099\n",
      "Training Loss: 0.0007844641921838047\n",
      "Training Loss: 0.000770007789178635\n",
      "Training Loss: 0.000517918558762176\n",
      "Validation Loss: 0.0015192181930383701\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005094386492855847\n",
      "Training Loss: 0.0048859416606137525\n",
      "Training Loss: 0.004760918589308858\n",
      "Training Loss: 0.003265924989755149\n",
      "Training Loss: 0.0007827398239169269\n",
      "Training Loss: 0.0007681875713387853\n",
      "Training Loss: 0.000516966439463431\n",
      "Validation Loss: 0.001517323349623885\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005088575091795064\n",
      "Training Loss: 0.004879885305999778\n",
      "Training Loss: 0.004756028616684489\n",
      "Training Loss: 0.0032624334731372073\n",
      "Training Loss: 0.0007811292865517317\n",
      "Training Loss: 0.000766463617219415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [16:25<1:05:46, 493.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0005161427689017728\n",
      "Validation Loss: 0.001515435301408796\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.4331584782898426\n",
      "Training Loss: 0.3041079416126013\n",
      "Training Loss: 0.1975469421222806\n",
      "Training Loss: 0.10336027675308287\n",
      "Training Loss: 0.05518075878731907\n",
      "Training Loss: 0.04009647900238633\n",
      "Training Loss: 0.0365288518788293\n",
      "Validation Loss: 0.03563311874294437\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.03850629573687911\n",
      "Training Loss: 0.03684184634126723\n",
      "Training Loss: 0.034293300304561854\n",
      "Training Loss: 0.029956143673625774\n",
      "Training Loss: 0.02600044050719589\n",
      "Training Loss: 0.023219070576597004\n",
      "Training Loss: 0.02313539920374751\n",
      "Validation Loss: 0.023295728419994593\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.026155659463256598\n",
      "Training Loss: 0.02509173466358334\n",
      "Training Loss: 0.02296586333308369\n",
      "Training Loss: 0.019274641952943056\n",
      "Training Loss: 0.014906741224694996\n",
      "Training Loss: 0.013126742753665894\n",
      "Training Loss: 0.013072302993386985\n",
      "Validation Loss: 0.013971667096702459\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.017640774515457452\n",
      "Training Loss: 0.017216309863142668\n",
      "Training Loss: 0.015668753809295596\n",
      "Training Loss: 0.012089131405809894\n",
      "Training Loss: 0.007829072426538915\n",
      "Training Loss: 0.006734094671555795\n",
      "Training Loss: 0.006398655011435039\n",
      "Validation Loss: 0.007566262567189107\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.011497172242961824\n",
      "Training Loss: 0.011424987389473245\n",
      "Training Loss: 0.010813756844727322\n",
      "Training Loss: 0.008041520480182952\n",
      "Training Loss: 0.004636575655313209\n",
      "Training Loss: 0.004394573617610149\n",
      "Training Loss: 0.004506517067202367\n",
      "Validation Loss: 0.005954980526262483\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.00985549186123535\n",
      "Training Loss: 0.010097162296297028\n",
      "Training Loss: 0.009788069396745414\n",
      "Training Loss: 0.007130841866892296\n",
      "Training Loss: 0.003814900227007456\n",
      "Training Loss: 0.0037075234175426887\n",
      "Training Loss: 0.003754310682415962\n",
      "Validation Loss: 0.0052212557144121225\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.009058728738455101\n",
      "Training Loss: 0.009278116211062297\n",
      "Training Loss: 0.009082140654791147\n",
      "Training Loss: 0.0064844917380833065\n",
      "Training Loss: 0.0032714964542537926\n",
      "Training Loss: 0.003245525136444485\n",
      "Training Loss: 0.0032367255969438704\n",
      "Validation Loss: 0.004647972486688222\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.008466739975847303\n",
      "Training Loss: 0.00866550452192314\n",
      "Training Loss: 0.008546149956528097\n",
      "Training Loss: 0.00601887882410665\n",
      "Training Loss: 0.0029035133475554175\n",
      "Training Loss: 0.0029251747552189043\n",
      "Training Loss: 0.0028820813589845785\n",
      "Validation Loss: 0.004216815725410155\n",
      "Validation Accuracy: 0.11704119850187265\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.008020804797997699\n",
      "Training Loss: 0.008202784491004422\n",
      "Training Loss: 0.008137918962165713\n",
      "Training Loss: 0.0056826490625098815\n",
      "Training Loss: 0.00265467036224436\n",
      "Training Loss: 0.0027001002542237982\n",
      "Training Loss: 0.002637937342224177\n",
      "Validation Loss: 0.0039056093350910907\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.007682965418789536\n",
      "Training Loss: 0.007849388456670568\n",
      "Training Loss: 0.007822829325450585\n",
      "Training Loss: 0.005435745581635274\n",
      "Training Loss: 0.0024824839853681622\n",
      "Training Loss: 0.0025362785608740524\n",
      "Training Loss: 0.0024639451802067926\n",
      "Validation Loss: 0.0036753138513321735\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.007422256272984668\n",
      "Training Loss: 0.007574440552853048\n",
      "Training Loss: 0.007574335027020425\n",
      "Training Loss: 0.005248559485917213\n",
      "Training Loss: 0.0023560164944501595\n",
      "Training Loss: 0.0024097229560720735\n",
      "Training Loss: 0.002331101323798066\n",
      "Validation Loss: 0.003492557641192015\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.007215700906235725\n",
      "Training Loss: 0.007355686140945181\n",
      "Training Loss: 0.007373490310274064\n",
      "Training Loss: 0.00510011112986831\n",
      "Training Loss: 0.0022544439166085795\n",
      "Training Loss: 0.002304368107143091\n",
      "Training Loss: 0.002220411369635258\n",
      "Validation Loss: 0.003335901649846706\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.007047311646165327\n",
      "Training Loss: 0.00717764840577729\n",
      "Training Loss: 0.007207440914353356\n",
      "Training Loss: 0.004976880603644531\n",
      "Training Loss: 0.0021655177138745784\n",
      "Training Loss: 0.0022104447024321417\n",
      "Training Loss: 0.002121260772983078\n",
      "Validation Loss: 0.0031938769593869757\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.006907115960493684\n",
      "Training Loss: 0.00703073127893731\n",
      "Training Loss: 0.007068585682427511\n",
      "Training Loss: 0.004871642915386474\n",
      "Training Loss: 0.0020834473655850162\n",
      "Training Loss: 0.0021230861404183086\n",
      "Training Loss: 0.0020291408913908526\n",
      "Validation Loss: 0.0030618220337052665\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.0067900220036972315\n",
      "Training Loss: 0.006910003372468054\n",
      "Training Loss: 0.006953419039491564\n",
      "Training Loss: 0.00478156303986907\n",
      "Training Loss: 0.002006511148647405\n",
      "Training Loss: 0.0020407906999025728\n",
      "Training Loss: 0.001943128716229694\n",
      "Validation Loss: 0.002939168803964145\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.0066941429581493135\n",
      "Training Loss: 0.006813267731340602\n",
      "Training Loss: 0.006860629755537957\n",
      "Training Loss: 0.004705867447919445\n",
      "Training Loss: 0.0019348217439255676\n",
      "Training Loss: 0.001963784403487807\n",
      "Training Loss: 0.0018637053125712556\n",
      "Validation Loss: 0.002827186846067041\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.0066187701059971\n",
      "Training Loss: 0.006738896468887106\n",
      "Training Loss: 0.006789015677059069\n",
      "Training Loss: 0.0046439467468007934\n",
      "Training Loss: 0.0018689168269338551\n",
      "Training Loss: 0.001892797683904064\n",
      "Training Loss: 0.0017914771079085767\n",
      "Validation Loss: 0.0027273371204174007\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.0065626930422149595\n",
      "Training Loss: 0.006684373322641477\n",
      "Training Loss: 0.0067362399189732965\n",
      "Training Loss: 0.004594483293913072\n",
      "Training Loss: 0.001809193619847065\n",
      "Training Loss: 0.0018284056828997565\n",
      "Training Loss: 0.0017266795093019028\n",
      "Validation Loss: 0.0026403442118927306\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.00652351368800737\n",
      "Training Loss: 0.006646038824692368\n",
      "Training Loss: 0.006698769391514361\n",
      "Training Loss: 0.0045554322251700795\n",
      "Training Loss: 0.0017557193955872207\n",
      "Training Loss: 0.0017707106492889579\n",
      "Training Loss: 0.0016690445152926258\n",
      "Validation Loss: 0.0025658661744658476\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.0064978569734375925\n",
      "Training Loss: 0.006619678231654689\n",
      "Training Loss: 0.006672563194297254\n",
      "Training Loss: 0.004524404265248449\n",
      "Training Loss: 0.001708158065448515\n",
      "Training Loss: 0.0017192616490501677\n",
      "Training Loss: 0.001617840509134112\n",
      "Validation Loss: 0.0025026120138851656\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.006482066973112524\n",
      "Training Loss: 0.006601346449460834\n",
      "Training Loss: 0.006653895849594847\n",
      "Training Loss: 0.00449911136762239\n",
      "Training Loss: 0.00166580459830584\n",
      "Training Loss: 0.0016731411680666497\n",
      "Training Loss: 0.0015719908093160483\n",
      "Validation Loss: 0.002448688859419988\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.006472877109190449\n",
      "Training Loss: 0.006587927332147956\n",
      "Training Loss: 0.006639855017419904\n",
      "Training Loss: 0.004477632556809112\n",
      "Training Loss: 0.0016276732967526186\n",
      "Training Loss: 0.0016311107577348593\n",
      "Training Loss: 0.0015301776213163976\n",
      "Validation Loss: 0.002401907098229298\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.006467752702301368\n",
      "Training Loss: 0.006577249321853742\n",
      "Training Loss: 0.006628404096700251\n",
      "Training Loss: 0.00445842768123839\n",
      "Training Loss: 0.0015925239466014318\n",
      "Training Loss: 0.0015916698036016897\n",
      "Training Loss: 0.0014908365161682014\n",
      "Validation Loss: 0.002359921137945315\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.00646488503436558\n",
      "Training Loss: 0.006567858934868127\n",
      "Training Loss: 0.006618117932230234\n",
      "Training Loss: 0.004440165871928911\n",
      "Training Loss: 0.0015588086596108042\n",
      "Training Loss: 0.0015529803404933772\n",
      "Training Loss: 0.0014520093497412746\n",
      "Validation Loss: 0.002320148897739232\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.006462990561267361\n",
      "Training Loss: 0.006558682091999799\n",
      "Training Loss: 0.006607816810719669\n",
      "Training Loss: 0.00442144959102734\n",
      "Training Loss: 0.0015244336586329155\n",
      "Training Loss: 0.001512556792367832\n",
      "Training Loss: 0.001410960130742751\n",
      "Validation Loss: 0.0022794392023345435\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.006461044785100967\n",
      "Training Loss: 0.006548661536071449\n",
      "Training Loss: 0.006596147972159088\n",
      "Training Loss: 0.004400409863883396\n",
      "Training Loss: 0.0014862348734459374\n",
      "Training Loss: 0.0014665450363827404\n",
      "Training Loss: 0.0013633298644708703\n",
      "Validation Loss: 0.0022332502280715994\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.006457904166309163\n",
      "Training Loss: 0.006536300866864622\n",
      "Training Loss: 0.00658095513121225\n",
      "Training Loss: 0.004373940790974302\n",
      "Training Loss: 0.0014387243741657584\n",
      "Training Loss: 0.0014080054925580043\n",
      "Training Loss: 0.0013011659323092317\n",
      "Validation Loss: 0.002173769217185512\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.006451638168655336\n",
      "Training Loss: 0.006518841854995116\n",
      "Training Loss: 0.006557989118155092\n",
      "Training Loss: 0.0043360270594712345\n",
      "Training Loss: 0.0013711925178358797\n",
      "Training Loss: 0.001323036768408201\n",
      "Training Loss: 0.0012087869011156726\n",
      "Validation Loss: 0.002086432813917468\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.00643852801527828\n",
      "Training Loss: 0.00649102886323817\n",
      "Training Loss: 0.006518555546645075\n",
      "Training Loss: 0.0042752557619678555\n",
      "Training Loss: 0.0012640020884282421\n",
      "Training Loss: 0.001186893199046608\n",
      "Training Loss: 0.001062431739992462\n",
      "Validation Loss: 0.001953670599372283\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006416531621944159\n",
      "Training Loss: 0.006448951897909865\n",
      "Training Loss: 0.006453133758623153\n",
      "Training Loss: 0.004184228725498542\n",
      "Training Loss: 0.0011103061986796091\n",
      "Training Loss: 0.0010012042104790453\n",
      "Training Loss: 0.0008877456002664985\n",
      "Validation Loss: 0.0018143815672210825\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006404822529293597\n",
      "Training Loss: 0.006407655553193763\n",
      "Training Loss: 0.006379497252637521\n",
      "Training Loss: 0.004100446269803797\n",
      "Training Loss: 0.0009876865756814368\n",
      "Training Loss: 0.0008720257451204816\n",
      "Training Loss: 0.0007912466407287865\n",
      "Validation Loss: 0.0017429000065484945\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.0063923596858512614\n",
      "Training Loss: 0.006364685362204909\n",
      "Training Loss: 0.006316197477281094\n",
      "Training Loss: 0.0040469519868202046\n",
      "Training Loss: 0.00093148768915853\n",
      "Training Loss: 0.0008171560092887375\n",
      "Training Loss: 0.0007521811292463098\n",
      "Validation Loss: 0.0017099289726274095\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.006356747737736441\n",
      "Training Loss: 0.006315403694752603\n",
      "Training Loss: 0.006260900758206844\n",
      "Training Loss: 0.004007491782031139\n",
      "Training Loss: 0.0009011522238870384\n",
      "Training Loss: 0.0007858229007251794\n",
      "Training Loss: 0.0007286148724597297\n",
      "Validation Loss: 0.0016903380575818306\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006316590020433068\n",
      "Training Loss: 0.006269267920870334\n",
      "Training Loss: 0.006213279251242056\n",
      "Training Loss: 0.00397487515139801\n",
      "Training Loss: 0.0008782852454896783\n",
      "Training Loss: 0.0007613234235031996\n",
      "Training Loss: 0.0007094209139904705\n",
      "Validation Loss: 0.001675524535270195\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006279805653030053\n",
      "Training Loss: 0.006228688342962414\n",
      "Training Loss: 0.006172100743278861\n",
      "Training Loss: 0.003946531190522364\n",
      "Training Loss: 0.0008580779887051904\n",
      "Training Loss: 0.0007397753175973776\n",
      "Training Loss: 0.0006919745326376869\n",
      "Validation Loss: 0.0016625117906674477\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.006247189087443985\n",
      "Training Loss: 0.006193059524521232\n",
      "Training Loss: 0.0061359195620752875\n",
      "Training Loss: 0.003921214296860853\n",
      "Training Loss: 0.0008392783934687031\n",
      "Training Loss: 0.0007200556320822216\n",
      "Training Loss: 0.000675595505090314\n",
      "Validation Loss: 0.0016502179782044086\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006218090660404414\n",
      "Training Loss: 0.00616135370451957\n",
      "Training Loss: 0.006103559078183025\n",
      "Training Loss: 0.0038981876925390678\n",
      "Training Loss: 0.000821527738153236\n",
      "Training Loss: 0.0007017299868311966\n",
      "Training Loss: 0.0006600739025088842\n",
      "Validation Loss: 0.0016382539584667663\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006191794923506677\n",
      "Training Loss: 0.006132691995007917\n",
      "Training Loss: 0.006074144419981167\n",
      "Training Loss: 0.0038769669758039528\n",
      "Training Loss: 0.0008047020406229421\n",
      "Training Loss: 0.0006845806513229036\n",
      "Training Loss: 0.000645335237350082\n",
      "Validation Loss: 0.0016265038582596628\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006167707467684522\n",
      "Training Loss: 0.006106374970404432\n",
      "Training Loss: 0.006047017860109918\n",
      "Training Loss: 0.00385720515572757\n",
      "Training Loss: 0.0007887545311314171\n",
      "Training Loss: 0.000668486474569363\n",
      "Training Loss: 0.0006313451507230639\n",
      "Validation Loss: 0.0016149449328305933\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.0061453688563779\n",
      "Training Loss: 0.006081867549801246\n",
      "Training Loss: 0.006021686734748073\n",
      "Training Loss: 0.0038386473064747407\n",
      "Training Loss: 0.0007736553069844376\n",
      "Training Loss: 0.000653361710574245\n",
      "Training Loss: 0.000618081795728358\n",
      "Validation Loss: 0.0016036019410915353\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006124418994877487\n",
      "Training Loss: 0.0060587498429231345\n",
      "Training Loss: 0.005997773603885434\n",
      "Training Loss: 0.003821093790829764\n",
      "Training Loss: 0.00075937752979371\n",
      "Training Loss: 0.0006391438316859421\n",
      "Training Loss: 0.0006055224496594746\n",
      "Validation Loss: 0.0015925102325876623\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006104572371696122\n",
      "Training Loss: 0.006036687283776701\n",
      "Training Loss: 0.0059749788220506165\n",
      "Training Loss: 0.003804380890869652\n",
      "Training Loss: 0.0007458901118661742\n",
      "Training Loss: 0.0006257838181045372\n",
      "Training Loss: 0.0005936444418694009\n",
      "Validation Loss: 0.0015816849235978068\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006085590174188837\n",
      "Training Loss: 0.006015408985549584\n",
      "Training Loss: 0.00595306730363518\n",
      "Training Loss: 0.0037883780019910772\n",
      "Training Loss: 0.0007331587020962616\n",
      "Training Loss: 0.0006132306606741622\n",
      "Training Loss: 0.0005824148517058347\n",
      "Validation Loss: 0.0015711444744527208\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006067285074386745\n",
      "Training Loss: 0.005994695938425138\n",
      "Training Loss: 0.00593184900470078\n",
      "Training Loss: 0.003772974207386142\n",
      "Training Loss: 0.0007211402306347736\n",
      "Training Loss: 0.0006014374102232977\n",
      "Training Loss: 0.0005717976637242828\n",
      "Validation Loss: 0.0015608940264476073\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006049498911597766\n",
      "Training Loss: 0.005974373132921755\n",
      "Training Loss: 0.005911173371132463\n",
      "Training Loss: 0.0037580756088573253\n",
      "Training Loss: 0.0007097837184119271\n",
      "Training Loss: 0.0005903508696064819\n",
      "Training Loss: 0.0005617480169894406\n",
      "Validation Loss: 0.0015509280177532635\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006032104509649798\n",
      "Training Loss: 0.0059542969072936104\n",
      "Training Loss: 0.005890917971264571\n",
      "Training Loss: 0.003743602798494976\n",
      "Training Loss: 0.0006990322937781457\n",
      "Training Loss: 0.0005799180989197339\n",
      "Training Loss: 0.0005522182495769812\n",
      "Validation Loss: 0.0015412314751029384\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.0060149947210447864\n",
      "Training Loss: 0.005934358324157074\n",
      "Training Loss: 0.005870986357331276\n",
      "Training Loss: 0.0037294917549297677\n",
      "Training Loss: 0.0006888281814462971\n",
      "Training Loss: 0.0005700828972476302\n",
      "Training Loss: 0.0005431573765599751\n",
      "Validation Loss: 0.0015317928237833557\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.005998091924702749\n",
      "Training Loss: 0.00591447644343134\n",
      "Training Loss: 0.005851307086413726\n",
      "Training Loss: 0.0037156895935186187\n",
      "Training Loss: 0.0006791074910142925\n",
      "Training Loss: 0.0005607872704422334\n",
      "Training Loss: 0.0005345142945589032\n",
      "Validation Loss: 0.0015225877854772511\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.005981328908819705\n",
      "Training Loss: 0.005894591694814153\n",
      "Training Loss: 0.005831825219793245\n",
      "Training Loss: 0.0037021548959455686\n",
      "Training Loss: 0.0006698108253476676\n",
      "Training Loss: 0.0005519780920440098\n",
      "Training Loss: 0.0005262406161637046\n",
      "Validation Loss: 0.0015136058814431217\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.005964661142206751\n",
      "Training Loss: 0.00587466865661554\n",
      "Training Loss: 0.005812499971943908\n",
      "Training Loss: 0.0036888581968378274\n",
      "Training Loss: 0.0006608804102506838\n",
      "Training Loss: 0.0005436004549483187\n",
      "Training Loss: 0.0005182902470551199\n",
      "Validation Loss: 0.001504829854363042\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.005948060781811364\n",
      "Training Loss: 0.005854692592984065\n",
      "Training Loss: 0.005793310750159435\n",
      "Training Loss: 0.0036757786013913575\n",
      "Training Loss: 0.0006522598327865126\n",
      "Training Loss: 0.000535604027354566\n",
      "Training Loss: 0.0005106215655541746\n",
      "Validation Loss: 0.0014962495654217082\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.005931505774497054\n",
      "Training Loss: 0.005834662616252899\n",
      "Training Loss: 0.00577424380404409\n",
      "Training Loss: 0.003662902846263023\n",
      "Training Loss: 0.0006439026940643089\n",
      "Training Loss: 0.0005279459455414326\n",
      "Training Loss: 0.0005032014007883845\n",
      "Validation Loss: 0.0014878586631713868\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.0059149907890241596\n",
      "Training Loss: 0.0058145894005429\n",
      "Training Loss: 0.005755295513663441\n",
      "Training Loss: 0.003650226383761037\n",
      "Training Loss: 0.0006357689887954621\n",
      "Training Loss: 0.0005205885116447462\n",
      "Training Loss: 0.0004960019314603414\n",
      "Validation Loss: 0.0014796610407671656\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.005898516838205978\n",
      "Training Loss: 0.005794497279566713\n",
      "Training Loss: 0.005736470192205161\n",
      "Training Loss: 0.0036377478583017363\n",
      "Training Loss: 0.0006278288735848037\n",
      "Training Loss: 0.0005135033000260591\n",
      "Training Loss: 0.0004890060301113408\n",
      "Validation Loss: 0.0014716606168163598\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.005882084940094501\n",
      "Training Loss: 0.005774412261671386\n",
      "Training Loss: 0.005717779137194156\n",
      "Training Loss: 0.0036254722050216514\n",
      "Training Loss: 0.0006200592641107505\n",
      "Training Loss: 0.0005066657457064138\n",
      "Training Loss: 0.000482201255763357\n",
      "Validation Loss: 0.0014638715107098558\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.00586571286607068\n",
      "Training Loss: 0.005754371585790068\n",
      "Training Loss: 0.005699234878411516\n",
      "Training Loss: 0.003613405330979731\n",
      "Training Loss: 0.0006124464314052603\n",
      "Training Loss: 0.0005000633167219348\n",
      "Training Loss: 0.00047558449859934625\n",
      "Validation Loss: 0.0014563090017406367\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.005849406776833348\n",
      "Training Loss: 0.0057344062771881\n",
      "Training Loss: 0.00568085161736235\n",
      "Training Loss: 0.0036015548050636424\n",
      "Training Loss: 0.0006049887822882738\n",
      "Training Loss: 0.0004936874547274784\n",
      "Training Loss: 0.0004691593668394489\n",
      "Validation Loss: 0.0014489926359104342\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.005833180324989371\n",
      "Training Loss: 0.005714553047437221\n",
      "Training Loss: 0.005662644729018212\n",
      "Training Loss: 0.003589929140289314\n",
      "Training Loss: 0.0005976863984687952\n",
      "Training Loss: 0.0004875380815428798\n",
      "Training Loss: 0.0004629345715875388\n",
      "Validation Loss: 0.0014419443385953726\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.005817044674186036\n",
      "Training Loss: 0.005694844886893407\n",
      "Training Loss: 0.005644630654715001\n",
      "Training Loss: 0.0035785375024715905\n",
      "Training Loss: 0.0005905493837053654\n",
      "Training Loss: 0.00048161705512029585\n",
      "Training Loss: 0.00045692286686971784\n",
      "Validation Loss: 0.0014351844798606971\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.005801007493282669\n",
      "Training Loss: 0.005675309273647145\n",
      "Training Loss: 0.00562682279618457\n",
      "Training Loss: 0.0035673864060663616\n",
      "Training Loss: 0.0005835912053589709\n",
      "Training Loss: 0.0004759309396831668\n",
      "Training Loss: 0.00045113998610759156\n",
      "Validation Loss: 0.0014287371715759223\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.005785079931956716\n",
      "Training Loss: 0.005655979742878117\n",
      "Training Loss: 0.00560923540382646\n",
      "Training Loss: 0.003556480728293536\n",
      "Training Loss: 0.000576824551608297\n",
      "Training Loss: 0.0004704877130279783\n",
      "Training Loss: 0.00044560086247656726\n",
      "Validation Loss: 0.001422620834939684\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.005769264771370217\n",
      "Training Loss: 0.005636874798801728\n",
      "Training Loss: 0.005591874157544225\n",
      "Training Loss: 0.003545824437169358\n",
      "Training Loss: 0.0005702726259914925\n",
      "Training Loss: 0.00046529927625670096\n",
      "Training Loss: 0.000440323421935318\n",
      "Validation Loss: 0.0014168510436623667\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.0057535631238715725\n",
      "Training Loss: 0.005618013725616038\n",
      "Training Loss: 0.0055747460224665705\n",
      "Training Loss: 0.003535418682586169\n",
      "Training Loss: 0.0005639486596192001\n",
      "Training Loss: 0.0004603710226729163\n",
      "Training Loss: 0.0004353214726143051\n",
      "Validation Loss: 0.0014114372152856931\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.0057379744423087685\n",
      "Training Loss: 0.005599412943702191\n",
      "Training Loss: 0.005557850598706864\n",
      "Training Loss: 0.0035252601494721605\n",
      "Training Loss: 0.0005578688663808862\n",
      "Training Loss: 0.00045570943861093836\n",
      "Training Loss: 0.00043060523272288266\n",
      "Validation Loss: 0.0014063859649359211\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.005722491568303667\n",
      "Training Loss: 0.00558108021330554\n",
      "Training Loss: 0.005541184870526195\n",
      "Training Loss: 0.0035153441404690965\n",
      "Training Loss: 0.0005520425394206541\n",
      "Training Loss: 0.0004513154210508219\n",
      "Training Loss: 0.0004261807638249593\n",
      "Validation Loss: 0.0014016985946552032\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.005707108819042333\n",
      "Training Loss: 0.005563022199203261\n",
      "Training Loss: 0.005524740827386267\n",
      "Training Loss: 0.0035056593563058415\n",
      "Training Loss: 0.0005464771434344584\n",
      "Training Loss: 0.0004471865038794931\n",
      "Training Loss: 0.00042205118003039386\n",
      "Validation Loss: 0.0013973669106252353\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.005691808685660362\n",
      "Training Loss: 0.005545232994481921\n",
      "Training Loss: 0.005508501909789629\n",
      "Training Loss: 0.0034961943273810903\n",
      "Training Loss: 0.0005411759691196494\n",
      "Training Loss: 0.00044331731616694016\n",
      "Training Loss: 0.0004182121346821077\n",
      "Validation Loss: 0.0013933781674393359\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0056765807460760695\n",
      "Training Loss: 0.0055277103028493\n",
      "Training Loss: 0.005492452188045718\n",
      "Training Loss: 0.0034869312054070178\n",
      "Training Loss: 0.0005361362700932659\n",
      "Training Loss: 0.0004396979564262438\n",
      "Training Loss: 0.0004146568156284047\n",
      "Validation Loss: 0.0013897164239921372\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.005661404012935236\n",
      "Training Loss: 0.005510442118975334\n",
      "Training Loss: 0.005476568280719221\n",
      "Training Loss: 0.0034778522112901555\n",
      "Training Loss: 0.0005313500452029984\n",
      "Training Loss: 0.0004363117753018741\n",
      "Training Loss: 0.0004113706916905357\n",
      "Validation Loss: 0.00138635887263634\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.005646263586822897\n",
      "Training Loss: 0.005493418702972121\n",
      "Training Loss: 0.005460828435607254\n",
      "Training Loss: 0.003468936139543075\n",
      "Training Loss: 0.0005268052843894111\n",
      "Training Loss: 0.00043314304937666745\n",
      "Training Loss: 0.00040833822244167094\n",
      "Validation Loss: 0.0013832790077941263\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.005631142376805656\n",
      "Training Loss: 0.005476623481954448\n",
      "Training Loss: 0.00544520708033815\n",
      "Training Loss: 0.003460163417184958\n",
      "Training Loss: 0.000522488741480629\n",
      "Training Loss: 0.0004301726982157561\n",
      "Training Loss: 0.0004055401338337106\n",
      "Validation Loss: 0.0013804539990365464\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.005616021389141679\n",
      "Training Loss: 0.0054600425926037135\n",
      "Training Loss: 0.005429681718815118\n",
      "Training Loss: 0.0034515119211573617\n",
      "Training Loss: 0.0005183807676075958\n",
      "Training Loss: 0.00042737767951621207\n",
      "Training Loss: 0.0004029553715736256\n",
      "Validation Loss: 0.0013778516549110406\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.005600888665067032\n",
      "Training Loss: 0.005443661839235574\n",
      "Training Loss: 0.005414229617454112\n",
      "Training Loss: 0.003442964129790198\n",
      "Training Loss: 0.0005144666288833832\n",
      "Training Loss: 0.0004247397286962951\n",
      "Training Loss: 0.0004005618805240374\n",
      "Validation Loss: 0.0013754448711947094\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.005585732633480802\n",
      "Training Loss: 0.005427468068664894\n",
      "Training Loss: 0.005398833638173528\n",
      "Training Loss: 0.0034345019592728933\n",
      "Training Loss: 0.0005107219373894623\n",
      "Training Loss: 0.0004222347525137593\n",
      "Training Loss: 0.0003983369720663177\n",
      "Validation Loss: 0.0013732099676703553\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.005570548397954554\n",
      "Training Loss: 0.005411452496191487\n",
      "Training Loss: 0.005383479822194203\n",
      "Training Loss: 0.003426110919463099\n",
      "Training Loss: 0.0005071329291968141\n",
      "Training Loss: 0.00041984770989074606\n",
      "Training Loss: 0.00039626166006200946\n",
      "Validation Loss: 0.0013711194139674188\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.005555331193027087\n",
      "Training Loss: 0.00539560777542647\n",
      "Training Loss: 0.005368159245699644\n",
      "Training Loss: 0.003417780608360772\n",
      "Training Loss: 0.0005036784237745451\n",
      "Training Loss: 0.00041755832866329\n",
      "Training Loss: 0.0003943144102959195\n",
      "Validation Loss: 0.0013691528389851253\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.005540086203836836\n",
      "Training Loss: 0.005379933452350088\n",
      "Training Loss: 0.00535287051461637\n",
      "Training Loss: 0.0034095036415965297\n",
      "Training Loss: 0.0005003435630351306\n",
      "Training Loss: 0.0004153519465762656\n",
      "Training Loss: 0.0003924774748884374\n",
      "Validation Loss: 0.0013672932813489941\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.005524823618470691\n",
      "Training Loss: 0.005364431033958681\n",
      "Training Loss: 0.005337613216252066\n",
      "Training Loss: 0.0034012764479120963\n",
      "Training Loss: 0.000497117019694997\n",
      "Training Loss: 0.00041321754204545866\n",
      "Training Loss: 0.0003907376306233345\n",
      "Validation Loss: 0.0013655190685156154\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.005509550731512718\n",
      "Training Loss: 0.0053491050167940556\n",
      "Training Loss: 0.0053223954990971835\n",
      "Training Loss: 0.003393097926164046\n",
      "Training Loss: 0.0004939854125404963\n",
      "Training Loss: 0.0004111441114946501\n",
      "Training Loss: 0.0003890804083857802\n",
      "Validation Loss: 0.0013638201204223615\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.005494290735805407\n",
      "Training Loss: 0.005333966460893862\n",
      "Training Loss: 0.0053072270058328284\n",
      "Training Loss: 0.003384969695180189\n",
      "Training Loss: 0.0004909451179264579\n",
      "Training Loss: 0.0004091273822632502\n",
      "Training Loss: 0.0003874982297202223\n",
      "Validation Loss: 0.0013621840023214159\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.005479054659954272\n",
      "Training Loss: 0.005319022904150188\n",
      "Training Loss: 0.005292124327388592\n",
      "Training Loss: 0.0033768988541851286\n",
      "Training Loss: 0.0004879883331159363\n",
      "Training Loss: 0.0004071604340788326\n",
      "Training Loss: 0.00038598116840148577\n",
      "Validation Loss: 0.0013606021920771056\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.005463873330736533\n",
      "Training Loss: 0.005304292635992169\n",
      "Training Loss: 0.005277103295084089\n",
      "Training Loss: 0.0033688906527095243\n",
      "Training Loss: 0.0004851144961867249\n",
      "Training Loss: 0.000405242004635511\n",
      "Training Loss: 0.00038452420463727324\n",
      "Validation Loss: 0.0013590643590831544\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.005448766615591012\n",
      "Training Loss: 0.005289787518559024\n",
      "Training Loss: 0.0052621857455233114\n",
      "Training Loss: 0.003360954487870913\n",
      "Training Loss: 0.0004823217672674218\n",
      "Training Loss: 0.00040337103975616627\n",
      "Training Loss: 0.00038312207980197857\n",
      "Validation Loss: 0.001357567428852334\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.00543376735469792\n",
      "Training Loss: 0.005275527497869917\n",
      "Training Loss: 0.0052473918010946365\n",
      "Training Loss: 0.003353099007872515\n",
      "Training Loss: 0.00047961302610929126\n",
      "Training Loss: 0.0004015496146894293\n",
      "Training Loss: 0.00038177501566678986\n",
      "Validation Loss: 0.0013561020646687277\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.0054188950668321925\n",
      "Training Loss: 0.005261522503569722\n",
      "Training Loss: 0.005232744081295096\n",
      "Training Loss: 0.0033453371127689026\n",
      "Training Loss: 0.00047699319155071864\n",
      "Training Loss: 0.0003997796896874206\n",
      "Training Loss: 0.00038047987800382543\n",
      "Validation Loss: 0.0013546652664942318\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.00540418112417683\n",
      "Training Loss: 0.0052477945323335\n",
      "Training Loss: 0.005218264865106903\n",
      "Training Loss: 0.0033376778421370544\n",
      "Training Loss: 0.0004744657153059961\n",
      "Training Loss: 0.0003980640098961885\n",
      "Training Loss: 0.00037923843676253456\n",
      "Validation Loss: 0.001353251256564482\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.005389648590935394\n",
      "Training Loss: 0.0052343509148340675\n",
      "Training Loss: 0.005203976587508805\n",
      "Training Loss: 0.0033301326535729457\n",
      "Training Loss: 0.0004720369548886083\n",
      "Training Loss: 0.00039640629027417164\n",
      "Training Loss: 0.00037805004943948007\n",
      "Validation Loss: 0.0013518549916870745\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.005375324529013597\n",
      "Training Loss: 0.005221206986461766\n",
      "Training Loss: 0.0051898994779912755\n",
      "Training Loss: 0.0033227109861036297\n",
      "Training Loss: 0.000469709900644375\n",
      "Training Loss: 0.00039480822120822266\n",
      "Training Loss: 0.00037691344634367853\n",
      "Validation Loss: 0.0013504697027143565\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.0053612339578103275\n",
      "Training Loss: 0.005208376489463262\n",
      "Training Loss: 0.0051760538754751905\n",
      "Training Loss: 0.0033154215355170893\n",
      "Training Loss: 0.0004674876933859196\n",
      "Training Loss: 0.00039327141344983827\n",
      "Training Loss: 0.000375829222430184\n",
      "Validation Loss: 0.0013490924531025358\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.005347395028802566\n",
      "Training Loss: 0.005195863238768652\n",
      "Training Loss: 0.00516245678300038\n",
      "Training Loss: 0.0033082704624393957\n",
      "Training Loss: 0.00046537704787624534\n",
      "Training Loss: 0.0003917996257223422\n",
      "Training Loss: 0.0003747979921172373\n",
      "Validation Loss: 0.0013477158191402823\n",
      "Validation Accuracy: 0.11704119850187265\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.00533382723282557\n",
      "Training Loss: 0.005183673975989222\n",
      "Training Loss: 0.005149123900919221\n",
      "Training Loss: 0.0033012648106523555\n",
      "Training Loss: 0.0004633777063281741\n",
      "Training Loss: 0.0003903924655242008\n",
      "Training Loss: 0.00037381880054454086\n",
      "Validation Loss: 0.0013463364919564296\n",
      "Validation Accuracy: 0.11704119850187265\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.005320545609574765\n",
      "Training Loss: 0.00517181092640385\n",
      "Training Loss: 0.005136064133839682\n",
      "Training Loss: 0.0032944080372544703\n",
      "Training Loss: 0.00046149432673701083\n",
      "Training Loss: 0.0003890514803060796\n",
      "Training Loss: 0.0003728914718158194\n",
      "Validation Loss: 0.001344949262237834\n",
      "Validation Accuracy: 0.11704119850187265\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.005307564321556129\n",
      "Training Loss: 0.005160276376409456\n",
      "Training Loss: 0.005123292281059549\n",
      "Training Loss: 0.003287703452551796\n",
      "Training Loss: 0.00045972373605764007\n",
      "Training Loss: 0.0003877739005474723\n",
      "Training Loss: 0.00037201177525275853\n",
      "Validation Loss: 0.0013435481504553734\n",
      "Validation Accuracy: 0.11704119850187265\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.005294892797246575\n",
      "Training Loss: 0.005149067702004686\n",
      "Training Loss: 0.005110813021892682\n",
      "Training Loss: 0.003281152584022493\n",
      "Training Loss: 0.0004580679151695222\n",
      "Training Loss: 0.00038655925112834666\n",
      "Training Loss: 0.0003711795571871335\n",
      "Validation Loss: 0.0013421297267989312\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0052825389563804495\n",
      "Training Loss: 0.005138182548689656\n",
      "Training Loss: 0.005098632371518761\n",
      "Training Loss: 0.0032747549092164265\n",
      "Training Loss: 0.00045652139124285895\n",
      "Training Loss: 0.00038540350946277615\n",
      "Training Loss: 0.00037039063401607565\n",
      "Validation Loss: 0.0013406913631490156\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.00527050769480411\n",
      "Training Loss: 0.005127614266239106\n",
      "Training Loss: 0.005086751407361589\n",
      "Training Loss: 0.003268509643748985\n",
      "Training Loss: 0.00045508542549214324\n",
      "Training Loss: 0.00038430671229434666\n",
      "Training Loss: 0.00036964372604415986\n",
      "Validation Loss: 0.00133922862634913\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.00525880168133881\n",
      "Training Loss: 0.005117357063572854\n",
      "Training Loss: 0.0050751721725100655\n",
      "Training Loss: 0.00326241471193498\n",
      "Training Loss: 0.0004537513729155762\n",
      "Training Loss: 0.000383261733877589\n",
      "Training Loss: 0.0003689323070284445\n",
      "Validation Loss: 0.0013377401221404898\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.005247422651736997\n",
      "Training Loss: 0.0051074036955833435\n",
      "Training Loss: 0.00506389275600668\n",
      "Training Loss: 0.0032564664809615353\n",
      "Training Loss: 0.00045251565214130094\n",
      "Training Loss: 0.0003822661705271457\n",
      "Training Loss: 0.00036825450799369717\n",
      "Validation Loss: 0.001336224409501977\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.0052363692747894675\n",
      "Training Loss: 0.005097742227371782\n",
      "Training Loss: 0.00505290761240758\n",
      "Training Loss: 0.00325066210469231\n",
      "Training Loss: 0.0004513735944783548\n",
      "Training Loss: 0.00038131560515466847\n",
      "Training Loss: 0.00036760633935045917\n",
      "Validation Loss: 0.0013346805126649983\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.005225636882823892\n",
      "Training Loss: 0.005088365414994769\n",
      "Training Loss: 0.005042214743443765\n",
      "Training Loss: 0.003244997138644976\n",
      "Training Loss: 0.0004503199313330697\n",
      "Training Loss: 0.0003804073697756394\n",
      "Training Loss: 0.000366984083702846\n",
      "Validation Loss: 0.001333110480179508\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.005215222837869078\n",
      "Training Loss: 0.005079262343933806\n",
      "Training Loss: 0.005031805513426662\n",
      "Training Loss: 0.003239467301100376\n",
      "Training Loss: 0.0004493474552873522\n",
      "Training Loss: 0.00037953613929857965\n",
      "Training Loss: 0.00036638390622101725\n",
      "Validation Loss: 0.0013315123383806887\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.005205122621264309\n",
      "Training Loss: 0.005070424006553367\n",
      "Training Loss: 0.005021673122537322\n",
      "Training Loss: 0.0032340695035964016\n",
      "Training Loss: 0.0004484518977551488\n",
      "Training Loss: 0.0003786987142302678\n",
      "Training Loss: 0.0003658021073351847\n",
      "Validation Loss: 0.0013298879174580135\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.0051953290740493685\n",
      "Training Loss: 0.005061838717083446\n",
      "Training Loss: 0.005011810270370915\n",
      "Training Loss: 0.00322879669660324\n",
      "Training Loss: 0.00044762298413843384\n",
      "Training Loss: 0.00037789046502439305\n",
      "Training Loss: 0.0003652343676003511\n",
      "Validation Loss: 0.0013282367527312233\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.005185835133306682\n",
      "Training Loss: 0.0050534955970942975\n",
      "Training Loss: 0.0050022070138948035\n",
      "Training Loss: 0.0032236469378221955\n",
      "Training Loss: 0.0004468596515653189\n",
      "Training Loss: 0.0003771094203693792\n",
      "Training Loss: 0.0003646793551160954\n",
      "Validation Loss: 0.00132656468461422\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.005176632851944305\n",
      "Training Loss: 0.005045385867124424\n",
      "Training Loss: 0.0049928557174280285\n",
      "Training Loss: 0.003218614200413867\n",
      "Training Loss: 0.0004461520216136705\n",
      "Training Loss: 0.00037635078668245113\n",
      "Training Loss: 0.00036413231759070187\n",
      "Validation Loss: 0.0013248698589095954\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0051677150203613565\n",
      "Training Loss: 0.0050374985876260325\n",
      "Training Loss: 0.004983746748766862\n",
      "Training Loss: 0.0032136953428562264\n",
      "Training Loss: 0.00044549700505740474\n",
      "Training Loss: 0.00037561346813163254\n",
      "Training Loss: 0.0003635927333016298\n",
      "Validation Loss: 0.0013231562903654075\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.005159070125082508\n",
      "Training Loss: 0.005029822200303897\n",
      "Training Loss: 0.004974869639845565\n",
      "Training Loss: 0.003208885353542428\n",
      "Training Loss: 0.00044488772444310597\n",
      "Training Loss: 0.00037489320544409566\n",
      "Training Loss: 0.0003630565850107814\n",
      "Validation Loss: 0.0013214263198507012\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.005150691703311168\n",
      "Training Loss: 0.005022349079372362\n",
      "Training Loss: 0.004966214779415168\n",
      "Training Loss: 0.00320418184317532\n",
      "Training Loss: 0.0004443198034277884\n",
      "Training Loss: 0.00037418869302200617\n",
      "Training Loss: 0.0003625229655517614\n",
      "Validation Loss: 0.001319681433459381\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.005142567034927197\n",
      "Training Loss: 0.00501506810891442\n",
      "Training Loss: 0.004957771692425013\n",
      "Training Loss: 0.0031995789005668483\n",
      "Training Loss: 0.0004437893425347283\n",
      "Training Loss: 0.00037349850113969294\n",
      "Training Loss: 0.00036199086243868803\n",
      "Validation Loss: 0.0013179257739091568\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.005134688485995866\n",
      "Training Loss: 0.005007971185259521\n",
      "Training Loss: 0.004949531832826324\n",
      "Training Loss: 0.0031950752935517813\n",
      "Training Loss: 0.0004432914777135011\n",
      "Training Loss: 0.0003728193439746974\n",
      "Training Loss: 0.0003614578907217947\n",
      "Validation Loss: 0.0013161576595000421\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.00512704506458249\n",
      "Training Loss: 0.00500104917504359\n",
      "Training Loss: 0.004941486836178228\n",
      "Training Loss: 0.003190664927496982\n",
      "Training Loss: 0.0004428218751854729\n",
      "Training Loss: 0.0003721505888461252\n",
      "Training Loss: 0.0003609235396288568\n",
      "Validation Loss: 0.0013143819000215307\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.005119626756641083\n",
      "Training Loss: 0.004994294125353917\n",
      "Training Loss: 0.004933626111014746\n",
      "Training Loss: 0.0031863458860243556\n",
      "Training Loss: 0.0004423762126680231\n",
      "Training Loss: 0.0003714904324078816\n",
      "Training Loss: 0.00036038528807694095\n",
      "Validation Loss: 0.0013126012999873978\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.005112425996921957\n",
      "Training Loss: 0.00498769996338524\n",
      "Training Loss: 0.004925942712579854\n",
      "Training Loss: 0.0031821148073504445\n",
      "Training Loss: 0.0004419514432811411\n",
      "Training Loss: 0.0003708372094115475\n",
      "Training Loss: 0.00035984448240924396\n",
      "Validation Loss: 0.0013108166381976157\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.005105426685186103\n",
      "Training Loss: 0.004981253787991591\n",
      "Training Loss: 0.0049184261891059575\n",
      "Training Loss: 0.0031779688239657843\n",
      "Training Loss: 0.0004415467472426826\n",
      "Training Loss: 0.0003701912884571357\n",
      "Training Loss: 0.00035930060870668966\n",
      "Validation Loss: 0.001309030656189412\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.005098623642115854\n",
      "Training Loss: 0.0049749524611979725\n",
      "Training Loss: 0.004911070788512007\n",
      "Training Loss: 0.003173904959967331\n",
      "Training Loss: 0.0004411556092236424\n",
      "Training Loss: 0.0003695491702092113\n",
      "Training Loss: 0.00035875115605449537\n",
      "Validation Loss: 0.0013072450520949916\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.005092007206985727\n",
      "Training Loss: 0.0049687889276538045\n",
      "Training Loss: 0.004903867379180155\n",
      "Training Loss: 0.003169918852236151\n",
      "Training Loss: 0.00044077875354560093\n",
      "Training Loss: 0.00036891245756123683\n",
      "Training Loss: 0.00035819812710542466\n",
      "Validation Loss: 0.0013054590501937202\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.005085566312773153\n",
      "Training Loss: 0.004962754481821321\n",
      "Training Loss: 0.004896808521589264\n",
      "Training Loss: 0.0031660093655500533\n",
      "Training Loss: 0.0004404144080035621\n",
      "Training Loss: 0.00036828061733103825\n",
      "Training Loss: 0.0003576424111815868\n",
      "Validation Loss: 0.0013036780314241389\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.005079291752772406\n",
      "Training Loss: 0.004956843702238985\n",
      "Training Loss: 0.004889887204626575\n",
      "Training Loss: 0.0031621730399183433\n",
      "Training Loss: 0.000440058572057751\n",
      "Training Loss: 0.00036765161268704106\n",
      "Training Loss: 0.0003570810904056998\n",
      "Validation Loss: 0.001301903367617642\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.005073176175355912\n",
      "Training Loss: 0.004951052261167206\n",
      "Training Loss: 0.004883099099970423\n",
      "Training Loss: 0.0031584079501772067\n",
      "Training Loss: 0.00043971116880129555\n",
      "Training Loss: 0.00036702606827020644\n",
      "Training Loss: 0.0003565161677943252\n",
      "Validation Loss: 0.0013001348557286855\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.005067210422130302\n",
      "Training Loss: 0.0049453723069746046\n",
      "Training Loss: 0.0048764343623770405\n",
      "Training Loss: 0.003154710853468714\n",
      "Training Loss: 0.00043937235030170994\n",
      "Training Loss: 0.000366404658780084\n",
      "Training Loss: 0.00035594809480244294\n",
      "Validation Loss: 0.0012983753694335177\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0050613863702164965\n",
      "Training Loss: 0.0049398007430136205\n",
      "Training Loss: 0.004869889441179112\n",
      "Training Loss: 0.003151080210272994\n",
      "Training Loss: 0.0004390362202684628\n",
      "Training Loss: 0.0003657840379673871\n",
      "Training Loss: 0.0003553757141889946\n",
      "Validation Loss: 0.0012966248272523137\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.005055696003837511\n",
      "Training Loss: 0.004934330511605367\n",
      "Training Loss: 0.004863458808977157\n",
      "Training Loss: 0.003147512780597026\n",
      "Training Loss: 0.0004387056491395924\n",
      "Training Loss: 0.00036516688767733284\n",
      "Training Loss: 0.00035479875070450363\n",
      "Validation Loss: 0.0012948850777143202\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0050501330726547165\n",
      "Training Loss: 0.004928960422403179\n",
      "Training Loss: 0.004857137868530117\n",
      "Training Loss: 0.0031440066357117757\n",
      "Training Loss: 0.00043837788965902293\n",
      "Training Loss: 0.00036455212233704515\n",
      "Training Loss: 0.00035421959371888077\n",
      "Validation Loss: 0.001293158848949382\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.005044688598718494\n",
      "Training Loss: 0.004923681208747439\n",
      "Training Loss: 0.0048509193112840875\n",
      "Training Loss: 0.003140559688108624\n",
      "Training Loss: 0.00043805511653772556\n",
      "Training Loss: 0.0003639410311734537\n",
      "Training Loss: 0.00035363863651582505\n",
      "Validation Loss: 0.001291446553997535\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.005039358044159599\n",
      "Training Loss: 0.004918493368895724\n",
      "Training Loss: 0.004844801010913216\n",
      "Training Loss: 0.0031371704649700407\n",
      "Training Loss: 0.00043773320663603954\n",
      "Training Loss: 0.00036333186591946286\n",
      "Training Loss: 0.00035305380280988176\n",
      "Validation Loss: 0.001289747751586938\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.005034135085879825\n",
      "Training Loss: 0.004913389777648262\n",
      "Training Loss: 0.004838777070981451\n",
      "Training Loss: 0.0031338364635666947\n",
      "Training Loss: 0.00043741402339946944\n",
      "Training Loss: 0.0003627263084126753\n",
      "Training Loss: 0.0003524685388947546\n",
      "Validation Loss: 0.0012880657089495689\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.005029013102757745\n",
      "Training Loss: 0.004908368368633092\n",
      "Training Loss: 0.004832844596821814\n",
      "Training Loss: 0.0031305558969052074\n",
      "Training Loss: 0.0004370976613427047\n",
      "Training Loss: 0.0003621242422377691\n",
      "Training Loss: 0.00035188119358281255\n",
      "Validation Loss: 0.0012863986267983366\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.005023987091262825\n",
      "Training Loss: 0.004903424351359717\n",
      "Training Loss: 0.004826998764765449\n",
      "Training Loss: 0.003127328141754333\n",
      "Training Loss: 0.00043678170404746195\n",
      "Training Loss: 0.00036152543896605493\n",
      "Training Loss: 0.00035129211131788905\n",
      "Validation Loss: 0.0012847505211620854\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00501905199023895\n",
      "Training Loss: 0.004898558724089525\n",
      "Training Loss: 0.0048212382604833695\n",
      "Training Loss: 0.003124149994819163\n",
      "Training Loss: 0.00043646755595545984\n",
      "Training Loss: 0.00036092998230742525\n",
      "Training Loss: 0.0003507025654107565\n",
      "Validation Loss: 0.0012831186696336332\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.005014204272301867\n",
      "Training Loss: 0.004893766038585454\n",
      "Training Loss: 0.0048155580152524635\n",
      "Training Loss: 0.003121020430153294\n",
      "Training Loss: 0.00043615449660137527\n",
      "Training Loss: 0.00036033877069712616\n",
      "Training Loss: 0.00035011259093153057\n",
      "Validation Loss: 0.001281507509348091\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.005009438465931453\n",
      "Training Loss: 0.004889042418217287\n",
      "Training Loss: 0.004809956120443531\n",
      "Training Loss: 0.0031179393779621023\n",
      "Training Loss: 0.0004358433844026877\n",
      "Training Loss: 0.00035975306527689097\n",
      "Training Loss: 0.0003495235321315704\n",
      "Validation Loss: 0.0012799139565529655\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.005004749522777274\n",
      "Training Loss: 0.004884385530021973\n",
      "Training Loss: 0.004804428175557405\n",
      "Training Loss: 0.0031149029794141827\n",
      "Training Loss: 0.00043553393061301904\n",
      "Training Loss: 0.00035917163717385847\n",
      "Training Loss: 0.00034893558013209257\n",
      "Validation Loss: 0.0012783425600843047\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.0050001353805419054\n",
      "Training Loss: 0.004879793559084646\n",
      "Training Loss: 0.004798972921562381\n",
      "Training Loss: 0.003111911621372201\n",
      "Training Loss: 0.00043522607007616897\n",
      "Training Loss: 0.00035859492174495245\n",
      "Training Loss: 0.0003483464066266606\n",
      "Validation Loss: 0.001276790239514211\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.00499559345364105\n",
      "Training Loss: 0.004875266962917521\n",
      "Training Loss: 0.004793588747270405\n",
      "Training Loss: 0.0031089638608227687\n",
      "Training Loss: 0.00043491917611390815\n",
      "Training Loss: 0.0003580241647796356\n",
      "Training Loss: 0.00034776078295180925\n",
      "Validation Loss: 0.0012752589007620683\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.004991118361940608\n",
      "Training Loss: 0.00487079955288209\n",
      "Training Loss: 0.004788271191064269\n",
      "Training Loss: 0.003106057965505897\n",
      "Training Loss: 0.000434613783218083\n",
      "Training Loss: 0.0003574594184101443\n",
      "Training Loss: 0.0003471749327763973\n",
      "Validation Loss: 0.0012737466790299157\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.004986708509386517\n",
      "Training Loss: 0.0048663928441237654\n",
      "Training Loss: 0.004783020205795765\n",
      "Training Loss: 0.0031031934135307892\n",
      "Training Loss: 0.00043431164747744335\n",
      "Training Loss: 0.00035690127908310387\n",
      "Training Loss: 0.0003465933681764\n",
      "Validation Loss: 0.0012722574135980548\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00498235936218407\n",
      "Training Loss: 0.004862041606684215\n",
      "Training Loss: 0.004777833184343763\n",
      "Training Loss: 0.0031003692699187015\n",
      "Training Loss: 0.00043401209033618217\n",
      "Training Loss: 0.00035635054226077043\n",
      "Training Loss: 0.0003460139252092631\n",
      "Validation Loss: 0.0012707895479020697\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.004978071350487881\n",
      "Training Loss: 0.004857745337067172\n",
      "Training Loss: 0.00477270687057171\n",
      "Training Loss: 0.00309758506964954\n",
      "Training Loss: 0.0004337152309744852\n",
      "Training Loss: 0.0003558063031960046\n",
      "Training Loss: 0.0003454369635801413\n",
      "Validation Loss: 0.0012693428327335997\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.004973838963778689\n",
      "Training Loss: 0.004853502037585713\n",
      "Training Loss: 0.0047676413116278125\n",
      "Training Loss: 0.003094838133306439\n",
      "Training Loss: 0.0004334214898335631\n",
      "Training Loss: 0.0003552694906466058\n",
      "Training Loss: 0.00034486393891711484\n",
      "Validation Loss: 0.0012679185010268255\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.004969663669471629\n",
      "Training Loss: 0.004849313650047407\n",
      "Training Loss: 0.004762633907375857\n",
      "Training Loss: 0.003092128842431521\n",
      "Training Loss: 0.00043312997015164\n",
      "Training Loss: 0.0003547408176200406\n",
      "Training Loss: 0.0003442937062573037\n",
      "Validation Loss: 0.0012665150609651793\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.0049655410810373725\n",
      "Training Loss: 0.004845174401416443\n",
      "Training Loss: 0.004757683584466576\n",
      "Training Loss: 0.0030894565946073273\n",
      "Training Loss: 0.0004328432882903144\n",
      "Training Loss: 0.0003542208209000819\n",
      "Training Loss: 0.0003437288020359119\n",
      "Validation Loss: 0.001265134833333883\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.004961470089619979\n",
      "Training Loss: 0.004841085767839104\n",
      "Training Loss: 0.004752789323101751\n",
      "Training Loss: 0.0030868193322567093\n",
      "Training Loss: 0.00043255944874545096\n",
      "Training Loss: 0.0003537086403594003\n",
      "Training Loss: 0.0003431675814135815\n",
      "Validation Loss: 0.0012637756394877706\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.004957448140485213\n",
      "Training Loss: 0.004837045178865082\n",
      "Training Loss: 0.004747949158772826\n",
      "Training Loss: 0.0030842174758390684\n",
      "Training Loss: 0.0004322806776690413\n",
      "Training Loss: 0.00035320587878231893\n",
      "Training Loss: 0.0003426116066111717\n",
      "Validation Loss: 0.0012624364155837359\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.004953475243528373\n",
      "Training Loss: 0.004833053492475301\n",
      "Training Loss: 0.004743163184029981\n",
      "Training Loss: 0.0030816506266000943\n",
      "Training Loss: 0.0004320049019588623\n",
      "Training Loss: 0.00035271099332021547\n",
      "Training Loss: 0.0003420602583355503\n",
      "Validation Loss: 0.0012611192806422438\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.004949550593737513\n",
      "Training Loss: 0.0048291078023612495\n",
      "Training Loss: 0.004738429229473695\n",
      "Training Loss: 0.0030791174312071235\n",
      "Training Loss: 0.0004317346400785027\n",
      "Training Loss: 0.00035222606458773953\n",
      "Training Loss: 0.00034151402269344544\n",
      "Validation Loss: 0.0012598231815024931\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.004945670226588845\n",
      "Training Loss: 0.00482520688965451\n",
      "Training Loss: 0.00473374527588021\n",
      "Training Loss: 0.0030766178583098735\n",
      "Training Loss: 0.00043147103486262494\n",
      "Training Loss: 0.0003517520273089758\n",
      "Training Loss: 0.00034097659645340175\n",
      "Validation Loss: 0.0012585502194991938\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.004941831201431341\n",
      "Training Loss: 0.004821348123368807\n",
      "Training Loss: 0.004729110669577494\n",
      "Training Loss: 0.0030741505818650695\n",
      "Training Loss: 0.0004312141367336153\n",
      "Training Loss: 0.0003512882262111816\n",
      "Training Loss: 0.00034044403626467103\n",
      "Validation Loss: 0.00125729490242888\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.004938036830862984\n",
      "Training Loss: 0.004817534720641561\n",
      "Training Loss: 0.00472452579298988\n",
      "Training Loss: 0.0030717158900779393\n",
      "Training Loss: 0.00043096029057778653\n",
      "Training Loss: 0.0003508332872297615\n",
      "Training Loss: 0.00033991691416304095\n",
      "Validation Loss: 0.0012560614171555206\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.004934286315692589\n",
      "Training Loss: 0.004813763573765755\n",
      "Training Loss: 0.004719989013974555\n",
      "Training Loss: 0.0030693113886445644\n",
      "Training Loss: 0.00043071290085208605\n",
      "Training Loss: 0.00035039035601585055\n",
      "Training Loss: 0.0003393982630950632\n",
      "Validation Loss: 0.001254847969215412\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.004930574393365532\n",
      "Training Loss: 0.004810032659443095\n",
      "Training Loss: 0.004715498855803162\n",
      "Training Loss: 0.003066939470199941\n",
      "Training Loss: 0.0004304733299795771\n",
      "Training Loss: 0.00034995708221686073\n",
      "Training Loss: 0.0003388859406550182\n",
      "Validation Loss: 0.0012536545125002387\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.00492690323444549\n",
      "Training Loss: 0.0048063441237900404\n",
      "Training Loss: 0.004711056019295938\n",
      "Training Loss: 0.003064597504817357\n",
      "Training Loss: 0.00043023999340221055\n",
      "Training Loss: 0.00034953532018334955\n",
      "Training Loss: 0.0003383813379514322\n",
      "Validation Loss: 0.0012524800565532385\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.004923272054293193\n",
      "Training Loss: 0.004802695447579026\n",
      "Training Loss: 0.004706658187205903\n",
      "Training Loss: 0.003062286051749652\n",
      "Training Loss: 0.00043001305806683376\n",
      "Training Loss: 0.0003491240033326903\n",
      "Training Loss: 0.0003378832856469671\n",
      "Validation Loss: 0.0012513254317601057\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.004919681135215796\n",
      "Training Loss: 0.0047990868560737\n",
      "Training Loss: 0.004702306329272687\n",
      "Training Loss: 0.0030600043060485405\n",
      "Training Loss: 0.00042979274883691687\n",
      "Training Loss: 0.0003487226759534678\n",
      "Training Loss: 0.0003373928587097907\n",
      "Validation Loss: 0.0012501893878551654\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.0049161260790424426\n",
      "Training Loss: 0.004795515974401496\n",
      "Training Loss: 0.004697998788324185\n",
      "Training Loss: 0.0030577513909520347\n",
      "Training Loss: 0.000429580233321758\n",
      "Training Loss: 0.0003483343528387195\n",
      "Training Loss: 0.00033691098366034567\n",
      "Validation Loss: 0.0012490700053481459\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.00491260877228342\n",
      "Training Loss: 0.0047919856052612885\n",
      "Training Loss: 0.00469373503816314\n",
      "Training Loss: 0.003055527532128508\n",
      "Training Loss: 0.0004293737289481214\n",
      "Training Loss: 0.00034795604555256434\n",
      "Training Loss: 0.00033643652484897755\n",
      "Validation Loss: 0.0012479715879816014\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.004909128005383536\n",
      "Training Loss: 0.004788491379586048\n",
      "Training Loss: 0.004689512940240092\n",
      "Training Loss: 0.0030533325449187032\n",
      "Training Loss: 0.00042917701553960794\n",
      "Training Loss: 0.00034759048134219483\n",
      "Training Loss: 0.00033597102850762894\n",
      "Validation Loss: 0.0012468893333782499\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.0049056825699517505\n",
      "Training Loss: 0.0047850328753702345\n",
      "Training Loss: 0.004685333273955621\n",
      "Training Loss: 0.0030511655706595775\n",
      "Training Loss: 0.00042898672178125707\n",
      "Training Loss: 0.00034723541657513126\n",
      "Training Loss: 0.0003355134212324629\n",
      "Validation Loss: 0.0012458257780540935\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.0049022730684373525\n",
      "Training Loss: 0.004781613188097254\n",
      "Training Loss: 0.004681196091114543\n",
      "Training Loss: 0.0030490257234396267\n",
      "Training Loss: 0.0004288039978200686\n",
      "Training Loss: 0.00034689225771217025\n",
      "Training Loss: 0.0003350650841093739\n",
      "Validation Loss: 0.0012447763513588374\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.004898897254606709\n",
      "Training Loss: 0.004778227592469193\n",
      "Training Loss: 0.0046770995866972954\n",
      "Training Loss: 0.003046913021089495\n",
      "Training Loss: 0.0004286299228624557\n",
      "Training Loss: 0.0003465602846699767\n",
      "Training Loss: 0.0003346234907621692\n",
      "Validation Loss: 0.0012437455766490524\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.004895558332209476\n",
      "Training Loss: 0.004774878901662305\n",
      "Training Loss: 0.004673045396339148\n",
      "Training Loss: 0.0030448281224607853\n",
      "Training Loss: 0.00042846279902732933\n",
      "Training Loss: 0.0003462394432790461\n",
      "Training Loss: 0.00033419117587982325\n",
      "Validation Loss: 0.0012427292662023693\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.0048922505939844995\n",
      "Training Loss: 0.004771563469548709\n",
      "Training Loss: 0.004669029093929566\n",
      "Training Loss: 0.0030427695383014\n",
      "Training Loss: 0.00042830402762774613\n",
      "Training Loss: 0.00034593086600580135\n",
      "Training Loss: 0.00033376773482814316\n",
      "Validation Loss: 0.001241729206647894\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.00488897786475718\n",
      "Training Loss: 0.0047682848607655615\n",
      "Training Loss: 0.004665053826174699\n",
      "Training Loss: 0.003040736461043707\n",
      "Training Loss: 0.0004281538544819341\n",
      "Training Loss: 0.0003456335393275367\n",
      "Training Loss: 0.0003333531366661191\n",
      "Validation Loss: 0.0012407419151999972\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.00488573479349725\n",
      "Training Loss: 0.004765037060133181\n",
      "Training Loss: 0.0046611161920009185\n",
      "Training Loss: 0.0030387299687981793\n",
      "Training Loss: 0.00042801099003554556\n",
      "Training Loss: 0.00034534780301328285\n",
      "Training Loss: 0.0003329486004440696\n",
      "Validation Loss: 0.0012397727939438238\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.004882525581051595\n",
      "Training Loss: 0.004761822009459138\n",
      "Training Loss: 0.004657216649502516\n",
      "Training Loss: 0.003036749261282239\n",
      "Training Loss: 0.0004278780349704903\n",
      "Training Loss: 0.00034507407039200186\n",
      "Training Loss: 0.00033255205751629545\n",
      "Validation Loss: 0.0012388158976453555\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.0048793497122824195\n",
      "Training Loss: 0.004758641965454444\n",
      "Training Loss: 0.004653355096816085\n",
      "Training Loss: 0.003034793406195604\n",
      "Training Loss: 0.00042775173529662424\n",
      "Training Loss: 0.0003448107269468892\n",
      "Training Loss: 0.00033216396699572217\n",
      "Validation Loss: 0.0012378741249425532\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.00487620294559747\n",
      "Training Loss: 0.0047554920392576605\n",
      "Training Loss: 0.004649531270260922\n",
      "Training Loss: 0.0030328620639784275\n",
      "Training Loss: 0.00042763397133967375\n",
      "Training Loss: 0.00034455916387742035\n",
      "Training Loss: 0.00033178473640873565\n",
      "Validation Loss: 0.001236942734772384\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.0048730874719331045\n",
      "Training Loss: 0.004752375762327574\n",
      "Training Loss: 0.004645744659355842\n",
      "Training Loss: 0.0030309552645530857\n",
      "Training Loss: 0.0004275230098573957\n",
      "Training Loss: 0.0003443172936022165\n",
      "Training Loss: 0.0003314130357466638\n",
      "Validation Loss: 0.0012360244618457557\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.004870003552059643\n",
      "Training Loss: 0.00474929075455293\n",
      "Training Loss: 0.004641994686098769\n",
      "Training Loss: 0.003029072826011543\n",
      "Training Loss: 0.00042742196412291376\n",
      "Training Loss: 0.00034408868894388435\n",
      "Training Loss: 0.00033105237464042147\n",
      "Validation Loss: 0.0012351201156470166\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.0048669496428919955\n",
      "Training Loss: 0.004746234508929774\n",
      "Training Loss: 0.004638278195634484\n",
      "Training Loss: 0.0030272139993576274\n",
      "Training Loss: 0.00042732958198030244\n",
      "Training Loss: 0.0003438705112421303\n",
      "Training Loss: 0.0003306999380583875\n",
      "Validation Loss: 0.0012342265699397117\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.004863924039527774\n",
      "Training Loss: 0.004743208485888317\n",
      "Training Loss: 0.004634597405674868\n",
      "Training Loss: 0.0030253781286364756\n",
      "Training Loss: 0.0004272466327165603\n",
      "Training Loss: 0.00034366398340353045\n",
      "Training Loss: 0.00033035687865776706\n",
      "Validation Loss: 0.0012333457419931738\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.004860927537665702\n",
      "Training Loss: 0.004740211705793627\n",
      "Training Loss: 0.00463095027313102\n",
      "Training Loss: 0.003023565028674966\n",
      "Training Loss: 0.0004271677092037862\n",
      "Training Loss: 0.00034346575070230755\n",
      "Training Loss: 0.00033001980562403333\n",
      "Validation Loss: 0.0012324754230092518\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.00485796281893272\n",
      "Training Loss: 0.0047372472559800375\n",
      "Training Loss: 0.004627338779973797\n",
      "Training Loss: 0.00302177521376052\n",
      "Training Loss: 0.000427098654981819\n",
      "Training Loss: 0.0003432801038434263\n",
      "Training Loss: 0.00032969376579785605\n",
      "Validation Loss: 0.0012316162290326486\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.0048550243739737195\n",
      "Training Loss: 0.004734309130581096\n",
      "Training Loss: 0.004623759504756891\n",
      "Training Loss: 0.0030200070089313157\n",
      "Training Loss: 0.00042703761155280515\n",
      "Training Loss: 0.00034310421062400565\n",
      "Training Loss: 0.0003293760177257354\n",
      "Validation Loss: 0.0012307644755811054\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.004852113054366783\n",
      "Training Loss: 0.004731398065341636\n",
      "Training Loss: 0.004620213201269508\n",
      "Training Loss: 0.003018261975312271\n",
      "Training Loss: 0.0004269837582978653\n",
      "Training Loss: 0.0003429389138909755\n",
      "Training Loss: 0.00032906586464378053\n",
      "Validation Loss: 0.0012299251682818292\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.004849230839172378\n",
      "Training Loss: 0.004728514666785486\n",
      "Training Loss: 0.004616699119796976\n",
      "Training Loss: 0.0030165376010654653\n",
      "Training Loss: 0.0004269383729842957\n",
      "Training Loss: 0.0003427833045498119\n",
      "Training Loss: 0.0003287640274538717\n",
      "Validation Loss: 0.0012290937284046692\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.004846376837813296\n",
      "Training Loss: 0.004725661209085956\n",
      "Training Loss: 0.004613218256854452\n",
      "Training Loss: 0.0030148353251706794\n",
      "Training Loss: 0.00042690017147833714\n",
      "Training Loss: 0.00034263821416971043\n",
      "Training Loss: 0.0003284710027037363\n",
      "Validation Loss: 0.0012282716563365628\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.004843549083452672\n",
      "Training Loss: 0.004722831274848432\n",
      "Training Loss: 0.004609767199726775\n",
      "Training Loss: 0.003013153213455553\n",
      "Training Loss: 0.0004268701881665038\n",
      "Training Loss: 0.0003425023210365907\n",
      "Training Loss: 0.000328185746075178\n",
      "Validation Loss: 0.0012274573494729058\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.00484075014712289\n",
      "Training Loss: 0.004720029101590626\n",
      "Training Loss: 0.004606348184170201\n",
      "Training Loss: 0.0030114921755739488\n",
      "Training Loss: 0.000426845497895556\n",
      "Training Loss: 0.0003423761982776341\n",
      "Training Loss: 0.0003279092629782099\n",
      "Validation Loss: 0.00122665184153107\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.004837974652764388\n",
      "Training Loss: 0.004717250255052932\n",
      "Training Loss: 0.0046029578027082605\n",
      "Training Loss: 0.003009851009578597\n",
      "Training Loss: 0.0004268298540773685\n",
      "Training Loss: 0.0003422600467820303\n",
      "Training Loss: 0.00032764031220722245\n",
      "Validation Loss: 0.0012258543041010416\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.004835226059658453\n",
      "Training Loss: 0.004714499223628082\n",
      "Training Loss: 0.004599599645007402\n",
      "Training Loss: 0.00300822986217554\n",
      "Training Loss: 0.0004268204376057838\n",
      "Training Loss: 0.00034215161653264656\n",
      "Training Loss: 0.0003273781230018358\n",
      "Validation Loss: 0.001225061685300113\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.0048325043462682515\n",
      "Training Loss: 0.004711771794827655\n",
      "Training Loss: 0.00459626956784632\n",
      "Training Loss: 0.003006628789353272\n",
      "Training Loss: 0.0004268172211232013\n",
      "Training Loss: 0.0003420527036359999\n",
      "Training Loss: 0.00032712367621570595\n",
      "Validation Loss: 0.0012242788678817347\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.004829808827489615\n",
      "Training Loss: 0.0047090687480522316\n",
      "Training Loss: 0.004592967961798422\n",
      "Training Loss: 0.0030050470800597395\n",
      "Training Loss: 0.0004268234346091049\n",
      "Training Loss: 0.00034196402573797967\n",
      "Training Loss: 0.0003268791467962728\n",
      "Validation Loss: 0.0012235012018140964\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.004827136162202805\n",
      "Training Loss: 0.004706387692131102\n",
      "Training Loss: 0.00458969444676768\n",
      "Training Loss: 0.0030034850437505155\n",
      "Training Loss: 0.0004268358071567491\n",
      "Training Loss: 0.000341882688280748\n",
      "Training Loss: 0.00032664016705894025\n",
      "Validation Loss: 0.0012227308953367086\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.004824489110033028\n",
      "Training Loss: 0.004703730666660704\n",
      "Training Loss: 0.004586449435446411\n",
      "Training Loss: 0.0030019398742797423\n",
      "Training Loss: 0.0004268541251803981\n",
      "Training Loss: 0.00034181018338131253\n",
      "Training Loss: 0.00032640896573866486\n",
      "Validation Loss: 0.001221965269234872\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0048218658758560195\n",
      "Training Loss: 0.004701094577321783\n",
      "Training Loss: 0.004583229816053063\n",
      "Training Loss: 0.003000413974764342\n",
      "Training Loss: 0.00042687888068030586\n",
      "Training Loss: 0.0003417467079634662\n",
      "Training Loss: 0.0003261853525145852\n",
      "Validation Loss: 0.0012212074435454896\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.004819268382270821\n",
      "Training Loss: 0.004698478966020048\n",
      "Training Loss: 0.004580037051346153\n",
      "Training Loss: 0.002998906035240907\n",
      "Training Loss: 0.0004269117055082461\n",
      "Training Loss: 0.000341690694240242\n",
      "Training Loss: 0.00032596947605270546\n",
      "Validation Loss: 0.0012204551394767876\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.004816692631575279\n",
      "Training Loss: 0.004695885943365283\n",
      "Training Loss: 0.004576871736207977\n",
      "Training Loss: 0.0029974156445996413\n",
      "Training Loss: 0.0004269488303907565\n",
      "Training Loss: 0.0003416423724229389\n",
      "Training Loss: 0.00032575930255916317\n",
      "Validation Loss: 0.0012197053318109437\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.004814141236711294\n",
      "Training Loss: 0.004693313972093165\n",
      "Training Loss: 0.004573730807169341\n",
      "Training Loss: 0.0029959431338875218\n",
      "Training Loss: 0.0004269934565672884\n",
      "Training Loss: 0.0003416024732905498\n",
      "Training Loss: 0.0003255574010836426\n",
      "Validation Loss: 0.001218962586088732\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.004811611380428076\n",
      "Training Loss: 0.004690761363017373\n",
      "Training Loss: 0.0045706145226722585\n",
      "Training Loss: 0.0029944873928980085\n",
      "Training Loss: 0.00042704493484052364\n",
      "Training Loss: 0.0003415705850966333\n",
      "Training Loss: 0.0003253624224817031\n",
      "Validation Loss: 0.0012182241469704065\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.004809107495239004\n",
      "Training Loss: 0.004688227781443856\n",
      "Training Loss: 0.004567523617297411\n",
      "Training Loss: 0.0029930488121863164\n",
      "Training Loss: 0.0004271010752563598\n",
      "Training Loss: 0.00034154577617300674\n",
      "Training Loss: 0.00032517447989448555\n",
      "Validation Loss: 0.0012174900668728372\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.004806620981544257\n",
      "Training Loss: 0.004685711952624842\n",
      "Training Loss: 0.004564456437365152\n",
      "Training Loss: 0.002991627185242578\n",
      "Training Loss: 0.00042716284755442755\n",
      "Training Loss: 0.00034152736654505136\n",
      "Training Loss: 0.0003249919788504485\n",
      "Validation Loss: 0.0012167602668096459\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.004804160495405085\n",
      "Training Loss: 0.0046832166548119855\n",
      "Training Loss: 0.004561413236660883\n",
      "Training Loss: 0.002990220780106938\n",
      "Training Loss: 0.00042723038746771635\n",
      "Training Loss: 0.0003415164470425225\n",
      "Training Loss: 0.000324816788888711\n",
      "Validation Loss: 0.0012160346494546799\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.004801720815594308\n",
      "Training Loss: 0.004680738318129442\n",
      "Training Loss: 0.0045583915332099425\n",
      "Training Loss: 0.002988829725513824\n",
      "Training Loss: 0.00042730601693619975\n",
      "Training Loss: 0.00034151410998674693\n",
      "Training Loss: 0.00032464896963574575\n",
      "Validation Loss: 0.0012153128486801174\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.004799302545143292\n",
      "Training Loss: 0.0046782750787679106\n",
      "Training Loss: 0.004555392361362464\n",
      "Training Loss: 0.0029874552249066254\n",
      "Training Loss: 0.00042738608506624585\n",
      "Training Loss: 0.0003415177323222451\n",
      "Training Loss: 0.0003244868843285076\n",
      "Validation Loss: 0.0012145946229623163\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.004796902990783565\n",
      "Training Loss: 0.0046758297056658195\n",
      "Training Loss: 0.004552416568621993\n",
      "Training Loss: 0.002986095980131722\n",
      "Training Loss: 0.00042747007657453653\n",
      "Training Loss: 0.0003415274468534335\n",
      "Training Loss: 0.00032433114936793574\n",
      "Validation Loss: 0.0012138795395861797\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.004794525941251777\n",
      "Training Loss: 0.004673399620805867\n",
      "Training Loss: 0.004549460658454336\n",
      "Training Loss: 0.002984752053143893\n",
      "Training Loss: 0.000427561991600669\n",
      "Training Loss: 0.00034154438491896145\n",
      "Training Loss: 0.00032418184529888096\n",
      "Validation Loss: 0.0012131689052264929\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.004792167822015472\n",
      "Training Loss: 0.004670984658878297\n",
      "Training Loss: 0.004546525760670193\n",
      "Training Loss: 0.0029834228265190176\n",
      "Training Loss: 0.0004276569668581942\n",
      "Training Loss: 0.00034156640949731807\n",
      "Training Loss: 0.0003240376840767567\n",
      "Validation Loss: 0.00121246207784131\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.0047898334806086495\n",
      "Training Loss: 0.004668586181942373\n",
      "Training Loss: 0.004543613289133646\n",
      "Training Loss: 0.0029821071595188186\n",
      "Training Loss: 0.0004277564780204557\n",
      "Training Loss: 0.0003415953125113447\n",
      "Training Loss: 0.00032390041629696496\n",
      "Validation Loss: 0.0012117580139847325\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.00478751567483414\n",
      "Training Loss: 0.0046662001684308054\n",
      "Training Loss: 0.004540720558725298\n",
      "Training Loss: 0.002980806519117323\n",
      "Training Loss: 0.00042786288940988015\n",
      "Training Loss: 0.0003416305044993351\n",
      "Training Loss: 0.0003237690741480037\n",
      "Validation Loss: 0.0012110562633240354\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.004785217829048633\n",
      "Training Loss: 0.004663828609627671\n",
      "Training Loss: 0.004537847201572731\n",
      "Training Loss: 0.0029795201415026896\n",
      "Training Loss: 0.00042797395140951265\n",
      "Training Loss: 0.0003416708949407621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [24:45<57:56, 496.60s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00032364313754442266\n",
      "Validation Loss: 0.0012103567804594233\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.3924433323740959\n",
      "Training Loss: 0.24746670018881559\n",
      "Training Loss: 0.1129312271811068\n",
      "Training Loss: 0.05756975944561418\n",
      "Training Loss: 0.05184574466664344\n",
      "Training Loss: 0.047652226476930085\n",
      "Training Loss: 0.047502950797788795\n",
      "Validation Loss: 0.046736277200365335\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.048619393277913335\n",
      "Training Loss: 0.04678236362524331\n",
      "Training Loss: 0.04431066205725074\n",
      "Training Loss: 0.03920914492337033\n",
      "Training Loss: 0.03679335008841008\n",
      "Training Loss: 0.03254891183227301\n",
      "Training Loss: 0.032726947730407116\n",
      "Validation Loss: 0.03268221611931418\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.03491463930346072\n",
      "Training Loss: 0.033530050460249185\n",
      "Training Loss: 0.031173411346971987\n",
      "Training Loss: 0.026774085924844256\n",
      "Training Loss: 0.02356450374936685\n",
      "Training Loss: 0.020347384873311967\n",
      "Training Loss: 0.02018533576047048\n",
      "Validation Loss: 0.02069634534697631\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.02404996772762388\n",
      "Training Loss: 0.023284670189023016\n",
      "Training Loss: 0.021462555369362236\n",
      "Training Loss: 0.01766048916062573\n",
      "Training Loss: 0.014111105834599584\n",
      "Training Loss: 0.012219106694683433\n",
      "Training Loss: 0.01200450552161783\n",
      "Validation Loss: 0.013117474918899018\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.017295113606378434\n",
      "Training Loss: 0.017091362201608718\n",
      "Training Loss: 0.015911374010611327\n",
      "Training Loss: 0.0125406551049673\n",
      "Training Loss: 0.0089327474986203\n",
      "Training Loss: 0.008033454646356403\n",
      "Training Loss: 0.00791660211980343\n",
      "Validation Loss: 0.009811716467710665\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.013898199340328574\n",
      "Training Loss: 0.013947503296658397\n",
      "Training Loss: 0.013195122245233505\n",
      "Training Loss: 0.010009993500716519\n",
      "Training Loss: 0.006425837010610848\n",
      "Training Loss: 0.005980858019320294\n",
      "Training Loss: 0.005892782645532862\n",
      "Validation Loss: 0.008239536001591172\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.012039015810005367\n",
      "Training Loss: 0.012149326392682269\n",
      "Training Loss: 0.011637778002768755\n",
      "Training Loss: 0.008557559890614358\n",
      "Training Loss: 0.0049985770432977\n",
      "Training Loss: 0.0047645029955310746\n",
      "Training Loss: 0.00468240903574042\n",
      "Validation Loss: 0.007020711656816806\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.010799601238686592\n",
      "Training Loss: 0.010926423326600343\n",
      "Training Loss: 0.010555105241946876\n",
      "Training Loss: 0.007583345770399319\n",
      "Training Loss: 0.0040435697755310685\n",
      "Training Loss: 0.003937234007171355\n",
      "Training Loss: 0.0038535866932943464\n",
      "Validation Loss: 0.005953646418010669\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.00988463922869414\n",
      "Training Loss: 0.010024156454019249\n",
      "Training Loss: 0.009729144751327112\n",
      "Training Loss: 0.0068750431378430225\n",
      "Training Loss: 0.0033589862001826985\n",
      "Training Loss: 0.0033324709511362016\n",
      "Training Loss: 0.0032407489768229423\n",
      "Validation Loss: 0.005062541943819572\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.00917782744858414\n",
      "Training Loss: 0.009334406259004027\n",
      "Training Loss: 0.009081106436206028\n",
      "Training Loss: 0.006338677775493125\n",
      "Training Loss: 0.0028519074860378166\n",
      "Training Loss: 0.002870210291584954\n",
      "Training Loss: 0.002772234273434151\n",
      "Validation Loss: 0.004357428589708447\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.008627464460441842\n",
      "Training Loss: 0.008809044623048976\n",
      "Training Loss: 0.008585546609247104\n",
      "Training Loss: 0.00593532766593853\n",
      "Training Loss: 0.002473987022822257\n",
      "Training Loss: 0.0025160226464504376\n",
      "Training Loss: 0.002416087401797995\n",
      "Validation Loss: 0.003821204767879866\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.008211584337987006\n",
      "Training Loss: 0.008424678919836878\n",
      "Training Loss: 0.008227575309574605\n",
      "Training Loss: 0.005643038218549918\n",
      "Training Loss: 0.0021958100283518433\n",
      "Training Loss: 0.002250485062249936\n",
      "Training Loss: 0.0021501833412912673\n",
      "Validation Loss: 0.0034268456382575804\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.007910250803688542\n",
      "Training Loss: 0.008154606549069285\n",
      "Training Loss: 0.007980485847219825\n",
      "Training Loss: 0.0054365644746576436\n",
      "Training Loss: 0.0019936084136134014\n",
      "Training Loss: 0.0020554905870812947\n",
      "Training Loss: 0.0019545639518764803\n",
      "Validation Loss: 0.003144713176880032\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.0076958976313471796\n",
      "Training Loss: 0.00796464256127365\n",
      "Training Loss: 0.007808887225110084\n",
      "Training Loss: 0.005289269208587939\n",
      "Training Loss: 0.001848136977350805\n",
      "Training Loss: 0.0019142271966848057\n",
      "Training Loss: 0.0018124043761054054\n",
      "Validation Loss: 0.002945648788819822\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.0075393930752761665\n",
      "Training Loss: 0.007823252094676719\n",
      "Training Loss: 0.0076817622117232535\n",
      "Training Loss: 0.005179976813451503\n",
      "Training Loss: 0.0017448466666974128\n",
      "Training Loss: 0.00181280739911017\n",
      "Training Loss: 0.0017104437590751331\n",
      "Validation Loss: 0.0028048753748213602\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.0074179504322819416\n",
      "Training Loss: 0.0077086381218396125\n",
      "Training Loss: 0.007578789384569973\n",
      "Training Loss: 0.005094793287862558\n",
      "Training Loss: 0.0016728248502477071\n",
      "Training Loss: 0.0017405339320248458\n",
      "Training Loss: 0.0016384609050874132\n",
      "Validation Loss: 0.002703874012235594\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.007317261743592099\n",
      "Training Loss: 0.0076085614738985894\n",
      "Training Loss: 0.007488997830078006\n",
      "Training Loss: 0.005025480131589575\n",
      "Training Loss: 0.0016238898468145635\n",
      "Training Loss: 0.0016895160511194262\n",
      "Training Loss: 0.001588729268842144\n",
      "Validation Loss: 0.002629875710468621\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.007229594254167751\n",
      "Training Loss: 0.007517179743153975\n",
      "Training Loss: 0.0074072713777422906\n",
      "Training Loss: 0.004967357374553103\n",
      "Training Loss: 0.0015919796346861403\n",
      "Training Loss: 0.001654075163678499\n",
      "Training Loss: 0.0015555155278707388\n",
      "Validation Loss: 0.002574433697834925\n",
      "Validation Accuracy: 0.11704119850187265\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.007151199417421594\n",
      "Training Loss: 0.007432176661677658\n",
      "Training Loss: 0.007331591617548839\n",
      "Training Loss: 0.0049177710966614545\n",
      "Training Loss: 0.0015726282983087004\n",
      "Training Loss: 0.001630130410048878\n",
      "Training Loss: 0.0015345740725751966\n",
      "Validation Loss: 0.0025320217367183167\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.007080414197407663\n",
      "Training Loss: 0.007352931842906401\n",
      "Training Loss: 0.007261401289142668\n",
      "Training Loss: 0.0048751647668541405\n",
      "Training Loss: 0.0015625337585515808\n",
      "Training Loss: 0.0016147139805252663\n",
      "Training Loss: 0.0015227341308491306\n",
      "Validation Loss: 0.0024990217612179044\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.007016549261752516\n",
      "Training Loss: 0.007279515713453293\n",
      "Training Loss: 0.007196729065617547\n",
      "Training Loss: 0.004838536253955681\n",
      "Training Loss: 0.0015591988290543667\n",
      "Training Loss: 0.001605614333820995\n",
      "Training Loss: 0.0015175724886648822\n",
      "Validation Loss: 0.002473017913380931\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.006959295482374728\n",
      "Training Loss: 0.007212176605826244\n",
      "Training Loss: 0.007137746121734381\n",
      "Training Loss: 0.004807148924592184\n",
      "Training Loss: 0.0015606952243251726\n",
      "Training Loss: 0.0016011334721406456\n",
      "Training Loss: 0.0015171960250881967\n",
      "Validation Loss: 0.0024523614704772857\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.006908423813292756\n",
      "Training Loss: 0.00715109797893092\n",
      "Training Loss: 0.007084561705123633\n",
      "Training Loss: 0.004780382264725631\n",
      "Training Loss: 0.0015654963463020977\n",
      "Training Loss: 0.0015999406662012916\n",
      "Training Loss: 0.0015201041789259762\n",
      "Validation Loss: 0.002435864156246367\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.006863649169681594\n",
      "Training Loss: 0.0070962973323185\n",
      "Training Loss: 0.007037137073930353\n",
      "Training Loss: 0.004757664239004953\n",
      "Training Loss: 0.001572386553452816\n",
      "Training Loss: 0.0016009784792549908\n",
      "Training Loss: 0.0015251025363977532\n",
      "Validation Loss: 0.002422640792236747\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.006824592344928533\n",
      "Training Loss: 0.0070476051850710065\n",
      "Training Loss: 0.00699526458629407\n",
      "Training Loss: 0.004738444250324392\n",
      "Training Loss: 0.0015804015829053242\n",
      "Training Loss: 0.0016034092426707503\n",
      "Training Loss: 0.0015312498941784725\n",
      "Validation Loss: 0.0024119918112562836\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.006790783262113109\n",
      "Training Loss: 0.0070046949479728934\n",
      "Training Loss: 0.006958601750666276\n",
      "Training Loss: 0.004722196560614975\n",
      "Training Loss: 0.0015887895954074338\n",
      "Training Loss: 0.0016065758544573329\n",
      "Training Loss: 0.0015378240530844777\n",
      "Validation Loss: 0.0024033502132430403\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.006761689773993567\n",
      "Training Loss: 0.0069671106745954605\n",
      "Training Loss: 0.0069266939396038655\n",
      "Training Loss: 0.004708426383440383\n",
      "Training Loss: 0.0015969976644555572\n",
      "Training Loss: 0.0016099882425623946\n",
      "Training Loss: 0.0015442962330416777\n",
      "Validation Loss: 0.002396257151457157\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.0067367467808071525\n",
      "Training Loss: 0.006934327152557671\n",
      "Training Loss: 0.006899034674279392\n",
      "Training Loss: 0.004696687227042275\n",
      "Training Loss: 0.0016046349583484698\n",
      "Training Loss: 0.0016132925295096357\n",
      "Training Loss: 0.0015503022595657967\n",
      "Validation Loss: 0.002390338227301941\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.0067153985716868194\n",
      "Training Loss: 0.006905784987611696\n",
      "Training Loss: 0.00687509475974366\n",
      "Training Loss: 0.004686587725445861\n",
      "Training Loss: 0.0016114579304121435\n",
      "Training Loss: 0.001616255928238388\n",
      "Training Loss: 0.0015556171794014516\n",
      "Validation Loss: 0.00238529650518655\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006697111879475415\n",
      "Training Loss: 0.00688092544209212\n",
      "Training Loss: 0.006854355033719912\n",
      "Training Loss: 0.004677793747905525\n",
      "Training Loss: 0.001617338772630319\n",
      "Training Loss: 0.001618740550838993\n",
      "Training Loss: 0.001560123610397568\n",
      "Validation Loss: 0.0023809039120170836\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006681403296533972\n",
      "Training Loss: 0.0068592244375031445\n",
      "Training Loss: 0.006836335718398914\n",
      "Training Loss: 0.004670029200860881\n",
      "Training Loss: 0.0016222342105174903\n",
      "Training Loss: 0.0016206808892457048\n",
      "Training Loss: 0.0015637837354734074\n",
      "Validation Loss: 0.002376983097661984\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006667840639129281\n",
      "Training Loss: 0.0068401988549157975\n",
      "Training Loss: 0.006820603589294478\n",
      "Training Loss: 0.004663072288967669\n",
      "Training Loss: 0.0016261644853511825\n",
      "Training Loss: 0.0016220638884260551\n",
      "Training Loss: 0.001566616541822441\n",
      "Validation Loss: 0.0023734053923972543\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.00665605025482364\n",
      "Training Loss: 0.006823421556036919\n",
      "Training Loss: 0.006806772043928504\n",
      "Training Loss: 0.004656741071376019\n",
      "Training Loss: 0.0016291943471878768\n",
      "Training Loss: 0.001622912525490392\n",
      "Training Loss: 0.0015686781820841134\n",
      "Validation Loss: 0.002370084661856112\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006645714455517009\n",
      "Training Loss: 0.006808518596226349\n",
      "Training Loss: 0.0067945170996244995\n",
      "Training Loss: 0.004650899261614541\n",
      "Training Loss: 0.0016314033215167\n",
      "Training Loss: 0.0016232692990888608\n",
      "Training Loss: 0.0015700406685937196\n",
      "Validation Loss: 0.0023669517631856167\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006636563878273592\n",
      "Training Loss: 0.006795170027762651\n",
      "Training Loss: 0.0067835580697283146\n",
      "Training Loss: 0.0046454363977682074\n",
      "Training Loss: 0.0016328900493681432\n",
      "Training Loss: 0.0016231882813735866\n",
      "Training Loss: 0.0015707872626080643\n",
      "Validation Loss: 0.0023639617769670245\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.006628378666937352\n",
      "Training Loss: 0.006783106625080109\n",
      "Training Loss: 0.006773665670771152\n",
      "Training Loss: 0.0046402690919057936\n",
      "Training Loss: 0.001633746267616516\n",
      "Training Loss: 0.0016227246540802297\n",
      "Training Loss: 0.0015709966815484222\n",
      "Validation Loss: 0.002361084877605784\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006620978070423007\n",
      "Training Loss: 0.006772106635617092\n",
      "Training Loss: 0.006764649552060291\n",
      "Training Loss: 0.004635330706078094\n",
      "Training Loss: 0.001634059326024726\n",
      "Training Loss: 0.0016219331487081945\n",
      "Training Loss: 0.0015707471944915597\n",
      "Validation Loss: 0.0023583003817617106\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006614209921099246\n",
      "Training Loss: 0.0067619781452231105\n",
      "Training Loss: 0.0067563488543964925\n",
      "Training Loss: 0.0046305722233228154\n",
      "Training Loss: 0.0016339149260602426\n",
      "Training Loss: 0.0016208671111962757\n",
      "Training Loss: 0.001570109573949594\n",
      "Validation Loss: 0.002355593532484726\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006607955830404535\n",
      "Training Loss: 0.006752571152755991\n",
      "Training Loss: 0.006748640851583332\n",
      "Training Loss: 0.004625954603470745\n",
      "Training Loss: 0.0016333828111237381\n",
      "Training Loss: 0.0016195685826824048\n",
      "Training Loss: 0.0015691420742950867\n",
      "Validation Loss: 0.002352950288702678\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006602115714922547\n",
      "Training Loss: 0.006743755876086653\n",
      "Training Loss: 0.0067414179793559015\n",
      "Training Loss: 0.004621447273748345\n",
      "Training Loss: 0.001632525854947744\n",
      "Training Loss: 0.001618077300809091\n",
      "Training Loss: 0.0015678953830501996\n",
      "Validation Loss: 0.0023503626763475326\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.00659661483601667\n",
      "Training Loss: 0.006735434134025126\n",
      "Training Loss: 0.006734600521158427\n",
      "Training Loss: 0.004617028287684661\n",
      "Training Loss: 0.0016313986653403844\n",
      "Training Loss: 0.0016164285767445107\n",
      "Training Loss: 0.0015664170190575533\n",
      "Validation Loss: 0.0023478252286953005\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006591386469081044\n",
      "Training Loss: 0.006727519668638706\n",
      "Training Loss: 0.006728121215710417\n",
      "Training Loss: 0.004612678063786006\n",
      "Training Loss: 0.0016300414149009157\n",
      "Training Loss: 0.001614646164671285\n",
      "Training Loss: 0.001564737909793621\n",
      "Validation Loss: 0.0023453293991391112\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006586384220281616\n",
      "Training Loss: 0.006719948765821755\n",
      "Training Loss: 0.006721927431644872\n",
      "Training Loss: 0.004608385178507888\n",
      "Training Loss: 0.0016284942827769556\n",
      "Training Loss: 0.0016127559331653174\n",
      "Training Loss: 0.0015628906898200513\n",
      "Validation Loss: 0.0023428700142823448\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006581565598025918\n",
      "Training Loss: 0.00671266479883343\n",
      "Training Loss: 0.006715977404965088\n",
      "Training Loss: 0.004604137092464953\n",
      "Training Loss: 0.0016267858377250377\n",
      "Training Loss: 0.0016107753870164743\n",
      "Training Loss: 0.0015608992183115333\n",
      "Validation Loss: 0.0023404421113223913\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006576901025837288\n",
      "Training Loss: 0.006705626611365006\n",
      "Training Loss: 0.00671023843344301\n",
      "Training Loss: 0.004599927819217555\n",
      "Training Loss: 0.0016249444939603563\n",
      "Training Loss: 0.001608721747106756\n",
      "Training Loss: 0.0015587843059620353\n",
      "Validation Loss: 0.0023380418673566673\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006572364096064121\n",
      "Training Loss: 0.006698797680437565\n",
      "Training Loss: 0.006704684826545417\n",
      "Training Loss: 0.004595752266759519\n",
      "Training Loss: 0.0016229885786015075\n",
      "Training Loss: 0.0016066063898324502\n",
      "Training Loss: 0.0015565595705993474\n",
      "Validation Loss: 0.0023356646479544326\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006567938987864182\n",
      "Training Loss: 0.006692154593765736\n",
      "Training Loss: 0.006699296748265624\n",
      "Training Loss: 0.004591605317837093\n",
      "Training Loss: 0.0016209371808508877\n",
      "Training Loss: 0.0016044402679835912\n",
      "Training Loss: 0.0015542412453214637\n",
      "Validation Loss: 0.002333305444368266\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.00656360647873953\n",
      "Training Loss: 0.0066856725059915335\n",
      "Training Loss: 0.006694057629210874\n",
      "Training Loss: 0.004587485510710394\n",
      "Training Loss: 0.0016188019653782248\n",
      "Training Loss: 0.001602232457807986\n",
      "Training Loss: 0.0015518396612606011\n",
      "Validation Loss: 0.002330962145044416\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006559359937673434\n",
      "Training Loss: 0.00667933787452057\n",
      "Training Loss: 0.006688956452999264\n",
      "Training Loss: 0.00458339184660872\n",
      "Training Loss: 0.0016165969127905555\n",
      "Training Loss: 0.0015999898060545092\n",
      "Training Loss: 0.001549363216618076\n",
      "Validation Loss: 0.0023286334081000933\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006555189180653542\n",
      "Training Loss: 0.006673135018208995\n",
      "Training Loss: 0.006683983184630051\n",
      "Training Loss: 0.004579324447840918\n",
      "Training Loss: 0.0016143317279056647\n",
      "Training Loss: 0.001597718484190409\n",
      "Training Loss: 0.0015468198638700415\n",
      "Validation Loss: 0.002326313472012396\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006551087892148644\n",
      "Training Loss: 0.0066670568310655655\n",
      "Training Loss: 0.00667913094162941\n",
      "Training Loss: 0.0045752841674402585\n",
      "Training Loss: 0.001612013905250933\n",
      "Training Loss: 0.0015954230150964576\n",
      "Training Loss: 0.00154421757964883\n",
      "Validation Loss: 0.0023240024863335927\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006547051339875907\n",
      "Training Loss: 0.006661091675050556\n",
      "Training Loss: 0.0066743917495477945\n",
      "Training Loss: 0.004571269494772423\n",
      "Training Loss: 0.0016096536828263197\n",
      "Training Loss: 0.001593109559325967\n",
      "Training Loss: 0.0015415633331576828\n",
      "Validation Loss: 0.002321697906168066\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006543075228109956\n",
      "Training Loss: 0.006655233213678002\n",
      "Training Loss: 0.006669757636263967\n",
      "Training Loss: 0.004567284650111105\n",
      "Training Loss: 0.0016072557598818093\n",
      "Training Loss: 0.0015907802031870234\n",
      "Training Loss: 0.001538861156150233\n",
      "Validation Loss: 0.002319397309248698\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006539158389205113\n",
      "Training Loss: 0.006649477672763169\n",
      "Training Loss: 0.006665226912591606\n",
      "Training Loss: 0.004563328442600323\n",
      "Training Loss: 0.001604829622519901\n",
      "Training Loss: 0.001588442758875317\n",
      "Training Loss: 0.0015361217643658164\n",
      "Validation Loss: 0.0023171057039414\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006535293150227517\n",
      "Training Loss: 0.006643811333924532\n",
      "Training Loss: 0.006660787760047242\n",
      "Training Loss: 0.004559403603561805\n",
      "Training Loss: 0.001602380046097096\n",
      "Training Loss: 0.0015860981574223844\n",
      "Training Loss: 0.0015333486055897083\n",
      "Validation Loss: 0.002314819967743072\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.006531481661368161\n",
      "Training Loss: 0.006638234367128462\n",
      "Training Loss: 0.006656438196077943\n",
      "Training Loss: 0.004555509976271423\n",
      "Training Loss: 0.0015999139967607335\n",
      "Training Loss: 0.0015837498939072247\n",
      "Training Loss: 0.001530547142465366\n",
      "Validation Loss: 0.0023125376881968774\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006527718873694539\n",
      "Training Loss: 0.006632739214692265\n",
      "Training Loss: 0.006652170077431947\n",
      "Training Loss: 0.004551649092682055\n",
      "Training Loss: 0.0015974350468604826\n",
      "Training Loss: 0.0015814027894521133\n",
      "Training Loss: 0.0015277245178003794\n",
      "Validation Loss: 0.0023102634774424197\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.00652400246122852\n",
      "Training Loss: 0.006627318233950063\n",
      "Training Loss: 0.006647975639207288\n",
      "Training Loss: 0.004547820702282479\n",
      "Training Loss: 0.0015949491472565568\n",
      "Training Loss: 0.001579057308845222\n",
      "Training Loss: 0.0015248837393301074\n",
      "Validation Loss: 0.0023079927996422726\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006520328398328275\n",
      "Training Loss: 0.0066219661256764085\n",
      "Training Loss: 0.006643849267857149\n",
      "Training Loss: 0.004544026537623722\n",
      "Training Loss: 0.0015924616370466538\n",
      "Training Loss: 0.0015767169682658277\n",
      "Training Loss: 0.0015220319989020937\n",
      "Validation Loss: 0.0023057305325766197\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006516694846795872\n",
      "Training Loss: 0.006616675104014575\n",
      "Training Loss: 0.006639781552366913\n",
      "Training Loss: 0.004540263022936415\n",
      "Training Loss: 0.0015899780792096863\n",
      "Training Loss: 0.0015743853156163822\n",
      "Training Loss: 0.0015191754064289853\n",
      "Validation Loss: 0.002303475059674833\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006513095787959174\n",
      "Training Loss: 0.0066114387300331146\n",
      "Training Loss: 0.006635765180690214\n",
      "Training Loss: 0.004536531302510412\n",
      "Training Loss: 0.0015874996389175066\n",
      "Training Loss: 0.0015720631088333903\n",
      "Training Loss: 0.001516315152257448\n",
      "Validation Loss: 0.0023012268883463916\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006509529714239761\n",
      "Training Loss: 0.006606249684700742\n",
      "Training Loss: 0.0066317948838695885\n",
      "Training Loss: 0.004532830120006111\n",
      "Training Loss: 0.001585029474881594\n",
      "Training Loss: 0.0015697517394437455\n",
      "Training Loss: 0.0015134560399746987\n",
      "Validation Loss: 0.0022989870159152532\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006505994803737849\n",
      "Training Loss: 0.006601102079730481\n",
      "Training Loss: 0.00662786046974361\n",
      "Training Loss: 0.004529157972283429\n",
      "Training Loss: 0.0015825711900833994\n",
      "Training Loss: 0.001567451122100465\n",
      "Training Loss: 0.0015106003891560249\n",
      "Validation Loss: 0.0022967526189283526\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.0065024828724563125\n",
      "Training Loss: 0.006595988281769678\n",
      "Training Loss: 0.0066239550663158295\n",
      "Training Loss: 0.004525512374311802\n",
      "Training Loss: 0.001580126907574595\n",
      "Training Loss: 0.0015651637618429959\n",
      "Training Loss: 0.0015077542532526422\n",
      "Validation Loss: 0.0022945278765649398\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.0064989935630001125\n",
      "Training Loss: 0.006590899418806657\n",
      "Training Loss: 0.006620072409277782\n",
      "Training Loss: 0.004521892001939705\n",
      "Training Loss: 0.0015776986681157723\n",
      "Training Loss: 0.0015628908093640347\n",
      "Training Loss: 0.0015049175954482052\n",
      "Validation Loss: 0.0022923117560205443\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006495520377065986\n",
      "Training Loss: 0.006585832462878898\n",
      "Training Loss: 0.006616204365855083\n",
      "Training Loss: 0.004518293041037395\n",
      "Training Loss: 0.0015752857673942344\n",
      "Training Loss: 0.0015606293230666778\n",
      "Training Loss: 0.001502090304129524\n",
      "Validation Loss: 0.0022901013966720404\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.00649206308182329\n",
      "Training Loss: 0.006580780486110598\n",
      "Training Loss: 0.006612346260808409\n",
      "Training Loss: 0.00451471368141938\n",
      "Training Loss: 0.0015728893287450774\n",
      "Training Loss: 0.0015583804898778907\n",
      "Training Loss: 0.0014992753615661058\n",
      "Validation Loss: 0.0022878983924719105\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.006488615634152666\n",
      "Training Loss: 0.0065757366246543825\n",
      "Training Loss: 0.006608492722734809\n",
      "Training Loss: 0.004511152318300447\n",
      "Training Loss: 0.0015705093664291781\n",
      "Training Loss: 0.0015561442599573638\n",
      "Training Loss: 0.0014964734221575781\n",
      "Validation Loss: 0.002285702568960676\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006485174779081717\n",
      "Training Loss: 0.006570696029812097\n",
      "Training Loss: 0.006604634709656238\n",
      "Training Loss: 0.004507604081154568\n",
      "Training Loss: 0.0015681469531409675\n",
      "Training Loss: 0.0015539202996296807\n",
      "Training Loss: 0.001493684206652688\n",
      "Validation Loss: 0.002283512656415607\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.0064817382325418295\n",
      "Training Loss: 0.006565653252182529\n",
      "Training Loss: 0.006600772187812254\n",
      "Training Loss: 0.004504068546521012\n",
      "Training Loss: 0.0015657979923707898\n",
      "Training Loss: 0.0015517049652407878\n",
      "Training Loss: 0.00149090551523841\n",
      "Validation Loss: 0.002281326679700344\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.00647830436588265\n",
      "Training Loss: 0.006560606533894315\n",
      "Training Loss: 0.006596899846335873\n",
      "Training Loss: 0.004500539523724001\n",
      "Training Loss: 0.001563460338729783\n",
      "Training Loss: 0.001549496533916681\n",
      "Training Loss: 0.001488136156258406\n",
      "Validation Loss: 0.002279142893457084\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006474869959056377\n",
      "Training Loss: 0.006555552976205945\n",
      "Training Loss: 0.006593015184625984\n",
      "Training Loss: 0.004497017648027395\n",
      "Training Loss: 0.0015611337903828825\n",
      "Training Loss: 0.0015472953989956295\n",
      "Training Loss: 0.001485376454947982\n",
      "Validation Loss: 0.0022769634751864613\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006471431644167751\n",
      "Training Loss: 0.006550484475446865\n",
      "Training Loss: 0.006589112391229719\n",
      "Training Loss: 0.004493497540606768\n",
      "Training Loss: 0.001558814722520765\n",
      "Training Loss: 0.001545098854112439\n",
      "Training Loss: 0.0014826240221736953\n",
      "Validation Loss: 0.0022747851884771272\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006467988392105326\n",
      "Training Loss: 0.006545402475167066\n",
      "Training Loss: 0.006585190839832649\n",
      "Training Loss: 0.004489978233468719\n",
      "Training Loss: 0.00155650094988232\n",
      "Training Loss: 0.0015429004038742277\n",
      "Training Loss: 0.001479872101044748\n",
      "Validation Loss: 0.002272603325048734\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006464538938598707\n",
      "Training Loss: 0.006540305588860065\n",
      "Training Loss: 0.006581251265015453\n",
      "Training Loss: 0.004486454632424284\n",
      "Training Loss: 0.0015541875137569149\n",
      "Training Loss: 0.0015407030798087363\n",
      "Training Loss: 0.0014771219207614194\n",
      "Validation Loss: 0.002270423073994348\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006461081843590364\n",
      "Training Loss: 0.006535190149443224\n",
      "Training Loss: 0.006577287997351959\n",
      "Training Loss: 0.004482926222262904\n",
      "Training Loss: 0.0015518743363645627\n",
      "Training Loss: 0.0015385025349678471\n",
      "Training Loss: 0.001474372354132356\n",
      "Validation Loss: 0.002268239442317499\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006457613951060921\n",
      "Training Loss: 0.0065300541720353066\n",
      "Training Loss: 0.00657330151530914\n",
      "Training Loss: 0.004479389348853146\n",
      "Training Loss: 0.0015495548219041665\n",
      "Training Loss: 0.0015362947378162062\n",
      "Training Loss: 0.0014716175661305896\n",
      "Validation Loss: 0.002266049407307435\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.006454132731305435\n",
      "Training Loss: 0.006524894795147702\n",
      "Training Loss: 0.006569288313621655\n",
      "Training Loss: 0.004475840218365192\n",
      "Training Loss: 0.001547229393327143\n",
      "Training Loss: 0.0015340797943645156\n",
      "Training Loss: 0.0014688561081129593\n",
      "Validation Loss: 0.0022638558339474076\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006450639478862286\n",
      "Training Loss: 0.00651971353800036\n",
      "Training Loss: 0.006565248423721641\n",
      "Training Loss: 0.004472278689281666\n",
      "Training Loss: 0.001544891702287714\n",
      "Training Loss: 0.0015318519395805196\n",
      "Training Loss: 0.0014660824569000398\n",
      "Validation Loss: 0.002261654293168535\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006447131996974349\n",
      "Training Loss: 0.006514508253894746\n",
      "Training Loss: 0.00656118325307034\n",
      "Training Loss: 0.004468699036951875\n",
      "Training Loss: 0.0015425357293861453\n",
      "Training Loss: 0.0015296087392198388\n",
      "Training Loss: 0.001463294774730457\n",
      "Validation Loss: 0.0022594440612991686\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0064436083065811545\n",
      "Training Loss: 0.006509279174497351\n",
      "Training Loss: 0.006557088464032859\n",
      "Training Loss: 0.004465099557637586\n",
      "Training Loss: 0.0015401625147933374\n",
      "Training Loss: 0.0015273484302451834\n",
      "Training Loss: 0.0014604887497262097\n",
      "Validation Loss: 0.002257222594751729\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006440066769719124\n",
      "Training Loss: 0.006504024104215205\n",
      "Training Loss: 0.0065529667981900274\n",
      "Training Loss: 0.004461479949095519\n",
      "Training Loss: 0.0015377625094697579\n",
      "Training Loss: 0.0015250655727868435\n",
      "Training Loss: 0.0014576599771680776\n",
      "Validation Loss: 0.002254988613012177\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006436507790349424\n",
      "Training Loss: 0.0064987430057954045\n",
      "Training Loss: 0.00654881298658438\n",
      "Training Loss: 0.00445783404946269\n",
      "Training Loss: 0.001535339640759048\n",
      "Training Loss: 0.0015227627565764124\n",
      "Training Loss: 0.0014548108709277585\n",
      "Validation Loss: 0.002252742206171584\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006432923583779484\n",
      "Training Loss: 0.00649343108991161\n",
      "Training Loss: 0.006544625617098063\n",
      "Training Loss: 0.0044541609403677285\n",
      "Training Loss: 0.0015328833294188371\n",
      "Training Loss: 0.0015204309723048936\n",
      "Training Loss: 0.0014519305057183374\n",
      "Validation Loss: 0.0022504782566429233\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.00642932101036422\n",
      "Training Loss: 0.006488094818778336\n",
      "Training Loss: 0.00654040947323665\n",
      "Training Loss: 0.0044504570085700836\n",
      "Training Loss: 0.001530390913685551\n",
      "Training Loss: 0.0015180699044867651\n",
      "Training Loss: 0.0014490185164322612\n",
      "Validation Loss: 0.0022481990052031333\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006425692113116384\n",
      "Training Loss: 0.006482727096881718\n",
      "Training Loss: 0.006536157232476398\n",
      "Training Loss: 0.004446719414045219\n",
      "Training Loss: 0.00152786096179625\n",
      "Training Loss: 0.0015156765573919983\n",
      "Training Loss: 0.0014460707633406856\n",
      "Validation Loss: 0.002245901137394639\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006422038458986208\n",
      "Training Loss: 0.006477330897469074\n",
      "Training Loss: 0.006531871829647571\n",
      "Training Loss: 0.004442945198534289\n",
      "Training Loss: 0.0015252861006592865\n",
      "Training Loss: 0.0015132480003376258\n",
      "Training Loss: 0.001443085525825154\n",
      "Validation Loss: 0.0022435850989856593\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006418353904737159\n",
      "Training Loss: 0.00647190077113919\n",
      "Training Loss: 0.006527546806028113\n",
      "Training Loss: 0.004439131594481296\n",
      "Training Loss: 0.0015226666553644463\n",
      "Training Loss: 0.0015107818552496611\n",
      "Training Loss: 0.0014400573298917153\n",
      "Validation Loss: 0.002241246473224666\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006414639218710363\n",
      "Training Loss: 0.006466438641073182\n",
      "Training Loss: 0.006523184133693576\n",
      "Training Loss: 0.004435272937116679\n",
      "Training Loss: 0.0015199954045237974\n",
      "Training Loss: 0.0015082736651675077\n",
      "Training Loss: 0.0014369828093913383\n",
      "Validation Loss: 0.0022388845275562565\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006410889186663553\n",
      "Training Loss: 0.006460940000833943\n",
      "Training Loss: 0.006518777578603477\n",
      "Training Loss: 0.0044313671640702525\n",
      "Training Loss: 0.001517272246273933\n",
      "Training Loss: 0.0015057239287853009\n",
      "Training Loss: 0.0014338606323872228\n",
      "Validation Loss: 0.002236499357031142\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006407101885415613\n",
      "Training Loss: 0.006455405856249854\n",
      "Training Loss: 0.006514328646007925\n",
      "Training Loss: 0.004427409887866816\n",
      "Training Loss: 0.0015144883241009666\n",
      "Training Loss: 0.0015031260615796782\n",
      "Training Loss: 0.0014306843961821869\n",
      "Validation Loss: 0.00223408675396576\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006403273954056203\n",
      "Training Loss: 0.006449833634542301\n",
      "Training Loss: 0.006509833192685619\n",
      "Training Loss: 0.004423398063881905\n",
      "Training Loss: 0.0015116451569338095\n",
      "Training Loss: 0.0015004809571109944\n",
      "Training Loss: 0.0014274534610740374\n",
      "Validation Loss: 0.002231649180958379\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006399402123643086\n",
      "Training Loss: 0.006444219659315422\n",
      "Training Loss: 0.006505286804167554\n",
      "Training Loss: 0.004419326973729767\n",
      "Training Loss: 0.0015087343321647496\n",
      "Training Loss: 0.0014977834826277103\n",
      "Training Loss: 0.0014241623779525981\n",
      "Validation Loss: 0.002229180101146974\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.0063954819715581835\n",
      "Training Loss: 0.006438564616255462\n",
      "Training Loss: 0.0065006902127061035\n",
      "Training Loss: 0.004415190541258198\n",
      "Training Loss: 0.0015057539578992873\n",
      "Training Loss: 0.0014950320863135857\n",
      "Training Loss: 0.0014208097393566277\n",
      "Validation Loss: 0.0022266820126648273\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006391506385989487\n",
      "Training Loss: 0.0064328614179976285\n",
      "Training Loss: 0.006496035036398098\n",
      "Training Loss: 0.004410985312279081\n",
      "Training Loss: 0.0015027005595038645\n",
      "Training Loss: 0.0014922251619282178\n",
      "Training Loss: 0.0014173939138709102\n",
      "Validation Loss: 0.0022241531756042924\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006387472036294639\n",
      "Training Loss: 0.006427106658229605\n",
      "Training Loss: 0.006491317095933482\n",
      "Training Loss: 0.00440670609852532\n",
      "Training Loss: 0.001499573065812001\n",
      "Training Loss: 0.0014893604937242344\n",
      "Training Loss: 0.0014139102390618064\n",
      "Validation Loss: 0.0022215904644101838\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006383371696574614\n",
      "Training Loss: 0.006421299490612\n",
      "Training Loss: 0.006486537286546082\n",
      "Training Loss: 0.004402346342831151\n",
      "Training Loss: 0.0014963646682736\n",
      "Training Loss: 0.0014864369116548914\n",
      "Training Loss: 0.0014103567801066674\n",
      "Validation Loss: 0.002218996100620391\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006379203216638416\n",
      "Training Loss: 0.006415436383103951\n",
      "Training Loss: 0.006481685251928866\n",
      "Training Loss: 0.004397900619878783\n",
      "Training Loss: 0.0014930728038598318\n",
      "Training Loss: 0.0014834481002617395\n",
      "Training Loss: 0.0014067280436574946\n",
      "Validation Loss: 0.0022163643772135687\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006374957065563649\n",
      "Training Loss: 0.006409513076068834\n",
      "Training Loss: 0.00647676124703139\n",
      "Training Loss: 0.004393362534901826\n",
      "Training Loss: 0.001489692826726241\n",
      "Training Loss: 0.0014803956798277796\n",
      "Training Loss: 0.0014030261705920567\n",
      "Validation Loss: 0.002213695077927546\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006370625565759838\n",
      "Training Loss: 0.006403521273750812\n",
      "Training Loss: 0.006471754923695699\n",
      "Training Loss: 0.00438872632359562\n",
      "Training Loss: 0.0014862234545580577\n",
      "Training Loss: 0.001477277067315299\n",
      "Training Loss: 0.0013992469293589238\n",
      "Validation Loss: 0.002210989727007316\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006366200952325016\n",
      "Training Loss: 0.006397459921427071\n",
      "Training Loss: 0.00646666471264325\n",
      "Training Loss: 0.0043839843658497555\n",
      "Training Loss: 0.0014826611845637672\n",
      "Training Loss: 0.0014740909924876179\n",
      "Training Loss: 0.0013953872535785194\n",
      "Validation Loss: 0.0022082452773799326\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006361676889937371\n",
      "Training Loss: 0.00639132261974737\n",
      "Training Loss: 0.006461482354206964\n",
      "Training Loss: 0.004379129583976464\n",
      "Training Loss: 0.0014790036059275735\n",
      "Training Loss: 0.001470836502048769\n",
      "Training Loss: 0.0013914483551343438\n",
      "Validation Loss: 0.0022054626376953232\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006357041250448674\n",
      "Training Loss: 0.006385101914638654\n",
      "Training Loss: 0.006456201785476878\n",
      "Training Loss: 0.004374155042605707\n",
      "Training Loss: 0.0014752458420116453\n",
      "Training Loss: 0.00146751066968136\n",
      "Training Loss: 0.001387427934241714\n",
      "Validation Loss: 0.0022026415640590295\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006352286436595023\n",
      "Training Loss: 0.006378794609336182\n",
      "Training Loss: 0.006450820044847205\n",
      "Training Loss: 0.004369051617613877\n",
      "Training Loss: 0.0014713864545046818\n",
      "Training Loss: 0.0014641136715363246\n",
      "Training Loss: 0.001383323521731654\n",
      "Validation Loss: 0.0021997803314423277\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006347404578700661\n",
      "Training Loss: 0.006372395369689912\n",
      "Training Loss: 0.006445329653797671\n",
      "Training Loss: 0.004363813797972398\n",
      "Training Loss: 0.0014674236602149904\n",
      "Training Loss: 0.0014606432674918325\n",
      "Training Loss: 0.0013791364527423865\n",
      "Validation Loss: 0.002196884274239432\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0063423838512971994\n",
      "Training Loss: 0.006365894734626636\n",
      "Training Loss: 0.006439724115189165\n",
      "Training Loss: 0.004358430762003991\n",
      "Training Loss: 0.0014633565953408834\n",
      "Training Loss: 0.0014571040103328414\n",
      "Training Loss: 0.0013748695123649668\n",
      "Validation Loss: 0.0021939539069450228\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006337211633799598\n",
      "Training Loss: 0.006359288850799203\n",
      "Training Loss: 0.006433998392894864\n",
      "Training Loss: 0.0043528970298939386\n",
      "Training Loss: 0.0014591821846261155\n",
      "Training Loss: 0.001453490679487004\n",
      "Training Loss: 0.001370518202020321\n",
      "Validation Loss: 0.0021909892334832115\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006331882157828659\n",
      "Training Loss: 0.006352572485338897\n",
      "Training Loss: 0.006428147081751376\n",
      "Training Loss: 0.004347202857752563\n",
      "Training Loss: 0.0014548989340255503\n",
      "Training Loss: 0.0014498052074486622\n",
      "Training Loss: 0.0013660855418129358\n",
      "Validation Loss: 0.002187994615013941\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006326384266139939\n",
      "Training Loss: 0.0063457424321677535\n",
      "Training Loss: 0.006422170200385153\n",
      "Training Loss: 0.004341342455809354\n",
      "Training Loss: 0.0014505059442308265\n",
      "Training Loss: 0.0014460471758502536\n",
      "Training Loss: 0.001361570641747676\n",
      "Validation Loss: 0.0021849742176360885\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006320706489495933\n",
      "Training Loss: 0.006338789578294381\n",
      "Training Loss: 0.006416060803458095\n",
      "Training Loss: 0.004335307065484812\n",
      "Training Loss: 0.0014460009589674883\n",
      "Training Loss: 0.0014422168685268844\n",
      "Training Loss: 0.001356974919763161\n",
      "Validation Loss: 0.0021819291659347425\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.0063148427614942194\n",
      "Training Loss: 0.0063317155605182055\n",
      "Training Loss: 0.006409818551037461\n",
      "Training Loss: 0.004329088461672654\n",
      "Training Loss: 0.0014413853980659041\n",
      "Training Loss: 0.0014383166340849129\n",
      "Training Loss: 0.0013523006057948805\n",
      "Validation Loss: 0.002178873542021726\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006308781974948943\n",
      "Training Loss: 0.006324512440478429\n",
      "Training Loss: 0.006403441614238546\n",
      "Training Loss: 0.00432267955635325\n",
      "Training Loss: 0.001436658744351007\n",
      "Training Loss: 0.0014343475452187704\n",
      "Training Loss: 0.0013475499248306732\n",
      "Validation Loss: 0.002175806479267811\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006302512875990942\n",
      "Training Loss: 0.006317176745506003\n",
      "Training Loss: 0.0063969275390263645\n",
      "Training Loss: 0.004316075010501663\n",
      "Training Loss: 0.0014318217098480092\n",
      "Training Loss: 0.0014303105208819033\n",
      "Training Loss: 0.0013427210530790035\n",
      "Validation Loss: 0.0021727421639372913\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006296032624086365\n",
      "Training Loss: 0.006309709813212976\n",
      "Training Loss: 0.006390281161293388\n",
      "Training Loss: 0.004309264530165819\n",
      "Training Loss: 0.0014268705721769948\n",
      "Training Loss: 0.0014262061679619364\n",
      "Training Loss: 0.0013378158039995469\n",
      "Validation Loss: 0.0021696896349495862\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.0062893307430204005\n",
      "Training Loss: 0.006302108140662312\n",
      "Training Loss: 0.006383503692923113\n",
      "Training Loss: 0.004302242396370275\n",
      "Training Loss: 0.0014218057120160665\n",
      "Training Loss: 0.0014220366634253878\n",
      "Training Loss: 0.001332830894680228\n",
      "Validation Loss: 0.002166654546241527\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006282401594799012\n",
      "Training Loss: 0.006294373255223036\n",
      "Training Loss: 0.006376599642680958\n",
      "Training Loss: 0.004295001554783085\n",
      "Training Loss: 0.0014166244135412854\n",
      "Training Loss: 0.001417800146373338\n",
      "Training Loss: 0.0013277655550336931\n",
      "Validation Loss: 0.0021636490409231083\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006275235927896574\n",
      "Training Loss: 0.0062864999927114695\n",
      "Training Loss: 0.006369570344686508\n",
      "Training Loss: 0.004287531540830969\n",
      "Training Loss: 0.0014113238829304463\n",
      "Training Loss: 0.0014134953205211787\n",
      "Training Loss: 0.0013226131000556051\n",
      "Validation Loss: 0.002160685409912543\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006267829415155575\n",
      "Training Loss: 0.006278490743134171\n",
      "Training Loss: 0.006362422601087019\n",
      "Training Loss: 0.004279822486496414\n",
      "Training Loss: 0.0014058977087552194\n",
      "Training Loss: 0.0014091208924219246\n",
      "Training Loss: 0.0013173649983946234\n",
      "Validation Loss: 0.0021577729840940806\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006260174707276747\n",
      "Training Loss: 0.006270344732329249\n",
      "Training Loss: 0.006355160636594519\n",
      "Training Loss: 0.004271865084883757\n",
      "Training Loss: 0.0014003366058750544\n",
      "Training Loss: 0.0014046720985061256\n",
      "Training Loss: 0.0013120082434033974\n",
      "Validation Loss: 0.002154920772520288\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.0062522606726270165\n",
      "Training Loss: 0.006262056460836902\n",
      "Training Loss: 0.006347786319674924\n",
      "Training Loss: 0.004263641463548993\n",
      "Training Loss: 0.0013946306151046883\n",
      "Training Loss: 0.001400142095371848\n",
      "Training Loss: 0.0013065269755315966\n",
      "Validation Loss: 0.002152138262712885\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006244082385674119\n",
      "Training Loss: 0.0062536261091008785\n",
      "Training Loss: 0.006340305061312393\n",
      "Training Loss: 0.00425513728063379\n",
      "Training Loss: 0.001388766290328931\n",
      "Training Loss: 0.0013955230456485879\n",
      "Training Loss: 0.001300899050111184\n",
      "Validation Loss: 0.002149418938629441\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.00623562591150403\n",
      "Training Loss: 0.0062450471648480745\n",
      "Training Loss: 0.006332716346951202\n",
      "Training Loss: 0.004246332646871452\n",
      "Training Loss: 0.0013827219442464412\n",
      "Training Loss: 0.0013907977007329463\n",
      "Training Loss: 0.0012950931178056634\n",
      "Validation Loss: 0.0021467607979373205\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006226885257055983\n",
      "Training Loss: 0.006236316997092217\n",
      "Training Loss: 0.006325022001983598\n",
      "Training Loss: 0.0042372049571713435\n",
      "Training Loss: 0.001376475898869103\n",
      "Training Loss: 0.001385953837452689\n",
      "Training Loss: 0.001289077902183635\n",
      "Validation Loss: 0.0021441471015193852\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.006217849212698638\n",
      "Training Loss: 0.006227429473074153\n",
      "Training Loss: 0.006317219268530607\n",
      "Training Loss: 0.004227729312187876\n",
      "Training Loss: 0.0013700047010206618\n",
      "Training Loss: 0.0013809703518199968\n",
      "Training Loss: 0.001282814195146784\n",
      "Validation Loss: 0.002141541222259507\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006208511497825384\n",
      "Training Loss: 0.006218382766237482\n",
      "Training Loss: 0.006309309080243111\n",
      "Training Loss: 0.0042178881783911494\n",
      "Training Loss: 0.0013632782135391608\n",
      "Training Loss: 0.0013758234459237428\n",
      "Training Loss: 0.0012762590326019564\n",
      "Validation Loss: 0.0021388897979030324\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006198875629343093\n",
      "Training Loss: 0.006209181962767616\n",
      "Training Loss: 0.006301291682757437\n",
      "Training Loss: 0.004207662592089037\n",
      "Training Loss: 0.0013562728760007302\n",
      "Training Loss: 0.0013704913046240109\n",
      "Training Loss: 0.0012693721200048459\n",
      "Validation Loss: 0.002136101885955983\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.006188959400169551\n",
      "Training Loss: 0.006199840316548943\n",
      "Training Loss: 0.006293174857273698\n",
      "Training Loss: 0.004197054454416502\n",
      "Training Loss: 0.0013489770688465796\n",
      "Training Loss: 0.0013649554675794207\n",
      "Training Loss: 0.0012621286405192222\n",
      "Validation Loss: 0.0021330729482934396\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006178801225032658\n",
      "Training Loss: 0.0061903908348176625\n",
      "Training Loss: 0.006284978828625753\n",
      "Training Loss: 0.0041860871103563116\n",
      "Training Loss: 0.0013413825754832942\n",
      "Training Loss: 0.0013591998668562155\n",
      "Training Loss: 0.001254512273153523\n",
      "Validation Loss: 0.002129657250481975\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006168470615521073\n",
      "Training Loss: 0.006180885515641421\n",
      "Training Loss: 0.0062767348263878375\n",
      "Training Loss: 0.004174818286992376\n",
      "Training Loss: 0.0013335144355369267\n",
      "Training Loss: 0.0013532247285183985\n",
      "Training Loss: 0.0012465383182279765\n",
      "Validation Loss: 0.002125697712608428\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.0061580733535811305\n",
      "Training Loss: 0.00617140295682475\n",
      "Training Loss: 0.006268496678676456\n",
      "Training Loss: 0.0041633483194164\n",
      "Training Loss: 0.0013254166519618593\n",
      "Training Loss: 0.0013470421040256042\n",
      "Training Loss: 0.0012382509061717429\n",
      "Validation Loss: 0.002121056758904174\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.0061477512726560235\n",
      "Training Loss: 0.006162040419876575\n",
      "Training Loss: 0.006260335099650547\n",
      "Training Loss: 0.00415181716234656\n",
      "Training Loss: 0.001317160965554649\n",
      "Training Loss: 0.001340681568835862\n",
      "Training Loss: 0.0012297209147072864\n",
      "Validation Loss: 0.002115639476981956\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.00613767571747303\n",
      "Training Loss: 0.006152901897439733\n",
      "Training Loss: 0.0062523282039910555\n",
      "Training Loss: 0.004140397809096612\n",
      "Training Loss: 0.001308832544455072\n",
      "Training Loss: 0.0013341785877128132\n",
      "Training Loss: 0.0012210365504142827\n",
      "Validation Loss: 0.0021094485484272913\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006128021442564204\n",
      "Training Loss: 0.006144085462437943\n",
      "Training Loss: 0.006244556216988713\n",
      "Training Loss: 0.004129266834061127\n",
      "Training Loss: 0.0013005137087020557\n",
      "Training Loss: 0.0013275687984423711\n",
      "Training Loss: 0.0012122854399785866\n",
      "Validation Loss: 0.002102588089741069\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.00611894476111047\n",
      "Training Loss: 0.006135658074636012\n",
      "Training Loss: 0.006237078929552808\n",
      "Training Loss: 0.004118580309732351\n",
      "Training Loss: 0.001292281016067136\n",
      "Training Loss: 0.0013208922295598314\n",
      "Training Loss: 0.0012035506052779964\n",
      "Validation Loss: 0.002095235877584017\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006110550330486148\n",
      "Training Loss: 0.006127645516535267\n",
      "Training Loss: 0.0062299285153858365\n",
      "Training Loss: 0.004108454484667163\n",
      "Training Loss: 0.0012841832767298911\n",
      "Training Loss: 0.0013141759697464295\n",
      "Training Loss: 0.001194895177759463\n",
      "Validation Loss: 0.0020876104018235867\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006102891821647063\n",
      "Training Loss: 0.006120035974308848\n",
      "Training Loss: 0.0062231061176862565\n",
      "Training Loss: 0.004098951492487686\n",
      "Training Loss: 0.0012762555872905068\n",
      "Training Loss: 0.001307445793208899\n",
      "Training Loss: 0.0011863736862142106\n",
      "Validation Loss: 0.002079928376993569\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.006095952686155215\n",
      "Training Loss: 0.006112783217104152\n",
      "Training Loss: 0.006216585977235809\n",
      "Training Loss: 0.004090084783383645\n",
      "Training Loss: 0.0012685087334830315\n",
      "Training Loss: 0.0013007175267557613\n",
      "Training Loss: 0.001178016459889477\n",
      "Validation Loss: 0.0020723653974034214\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006089682671008631\n",
      "Training Loss: 0.0061058260197751225\n",
      "Training Loss: 0.006210322213592008\n",
      "Training Loss: 0.004081827736881678\n",
      "Training Loss: 0.001260940831998596\n",
      "Training Loss: 0.001294006070093019\n",
      "Training Loss: 0.0011698462715139613\n",
      "Validation Loss: 0.002065040091356116\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.006083998640533537\n",
      "Training Loss: 0.006099098334088921\n",
      "Training Loss: 0.006204266966087744\n",
      "Training Loss: 0.004074127454223344\n",
      "Training Loss: 0.0012535522949474397\n",
      "Training Loss: 0.0012873304636741522\n",
      "Training Loss: 0.001161885053734295\n",
      "Validation Loss: 0.0020580103351431115\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006078799669630826\n",
      "Training Loss: 0.006092530450550839\n",
      "Training Loss: 0.006198361148126424\n",
      "Training Loss: 0.004066921489938977\n",
      "Training Loss: 0.0012463350301550236\n",
      "Training Loss: 0.0012807006458751857\n",
      "Training Loss: 0.001154139804566512\n",
      "Validation Loss: 0.0020513118873437524\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.0060739945259410885\n",
      "Training Loss: 0.006086069911252707\n",
      "Training Loss: 0.006192557953763753\n",
      "Training Loss: 0.004060140794390463\n",
      "Training Loss: 0.0012392818147782237\n",
      "Training Loss: 0.001274128424556693\n",
      "Training Loss: 0.0011466128710890188\n",
      "Validation Loss: 0.0020449520812512896\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.0060694978083483874\n",
      "Training Loss: 0.0060796719242352994\n",
      "Training Loss: 0.006186819823924452\n",
      "Training Loss: 0.004053722087992356\n",
      "Training Loss: 0.0012323861435288564\n",
      "Training Loss: 0.0012676236291008535\n",
      "Training Loss: 0.0011393070180201904\n",
      "Validation Loss: 0.002038912791065516\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006065234470879659\n",
      "Training Loss: 0.006073302797740325\n",
      "Training Loss: 0.006181111900368705\n",
      "Training Loss: 0.004047605164450943\n",
      "Training Loss: 0.0012256376765435561\n",
      "Training Loss: 0.0012611918787297327\n",
      "Training Loss: 0.0011322109239699785\n",
      "Validation Loss: 0.002033169986178736\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006061150531750172\n",
      "Training Loss: 0.006066943034529686\n",
      "Training Loss: 0.006175412856973708\n",
      "Training Loss: 0.004041741317050765\n",
      "Training Loss: 0.0012190260984061751\n",
      "Training Loss: 0.0012548364598478656\n",
      "Training Loss: 0.0011253189681156074\n",
      "Validation Loss: 0.002027696797433257\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.006057188925333321\n",
      "Training Loss: 0.006060574405128136\n",
      "Training Loss: 0.006169703641207888\n",
      "Training Loss: 0.0040360848951968365\n",
      "Training Loss: 0.0012125424173427745\n",
      "Training Loss: 0.001248556153877871\n",
      "Training Loss: 0.001118617465835996\n",
      "Validation Loss: 0.0020224725744078347\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0060533204348757865\n",
      "Training Loss: 0.006054191204020754\n",
      "Training Loss: 0.006163973953807727\n",
      "Training Loss: 0.0040305999582778895\n",
      "Training Loss: 0.0012061757204355672\n",
      "Training Loss: 0.0012423517132992856\n",
      "Training Loss: 0.0011120932808262297\n",
      "Validation Loss: 0.0020174688441820148\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006049512949539348\n",
      "Training Loss: 0.006047786046401597\n",
      "Training Loss: 0.00615821466781199\n",
      "Training Loss: 0.00402525598052307\n",
      "Training Loss: 0.0011999157586251386\n",
      "Training Loss: 0.0012362215934263077\n",
      "Training Loss: 0.0011057349741167854\n",
      "Validation Loss: 0.002012642131301257\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.0060457428079098464\n",
      "Training Loss: 0.006041358285583556\n",
      "Training Loss: 0.00615241885650903\n",
      "Training Loss: 0.004020027533188113\n",
      "Training Loss: 0.0011937577444768976\n",
      "Training Loss: 0.0012301645198022015\n",
      "Training Loss: 0.001099531197687611\n",
      "Validation Loss: 0.002007988069863983\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006041992546524853\n",
      "Training Loss: 0.006034907263237983\n",
      "Training Loss: 0.006146584113594145\n",
      "Training Loss: 0.004014893190105795\n",
      "Training Loss: 0.0011876878143812063\n",
      "Training Loss: 0.001224170703062555\n",
      "Training Loss: 0.0010934648317925165\n",
      "Validation Loss: 0.002003476876950359\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006038252027938142\n",
      "Training Loss: 0.006028435077751055\n",
      "Training Loss: 0.00614070886047557\n",
      "Training Loss: 0.00400983662577346\n",
      "Training Loss: 0.0011816968887433177\n",
      "Training Loss: 0.001218239784211619\n",
      "Training Loss: 0.0010875223646871746\n",
      "Validation Loss: 0.001999084099674639\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006034510944737122\n",
      "Training Loss: 0.006021945011452772\n",
      "Training Loss: 0.006134790170472115\n",
      "Training Loss: 0.004004841763016884\n",
      "Training Loss: 0.001175782060236088\n",
      "Training Loss: 0.0012123701446398627\n",
      "Training Loss: 0.0010816974595945794\n",
      "Validation Loss: 0.0019947920576233263\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006030753862578422\n",
      "Training Loss: 0.006015431348350831\n",
      "Training Loss: 0.006128823994658888\n",
      "Training Loss: 0.0039998983372788646\n",
      "Training Loss: 0.0011699373625742738\n",
      "Training Loss: 0.001206553217052715\n",
      "Training Loss: 0.0010759766161208973\n",
      "Validation Loss: 0.0019905850648128318\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006026980598689989\n",
      "Training Loss: 0.006008902449393645\n",
      "Training Loss: 0.006122812562389299\n",
      "Training Loss: 0.003994997328409227\n",
      "Training Loss: 0.0011641571471409406\n",
      "Training Loss: 0.0012007901084143669\n",
      "Training Loss: 0.0010703529872989747\n",
      "Validation Loss: 0.0019864339849297724\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006023181267082691\n",
      "Training Loss: 0.006002357443212532\n",
      "Training Loss: 0.006116754459217191\n",
      "Training Loss: 0.003990129934754805\n",
      "Training Loss: 0.001158433100063121\n",
      "Training Loss: 0.0011950735504797194\n",
      "Training Loss: 0.0010648159410629888\n",
      "Validation Loss: 0.001982336684020257\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.006019352314760909\n",
      "Training Loss: 0.005995795679045841\n",
      "Training Loss: 0.0061106472578831015\n",
      "Training Loss: 0.003985290498530958\n",
      "Training Loss: 0.0011527701202430761\n",
      "Training Loss: 0.0011894062142528128\n",
      "Training Loss: 0.0010593589386553503\n",
      "Validation Loss: 0.0019782784186505925\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.00601549148093909\n",
      "Training Loss: 0.005989221094641835\n",
      "Training Loss: 0.006104492445010692\n",
      "Training Loss: 0.003980475533026038\n",
      "Training Loss: 0.0011471629522566218\n",
      "Training Loss: 0.0011837830580770968\n",
      "Training Loss: 0.0010539771874027793\n",
      "Validation Loss: 0.0019742422726280874\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006011591871501878\n",
      "Training Loss: 0.0059826304344460365\n",
      "Training Loss: 0.006098285159096122\n",
      "Training Loss: 0.003975679398135981\n",
      "Training Loss: 0.001141612433566479\n",
      "Training Loss: 0.0011782028088055086\n",
      "Training Loss: 0.0010486691654659807\n",
      "Validation Loss: 0.0019702198964234945\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006007649234961718\n",
      "Training Loss: 0.005976026175776496\n",
      "Training Loss: 0.006092028688872233\n",
      "Training Loss: 0.003970900839631213\n",
      "Training Loss: 0.0011361171290627681\n",
      "Training Loss: 0.0011726660717977211\n",
      "Training Loss: 0.001043424383387901\n",
      "Validation Loss: 0.00196619700921232\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.006003665360622108\n",
      "Training Loss: 0.005969410631805659\n",
      "Training Loss: 0.006085723255528137\n",
      "Training Loss: 0.003966139294134336\n",
      "Training Loss: 0.0011306754703400658\n",
      "Training Loss: 0.0011671687821217347\n",
      "Training Loss: 0.0010382407385623083\n",
      "Validation Loss: 0.0019621700559671242\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005999636250780895\n",
      "Training Loss: 0.005962783937575295\n",
      "Training Loss: 0.006079370442312211\n",
      "Training Loss: 0.003961391230186564\n",
      "Training Loss: 0.0011252922874700744\n",
      "Training Loss: 0.0011617165993084199\n",
      "Training Loss: 0.0010331175234750845\n",
      "Validation Loss: 0.0019581318030235\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.0059955575643107295\n",
      "Training Loss: 0.005956144886440597\n",
      "Training Loss: 0.00607296647853218\n",
      "Training Loss: 0.003956657776507199\n",
      "Training Loss: 0.0011199682250298793\n",
      "Training Loss: 0.0011563034490973223\n",
      "Training Loss: 0.001028053113259375\n",
      "Validation Loss: 0.0019540791686455117\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005991431748261675\n",
      "Training Loss: 0.005949495199602097\n",
      "Training Loss: 0.006066517299041152\n",
      "Training Loss: 0.003951938872341998\n",
      "Training Loss: 0.0011147038281342247\n",
      "Training Loss: 0.0011509341702912934\n",
      "Training Loss: 0.001023043867899105\n",
      "Validation Loss: 0.001949999426720029\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005987254647770897\n",
      "Training Loss: 0.005942835459718481\n",
      "Training Loss: 0.006060022887540981\n",
      "Training Loss: 0.003947234177176142\n",
      "Training Loss: 0.001109499799713376\n",
      "Training Loss: 0.0011456067422113848\n",
      "Training Loss: 0.0010180878106621095\n",
      "Validation Loss: 0.001945898491210803\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005983026322210208\n",
      "Training Loss: 0.005936167195905\n",
      "Training Loss: 0.006053484083386138\n",
      "Training Loss: 0.003942545546015026\n",
      "Training Loss: 0.0011043590485496678\n",
      "Training Loss: 0.001140320940030506\n",
      "Training Loss: 0.0010131860998808407\n",
      "Validation Loss: 0.001941768459392335\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005978745158063248\n",
      "Training Loss: 0.00592949083482381\n",
      "Training Loss: 0.006046903731767088\n",
      "Training Loss: 0.003937872413443983\n",
      "Training Loss: 0.0010992878911929439\n",
      "Training Loss: 0.001135085025598528\n",
      "Training Loss: 0.0010083415611006786\n",
      "Validation Loss: 0.0019376182638515093\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005974409213522449\n",
      "Training Loss: 0.005922804083675146\n",
      "Training Loss: 0.006040282379835844\n",
      "Training Loss: 0.003933215780634782\n",
      "Training Loss: 0.001094284364953637\n",
      "Training Loss: 0.0011298913668724708\n",
      "Training Loss: 0.0010035476155462674\n",
      "Validation Loss: 0.001933441378899121\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005970023678382858\n",
      "Training Loss: 0.005916112965205684\n",
      "Training Loss: 0.006033625829732045\n",
      "Training Loss: 0.003928578153063427\n",
      "Training Loss: 0.0010893522542755817\n",
      "Training Loss: 0.001124744807748357\n",
      "Training Loss: 0.0009988095730659552\n",
      "Validation Loss: 0.001929230763479258\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005965580770280212\n",
      "Training Loss: 0.005909413584158756\n",
      "Training Loss: 0.006026931919623166\n",
      "Training Loss: 0.003923959391759126\n",
      "Training Loss: 0.0010844932225882076\n",
      "Training Loss: 0.001119648660387611\n",
      "Training Loss: 0.000994125016295584\n",
      "Validation Loss: 0.0019250067157151128\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.005961086871102452\n",
      "Training Loss: 0.005902711050584913\n",
      "Training Loss: 0.006020206795074046\n",
      "Training Loss: 0.003919358803323121\n",
      "Training Loss: 0.001079710316771525\n",
      "Training Loss: 0.0011146033015393187\n",
      "Training Loss: 0.0009894996983348392\n",
      "Validation Loss: 0.001920757650538114\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005956534155411646\n",
      "Training Loss: 0.005896000598440878\n",
      "Training Loss: 0.006013451578328386\n",
      "Training Loss: 0.00391478146360896\n",
      "Training Loss: 0.001075005622478784\n",
      "Training Loss: 0.0011096082169387955\n",
      "Training Loss: 0.000984929833066417\n",
      "Validation Loss: 0.0019165013642320654\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005951931712916121\n",
      "Training Loss: 0.005889286695746705\n",
      "Training Loss: 0.006006667827023193\n",
      "Training Loss: 0.003910223309503635\n",
      "Training Loss: 0.001070381207173341\n",
      "Training Loss: 0.0011046670012001415\n",
      "Training Loss: 0.0009804173934389838\n",
      "Validation Loss: 0.0019122311120115096\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005947274084901437\n",
      "Training Loss: 0.005882568976376206\n",
      "Training Loss: 0.005999859404982999\n",
      "Training Loss: 0.0039056889934727225\n",
      "Training Loss: 0.0010658378359221388\n",
      "Training Loss: 0.0010997797858726698\n",
      "Training Loss: 0.0009759651741478592\n",
      "Validation Loss: 0.0019079458039543463\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005942560761468485\n",
      "Training Loss: 0.005875847919378429\n",
      "Training Loss: 0.00599302813410759\n",
      "Training Loss: 0.003901177136358456\n",
      "Training Loss: 0.001061376562793157\n",
      "Training Loss: 0.0010949495118984488\n",
      "Training Loss: 0.000971569942048518\n",
      "Validation Loss: 0.0019036674730772717\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005937797027872876\n",
      "Training Loss: 0.005869126645266078\n",
      "Training Loss: 0.005986178994644434\n",
      "Training Loss: 0.0038966877723578363\n",
      "Training Loss: 0.0010569976810074877\n",
      "Training Loss: 0.0010901745966111775\n",
      "Training Loss: 0.0009672359928663355\n",
      "Validation Loss: 0.0018993893696896886\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005932978471973911\n",
      "Training Loss: 0.0058624009630875665\n",
      "Training Loss: 0.005979310872498899\n",
      "Training Loss: 0.0038922232841287043\n",
      "Training Loss: 0.0010527033145626774\n",
      "Training Loss: 0.0010854552089585923\n",
      "Training Loss: 0.0009629633217991796\n",
      "Validation Loss: 0.0018951100426331888\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005928106911014765\n",
      "Training Loss: 0.00585567457776051\n",
      "Training Loss: 0.005972428381210193\n",
      "Training Loss: 0.0038877819326444294\n",
      "Training Loss: 0.0010484914254629985\n",
      "Training Loss: 0.001080793702567462\n",
      "Training Loss: 0.0009587511274730786\n",
      "Validation Loss: 0.0018908503477998804\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005923187879379839\n",
      "Training Loss: 0.005848949077771976\n",
      "Training Loss: 0.00596553475712426\n",
      "Training Loss: 0.0038833655837515833\n",
      "Training Loss: 0.0010443650475644973\n",
      "Training Loss: 0.0010761898595956154\n",
      "Training Loss: 0.0009546002963907085\n",
      "Validation Loss: 0.001886607383199957\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.0059182184399105605\n",
      "Training Loss: 0.005842226524837315\n",
      "Training Loss: 0.005958633178379386\n",
      "Training Loss: 0.00387897265231004\n",
      "Training Loss: 0.0010403212317760336\n",
      "Training Loss: 0.001071646679774858\n",
      "Training Loss: 0.0009505135047947988\n",
      "Validation Loss: 0.0018823795908519628\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.0059131950396113095\n",
      "Training Loss: 0.0058354996365960685\n",
      "Training Loss: 0.005951718933647499\n",
      "Training Loss: 0.003874603982840199\n",
      "Training Loss: 0.0010363645025790902\n",
      "Training Loss: 0.0010671642182569485\n",
      "Training Loss: 0.0009464890774688684\n",
      "Validation Loss: 0.0018781816989291557\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005908124020788818\n",
      "Training Loss: 0.005828775609261356\n",
      "Training Loss: 0.005944799917633645\n",
      "Training Loss: 0.003870259282703046\n",
      "Training Loss: 0.0010324895627854857\n",
      "Training Loss: 0.0010627400246448816\n",
      "Training Loss: 0.0009425277051923331\n",
      "Validation Loss: 0.0018740113783620772\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005903005454456434\n",
      "Training Loss: 0.005822054160526022\n",
      "Training Loss: 0.0059378774074139076\n",
      "Training Loss: 0.00386593929521041\n",
      "Training Loss: 0.0010286976520728786\n",
      "Training Loss: 0.0010583751344529447\n",
      "Training Loss: 0.0009386304928921163\n",
      "Validation Loss: 0.0018698702142739578\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005897837554803118\n",
      "Training Loss: 0.005815333620994352\n",
      "Training Loss: 0.005930953326751478\n",
      "Training Loss: 0.0038616427367378493\n",
      "Training Loss: 0.0010249873000429943\n",
      "Training Loss: 0.0010540718481934163\n",
      "Training Loss: 0.0009347958066791761\n",
      "Validation Loss: 0.0018657755172307418\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005892625325359404\n",
      "Training Loss: 0.005808617193833925\n",
      "Training Loss: 0.005924026748398319\n",
      "Training Loss: 0.003857367779855849\n",
      "Training Loss: 0.0010213574220688316\n",
      "Training Loss: 0.0010498277518490794\n",
      "Training Loss: 0.0009310244547668844\n",
      "Validation Loss: 0.0018617146419996067\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005887365755625069\n",
      "Training Loss: 0.005801903338870033\n",
      "Training Loss: 0.005917100457008928\n",
      "Training Loss: 0.0038531167460314464\n",
      "Training Loss: 0.0010178094774892087\n",
      "Training Loss: 0.001045642753742868\n",
      "Training Loss: 0.0009273166072671302\n",
      "Validation Loss: 0.001857694858651999\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005882062199525535\n",
      "Training Loss: 0.005795192359946668\n",
      "Training Loss: 0.005910177417099476\n",
      "Training Loss: 0.00384888878732454\n",
      "Training Loss: 0.0010143363903625869\n",
      "Training Loss: 0.0010415158183604944\n",
      "Training Loss: 0.0009236696966399904\n",
      "Validation Loss: 0.0018537203369929005\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005876716482453049\n",
      "Training Loss: 0.005788487401441671\n",
      "Training Loss: 0.0059032567287795245\n",
      "Training Loss: 0.003844682130002184\n",
      "Training Loss: 0.001010944083245704\n",
      "Training Loss: 0.0010374496575968806\n",
      "Training Loss: 0.0009200871978828218\n",
      "Validation Loss: 0.0018497918769961384\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005871327192289754\n",
      "Training Loss: 0.0057817852159496395\n",
      "Training Loss: 0.005896340670296922\n",
      "Training Loss: 0.0038404974337026944\n",
      "Training Loss: 0.0010076237175962888\n",
      "Training Loss: 0.0010334399000566919\n",
      "Training Loss: 0.0009165633311204146\n",
      "Validation Loss: 0.0018459136556609\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005865899362834171\n",
      "Training Loss: 0.0057750880747335035\n",
      "Training Loss: 0.00588942895818036\n",
      "Training Loss: 0.003836334816151066\n",
      "Training Loss: 0.0010043785830930573\n",
      "Training Loss: 0.0010294879383582157\n",
      "Training Loss: 0.0009130999895569403\n",
      "Validation Loss: 0.0018420823910125367\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0058604318252764645\n",
      "Training Loss: 0.005768397654173895\n",
      "Training Loss: 0.005882526281056926\n",
      "Training Loss: 0.0038321926462231205\n",
      "Training Loss: 0.0010012051387457176\n",
      "Training Loss: 0.0010255915540619753\n",
      "Training Loss: 0.000909697335009696\n",
      "Validation Loss: 0.0018382987502590868\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.0058549272106029095\n",
      "Training Loss: 0.005761714568943716\n",
      "Training Loss: 0.005875629165675491\n",
      "Training Loss: 0.0038280707721423824\n",
      "Training Loss: 0.000998103236197494\n",
      "Training Loss: 0.0010217519990692381\n",
      "Training Loss: 0.0009063534236338455\n",
      "Validation Loss: 0.001834565653702879\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005849385206820443\n",
      "Training Loss: 0.005755036844639108\n",
      "Training Loss: 0.005868740436853841\n",
      "Training Loss: 0.003823970341909444\n",
      "Training Loss: 0.000995068444899516\n",
      "Training Loss: 0.0010179657787375619\n",
      "Training Loss: 0.0009030660262214951\n",
      "Validation Loss: 0.0018308824563725095\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005843810817459598\n",
      "Training Loss: 0.005748370462097228\n",
      "Training Loss: 0.005861863979371265\n",
      "Training Loss: 0.003819890269078314\n",
      "Training Loss: 0.0009920991918625076\n",
      "Training Loss: 0.0010142344099585898\n",
      "Training Loss: 0.0008998364936269354\n",
      "Validation Loss: 0.0018272448337875824\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.0058381999947596346\n",
      "Training Loss: 0.005741708965506405\n",
      "Training Loss: 0.005854993076063693\n",
      "Training Loss: 0.0038158289341663474\n",
      "Training Loss: 0.0009891990579490085\n",
      "Training Loss: 0.0010105569746519905\n",
      "Training Loss: 0.0008966633892850951\n",
      "Validation Loss: 0.0018236625513187452\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005832556269597262\n",
      "Training Loss: 0.005735058275749907\n",
      "Training Loss: 0.005848136437125504\n",
      "Training Loss: 0.0038117869317648\n",
      "Training Loss: 0.0009863594195485348\n",
      "Training Loss: 0.001006930573785212\n",
      "Training Loss: 0.0008935445100360085\n",
      "Validation Loss: 0.001820128335951468\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.0058268850680906325\n",
      "Training Loss: 0.005728414995828644\n",
      "Training Loss: 0.005841287830844521\n",
      "Training Loss: 0.0038077657857502347\n",
      "Training Loss: 0.0009835830276279012\n",
      "Training Loss: 0.0010033564575132913\n",
      "Training Loss: 0.0008904814824927599\n",
      "Validation Loss: 0.0018166395716111032\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005821181724313646\n",
      "Training Loss: 0.005721782482578419\n",
      "Training Loss: 0.0058344516245415435\n",
      "Training Loss: 0.003803762663155794\n",
      "Training Loss: 0.000980866031968617\n",
      "Training Loss: 0.0009998310935043263\n",
      "Training Loss: 0.0008874691025994253\n",
      "Validation Loss: 0.001813194542588694\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005815451050875708\n",
      "Training Loss: 0.00571516148513183\n",
      "Training Loss: 0.005827628293191083\n",
      "Training Loss: 0.0037997796208946964\n",
      "Training Loss: 0.0009782075216935482\n",
      "Training Loss: 0.0009963534444977994\n",
      "Training Loss: 0.0008845083042979241\n",
      "Validation Loss: 0.0018097963259330416\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005809695079224184\n",
      "Training Loss: 0.005708552727592178\n",
      "Training Loss: 0.005820818665670231\n",
      "Training Loss: 0.00379581552289892\n",
      "Training Loss: 0.000975605612329673\n",
      "Training Loss: 0.0009929259323689622\n",
      "Training Loss: 0.0008816006063716486\n",
      "Validation Loss: 0.0018064408938702169\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.0058039136801380665\n",
      "Training Loss: 0.005701956222183071\n",
      "Training Loss: 0.005814022389240563\n",
      "Training Loss: 0.00379187127618934\n",
      "Training Loss: 0.0009730567553924629\n",
      "Training Loss: 0.0009895431612676475\n",
      "Training Loss: 0.0008787382602167781\n",
      "Validation Loss: 0.001803129688511369\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005798112109769136\n",
      "Training Loss: 0.005695374786737375\n",
      "Training Loss: 0.0058072412409819665\n",
      "Training Loss: 0.003787944864743622\n",
      "Training Loss: 0.0009705611776007572\n",
      "Training Loss: 0.000986206406232668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [32:59<49:32, 495.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0008759266749257222\n",
      "Validation Loss: 0.001799853902132676\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.20343043193221091\n",
      "Training Loss: 0.15976880200207233\n",
      "Training Loss: 0.12022062007337808\n",
      "Training Loss: 0.08320143061224371\n",
      "Training Loss: 0.06649290369823575\n",
      "Training Loss: 0.05521607018075883\n",
      "Training Loss: 0.05834310268983245\n",
      "Validation Loss: 0.056219128452325136\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.057070326562970876\n",
      "Training Loss: 0.05513267958536744\n",
      "Training Loss: 0.052670585867017505\n",
      "Training Loss: 0.048257585279643536\n",
      "Training Loss: 0.04653753682039678\n",
      "Training Loss: 0.04072064694017172\n",
      "Training Loss: 0.041497821817174554\n",
      "Validation Loss: 0.03968928298086263\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.04163965661078692\n",
      "Training Loss: 0.03940555213019252\n",
      "Training Loss: 0.03598705565556884\n",
      "Training Loss: 0.030444636822212486\n",
      "Training Loss: 0.02667360317427665\n",
      "Training Loss: 0.021773441256955267\n",
      "Training Loss: 0.020227558524347843\n",
      "Validation Loss: 0.019863236568212397\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.022899145679548382\n",
      "Training Loss: 0.021619922122918068\n",
      "Training Loss: 0.019465352296829223\n",
      "Training Loss: 0.015476983119733632\n",
      "Training Loss: 0.011566881026374176\n",
      "Training Loss: 0.010120758394477888\n",
      "Training Loss: 0.010158932278864085\n",
      "Validation Loss: 0.01221425905701624\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.01550942794419825\n",
      "Training Loss: 0.015471476064994931\n",
      "Training Loss: 0.014551884902175516\n",
      "Training Loss: 0.011199010839918628\n",
      "Training Loss: 0.007382055319612846\n",
      "Training Loss: 0.00679994169739075\n",
      "Training Loss: 0.0067148950276896355\n",
      "Validation Loss: 0.008867336888815022\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.012482788970228285\n",
      "Training Loss: 0.012540996596217156\n",
      "Training Loss: 0.011961053004488348\n",
      "Training Loss: 0.008807183430471923\n",
      "Training Loss: 0.005136183399590663\n",
      "Training Loss: 0.004904194635455497\n",
      "Training Loss: 0.004697503016213887\n",
      "Validation Loss: 0.006610470354794228\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.010495835321489722\n",
      "Training Loss: 0.010584504548460246\n",
      "Training Loss: 0.01022951652877964\n",
      "Training Loss: 0.0072898796966183\n",
      "Training Loss: 0.0038090481943800114\n",
      "Training Loss: 0.0037584742504986933\n",
      "Training Loss: 0.003542215161724016\n",
      "Validation Loss: 0.005158629080050447\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.00918742272653617\n",
      "Training Loss: 0.00929867972037755\n",
      "Training Loss: 0.009060875264694915\n",
      "Training Loss: 0.006344093885854818\n",
      "Training Loss: 0.00306498783553252\n",
      "Training Loss: 0.003088260450749658\n",
      "Training Loss: 0.0029017586878035215\n",
      "Validation Loss: 0.004262011962130052\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.00827703028335236\n",
      "Training Loss: 0.008394580099266023\n",
      "Training Loss: 0.00821463186526671\n",
      "Training Loss: 0.005716609015216818\n",
      "Training Loss: 0.0026451260421890767\n",
      "Training Loss: 0.002679090516467113\n",
      "Training Loss: 0.0025312431852216833\n",
      "Validation Loss: 0.0036920784337773974\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.0076003178162500264\n",
      "Training Loss: 0.007718621995300054\n",
      "Training Loss: 0.007581945488927886\n",
      "Training Loss: 0.005284459329268429\n",
      "Training Loss: 0.0024017961794743315\n",
      "Training Loss: 0.0024151944839104545\n",
      "Training Loss: 0.002302498629578622\n",
      "Validation Loss: 0.0033143194403659816\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.007102389286737889\n",
      "Training Loss: 0.007225624031852931\n",
      "Training Loss: 0.00712741858093068\n",
      "Training Loss: 0.004991074887657305\n",
      "Training Loss: 0.002251061850111\n",
      "Training Loss: 0.002233138326264452\n",
      "Training Loss: 0.0021484576952934733\n",
      "Validation Loss: 0.0030514981958680272\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.0067547461192589255\n",
      "Training Loss: 0.006889674525009468\n",
      "Training Loss: 0.006821226294850931\n",
      "Training Loss: 0.00479640316872974\n",
      "Training Loss: 0.002147231943381485\n",
      "Training Loss: 0.002098308962158626\n",
      "Training Loss: 0.002036003080138471\n",
      "Validation Loss: 0.002862066882034617\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.006524820070480928\n",
      "Training Loss: 0.006674904944375158\n",
      "Training Loss: 0.006624697837978601\n",
      "Training Loss: 0.0046669574371480844\n",
      "Training Loss: 0.002067589149373816\n",
      "Training Loss: 0.001992544477834599\n",
      "Training Loss: 0.0019480922176444437\n",
      "Validation Loss: 0.0027214594249446154\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.006374906224664301\n",
      "Training Loss: 0.006539368208032101\n",
      "Training Loss: 0.006497354103485123\n",
      "Training Loss: 0.004575874351867242\n",
      "Training Loss: 0.0020012273530301172\n",
      "Training Loss: 0.0019056219188496471\n",
      "Training Loss: 0.0018752464202407282\n",
      "Validation Loss: 0.0026120699353277166\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.006271670230198652\n",
      "Training Loss: 0.006448085226584226\n",
      "Training Loss: 0.00640774262486957\n",
      "Training Loss: 0.004505218643753323\n",
      "Training Loss: 0.0019435068819439038\n",
      "Training Loss: 0.001831565252505243\n",
      "Training Loss: 0.0018123888569243718\n",
      "Validation Loss: 0.002522630667054942\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.006192698460072279\n",
      "Training Loss: 0.006378736110636964\n",
      "Training Loss: 0.006336546104867011\n",
      "Training Loss: 0.004444871130108368\n",
      "Training Loss: 0.0018923341017216444\n",
      "Training Loss: 0.0017667264015472028\n",
      "Training Loss: 0.001756660231621936\n",
      "Validation Loss: 0.0024470740602018613\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.006125587864080444\n",
      "Training Loss: 0.006319396118633449\n",
      "Training Loss: 0.006273624303285033\n",
      "Training Loss: 0.004389736856828677\n",
      "Training Loss: 0.001846334092406323\n",
      "Training Loss: 0.0017087398942385334\n",
      "Training Loss: 0.0017062313057249411\n",
      "Validation Loss: 0.002382201015729499\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.006064429591642693\n",
      "Training Loss: 0.0062645908165723085\n",
      "Training Loss: 0.006214376126299612\n",
      "Training Loss: 0.004337575176614337\n",
      "Training Loss: 0.0018043647127342411\n",
      "Training Loss: 0.0016560159577056765\n",
      "Training Loss: 0.001659865627734689\n",
      "Validation Loss: 0.0023258566992202154\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.006006914742756635\n",
      "Training Loss: 0.006212328323163092\n",
      "Training Loss: 0.006157193460385315\n",
      "Training Loss: 0.0042877094952564225\n",
      "Training Loss: 0.0017654680521809495\n",
      "Training Loss: 0.0016074746174854226\n",
      "Training Loss: 0.0016167297067295294\n",
      "Validation Loss: 0.0022759828803718984\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.0059525167720858\n",
      "Training Loss: 0.006162311024963856\n",
      "Training Loss: 0.006101949235890061\n",
      "Training Loss: 0.004240228707640199\n",
      "Training Loss: 0.0017288045232999138\n",
      "Training Loss: 0.0015623117866925896\n",
      "Training Loss: 0.0015761942652170546\n",
      "Validation Loss: 0.0022305333186795565\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.0059014513029251245\n",
      "Training Loss: 0.006114883049158379\n",
      "Training Loss: 0.0060490811005001885\n",
      "Training Loss: 0.004195424773788545\n",
      "Training Loss: 0.0016935738873144145\n",
      "Training Loss: 0.0015198431805765722\n",
      "Training Loss: 0.0015376569231739267\n",
      "Validation Loss: 0.0021879026241825496\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.005854081698926166\n",
      "Training Loss: 0.0060704314010217785\n",
      "Training Loss: 0.005999042647890747\n",
      "Training Loss: 0.0041534531014622185\n",
      "Training Loss: 0.001659010187286185\n",
      "Training Loss: 0.0014794446207815782\n",
      "Training Loss: 0.0015005117443797644\n",
      "Validation Loss: 0.002147209470217859\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.005810612849891186\n",
      "Training Loss: 0.006029109521768987\n",
      "Training Loss: 0.005952048161998391\n",
      "Training Loss: 0.004114227802347159\n",
      "Training Loss: 0.0016244683320110198\n",
      "Training Loss: 0.0014405914659437258\n",
      "Training Loss: 0.001464219576882897\n",
      "Validation Loss: 0.0021081917167315085\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.005770984740229324\n",
      "Training Loss: 0.005990785245085135\n",
      "Training Loss: 0.005908025552053005\n",
      "Training Loss: 0.004077479009283707\n",
      "Training Loss: 0.0015895172022283078\n",
      "Training Loss: 0.0014028746014810168\n",
      "Training Loss: 0.001428380874422146\n",
      "Validation Loss: 0.0020708241047597514\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.005734901237301529\n",
      "Training Loss: 0.005955117748817429\n",
      "Training Loss: 0.0058667008008342235\n",
      "Training Loss: 0.004042867808457231\n",
      "Training Loss: 0.0015539595231530256\n",
      "Training Loss: 0.0013660108621115796\n",
      "Training Loss: 0.001392767487850506\n",
      "Validation Loss: 0.0020350448725952623\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.005701915210811421\n",
      "Training Loss: 0.00592167362337932\n",
      "Training Loss: 0.0058277059061219915\n",
      "Training Loss: 0.004010075888509164\n",
      "Training Loss: 0.0015177786005369854\n",
      "Training Loss: 0.0013298107220907696\n",
      "Training Loss: 0.001357299088849686\n",
      "Validation Loss: 0.0020007385368928077\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.0056715517246630045\n",
      "Training Loss: 0.005890041826060042\n",
      "Training Loss: 0.005790697389165871\n",
      "Training Loss: 0.003978846696409164\n",
      "Training Loss: 0.0014810600911732764\n",
      "Training Loss: 0.001294150117028039\n",
      "Training Loss: 0.0013219790811126585\n",
      "Validation Loss: 0.001967752225041278\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.005643374592764303\n",
      "Training Loss: 0.005859878591727465\n",
      "Training Loss: 0.005755388243123889\n",
      "Training Loss: 0.003948999544954859\n",
      "Training Loss: 0.0014439336133364121\n",
      "Training Loss: 0.0012589486660726833\n",
      "Training Loss: 0.0012868621465167963\n",
      "Validation Loss: 0.0019359418255545562\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.005617035698378459\n",
      "Training Loss: 0.005830948681104928\n",
      "Training Loss: 0.0057215970364632085\n",
      "Training Loss: 0.003920421094517223\n",
      "Training Loss: 0.0014065335552731996\n",
      "Training Loss: 0.001224154024675954\n",
      "Training Loss: 0.0012520136915554758\n",
      "Validation Loss: 0.0019052089790400286\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.005592291093198583\n",
      "Training Loss: 0.005803115429589525\n",
      "Training Loss: 0.0056892222200986\n",
      "Training Loss: 0.0038930561098095497\n",
      "Training Loss: 0.0013690039605717174\n",
      "Training Loss: 0.001189755129016703\n",
      "Training Loss: 0.0012175178866891657\n",
      "Validation Loss: 0.001875531691395109\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.005568989026360213\n",
      "Training Loss: 0.005776333034737036\n",
      "Training Loss: 0.005658248355612159\n",
      "Training Loss: 0.003866896227118559\n",
      "Training Loss: 0.00133149234650773\n",
      "Training Loss: 0.0011557821296446491\n",
      "Training Loss: 0.0011834685708163306\n",
      "Validation Loss: 0.001846947556920524\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.0055470767733640965\n",
      "Training Loss: 0.005750645325751975\n",
      "Training Loss: 0.0056287417380372065\n",
      "Training Loss: 0.003841979814314982\n",
      "Training Loss: 0.0012941881633014417\n",
      "Training Loss: 0.001122335335094249\n",
      "Training Loss: 0.0011499936151085422\n",
      "Validation Loss: 0.001819564737767602\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.005526585088809952\n",
      "Training Loss: 0.005726163672516122\n",
      "Training Loss: 0.005600833274656907\n",
      "Training Loss: 0.003818385830090847\n",
      "Training Loss: 0.0012573488253110553\n",
      "Training Loss: 0.0010896079968370032\n",
      "Training Loss: 0.0011172769873519428\n",
      "Validation Loss: 0.001793557151014941\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.005507626705802977\n",
      "Training Loss: 0.0057030730514088645\n",
      "Training Loss: 0.005574726988561452\n",
      "Training Loss: 0.003796234676556196\n",
      "Training Loss: 0.0012213135839556344\n",
      "Training Loss: 0.0010578915338555817\n",
      "Training Loss: 0.0010855684270791243\n",
      "Validation Loss: 0.001769154378686498\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.005490383928408846\n",
      "Training Loss: 0.005681604433339089\n",
      "Training Loss: 0.005550671826349571\n",
      "Training Loss: 0.003775677576340968\n",
      "Training Loss: 0.0011865354157635012\n",
      "Training Loss: 0.0010275952344818507\n",
      "Training Loss: 0.0010552204679697753\n",
      "Validation Loss: 0.0017465914734543305\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.005475048314547166\n",
      "Training Loss: 0.005661996090784669\n",
      "Training Loss: 0.005528929200372659\n",
      "Training Loss: 0.0037568828568328174\n",
      "Training Loss: 0.0011535502244078089\n",
      "Training Loss: 0.0009992024042003323\n",
      "Training Loss: 0.0010266519634751603\n",
      "Validation Loss: 0.0017261065784466806\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.005461767086526379\n",
      "Training Loss: 0.00564443088194821\n",
      "Training Loss: 0.005509706634329632\n",
      "Training Loss: 0.0037399984223884532\n",
      "Training Loss: 0.0011229337556869722\n",
      "Training Loss: 0.00097321369452402\n",
      "Training Loss: 0.001000322398467688\n",
      "Validation Loss: 0.0017078516785533268\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.005450558181619272\n",
      "Training Loss: 0.005628967849188485\n",
      "Training Loss: 0.005493084289482795\n",
      "Training Loss: 0.003725113339460222\n",
      "Training Loss: 0.0010952179561718367\n",
      "Training Loss: 0.0009500515449326485\n",
      "Training Loss: 0.0009766539178963285\n",
      "Validation Loss: 0.001691893899524654\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.005441237089689821\n",
      "Training Loss: 0.005615486106253229\n",
      "Training Loss: 0.0054789501236518845\n",
      "Training Loss: 0.0037122129132330884\n",
      "Training Loss: 0.0010707768198335544\n",
      "Training Loss: 0.000929957081534667\n",
      "Training Loss: 0.0009559265281131957\n",
      "Validation Loss: 0.0016781630948070813\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.005433440306223929\n",
      "Training Loss: 0.005603686461108736\n",
      "Training Loss: 0.005467005946557037\n",
      "Training Loss: 0.003701175729947863\n",
      "Training Loss: 0.0010497687717725057\n",
      "Training Loss: 0.0009129461321572308\n",
      "Training Loss: 0.0009382294112583623\n",
      "Validation Loss: 0.001666500118769503\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.005426697694929317\n",
      "Training Loss: 0.0055931670404970645\n",
      "Training Loss: 0.005456827972084284\n",
      "Training Loss: 0.0036917726542742457\n",
      "Training Loss: 0.0010321095971448813\n",
      "Training Loss: 0.0008988174887781497\n",
      "Training Loss: 0.0009234406631730963\n",
      "Validation Loss: 0.0016566641122531774\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.005420553024159744\n",
      "Training Loss: 0.005583522316883318\n",
      "Training Loss: 0.005447953423135914\n",
      "Training Loss: 0.003683722456917167\n",
      "Training Loss: 0.0010175050512771123\n",
      "Training Loss: 0.0008872168359812349\n",
      "Training Loss: 0.0009112648700829595\n",
      "Validation Loss: 0.0016483796952503353\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.00541466215159744\n",
      "Training Loss: 0.00557444721867796\n",
      "Training Loss: 0.0054399739659857\n",
      "Training Loss: 0.003676733232132392\n",
      "Training Loss: 0.0010055553348502144\n",
      "Training Loss: 0.0008777335449121893\n",
      "Training Loss: 0.0009013261462678202\n",
      "Validation Loss: 0.0016413653977494337\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.005408806347404606\n",
      "Training Loss: 0.005565738883451559\n",
      "Training Loss: 0.0054325637180591005\n",
      "Training Loss: 0.003670539220329374\n",
      "Training Loss: 0.0009958074254973325\n",
      "Training Loss: 0.0008699517534114421\n",
      "Training Loss: 0.0008932123307022266\n",
      "Validation Loss: 0.0016353684615942018\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.005402887543314136\n",
      "Training Loss: 0.0055573005042970185\n",
      "Training Loss: 0.005425504805170931\n",
      "Training Loss: 0.003664925258199219\n",
      "Training Loss: 0.0009878341133298819\n",
      "Training Loss: 0.0008635119684913662\n",
      "Training Loss: 0.000886547543341294\n",
      "Validation Loss: 0.0016301579277243898\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.005396868829848245\n",
      "Training Loss: 0.005549083455698565\n",
      "Training Loss: 0.005418654453242197\n",
      "Training Loss: 0.003659719203133136\n",
      "Training Loss: 0.0009812540040002205\n",
      "Training Loss: 0.0008581082274031359\n",
      "Training Loss: 0.0008810019867087249\n",
      "Validation Loss: 0.0016255562786886657\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.005390769275836646\n",
      "Training Loss: 0.005541064767749048\n",
      "Training Loss: 0.005411933052819222\n",
      "Training Loss: 0.0036548009006946813\n",
      "Training Loss: 0.0009757598031137605\n",
      "Training Loss: 0.0008535038679838181\n",
      "Training Loss: 0.0008763106890546623\n",
      "Validation Loss: 0.001621410609065984\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0053846091480227185\n",
      "Training Loss: 0.005533234340255148\n",
      "Training Loss: 0.005405299994745292\n",
      "Training Loss: 0.0036500848065770695\n",
      "Training Loss: 0.0009710970640298911\n",
      "Training Loss: 0.0008495129596849438\n",
      "Training Loss: 0.0008722655002202373\n",
      "Validation Loss: 0.0016176232277618002\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.005378429250558838\n",
      "Training Loss: 0.005525579628301785\n",
      "Training Loss: 0.005398737917421386\n",
      "Training Loss: 0.0036455140076577662\n",
      "Training Loss: 0.0009670783398905769\n",
      "Training Loss: 0.000845996476127766\n",
      "Training Loss: 0.0008687061475939117\n",
      "Validation Loss: 0.0016141056105161385\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.005372258284478448\n",
      "Training Loss: 0.0055180936033139005\n",
      "Training Loss: 0.005392244717222638\n",
      "Training Loss: 0.0036410526433610355\n",
      "Training Loss: 0.0009635530425293837\n",
      "Training Loss: 0.0008428493609244469\n",
      "Training Loss: 0.0008655141037888825\n",
      "Validation Loss: 0.001610785027401703\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.005366122212726623\n",
      "Training Loss: 0.005510766598745249\n",
      "Training Loss: 0.005385822973912582\n",
      "Training Loss: 0.0036366784085112157\n",
      "Training Loss: 0.0009604091107030399\n",
      "Training Loss: 0.0008399929880397395\n",
      "Training Loss: 0.0008626032543543261\n",
      "Validation Loss: 0.0016076268862814326\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.005360041488311254\n",
      "Training Loss: 0.005503588507999666\n",
      "Training Loss: 0.005379476873204112\n",
      "Training Loss: 0.0036323744631954467\n",
      "Training Loss: 0.0009575629438040778\n",
      "Training Loss: 0.0008373650671273936\n",
      "Training Loss: 0.0008599028536991682\n",
      "Validation Loss: 0.0016045886319050434\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.005354033478070051\n",
      "Training Loss: 0.005496552202966996\n",
      "Training Loss: 0.0053732141054933895\n",
      "Training Loss: 0.0036281354492530226\n",
      "Training Loss: 0.000954950506129535\n",
      "Training Loss: 0.0008349213332985528\n",
      "Training Loss: 0.0008573664576397278\n",
      "Validation Loss: 0.0016016590200566899\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.005348113044165075\n",
      "Training Loss: 0.005489649411174469\n",
      "Training Loss: 0.005367034024675377\n",
      "Training Loss: 0.003623951852496248\n",
      "Training Loss: 0.0009525244440010283\n",
      "Training Loss: 0.0008326296310406179\n",
      "Training Loss: 0.0008549607770692091\n",
      "Validation Loss: 0.0015988035414416747\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.005342280833283439\n",
      "Training Loss: 0.005482873998698778\n",
      "Training Loss: 0.0053609463851898905\n",
      "Training Loss: 0.0036198242705722805\n",
      "Training Loss: 0.0009502447380509693\n",
      "Training Loss: 0.000830459988937946\n",
      "Training Loss: 0.000852652005123673\n",
      "Validation Loss: 0.0015960162253056266\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.00533655189210549\n",
      "Training Loss: 0.005476226963801309\n",
      "Training Loss: 0.005354954279027879\n",
      "Training Loss: 0.003615746788564138\n",
      "Training Loss: 0.0009480778590659611\n",
      "Training Loss: 0.0008283898180525284\n",
      "Training Loss: 0.000850421003124211\n",
      "Validation Loss: 0.0015932884801323926\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.0053309205226833\n",
      "Training Loss: 0.00546969408809673\n",
      "Training Loss: 0.005349054255639203\n",
      "Training Loss: 0.003611719205218833\n",
      "Training Loss: 0.0009460097490227781\n",
      "Training Loss: 0.0008264076411433053\n",
      "Training Loss: 0.0008482564016594552\n",
      "Validation Loss: 0.0015906098709452185\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.005325390173238702\n",
      "Training Loss: 0.00546326931391377\n",
      "Training Loss: 0.005343244560062885\n",
      "Training Loss: 0.00360773903230438\n",
      "Training Loss: 0.000944023822521558\n",
      "Training Loss: 0.0008245002516196109\n",
      "Training Loss: 0.000846144613751676\n",
      "Validation Loss: 0.001587980032910694\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.005319961894419975\n",
      "Training Loss: 0.005456954677938484\n",
      "Training Loss: 0.005337527390802279\n",
      "Training Loss: 0.0036038054632081184\n",
      "Training Loss: 0.0009420989095815458\n",
      "Training Loss: 0.0008226528631348628\n",
      "Training Loss: 0.0008440737175988034\n",
      "Validation Loss: 0.0015853925711433456\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.005314633140806109\n",
      "Training Loss: 0.005450741824461147\n",
      "Training Loss: 0.005331901431782171\n",
      "Training Loss: 0.0035999144428933505\n",
      "Training Loss: 0.0009402278609923087\n",
      "Training Loss: 0.000820858981605852\n",
      "Training Loss: 0.0008420366056088824\n",
      "Validation Loss: 0.0015828389905947731\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.005309398587560282\n",
      "Training Loss: 0.005444624816882424\n",
      "Training Loss: 0.005326359021710232\n",
      "Training Loss: 0.0035960665479069574\n",
      "Training Loss: 0.0009384018833225127\n",
      "Training Loss: 0.0008191120810806751\n",
      "Training Loss: 0.0008400289506244007\n",
      "Validation Loss: 0.001580323166652558\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.0053042593598365785\n",
      "Training Loss: 0.005438603274524212\n",
      "Training Loss: 0.0053209043882088735\n",
      "Training Loss: 0.0035922583428327926\n",
      "Training Loss: 0.0009366107587993611\n",
      "Training Loss: 0.0008174054336268455\n",
      "Training Loss: 0.0008380450711410959\n",
      "Validation Loss: 0.0015778282383885863\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.005299211289384403\n",
      "Training Loss: 0.00543267163389828\n",
      "Training Loss: 0.005315529579529538\n",
      "Training Loss: 0.0035884896780771667\n",
      "Training Loss: 0.0009348526249232236\n",
      "Training Loss: 0.0008157358961761929\n",
      "Training Loss: 0.0008360837573127356\n",
      "Validation Loss: 0.0015753734161840117\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.005294245452387258\n",
      "Training Loss: 0.005426820953143761\n",
      "Training Loss: 0.005310229097958654\n",
      "Training Loss: 0.0035847570215992164\n",
      "Training Loss: 0.0009331226705398876\n",
      "Training Loss: 0.0008140988502418622\n",
      "Training Loss: 0.000834139801299898\n",
      "Validation Loss: 0.0015729443049790819\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.005289363000774756\n",
      "Training Loss: 0.005421052090823651\n",
      "Training Loss: 0.0053050020115915685\n",
      "Training Loss: 0.0035810593450150917\n",
      "Training Loss: 0.0009314145089592785\n",
      "Training Loss: 0.0008124888173188083\n",
      "Training Loss: 0.0008322091451555025\n",
      "Validation Loss: 0.0015705444206691203\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.005284562704036944\n",
      "Training Loss: 0.005415363836218603\n",
      "Training Loss: 0.005299846188863739\n",
      "Training Loss: 0.0035773964125837667\n",
      "Training Loss: 0.0009297214179241564\n",
      "Training Loss: 0.000810902202501893\n",
      "Training Loss: 0.0008302924747113138\n",
      "Validation Loss: 0.0015681709707034439\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0052798361598979686\n",
      "Training Loss: 0.0054097482905490326\n",
      "Training Loss: 0.005294757319497876\n",
      "Training Loss: 0.003573765939217992\n",
      "Training Loss: 0.0009280456783017143\n",
      "Training Loss: 0.0008093378954799846\n",
      "Training Loss: 0.0008283851007581688\n",
      "Validation Loss: 0.001565817255314879\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.005275180945172906\n",
      "Training Loss: 0.005404203717480413\n",
      "Training Loss: 0.0052897319640032945\n",
      "Training Loss: 0.003570164275879506\n",
      "Training Loss: 0.0009263783993083052\n",
      "Training Loss: 0.000807790442922851\n",
      "Training Loss: 0.0008264850461273454\n",
      "Validation Loss: 0.0015634880551618389\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.005270598194329068\n",
      "Training Loss: 0.00539873108966276\n",
      "Training Loss: 0.005284767020493746\n",
      "Training Loss: 0.003566593073483091\n",
      "Training Loss: 0.0009247211746696848\n",
      "Training Loss: 0.0008062597246316727\n",
      "Training Loss: 0.000824591574491933\n",
      "Validation Loss: 0.0015611785150277829\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.005266078111599199\n",
      "Training Loss: 0.005393324146862142\n",
      "Training Loss: 0.005279859020956792\n",
      "Training Loss: 0.003563047836651094\n",
      "Training Loss: 0.0009230649466917385\n",
      "Training Loss: 0.0008047390649153386\n",
      "Training Loss: 0.000822701011056779\n",
      "Validation Loss: 0.0015588869812265247\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0052616223064251245\n",
      "Training Loss: 0.0053879816847620535\n",
      "Training Loss: 0.005275006482261233\n",
      "Training Loss: 0.0035595293999358545\n",
      "Training Loss: 0.0009214175553643144\n",
      "Training Loss: 0.000803233058686601\n",
      "Training Loss: 0.0008208174284663983\n",
      "Validation Loss: 0.0015566099917102773\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.005257223056978546\n",
      "Training Loss: 0.005382695862208493\n",
      "Training Loss: 0.005270202172105201\n",
      "Training Loss: 0.003556035527435597\n",
      "Training Loss: 0.000919772195193218\n",
      "Training Loss: 0.0008017358856159262\n",
      "Training Loss: 0.0008189348589803558\n",
      "Validation Loss: 0.001554350641622358\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.005252879947656766\n",
      "Training Loss: 0.005377469970262609\n",
      "Training Loss: 0.005265446248231455\n",
      "Training Loss: 0.0035525638498074843\n",
      "Training Loss: 0.0009181251836707816\n",
      "Training Loss: 0.0008002443413715809\n",
      "Training Loss: 0.0008170544142194558\n",
      "Validation Loss: 0.0015521132460688957\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.005248590469127521\n",
      "Training Loss: 0.00537230139656458\n",
      "Training Loss: 0.005260737295611761\n",
      "Training Loss: 0.003549112801847514\n",
      "Training Loss: 0.0009164727994357236\n",
      "Training Loss: 0.0007987576581945178\n",
      "Training Loss: 0.0008151724802155513\n",
      "Validation Loss: 0.0015498901643678684\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.005244350998545997\n",
      "Training Loss: 0.005367184998467564\n",
      "Training Loss: 0.005256069983006455\n",
      "Training Loss: 0.0035456827750022057\n",
      "Training Loss: 0.000914819117897423\n",
      "Training Loss: 0.0007972745109873358\n",
      "Training Loss: 0.0008132893589208834\n",
      "Validation Loss: 0.0015476693843765191\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.005240158193046227\n",
      "Training Loss: 0.005362118798075244\n",
      "Training Loss: 0.00525144113402348\n",
      "Training Loss: 0.003542271077749319\n",
      "Training Loss: 0.0009131594710925128\n",
      "Training Loss: 0.0007957929222902749\n",
      "Training Loss: 0.0008114038128405809\n",
      "Validation Loss: 0.00154547000322177\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.005236011031665839\n",
      "Training Loss: 0.005357103560236283\n",
      "Training Loss: 0.005246849583927542\n",
      "Training Loss: 0.0035388762890943325\n",
      "Training Loss: 0.0009114908341143746\n",
      "Training Loss: 0.0007943099840485956\n",
      "Training Loss: 0.0008095141692319885\n",
      "Validation Loss: 0.0015432798015459508\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0052319048612844195\n",
      "Training Loss: 0.005352134497952648\n",
      "Training Loss: 0.005242294554482214\n",
      "Training Loss: 0.0035354980354895815\n",
      "Training Loss: 0.0009098137938417495\n",
      "Training Loss: 0.000792826409888221\n",
      "Training Loss: 0.0008076222175441216\n",
      "Validation Loss: 0.001541096709379996\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.005227836836129427\n",
      "Training Loss: 0.005347209156025201\n",
      "Training Loss: 0.005237769699888304\n",
      "Training Loss: 0.003532133188418811\n",
      "Training Loss: 0.0009081258464721031\n",
      "Training Loss: 0.0007913387524604332\n",
      "Training Loss: 0.0008057240291964263\n",
      "Validation Loss: 0.0015389163646374372\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.005223808267037384\n",
      "Training Loss: 0.005342328571132384\n",
      "Training Loss: 0.0052332769171334805\n",
      "Training Loss: 0.0035287817638891285\n",
      "Training Loss: 0.0009064232773380354\n",
      "Training Loss: 0.0007898448118066881\n",
      "Training Loss: 0.0008038173729437403\n",
      "Validation Loss: 0.001536748605939012\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.005219814625452273\n",
      "Training Loss: 0.005337488417862915\n",
      "Training Loss: 0.005228809897089377\n",
      "Training Loss: 0.0035254421159334015\n",
      "Training Loss: 0.0009047088275838177\n",
      "Training Loss: 0.0007883465169288684\n",
      "Training Loss: 0.0008019048106507398\n",
      "Validation Loss: 0.0015345836438243853\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.005215853242552839\n",
      "Training Loss: 0.005332686816691421\n",
      "Training Loss: 0.005224369659554213\n",
      "Training Loss: 0.003522114415682154\n",
      "Training Loss: 0.0009029820948489942\n",
      "Training Loss: 0.0007868405838962645\n",
      "Training Loss: 0.0007999855699017644\n",
      "Validation Loss: 0.0015324217139582816\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.005211920128786005\n",
      "Training Loss: 0.005327922106371261\n",
      "Training Loss: 0.005219951710896567\n",
      "Training Loss: 0.003518796273128828\n",
      "Training Loss: 0.0009012363295187242\n",
      "Training Loss: 0.0007853253635403234\n",
      "Training Loss: 0.0007980565418256447\n",
      "Validation Loss: 0.0015302633392286211\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.005208016328979283\n",
      "Training Loss: 0.0053231925558065996\n",
      "Training Loss: 0.005215554476017133\n",
      "Training Loss: 0.0035154843007330783\n",
      "Training Loss: 0.0008994719208567404\n",
      "Training Loss: 0.0007837976995506324\n",
      "Training Loss: 0.0007961168310430366\n",
      "Validation Loss: 0.0015281057737380518\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.005204138722037896\n",
      "Training Loss: 0.005318496800027788\n",
      "Training Loss: 0.00521117537165992\n",
      "Training Loss: 0.003512180472898763\n",
      "Training Loss: 0.0008976919457199983\n",
      "Training Loss: 0.0007822611658775714\n",
      "Training Loss: 0.0007941702174139209\n",
      "Validation Loss: 0.0015259511450092806\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.00520028188242577\n",
      "Training Loss: 0.005313830553204752\n",
      "Training Loss: 0.005206812903052196\n",
      "Training Loss: 0.003508883151371265\n",
      "Training Loss: 0.0008958931290544569\n",
      "Training Loss: 0.0007807113557646517\n",
      "Training Loss: 0.0007922108436468989\n",
      "Validation Loss: 0.0015237954082638933\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.005196450827061199\n",
      "Training Loss: 0.005309195431182161\n",
      "Training Loss: 0.005202467649360188\n",
      "Training Loss: 0.003505587276595179\n",
      "Training Loss: 0.000894068217749009\n",
      "Training Loss: 0.0007791452009405475\n",
      "Training Loss: 0.0007902368824579753\n",
      "Validation Loss: 0.0015216380571824418\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.005192638837033883\n",
      "Training Loss: 0.0053045890637440606\n",
      "Training Loss: 0.00519813392078504\n",
      "Training Loss: 0.003502298275125213\n",
      "Training Loss: 0.0008922249003080651\n",
      "Training Loss: 0.0007775658284663223\n",
      "Training Loss: 0.0007882548667839728\n",
      "Validation Loss: 0.0015194752423200854\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.005188844382646493\n",
      "Training Loss: 0.005300008939229883\n",
      "Training Loss: 0.005193811335484497\n",
      "Training Loss: 0.0034990100871073082\n",
      "Training Loss: 0.0008903559998725541\n",
      "Training Loss: 0.0007759669916413259\n",
      "Training Loss: 0.0007862538288463839\n",
      "Validation Loss: 0.0015173095257949322\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.005185070455772802\n",
      "Training Loss: 0.005295455285231582\n",
      "Training Loss: 0.005189499226398766\n",
      "Training Loss: 0.003495723693195032\n",
      "Training Loss: 0.0008884627943916712\n",
      "Training Loss: 0.0007743525259138551\n",
      "Training Loss: 0.0007842403172980994\n",
      "Validation Loss: 0.0015151402414889748\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.005181309996405616\n",
      "Training Loss: 0.005290922079002485\n",
      "Training Loss: 0.0051851934473961594\n",
      "Training Loss: 0.0034924372761452106\n",
      "Training Loss: 0.0008865460386732594\n",
      "Training Loss: 0.0007727206361596473\n",
      "Training Loss: 0.0007822143749217503\n",
      "Validation Loss: 0.0015129593729864708\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.005177558441646397\n",
      "Training Loss: 0.005286410389235243\n",
      "Training Loss: 0.005180892315111123\n",
      "Training Loss: 0.0034891504756524227\n",
      "Training Loss: 0.0008846043494122568\n",
      "Training Loss: 0.0007710681963362731\n",
      "Training Loss: 0.0007801755260152276\n",
      "Validation Loss: 0.00151078046784799\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.005173821880016476\n",
      "Training Loss: 0.005281918194959871\n",
      "Training Loss: 0.00517659483011812\n",
      "Training Loss: 0.0034858607541536912\n",
      "Training Loss: 0.0008826332457829266\n",
      "Training Loss: 0.0007693946677318308\n",
      "Training Loss: 0.0007781157677527517\n",
      "Validation Loss: 0.001508590967825192\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.005170095994835719\n",
      "Training Loss: 0.005277445547399111\n",
      "Training Loss: 0.005172299975529313\n",
      "Training Loss: 0.003482568216859363\n",
      "Training Loss: 0.0008806369284866378\n",
      "Training Loss: 0.000767700963333482\n",
      "Training Loss: 0.0007760429484187626\n",
      "Validation Loss: 0.0015063911805248537\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.0051663757575443016\n",
      "Training Loss: 0.005272986109484918\n",
      "Training Loss: 0.005168003346188926\n",
      "Training Loss: 0.0034792721764824817\n",
      "Training Loss: 0.0008786109765060246\n",
      "Training Loss: 0.0007659829538897611\n",
      "Training Loss: 0.0007739518597372808\n",
      "Validation Loss: 0.0015041868876899532\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.005162665935931727\n",
      "Training Loss: 0.005268544894061051\n",
      "Training Loss: 0.0051637050631688905\n",
      "Training Loss: 0.0034759712139202747\n",
      "Training Loss: 0.000876555151771754\n",
      "Training Loss: 0.0007642417796887458\n",
      "Training Loss: 0.0007718450232641771\n",
      "Validation Loss: 0.0015019691675249138\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.005158960307016969\n",
      "Training Loss: 0.005264113425510004\n",
      "Training Loss: 0.005159404053119943\n",
      "Training Loss: 0.003472664380969945\n",
      "Training Loss: 0.000874468154652277\n",
      "Training Loss: 0.0007624760956969112\n",
      "Training Loss: 0.0007697196902881842\n",
      "Validation Loss: 0.0014997452130359823\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.0051552609936334195\n",
      "Training Loss: 0.005259694628184661\n",
      "Training Loss: 0.005155098541872576\n",
      "Training Loss: 0.0034693507399060765\n",
      "Training Loss: 0.0008723515075689648\n",
      "Training Loss: 0.0007606861410022248\n",
      "Training Loss: 0.0007675772251968738\n",
      "Validation Loss: 0.0014974995830605567\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.005151563558029011\n",
      "Training Loss: 0.005255284431623295\n",
      "Training Loss: 0.005150783218559809\n",
      "Training Loss: 0.003466028704715427\n",
      "Training Loss: 0.0008702038999763317\n",
      "Training Loss: 0.0007588683097856119\n",
      "Training Loss: 0.000765414522611536\n",
      "Validation Loss: 0.0014952507868717854\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.005147875164402649\n",
      "Training Loss: 0.005250881934189238\n",
      "Training Loss: 0.005146458604140207\n",
      "Training Loss: 0.0034626981617475396\n",
      "Training Loss: 0.0008680263221322093\n",
      "Training Loss: 0.0007570254204620142\n",
      "Training Loss: 0.0007632334936351981\n",
      "Validation Loss: 0.0014929726434486118\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.005144187545520254\n",
      "Training Loss: 0.005246483230148442\n",
      "Training Loss: 0.005142122431425378\n",
      "Training Loss: 0.003459355343802599\n",
      "Training Loss: 0.0008658139289764222\n",
      "Training Loss: 0.0007551508661708795\n",
      "Training Loss: 0.0007610323232074734\n",
      "Validation Loss: 0.0014906759413738809\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.005140500632114709\n",
      "Training Loss: 0.0052420888608321545\n",
      "Training Loss: 0.005137769680586644\n",
      "Training Loss: 0.003455998244462535\n",
      "Training Loss: 0.0008635711186798289\n",
      "Training Loss: 0.0007532474193430971\n",
      "Training Loss: 0.0007588106210459955\n",
      "Validation Loss: 0.0014883641172976543\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.0051368121238192545\n",
      "Training Loss: 0.0052376937516964974\n",
      "Training Loss: 0.005133396051242016\n",
      "Training Loss: 0.0034526284075400327\n",
      "Training Loss: 0.000861301612603711\n",
      "Training Loss: 0.000751315353991231\n",
      "Training Loss: 0.0007565695756056812\n",
      "Validation Loss: 0.0014860030749380198\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.005133112479234115\n",
      "Training Loss: 0.005233296243241057\n",
      "Training Loss: 0.005129000722081401\n",
      "Training Loss: 0.0034492408084042837\n",
      "Training Loss: 0.0008590070830541663\n",
      "Training Loss: 0.0007493569888174533\n",
      "Training Loss: 0.0007543089397950098\n",
      "Validation Loss: 0.001483631192873082\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.005129401882295497\n",
      "Training Loss: 0.005228893772000447\n",
      "Training Loss: 0.0051245837775059045\n",
      "Training Loss: 0.0034458371080108917\n",
      "Training Loss: 0.0008566819939005655\n",
      "Training Loss: 0.0007473701330309268\n",
      "Training Loss: 0.0007520285427744966\n",
      "Validation Loss: 0.00148124387062657\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.005125682544894516\n",
      "Training Loss: 0.005224488504463807\n",
      "Training Loss: 0.0051201468228828165\n",
      "Training Loss: 0.0034424187074182555\n",
      "Training Loss: 0.0008543283294420689\n",
      "Training Loss: 0.000745356137631461\n",
      "Training Loss: 0.0007497287461592351\n",
      "Validation Loss: 0.001478852856444711\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.005121950453612954\n",
      "Training Loss: 0.005220081741572358\n",
      "Training Loss: 0.005115691458922811\n",
      "Training Loss: 0.003438984447566327\n",
      "Training Loss: 0.0008519421942764893\n",
      "Training Loss: 0.0007433138386113569\n",
      "Training Loss: 0.0007474136505334172\n",
      "Validation Loss: 0.001476452350926024\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.005118206518236548\n",
      "Training Loss: 0.005215670825564302\n",
      "Training Loss: 0.005111215772340074\n",
      "Training Loss: 0.0034355343189963606\n",
      "Training Loss: 0.0008495207522355486\n",
      "Training Loss: 0.0007412418842432089\n",
      "Training Loss: 0.0007450772802985739\n",
      "Validation Loss: 0.001474042147400611\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.0051144549029413615\n",
      "Training Loss: 0.005211258190101944\n",
      "Training Loss: 0.005106719603063538\n",
      "Training Loss: 0.003432067765679676\n",
      "Training Loss: 0.0008470636712445412\n",
      "Training Loss: 0.0007391397937317379\n",
      "Training Loss: 0.0007427243651181925\n",
      "Validation Loss: 0.0014716230473683428\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.005110686890548095\n",
      "Training Loss: 0.005206836848519743\n",
      "Training Loss: 0.005102198920794763\n",
      "Training Loss: 0.0034285854014160576\n",
      "Training Loss: 0.0008445731036772486\n",
      "Training Loss: 0.000737006845884025\n",
      "Training Loss: 0.0007403550895105582\n",
      "Validation Loss: 0.0014691969003939951\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.005106905549182556\n",
      "Training Loss: 0.00520240772573743\n",
      "Training Loss: 0.005097654204000719\n",
      "Training Loss: 0.0034250864981731867\n",
      "Training Loss: 0.0008420509309507906\n",
      "Training Loss: 0.0007348466377879959\n",
      "Training Loss: 0.0007379686762578786\n",
      "Validation Loss: 0.0014667598969278655\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.005103108873590827\n",
      "Training Loss: 0.0051979691250016915\n",
      "Training Loss: 0.0050930839305510745\n",
      "Training Loss: 0.0034215710364514963\n",
      "Training Loss: 0.0008394893749209587\n",
      "Training Loss: 0.0007326534602907487\n",
      "Training Loss: 0.0007355652883416042\n",
      "Validation Loss: 0.0014643058135463728\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.005099298621062189\n",
      "Training Loss: 0.00519351857539732\n",
      "Training Loss: 0.0050884858291829005\n",
      "Training Loss: 0.003418037356605055\n",
      "Training Loss: 0.0008368988055735826\n",
      "Training Loss: 0.0007304323122662026\n",
      "Training Loss: 0.0007331473143131007\n",
      "Validation Loss: 0.0014618479148380123\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0050954716757405545\n",
      "Training Loss: 0.005189056557137519\n",
      "Training Loss: 0.005083857413846999\n",
      "Training Loss: 0.003414484933018684\n",
      "Training Loss: 0.0008342751016607508\n",
      "Training Loss: 0.0007281824399251491\n",
      "Training Loss: 0.0007307133152789902\n",
      "Validation Loss: 0.001459380474869399\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.005091629929374904\n",
      "Training Loss: 0.0051845774683170025\n",
      "Training Loss: 0.005079199880710803\n",
      "Training Loss: 0.003410915042331908\n",
      "Training Loss: 0.000831619410746498\n",
      "Training Loss: 0.0007259024982340634\n",
      "Training Loss: 0.0007282651834248099\n",
      "Validation Loss: 0.0014568934839939147\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.0050877696758834644\n",
      "Training Loss: 0.005180084564490244\n",
      "Training Loss: 0.0050745118589838965\n",
      "Training Loss: 0.003407329135661712\n",
      "Training Loss: 0.0008289332530694082\n",
      "Training Loss: 0.0007235933790798299\n",
      "Training Loss: 0.0007258004654431716\n",
      "Validation Loss: 0.0014543945713392512\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.005083895396091975\n",
      "Training Loss: 0.005175577068584971\n",
      "Training Loss: 0.005069792232243344\n",
      "Training Loss: 0.00340372500999365\n",
      "Training Loss: 0.0008262175746494904\n",
      "Training Loss: 0.0007212572811113205\n",
      "Training Loss: 0.0007233219253248535\n",
      "Validation Loss: 0.0014518884638252873\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.005080002245958895\n",
      "Training Loss: 0.00517105309816543\n",
      "Training Loss: 0.005065041002817452\n",
      "Training Loss: 0.0034001030954823363\n",
      "Training Loss: 0.0008234702517802362\n",
      "Training Loss: 0.0007188928479445167\n",
      "Training Loss: 0.0007208290771814063\n",
      "Validation Loss: 0.001449371827482891\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.005076094984542578\n",
      "Training Loss: 0.005166509699774906\n",
      "Training Loss: 0.005060257334262133\n",
      "Training Loss: 0.0033964636336895637\n",
      "Training Loss: 0.000820695003203582\n",
      "Training Loss: 0.0007165014029305894\n",
      "Training Loss: 0.0007183233750401996\n",
      "Validation Loss: 0.0014468401828292437\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.0050721675646491346\n",
      "Training Loss: 0.005161946513107978\n",
      "Training Loss: 0.005055439215502702\n",
      "Training Loss: 0.003392806718038628\n",
      "Training Loss: 0.0008178927052358631\n",
      "Training Loss: 0.0007140828047704417\n",
      "Training Loss: 0.0007158037662156857\n",
      "Validation Loss: 0.0014443063707577505\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.005068224507849664\n",
      "Training Loss: 0.005157365077757277\n",
      "Training Loss: 0.005050590454484336\n",
      "Training Loss: 0.0033891327683522833\n",
      "Training Loss: 0.0008150630403542891\n",
      "Training Loss: 0.0007116395473713055\n",
      "Training Loss: 0.0007132717076456175\n",
      "Validation Loss: 0.0014417558484004413\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.0050642658147262406\n",
      "Training Loss: 0.005152767524123192\n",
      "Training Loss: 0.005045710214180872\n",
      "Training Loss: 0.0033854415656242053\n",
      "Training Loss: 0.0008122061671747361\n",
      "Training Loss: 0.0007091712727560662\n",
      "Training Loss: 0.0007107269602420275\n",
      "Validation Loss: 0.0014391947687648879\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.005060289212269709\n",
      "Training Loss: 0.005148146281717345\n",
      "Training Loss: 0.005040792725048959\n",
      "Training Loss: 0.003381736848386936\n",
      "Training Loss: 0.0008093292133708018\n",
      "Training Loss: 0.0007066816650331021\n",
      "Training Loss: 0.0007081740599824116\n",
      "Validation Loss: 0.0014366245996869738\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.005056292428052984\n",
      "Training Loss: 0.005143506021122448\n",
      "Training Loss: 0.005035845147795044\n",
      "Training Loss: 0.003378016045608092\n",
      "Training Loss: 0.0008064275684591849\n",
      "Training Loss: 0.0007041675825894345\n",
      "Training Loss: 0.0007056091618142091\n",
      "Validation Loss: 0.0014340484186515128\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.005052282406832091\n",
      "Training Loss: 0.005138845816836692\n",
      "Training Loss: 0.005030865774024278\n",
      "Training Loss: 0.0033742798562161626\n",
      "Training Loss: 0.0008035046096483711\n",
      "Training Loss: 0.0007016338150424417\n",
      "Training Loss: 0.0007030343827500473\n",
      "Validation Loss: 0.001431459341240063\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.005048255840665661\n",
      "Training Loss: 0.005134167305659503\n",
      "Training Loss: 0.005025854929699563\n",
      "Training Loss: 0.003370530110114487\n",
      "Training Loss: 0.0008005616543232463\n",
      "Training Loss: 0.0006990814210439567\n",
      "Training Loss: 0.0007004528219113126\n",
      "Validation Loss: 0.0014288666315179681\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.005044211709755473\n",
      "Training Loss: 0.00512946562259458\n",
      "Training Loss: 0.005020811085705646\n",
      "Training Loss: 0.003366768425767077\n",
      "Training Loss: 0.0007976015396707226\n",
      "Training Loss: 0.0006965093351027462\n",
      "Training Loss: 0.0006978609946963843\n",
      "Validation Loss: 0.001426266585668542\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.0050401544803753496\n",
      "Training Loss: 0.005124748444650322\n",
      "Training Loss: 0.005015740704839118\n",
      "Training Loss: 0.003362991677277023\n",
      "Training Loss: 0.0007946217304561288\n",
      "Training Loss: 0.0006939196919847746\n",
      "Training Loss: 0.000695261270593619\n",
      "Validation Loss: 0.0014236538473879465\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.005036080014542676\n",
      "Training Loss: 0.005120012204861269\n",
      "Training Loss: 0.005010641686967574\n",
      "Training Loss: 0.003359207376779523\n",
      "Training Loss: 0.0007916273479349911\n",
      "Training Loss: 0.0006913148183957673\n",
      "Training Loss: 0.0006926567482878454\n",
      "Validation Loss: 0.0014210287968790306\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.0050319923128699885\n",
      "Training Loss: 0.005115259342128411\n",
      "Training Loss: 0.0050055153778521344\n",
      "Training Loss: 0.0033554117768653667\n",
      "Training Loss: 0.0007886183401569724\n",
      "Training Loss: 0.0006886961945565417\n",
      "Training Loss: 0.0006900445533392486\n",
      "Validation Loss: 0.0014183974107823331\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.0050278923026053236\n",
      "Training Loss: 0.005110491046798416\n",
      "Training Loss: 0.005000365085434169\n",
      "Training Loss: 0.0033516070802579634\n",
      "Training Loss: 0.0007855967272189446\n",
      "Training Loss: 0.0006860670002060942\n",
      "Training Loss: 0.0006874260712356772\n",
      "Validation Loss: 0.00141576161049568\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.005023778845788911\n",
      "Training Loss: 0.005105706311296671\n",
      "Training Loss: 0.004995187604799866\n",
      "Training Loss: 0.003347794810833875\n",
      "Training Loss: 0.000782563183165621\n",
      "Training Loss: 0.0006834242035984061\n",
      "Training Loss: 0.0006848029544926249\n",
      "Validation Loss: 0.001413117459698751\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.005019655405194498\n",
      "Training Loss: 0.005100908597232774\n",
      "Training Loss: 0.0049899899464799094\n",
      "Training Loss: 0.0033439744816860183\n",
      "Training Loss: 0.0007795212291239295\n",
      "Training Loss: 0.0006807745779224206\n",
      "Training Loss: 0.0006821772269904613\n",
      "Validation Loss: 0.0014104648509144797\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.005015520850429311\n",
      "Training Loss: 0.005096098236390389\n",
      "Training Loss: 0.004984769893344492\n",
      "Training Loss: 0.0033401521616906395\n",
      "Training Loss: 0.0007764750521164388\n",
      "Training Loss: 0.0006781187930027954\n",
      "Training Loss: 0.0006795489277283195\n",
      "Validation Loss: 0.0014077976895609612\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005011375240865163\n",
      "Training Loss: 0.005091276561142877\n",
      "Training Loss: 0.004979531756835059\n",
      "Training Loss: 0.0033363230884424413\n",
      "Training Loss: 0.0007734189362963662\n",
      "Training Loss: 0.0006754558591637761\n",
      "Training Loss: 0.0006769165799778421\n",
      "Validation Loss: 0.0014051328966270238\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.005007221314008348\n",
      "Training Loss: 0.005086446776404046\n",
      "Training Loss: 0.004974279111484066\n",
      "Training Loss: 0.0033324926826753653\n",
      "Training Loss: 0.0007703606199356728\n",
      "Training Loss: 0.0006727896530355792\n",
      "Training Loss: 0.0006742840276274364\n",
      "Validation Loss: 0.001402456422919784\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.00500306082656607\n",
      "Training Loss: 0.005081609785556793\n",
      "Training Loss: 0.004969011212233454\n",
      "Training Loss: 0.003328661289997399\n",
      "Training Loss: 0.0007673008942219895\n",
      "Training Loss: 0.0006701227970188484\n",
      "Training Loss: 0.000671649083669763\n",
      "Validation Loss: 0.001399770942067532\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.0049988919193856415\n",
      "Training Loss: 0.005076766108395532\n",
      "Training Loss: 0.004963729400769807\n",
      "Training Loss: 0.0033248288033064454\n",
      "Training Loss: 0.0007642405683873221\n",
      "Training Loss: 0.0006674553103221115\n",
      "Training Loss: 0.0006690159434219823\n",
      "Validation Loss: 0.0013970806321594864\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.00499471825780347\n",
      "Training Loss: 0.0050719191029202195\n",
      "Training Loss: 0.004958437582245096\n",
      "Training Loss: 0.0033209996731602587\n",
      "Training Loss: 0.000761181763082277\n",
      "Training Loss: 0.0006647884525591508\n",
      "Training Loss: 0.0006663815473439172\n",
      "Validation Loss: 0.0013943857295143902\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.00499054406885989\n",
      "Training Loss: 0.005067072894889862\n",
      "Training Loss: 0.004953138938290067\n",
      "Training Loss: 0.0033171708781446797\n",
      "Training Loss: 0.0007581222789303866\n",
      "Training Loss: 0.0006621252468903549\n",
      "Training Loss: 0.0006637480690551456\n",
      "Validation Loss: 0.001391677016175216\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.004986362897907384\n",
      "Training Loss: 0.005062227750313468\n",
      "Training Loss: 0.0049478346534306186\n",
      "Training Loss: 0.0033133471781911796\n",
      "Training Loss: 0.0007550687919138\n",
      "Training Loss: 0.0006594656578090507\n",
      "Training Loss: 0.0006611175002763048\n",
      "Validation Loss: 0.0013889691299516429\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.004982181212981231\n",
      "Training Loss: 0.005057382564991712\n",
      "Training Loss: 0.0049425245838938285\n",
      "Training Loss: 0.003309528156678425\n",
      "Training Loss: 0.0007520224417385179\n",
      "Training Loss: 0.0006568130578671116\n",
      "Training Loss: 0.000658488901972305\n",
      "Validation Loss: 0.001386255332816406\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.004978001816780307\n",
      "Training Loss: 0.005052544838399627\n",
      "Training Loss: 0.004937214337987825\n",
      "Training Loss: 0.0033057170012034476\n",
      "Training Loss: 0.0007489844283554703\n",
      "Training Loss: 0.0006541687574645039\n",
      "Training Loss: 0.0006558649614453316\n",
      "Validation Loss: 0.0013835253490462956\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.004973817853606306\n",
      "Training Loss: 0.005047711640945636\n",
      "Training Loss: 0.004931902752141468\n",
      "Training Loss: 0.0033019126550061628\n",
      "Training Loss: 0.0007459523413854186\n",
      "Training Loss: 0.0006515318305173423\n",
      "Training Loss: 0.0006532429868821055\n",
      "Validation Loss: 0.0013807949060460811\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.004969639486516826\n",
      "Training Loss: 0.005042891117627732\n",
      "Training Loss: 0.004926595237920992\n",
      "Training Loss: 0.0032981177333567756\n",
      "Training Loss: 0.0007429299455543514\n",
      "Training Loss: 0.0006489040549786296\n",
      "Training Loss: 0.000650626347924117\n",
      "Validation Loss: 0.0013780552840029492\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.004965463879052549\n",
      "Training Loss: 0.005038080519880168\n",
      "Training Loss: 0.004921292085782625\n",
      "Training Loss: 0.003294332951627439\n",
      "Training Loss: 0.0007399172711302527\n",
      "Training Loss: 0.000646287257259246\n",
      "Training Loss: 0.0006480126979295165\n",
      "Validation Loss: 0.0013753137750069746\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.004961290013161488\n",
      "Training Loss: 0.005033280362840742\n",
      "Training Loss: 0.0049159923929255455\n",
      "Training Loss: 0.003290560293389717\n",
      "Training Loss: 0.0007369213410129305\n",
      "Training Loss: 0.0006436841849063057\n",
      "Training Loss: 0.0006454049074091018\n",
      "Validation Loss: 0.0013725611688403503\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.004957121758488938\n",
      "Training Loss: 0.005028497152379714\n",
      "Training Loss: 0.004910701076732948\n",
      "Training Loss: 0.0032867968920618296\n",
      "Training Loss: 0.000733936100587016\n",
      "Training Loss: 0.0006410929777484853\n",
      "Training Loss: 0.0006428026208595838\n",
      "Validation Loss: 0.0013698075606081328\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.004952957811765373\n",
      "Training Loss: 0.005023732127156109\n",
      "Training Loss: 0.00490541952138301\n",
      "Training Loss: 0.003283045143034542\n",
      "Training Loss: 0.0007309623943001498\n",
      "Training Loss: 0.0006385150652204175\n",
      "Training Loss: 0.0006402060789696407\n",
      "Validation Loss: 0.0013670517737307492\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.004948803475708701\n",
      "Training Loss: 0.0050189838814549146\n",
      "Training Loss: 0.004900147562730126\n",
      "Training Loss: 0.00327930912913871\n",
      "Training Loss: 0.0007280066788371187\n",
      "Training Loss: 0.0006359524345316459\n",
      "Training Loss: 0.0006376156097394415\n",
      "Validation Loss: 0.0013642840565719436\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.0049446546257240695\n",
      "Training Loss: 0.005014254678389989\n",
      "Training Loss: 0.004894887818954885\n",
      "Training Loss: 0.003275585731171304\n",
      "Training Loss: 0.0007250634056981653\n",
      "Training Loss: 0.0006334020310896449\n",
      "Training Loss: 0.0006350307163666002\n",
      "Validation Loss: 0.0013615141038683485\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.004940514407353476\n",
      "Training Loss: 0.005009547872468829\n",
      "Training Loss: 0.004889640377950854\n",
      "Training Loss: 0.003271877227234654\n",
      "Training Loss: 0.0007221382405259647\n",
      "Training Loss: 0.0006308681319933385\n",
      "Training Loss: 0.0006324529199628159\n",
      "Validation Loss: 0.0013587400149327289\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.004936383503372781\n",
      "Training Loss: 0.005004863602225669\n",
      "Training Loss: 0.0048844064568402245\n",
      "Training Loss: 0.0032681817404227333\n",
      "Training Loss: 0.0007192266547644976\n",
      "Training Loss: 0.0006283492385409773\n",
      "Training Loss: 0.000629878893669229\n",
      "Validation Loss: 0.0013559586058932353\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.004932261072681286\n",
      "Training Loss: 0.0050002035725628955\n",
      "Training Loss: 0.004879189059720374\n",
      "Training Loss: 0.0032645020198833665\n",
      "Training Loss: 0.0007163312079501338\n",
      "Training Loss: 0.0006258436501957476\n",
      "Training Loss: 0.0006273099065583665\n",
      "Validation Loss: 0.0013531759568343588\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.0049281505838735026\n",
      "Training Loss: 0.004995569757884368\n",
      "Training Loss: 0.004873986433958635\n",
      "Training Loss: 0.0032608352512761484\n",
      "Training Loss: 0.000713451910414733\n",
      "Training Loss: 0.0006233538036758546\n",
      "Training Loss: 0.0006247481674654409\n",
      "Validation Loss: 0.0013503878308052009\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.004924052418209613\n",
      "Training Loss: 0.00499096094048582\n",
      "Training Loss: 0.004868798864772543\n",
      "Training Loss: 0.0032571861741598697\n",
      "Training Loss: 0.0007105920914909803\n",
      "Training Loss: 0.000620881580398418\n",
      "Training Loss: 0.0006221931034815498\n",
      "Validation Loss: 0.0013476018499572542\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.00491996307449881\n",
      "Training Loss: 0.004986377460299991\n",
      "Training Loss: 0.004863626842270605\n",
      "Training Loss: 0.0032535492097667883\n",
      "Training Loss: 0.0007077466741611715\n",
      "Training Loss: 0.000618423566193087\n",
      "Training Loss: 0.0006196431399439462\n",
      "Validation Loss: 0.0013448103403625486\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.004915886418893933\n",
      "Training Loss: 0.004981822979752906\n",
      "Training Loss: 0.004858472541673109\n",
      "Training Loss: 0.003249928779405309\n",
      "Training Loss: 0.0007049174977873918\n",
      "Training Loss: 0.0006159801183093804\n",
      "Training Loss: 0.0006170968283549883\n",
      "Validation Loss: 0.0013420162718108253\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.004911824822193012\n",
      "Training Loss: 0.004977296494180336\n",
      "Training Loss: 0.004853334759827703\n",
      "Training Loss: 0.0032463225460378454\n",
      "Training Loss: 0.0007021037203958258\n",
      "Training Loss: 0.0006135499337688088\n",
      "Training Loss: 0.0006145560016739182\n",
      "Validation Loss: 0.0013392254306231921\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.004907773568993435\n",
      "Training Loss: 0.004972798315575347\n",
      "Training Loss: 0.004848214827943593\n",
      "Training Loss: 0.003242730089259567\n",
      "Training Loss: 0.0006993073388002813\n",
      "Training Loss: 0.000611135667131748\n",
      "Training Loss: 0.0006120182749873493\n",
      "Validation Loss: 0.0013364309385943852\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.00490373597072903\n",
      "Training Loss: 0.004968328062677756\n",
      "Training Loss: 0.004843111464870162\n",
      "Training Loss: 0.0032391525551793165\n",
      "Training Loss: 0.0006965277179551777\n",
      "Training Loss: 0.0006087351607857272\n",
      "Training Loss: 0.0006094837030104827\n",
      "Validation Loss: 0.001333632200240412\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.004899712343467399\n",
      "Training Loss: 0.004963887380436063\n",
      "Training Loss: 0.004838024963391945\n",
      "Training Loss: 0.003235586590162711\n",
      "Training Loss: 0.000693760572175961\n",
      "Training Loss: 0.0006063470939989202\n",
      "Training Loss: 0.0006069519500306341\n",
      "Validation Loss: 0.0013308383576521364\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.004895705072558485\n",
      "Training Loss: 0.004959476765361614\n",
      "Training Loss: 0.004832954821176827\n",
      "Training Loss: 0.0032320340521982873\n",
      "Training Loss: 0.0006910106283612549\n",
      "Training Loss: 0.000603972876269836\n",
      "Training Loss: 0.0006044229457620532\n",
      "Validation Loss: 0.0013280443679096617\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.004891710809897631\n",
      "Training Loss: 0.004955093844910152\n",
      "Training Loss: 0.004827902297256515\n",
      "Training Loss: 0.0032284929625166115\n",
      "Training Loss: 0.0006882728710479568\n",
      "Training Loss: 0.0006016081190318801\n",
      "Training Loss: 0.000601893262937665\n",
      "Validation Loss: 0.0013252537349179693\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.004887733231298625\n",
      "Training Loss: 0.0049507437838474284\n",
      "Training Loss: 0.0048228666809154675\n",
      "Training Loss: 0.003224965619156137\n",
      "Training Loss: 0.0006855469274159987\n",
      "Training Loss: 0.0005992555298143997\n",
      "Training Loss: 0.0005993664120614994\n",
      "Validation Loss: 0.0013224591651699446\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.004883769604493864\n",
      "Training Loss: 0.004946421390050091\n",
      "Training Loss: 0.004817848838283681\n",
      "Training Loss: 0.003221446837123949\n",
      "Training Loss: 0.0006828348588896916\n",
      "Training Loss: 0.0005969138274667785\n",
      "Training Loss: 0.0005968372555798851\n",
      "Validation Loss: 0.0013196785800849622\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.00487982414138969\n",
      "Training Loss: 0.004942128944094293\n",
      "Training Loss: 0.0048128445661859585\n",
      "Training Loss: 0.0032179407683725005\n",
      "Training Loss: 0.0006801369177992456\n",
      "Training Loss: 0.000594584048667457\n",
      "Training Loss: 0.0005943098607531283\n",
      "Validation Loss: 0.0013168946882816247\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.004875893032294698\n",
      "Training Loss: 0.0049378643231466415\n",
      "Training Loss: 0.004807858277927153\n",
      "Training Loss: 0.0032144423499994444\n",
      "Training Loss: 0.0006774469546508044\n",
      "Training Loss: 0.0005922604652005247\n",
      "Training Loss: 0.0005917770070664119\n",
      "Validation Loss: 0.0013141156479887217\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.004871980256284587\n",
      "Training Loss: 0.004933629142469726\n",
      "Training Loss: 0.004802886733668857\n",
      "Training Loss: 0.003210953949164832\n",
      "Training Loss: 0.0006747696490492672\n",
      "Training Loss: 0.000589945593237644\n",
      "Training Loss: 0.000589243709546281\n",
      "Validation Loss: 0.0013113443419025088\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.004868084657937288\n",
      "Training Loss: 0.004929425526061095\n",
      "Training Loss: 0.004797932067885995\n",
      "Training Loss: 0.0032074736258073245\n",
      "Training Loss: 0.0006721018339158036\n",
      "Training Loss: 0.0005876397595420712\n",
      "Training Loss: 0.0005867070202657487\n",
      "Validation Loss: 0.0013085801988466878\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.0048642073018709195\n",
      "Training Loss: 0.004925249342923052\n",
      "Training Loss: 0.00479299234924838\n",
      "Training Loss: 0.003204000819823705\n",
      "Training Loss: 0.0006694389812764712\n",
      "Training Loss: 0.0005853390197444241\n",
      "Training Loss: 0.0005841673245595302\n",
      "Validation Loss: 0.001305822278887438\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.004860347180510871\n",
      "Training Loss: 0.004921102360240184\n",
      "Training Loss: 0.00478806815459393\n",
      "Training Loss: 0.0032005351249244996\n",
      "Training Loss: 0.0006667850400845054\n",
      "Training Loss: 0.0005830445265746676\n",
      "Training Loss: 0.0005816193074861076\n",
      "Validation Loss: 0.0013030740542098363\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.004856509211822413\n",
      "Training Loss: 0.004916983239818365\n",
      "Training Loss: 0.004783159149228595\n",
      "Training Loss: 0.0031970756195369176\n",
      "Training Loss: 0.0006641405745176598\n",
      "Training Loss: 0.000580756131457747\n",
      "Training Loss: 0.0005790684920793865\n",
      "Validation Loss: 0.0013003399345347107\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.004852687689126469\n",
      "Training Loss: 0.004912891768617556\n",
      "Training Loss: 0.004778265252243727\n",
      "Training Loss: 0.0031936201748612803\n",
      "Training Loss: 0.0006615008132939693\n",
      "Training Loss: 0.0005784724147815723\n",
      "Training Loss: 0.0005765103742305655\n",
      "Validation Loss: 0.0012976144049586973\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.0048488869774155315\n",
      "Training Loss: 0.004908826971659437\n",
      "Training Loss: 0.004773385385633446\n",
      "Training Loss: 0.0031901709614612626\n",
      "Training Loss: 0.0006588679432752542\n",
      "Training Loss: 0.0005761927431012737\n",
      "Training Loss: 0.0005739451962290332\n",
      "Validation Loss: 0.0012949005250900997\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.0048451067344285545\n",
      "Training Loss: 0.004904790366417729\n",
      "Training Loss: 0.004768520755460486\n",
      "Training Loss: 0.0031867250257346315\n",
      "Training Loss: 0.0006562386502628214\n",
      "Training Loss: 0.0005739155805349583\n",
      "Training Loss: 0.0005713714992452879\n",
      "Validation Loss: 0.0012922005052939895\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.004841349189518951\n",
      "Training Loss: 0.004900781185715459\n",
      "Training Loss: 0.004763671214459464\n",
      "Training Loss: 0.003183281666279072\n",
      "Training Loss: 0.0006536147400038317\n",
      "Training Loss: 0.0005716432181361597\n",
      "Training Loss: 0.0005687916297756601\n",
      "Validation Loss: 0.0012895184262108044\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.004837610244867392\n",
      "Training Loss: 0.004896795832901262\n",
      "Training Loss: 0.004758834961685352\n",
      "Training Loss: 0.003179841770470375\n",
      "Training Loss: 0.0006509856530465185\n",
      "Training Loss: 0.0005693669676111313\n",
      "Training Loss: 0.0005661935385433026\n",
      "Validation Loss: 0.0012868563657668366\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.004833899836521596\n",
      "Training Loss: 0.004892843929119408\n",
      "Training Loss: 0.004754017252125777\n",
      "Training Loss: 0.0031764072277292144\n",
      "Training Loss: 0.0006483560592459981\n",
      "Training Loss: 0.0005670888644090155\n",
      "Training Loss: 0.00056358352638199\n",
      "Validation Loss: 0.001284205219804306\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.004830216460977681\n",
      "Training Loss: 0.004888923931866884\n",
      "Training Loss: 0.00474921460612677\n",
      "Training Loss: 0.0031729743245523423\n",
      "Training Loss: 0.0006457232830871363\n",
      "Training Loss: 0.0005648063116677804\n",
      "Training Loss: 0.0005609581012686249\n",
      "Validation Loss: 0.0012815697691996306\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.004826556128100492\n",
      "Training Loss: 0.004885028739809058\n",
      "Training Loss: 0.004744427992845885\n",
      "Training Loss: 0.0031695434912398924\n",
      "Training Loss: 0.0006430920962884557\n",
      "Training Loss: 0.0005625242788664764\n",
      "Training Loss: 0.0005583233569632284\n",
      "Validation Loss: 0.0012789590217518422\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.0048229202232323585\n",
      "Training Loss: 0.004881161337252706\n",
      "Training Loss: 0.004739658184698783\n",
      "Training Loss: 0.0031661125470418485\n",
      "Training Loss: 0.0006404592172475532\n",
      "Training Loss: 0.0005602389598061563\n",
      "Training Loss: 0.0005556765118672047\n",
      "Validation Loss: 0.001276370748129328\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.0048193081218050795\n",
      "Training Loss: 0.0048773153842194\n",
      "Training Loss: 0.004734901392948814\n",
      "Training Loss: 0.0031626822592807\n",
      "Training Loss: 0.0006378268623666372\n",
      "Training Loss: 0.0005579520492756273\n",
      "Training Loss: 0.0005530176578031387\n",
      "Validation Loss: 0.0012738050634393727\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.004815722418134101\n",
      "Training Loss: 0.004873495939536951\n",
      "Training Loss: 0.004730160962790251\n",
      "Training Loss: 0.003159252129116794\n",
      "Training Loss: 0.0006351880711736158\n",
      "Training Loss: 0.0005556601889111334\n",
      "Training Loss: 0.0005503442956251093\n",
      "Validation Loss: 0.0012712646768452802\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.004812162347952835\n",
      "Training Loss: 0.004869700630079024\n",
      "Training Loss: 0.004725436691660434\n",
      "Training Loss: 0.0031558188391500154\n",
      "Training Loss: 0.0006325485122215469\n",
      "Training Loss: 0.0005533689566073008\n",
      "Training Loss: 0.0005476600625843275\n",
      "Validation Loss: 0.00126875256369595\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.0048086282791337\n",
      "Training Loss: 0.004865925779449753\n",
      "Training Loss: 0.004720722397905774\n",
      "Training Loss: 0.003152385285065975\n",
      "Training Loss: 0.0006299110069812741\n",
      "Training Loss: 0.000551076248739264\n",
      "Training Loss: 0.0005449658210272901\n",
      "Validation Loss: 0.0012662659241374902\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.00480511560512241\n",
      "Training Loss: 0.004862170691485517\n",
      "Training Loss: 0.004716025185771287\n",
      "Training Loss: 0.0031489493358822073\n",
      "Training Loss: 0.0006272694436484016\n",
      "Training Loss: 0.0005487796578381676\n",
      "Training Loss: 0.0005422584821644705\n",
      "Validation Loss: 0.0012638169567079775\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.004801633702008985\n",
      "Training Loss: 0.0048584374401252715\n",
      "Training Loss: 0.004711341743823141\n",
      "Training Loss: 0.003145511616312433\n",
      "Training Loss: 0.0006246245287184138\n",
      "Training Loss: 0.0005464806778763886\n",
      "Training Loss: 0.0005395407455216628\n",
      "Validation Loss: 0.0012613963192958412\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.0047981776727829125\n",
      "Training Loss: 0.004854723609169014\n",
      "Training Loss: 0.004706672237953171\n",
      "Training Loss: 0.003142070783942472\n",
      "Training Loss: 0.0006219781118852552\n",
      "Training Loss: 0.0005441793261707062\n",
      "Training Loss: 0.0005368109472328797\n",
      "Validation Loss: 0.0012590136289184963\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.004794746328261681\n",
      "Training Loss: 0.004851028144476004\n",
      "Training Loss: 0.0047020167275331915\n",
      "Training Loss: 0.003138627447915496\n",
      "Training Loss: 0.0006193297884601634\n",
      "Training Loss: 0.0005418756183644291\n",
      "Training Loss: 0.0005340714144404046\n",
      "Validation Loss: 0.0012566682800636644\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.004791340575902723\n",
      "Training Loss: 0.004847349427291192\n",
      "Training Loss: 0.004697374792885967\n",
      "Training Loss: 0.0031351811398053543\n",
      "Training Loss: 0.0006166794593445956\n",
      "Training Loss: 0.000539571018962306\n",
      "Training Loss: 0.0005313214215857443\n",
      "Validation Loss: 0.001254362924668017\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.004787962034461088\n",
      "Training Loss: 0.004843687450629659\n",
      "Training Loss: 0.004692746802465991\n",
      "Training Loss: 0.0031317294905602467\n",
      "Training Loss: 0.000614025973918615\n",
      "Training Loss: 0.0005372636282118037\n",
      "Training Loss: 0.000528562176041305\n",
      "Validation Loss: 0.0012521017629484044\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.00478461139893625\n",
      "Training Loss: 0.004840043565491214\n",
      "Training Loss: 0.004688134421012364\n",
      "Training Loss: 0.0031282750054378995\n",
      "Training Loss: 0.0006113713381637354\n",
      "Training Loss: 0.0005349548846425023\n",
      "Training Loss: 0.0005257932006497867\n",
      "Validation Loss: 0.0012498865138838757\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.004781286779325456\n",
      "Training Loss: 0.0048364134505391124\n",
      "Training Loss: 0.0046835341755650005\n",
      "Training Loss: 0.0031248165456054266\n",
      "Training Loss: 0.0006087145172932651\n",
      "Training Loss: 0.0005326442228397355\n",
      "Training Loss: 0.0005230157465848606\n",
      "Validation Loss: 0.0012477194350378497\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.004777991483570077\n",
      "Training Loss: 0.004832794534158893\n",
      "Training Loss: 0.00467894686589716\n",
      "Training Loss: 0.00312135254353052\n",
      "Training Loss: 0.0006060541540500708\n",
      "Training Loss: 0.0005303320539678679\n",
      "Training Loss: 0.0005202279768127483\n",
      "Validation Loss: 0.001245597147422909\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.0047747192060342055\n",
      "Training Loss: 0.004829191151657142\n",
      "Training Loss: 0.0046743734276969914\n",
      "Training Loss: 0.0031178857485065236\n",
      "Training Loss: 0.0006033927855605726\n",
      "Training Loss: 0.0005280208731710445\n",
      "Training Loss: 0.0005174351576715708\n",
      "Validation Loss: 0.0012435247919527458\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.00477147513825912\n",
      "Training Loss: 0.004825599293108098\n",
      "Training Loss: 0.0046698133068275634\n",
      "Training Loss: 0.003114413372677518\n",
      "Training Loss: 0.0006007310014683753\n",
      "Training Loss: 0.0005257074536348228\n",
      "Training Loss: 0.000514634371647844\n",
      "Validation Loss: 0.0012415086675900966\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.004768260642304085\n",
      "Training Loss: 0.004822019802522845\n",
      "Training Loss: 0.004665267960226629\n",
      "Training Loss: 0.0031109365719021297\n",
      "Training Loss: 0.0005980652618745808\n",
      "Training Loss: 0.0005233931259135716\n",
      "Training Loss: 0.0005118261661846191\n",
      "Validation Loss: 0.0012395518988001157\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.004765070955618285\n",
      "Training Loss: 0.004818450062302872\n",
      "Training Loss: 0.0046607363046496176\n",
      "Training Loss: 0.003107454573328141\n",
      "Training Loss: 0.0005953984506777487\n",
      "Training Loss: 0.0005210778688342543\n",
      "Training Loss: 0.0005090128313167952\n",
      "Validation Loss: 0.0012376484969519217\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0047619112581014636\n",
      "Training Loss: 0.004814891161513515\n",
      "Training Loss: 0.004656215947470628\n",
      "Training Loss: 0.0031039681474794635\n",
      "Training Loss: 0.0005927328109100927\n",
      "Training Loss: 0.0005187661817035405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [41:18<41:24, 496.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0005061966100765858\n",
      "Validation Loss: 0.0012358045020168613\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.5419870032370091\n",
      "Training Loss: 0.4051355507969856\n",
      "Training Loss: 0.2417894660308957\n",
      "Training Loss: 0.09626479784958064\n",
      "Training Loss: 0.05380761332809925\n",
      "Training Loss: 0.046856916453689335\n",
      "Training Loss: 0.04886153196915984\n",
      "Validation Loss: 0.047827904925364234\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.049600860606878995\n",
      "Training Loss: 0.04834392454475164\n",
      "Training Loss: 0.046239545354619625\n",
      "Training Loss: 0.042980903633870186\n",
      "Training Loss: 0.04034460728056729\n",
      "Training Loss: 0.036064659231342375\n",
      "Training Loss: 0.03727126909885556\n",
      "Validation Loss: 0.03686216109073006\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.039454062720760706\n",
      "Training Loss: 0.038581420425325634\n",
      "Training Loss: 0.036302082063630225\n",
      "Training Loss: 0.03275818480178714\n",
      "Training Loss: 0.029423322686925532\n",
      "Training Loss: 0.02599576116539538\n",
      "Training Loss: 0.02618947054259479\n",
      "Validation Loss: 0.026656941882219544\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.029898831276223062\n",
      "Training Loss: 0.029231336303055286\n",
      "Training Loss: 0.0269553613755852\n",
      "Training Loss: 0.023184322114102544\n",
      "Training Loss: 0.019226404128130526\n",
      "Training Loss: 0.016806884922552855\n",
      "Training Loss: 0.01651735823135823\n",
      "Validation Loss: 0.017907320462474216\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.02147524553351104\n",
      "Training Loss: 0.02108233025763184\n",
      "Training Loss: 0.019225435508415104\n",
      "Training Loss: 0.015591239260393195\n",
      "Training Loss: 0.011449479399016128\n",
      "Training Loss: 0.010034900055034086\n",
      "Training Loss: 0.009905465220799669\n",
      "Validation Loss: 0.01141695564958068\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.015432157814502715\n",
      "Training Loss: 0.015332048947457225\n",
      "Training Loss: 0.014148270413279533\n",
      "Training Loss: 0.010907335090450943\n",
      "Training Loss: 0.007024925402365625\n",
      "Training Loss: 0.006402768805855885\n",
      "Training Loss: 0.006521162179997191\n",
      "Validation Loss: 0.008194731940839733\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.012241222215816378\n",
      "Training Loss: 0.012397805971559137\n",
      "Training Loss: 0.011793031445704401\n",
      "Training Loss: 0.008874196074320934\n",
      "Training Loss: 0.005218485131044872\n",
      "Training Loss: 0.004983533904887736\n",
      "Training Loss: 0.005021674741874449\n",
      "Validation Loss: 0.006646080459258781\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.010792854584287852\n",
      "Training Loss: 0.011001276833703742\n",
      "Training Loss: 0.010589976222254336\n",
      "Training Loss: 0.007793771778233349\n",
      "Training Loss: 0.004223560837563128\n",
      "Training Loss: 0.004151849607005716\n",
      "Training Loss: 0.004114679477643221\n",
      "Validation Loss: 0.0056495354484683005\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.00989841167232953\n",
      "Training Loss: 0.010122112808749079\n",
      "Training Loss: 0.00978852559113875\n",
      "Training Loss: 0.007086009396298323\n",
      "Training Loss: 0.0035841013048775495\n",
      "Training Loss: 0.003593180236057378\n",
      "Training Loss: 0.0035206120531074705\n",
      "Validation Loss: 0.004975510147672272\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.009279098738916218\n",
      "Training Loss: 0.009517074460163713\n",
      "Training Loss: 0.009223628658801317\n",
      "Training Loss: 0.006599710959126242\n",
      "Training Loss: 0.0031539016621536576\n",
      "Training Loss: 0.0032004826969932766\n",
      "Training Loss: 0.003111848171974998\n",
      "Validation Loss: 0.0044932142026506165\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.008828277062857524\n",
      "Training Loss: 0.009079006665851921\n",
      "Training Loss: 0.008808736178325489\n",
      "Training Loss: 0.0062498664041049775\n",
      "Training Loss: 0.0028490012502879834\n",
      "Training Loss: 0.002910648800607305\n",
      "Training Loss: 0.002815832748601679\n",
      "Validation Loss: 0.004130180477346067\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.008487421079771594\n",
      "Training Loss: 0.008747973213903606\n",
      "Training Loss: 0.00849214082234539\n",
      "Training Loss: 0.005987508842226816\n",
      "Training Loss: 0.0026229425097699277\n",
      "Training Loss: 0.0026877482756390235\n",
      "Training Loss: 0.0025920932178269142\n",
      "Validation Loss: 0.003845773961252013\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.008221807937370613\n",
      "Training Loss: 0.008488832246512175\n",
      "Training Loss: 0.008242494623409584\n",
      "Training Loss: 0.005783989290212049\n",
      "Training Loss: 0.0024498447275254874\n",
      "Training Loss: 0.002511174925020896\n",
      "Training Loss: 0.0024175788727006874\n",
      "Validation Loss: 0.003616562224349028\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.008009768052725122\n",
      "Training Loss: 0.008279743428574874\n",
      "Training Loss: 0.008039917488349601\n",
      "Training Loss: 0.005621701859781752\n",
      "Training Loss: 0.0023145913344342264\n",
      "Training Loss: 0.0023686079202161637\n",
      "Training Loss: 0.002278478724474553\n",
      "Validation Loss: 0.0034284113478159693\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.0078370318969246\n",
      "Training Loss: 0.008106304787797853\n",
      "Training Loss: 0.007871257830411196\n",
      "Training Loss: 0.00548930465374724\n",
      "Training Loss: 0.002207758873992134\n",
      "Training Loss: 0.0022522315790411084\n",
      "Training Loss: 0.0021659507349249905\n",
      "Validation Loss: 0.0032722740501692094\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.007693929228698835\n",
      "Training Loss: 0.00795893552713096\n",
      "Training Loss: 0.007727900451282039\n",
      "Training Loss: 0.005379363740503323\n",
      "Training Loss: 0.002123019062419189\n",
      "Training Loss: 0.0021567036594206\n",
      "Training Loss: 0.002073902120173443\n",
      "Validation Loss: 0.0031420022383513635\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.007574075035518035\n",
      "Training Loss: 0.00783170703216456\n",
      "Training Loss: 0.007604675512993708\n",
      "Training Loss: 0.0052871022844919935\n",
      "Training Loss: 0.002055737246118952\n",
      "Training Loss: 0.0020780360708886293\n",
      "Training Loss: 0.001997825770667987\n",
      "Validation Loss: 0.0030331276331013175\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.007473483291687444\n",
      "Training Loss: 0.007721483649220318\n",
      "Training Loss: 0.007498900715727359\n",
      "Training Loss: 0.005209466567175696\n",
      "Training Loss: 0.0020021436251408888\n",
      "Training Loss: 0.0020129880897002293\n",
      "Training Loss: 0.0019342155836056917\n",
      "Validation Loss: 0.002942159684716762\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.007389619593741372\n",
      "Training Loss: 0.007626791531220078\n",
      "Training Loss: 0.007409194280626252\n",
      "Training Loss: 0.005144300633401144\n",
      "Training Loss: 0.0019590033126587513\n",
      "Training Loss: 0.0019588251058303284\n",
      "Training Loss: 0.0018803374633716884\n",
      "Validation Loss: 0.002866170287227314\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.007320439707254991\n",
      "Training Loss: 0.007546584149822593\n",
      "Training Loss: 0.0073343320959247645\n",
      "Training Loss: 0.005089722908014665\n",
      "Training Loss: 0.0019236128394550178\n",
      "Training Loss: 0.0019132899033138528\n",
      "Training Loss: 0.0018341393476293889\n",
      "Validation Loss: 0.0028025996934514393\n",
      "Validation Accuracy: 0.11704119850187265\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.007263789280550555\n",
      "Training Loss: 0.0074794571322854605\n",
      "Training Loss: 0.007272632470121607\n",
      "Training Loss: 0.0050438915184349755\n",
      "Training Loss: 0.00189390128623927\n",
      "Training Loss: 0.0018746024975553155\n",
      "Training Loss: 0.0017941322505066637\n",
      "Validation Loss: 0.0027491967434679202\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.007217299395706505\n",
      "Training Loss: 0.007423469059867785\n",
      "Training Loss: 0.0072219447826500985\n",
      "Training Loss: 0.005005025610153098\n",
      "Training Loss: 0.0018684290576493367\n",
      "Training Loss: 0.0018414167847367936\n",
      "Training Loss: 0.0017592444735055324\n",
      "Validation Loss: 0.002704025627931838\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.00717860680189915\n",
      "Training Loss: 0.007376402405789122\n",
      "Training Loss: 0.007179952404694632\n",
      "Training Loss: 0.004971527219895506\n",
      "Training Loss: 0.0018462525724316948\n",
      "Training Loss: 0.0018127336773613933\n",
      "Training Loss: 0.0017286808442440816\n",
      "Validation Loss: 0.002665475060359299\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.00714558381238021\n",
      "Training Loss: 0.007336086992872879\n",
      "Training Loss: 0.007144480150891468\n",
      "Training Loss: 0.004942061560577713\n",
      "Training Loss: 0.0018267489496793133\n",
      "Training Loss: 0.001787798261793796\n",
      "Training Loss: 0.0017018182258470916\n",
      "Validation Loss: 0.002632245415305433\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.007116486771265045\n",
      "Training Loss: 0.007300633294507861\n",
      "Training Loss: 0.007113675163127482\n",
      "Training Loss: 0.004915576969651738\n",
      "Training Loss: 0.0018095004776841961\n",
      "Training Loss: 0.001766026902332669\n",
      "Training Loss: 0.0016781525021360721\n",
      "Validation Loss: 0.0026033130838022975\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007089972430840134\n",
      "Training Loss: 0.00726851423853077\n",
      "Training Loss: 0.007086069318465889\n",
      "Training Loss: 0.004891272630047751\n",
      "Training Loss: 0.0017941976753354538\n",
      "Training Loss: 0.0017469531662936788\n",
      "Training Loss: 0.001657260211359244\n",
      "Validation Loss: 0.00257786877062224\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.007065052442485466\n",
      "Training Loss: 0.007238569383043796\n",
      "Training Loss: 0.007060555989155546\n",
      "Training Loss: 0.0048685612612462134\n",
      "Training Loss: 0.0017805904794659\n",
      "Training Loss: 0.001730187889043009\n",
      "Training Loss: 0.0016387744092207868\n",
      "Validation Loss: 0.0025552865179920086\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.007041040527401492\n",
      "Training Loss: 0.007209950957912952\n",
      "Training Loss: 0.007036334174918011\n",
      "Training Loss: 0.0048470127138716635\n",
      "Training Loss: 0.0017684639085200615\n",
      "Training Loss: 0.0017154023102193606\n",
      "Training Loss: 0.0016223781346343458\n",
      "Validation Loss: 0.0025350619596908783\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.007017466592369601\n",
      "Training Loss: 0.007182070181006566\n",
      "Training Loss: 0.007012848089216277\n",
      "Training Loss: 0.004826320837019012\n",
      "Training Loss: 0.0017576191897387616\n",
      "Training Loss: 0.001702308556123171\n",
      "Training Loss: 0.0016077908083389047\n",
      "Validation Loss: 0.0025167904057573137\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006994023795705288\n",
      "Training Loss: 0.007154527043458075\n",
      "Training Loss: 0.006989725207677111\n",
      "Training Loss: 0.0048062710468366274\n",
      "Training Loss: 0.0017478833787026816\n",
      "Training Loss: 0.0016906557488982799\n",
      "Training Loss: 0.0015947648209112231\n",
      "Validation Loss: 0.002500146509593785\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.00697053263313137\n",
      "Training Loss: 0.007127067961264401\n",
      "Training Loss: 0.006966727736871689\n",
      "Training Loss: 0.004786710881453473\n",
      "Training Loss: 0.0017391001521900762\n",
      "Training Loss: 0.0016802245707367546\n",
      "Training Loss: 0.0015830832098436077\n",
      "Validation Loss: 0.002484854676020384\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006946884968783706\n",
      "Training Loss: 0.0070995361858513205\n",
      "Training Loss: 0.006943712846841663\n",
      "Training Loss: 0.004767536962899613\n",
      "Training Loss: 0.0017311327904462814\n",
      "Training Loss: 0.001670820477011148\n",
      "Training Loss: 0.0015725520765408874\n",
      "Validation Loss: 0.0024706839042286694\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.00692303421208635\n",
      "Training Loss: 0.007071846032049507\n",
      "Training Loss: 0.006920602986356244\n",
      "Training Loss: 0.004748674184666016\n",
      "Training Loss: 0.0017238617595285177\n",
      "Training Loss: 0.001662277363357134\n",
      "Training Loss: 0.0015630030356987845\n",
      "Validation Loss: 0.0024574492734940616\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006898966202279553\n",
      "Training Loss: 0.0070439537987113\n",
      "Training Loss: 0.006897358654532582\n",
      "Training Loss: 0.004730074494145811\n",
      "Training Loss: 0.001717190876370296\n",
      "Training Loss: 0.0016544489155785414\n",
      "Training Loss: 0.0015542895910039079\n",
      "Validation Loss: 0.0024449871218595863\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006874696677550673\n",
      "Training Loss: 0.007015847674338147\n",
      "Training Loss: 0.006873970376327634\n",
      "Training Loss: 0.004711703493230743\n",
      "Training Loss: 0.00171103852786473\n",
      "Training Loss: 0.0016472119440732059\n",
      "Training Loss: 0.0015462812184705399\n",
      "Validation Loss: 0.002433161176906808\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.006850246926769614\n",
      "Training Loss: 0.006987525965087116\n",
      "Training Loss: 0.006850437426473946\n",
      "Training Loss: 0.004693540454609319\n",
      "Training Loss: 0.0017053525859955698\n",
      "Training Loss: 0.0016404673158103833\n",
      "Training Loss: 0.0015388760129280855\n",
      "Validation Loss: 0.002421866396698431\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006825643200427294\n",
      "Training Loss: 0.006958987452089786\n",
      "Training Loss: 0.006826763621065765\n",
      "Training Loss: 0.0046755725666298535\n",
      "Training Loss: 0.0017000887281028553\n",
      "Training Loss: 0.0016341327412374084\n",
      "Training Loss: 0.0015319799529970623\n",
      "Validation Loss: 0.002411014749123523\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006800919682718814\n",
      "Training Loss: 0.006930238949134946\n",
      "Training Loss: 0.006802951875142753\n",
      "Training Loss: 0.004657792301732116\n",
      "Training Loss: 0.0016952187282731756\n",
      "Training Loss: 0.0016281448525114683\n",
      "Training Loss: 0.0015255187625007238\n",
      "Validation Loss: 0.0024005432945871483\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006776112804654986\n",
      "Training Loss: 0.006901284219929948\n",
      "Training Loss: 0.006779006050201133\n",
      "Training Loss: 0.004640201714937575\n",
      "Training Loss: 0.0016907269228249788\n",
      "Training Loss: 0.0016224577309913002\n",
      "Training Loss: 0.001519433402645518\n",
      "Validation Loss: 0.0023903988797936157\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006751248376676813\n",
      "Training Loss: 0.006872120023472234\n",
      "Training Loss: 0.006754924250999466\n",
      "Training Loss: 0.0046228076409897765\n",
      "Training Loss: 0.0016866011481033638\n",
      "Training Loss: 0.0016170373935165117\n",
      "Training Loss: 0.0015136779793101596\n",
      "Validation Loss: 0.002380562034498356\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006726362217450515\n",
      "Training Loss: 0.006842749909264967\n",
      "Training Loss: 0.00673070898745209\n",
      "Training Loss: 0.004605627654673299\n",
      "Training Loss: 0.0016828394573531113\n",
      "Training Loss: 0.0016118651763099478\n",
      "Training Loss: 0.0015082160189194838\n",
      "Validation Loss: 0.002371022114220637\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006701492734719068\n",
      "Training Loss: 0.006813181301113218\n",
      "Training Loss: 0.0067063651047647\n",
      "Training Loss: 0.004588689696829533\n",
      "Training Loss: 0.0016794363295775838\n",
      "Training Loss: 0.0016069339308160124\n",
      "Training Loss: 0.00150302525384177\n",
      "Validation Loss: 0.0023617875517680824\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006676682677352801\n",
      "Training Loss: 0.00678344146348536\n",
      "Training Loss: 0.006681917284149677\n",
      "Training Loss: 0.004572037284087855\n",
      "Training Loss: 0.0016763787738454995\n",
      "Training Loss: 0.001602239111380186\n",
      "Training Loss: 0.0014980861942603951\n",
      "Validation Loss: 0.0023528738908752214\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006651996660511941\n",
      "Training Loss: 0.006753582351375371\n",
      "Training Loss: 0.006657411558553577\n",
      "Training Loss: 0.004555729239655193\n",
      "Training Loss: 0.0016736484532884788\n",
      "Training Loss: 0.0015977805964212167\n",
      "Training Loss: 0.001493383391716634\n",
      "Validation Loss: 0.0023443195119732117\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006627521424088627\n",
      "Training Loss: 0.006723693402018398\n",
      "Training Loss: 0.006632925724843517\n",
      "Training Loss: 0.004539841508376412\n",
      "Training Loss: 0.0016712162049952895\n",
      "Training Loss: 0.0015935591894231038\n",
      "Training Loss: 0.0014889072189544095\n",
      "Validation Loss: 0.002336156724455979\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006603361194720492\n",
      "Training Loss: 0.006693900423124432\n",
      "Training Loss: 0.006608568618539721\n",
      "Training Loss: 0.004524463009365718\n",
      "Training Loss: 0.0016690418597136158\n",
      "Training Loss: 0.0015895714976795717\n",
      "Training Loss: 0.001484644644151558\n",
      "Validation Loss: 0.002328424774457383\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006579650558996945\n",
      "Training Loss: 0.0066643717826809735\n",
      "Training Loss: 0.0065844869730062785\n",
      "Training Loss: 0.004509693380605313\n",
      "Training Loss: 0.0016670700609392952\n",
      "Training Loss: 0.0015858025913621531\n",
      "Training Loss: 0.0014805721917946358\n",
      "Validation Loss: 0.002321157497850906\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006556547192158178\n",
      "Training Loss: 0.006635314261075109\n",
      "Training Loss: 0.006560855417046696\n",
      "Training Loss: 0.004495632813122938\n",
      "Training Loss: 0.0016652399000304284\n",
      "Training Loss: 0.0015822339482838288\n",
      "Training Loss: 0.0014766679662716342\n",
      "Validation Loss: 0.00231437847091928\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006534219435416162\n",
      "Training Loss: 0.006606960471253842\n",
      "Training Loss: 0.006537870189640671\n",
      "Training Loss: 0.004482365752119221\n",
      "Training Loss: 0.0016634698781126645\n",
      "Training Loss: 0.001578828125129803\n",
      "Training Loss: 0.0014728913986618863\n",
      "Validation Loss: 0.0023081027367457484\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006512837720802054\n",
      "Training Loss: 0.006579541681567207\n",
      "Training Loss: 0.006515720636816696\n",
      "Training Loss: 0.004469960163187352\n",
      "Training Loss: 0.0016616888377757278\n",
      "Training Loss: 0.0015755455826729303\n",
      "Training Loss: 0.0014691994828899624\n",
      "Validation Loss: 0.0023023200152469765\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006492557298624888\n",
      "Training Loss: 0.006553283997345716\n",
      "Training Loss: 0.006494588888017461\n",
      "Training Loss: 0.004458452886028681\n",
      "Training Loss: 0.0016598088818136603\n",
      "Training Loss: 0.001572332232099143\n",
      "Training Loss: 0.0014655416586901993\n",
      "Validation Loss: 0.0022970132172968386\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006473501694854349\n",
      "Training Loss: 0.006528371254680678\n",
      "Training Loss: 0.00647461395827122\n",
      "Training Loss: 0.004447843903035391\n",
      "Training Loss: 0.0016577647173835431\n",
      "Training Loss: 0.001569138068734901\n",
      "Training Loss: 0.0014618673061340814\n",
      "Validation Loss: 0.002292148705680021\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006455756520153955\n",
      "Training Loss: 0.006504943006439135\n",
      "Training Loss: 0.006455898499116302\n",
      "Training Loss: 0.004438101240593823\n",
      "Training Loss: 0.0016554892348358407\n",
      "Training Loss: 0.001565909487690078\n",
      "Training Loss: 0.0014581251936760964\n",
      "Validation Loss: 0.0022876781189320643\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006439357521012426\n",
      "Training Loss: 0.006483079160097986\n",
      "Training Loss: 0.0064384893502574415\n",
      "Training Loss: 0.004429162963206181\n",
      "Training Loss: 0.001652936218160903\n",
      "Training Loss: 0.0015625998859468383\n",
      "Training Loss: 0.001454274005809566\n",
      "Validation Loss: 0.002283545909558461\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006424296369077638\n",
      "Training Loss: 0.006462798619177193\n",
      "Training Loss: 0.006422388238133863\n",
      "Training Loss: 0.004420944837038405\n",
      "Training Loss: 0.001650071249823668\n",
      "Training Loss: 0.0015591695475450252\n",
      "Training Loss: 0.0014502801207709126\n",
      "Validation Loss: 0.0022797009859483494\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.006410521237412468\n",
      "Training Loss: 0.006444063243689016\n",
      "Training Loss: 0.006407545084366575\n",
      "Training Loss: 0.004413350357208401\n",
      "Training Loss: 0.0016468809669459006\n",
      "Training Loss: 0.001555596069447347\n",
      "Training Loss: 0.0014461256911454258\n",
      "Validation Loss: 0.002276085871797184\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006397946160286665\n",
      "Training Loss: 0.0064267982286401095\n",
      "Training Loss: 0.006393884067656472\n",
      "Training Loss: 0.004406280531475204\n",
      "Training Loss: 0.0016433655200671637\n",
      "Training Loss: 0.001551864924840629\n",
      "Training Loss: 0.0014418019758886658\n",
      "Validation Loss: 0.0022726541085623044\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006386460688663647\n",
      "Training Loss: 0.006410889435792342\n",
      "Training Loss: 0.006381304663373158\n",
      "Training Loss: 0.0043996434095606675\n",
      "Training Loss: 0.0016395365550852148\n",
      "Training Loss: 0.001547967805454391\n",
      "Training Loss: 0.0014373072850867176\n",
      "Validation Loss: 0.002269356748887533\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006375951734371483\n",
      "Training Loss: 0.00639621413894929\n",
      "Training Loss: 0.006369694852037355\n",
      "Training Loss: 0.0043933521163853585\n",
      "Training Loss: 0.0016354160894843518\n",
      "Training Loss: 0.0015439115739718544\n",
      "Training Loss: 0.001432655294847791\n",
      "Validation Loss: 0.002266160642910463\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.0063662907155230646\n",
      "Training Loss: 0.006382632703753189\n",
      "Training Loss: 0.006358938456978649\n",
      "Training Loss: 0.004387338097876637\n",
      "Training Loss: 0.0016310292003618087\n",
      "Training Loss: 0.001539702148293145\n",
      "Training Loss: 0.0014278573117917403\n",
      "Validation Loss: 0.002263030395686124\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006357364481082186\n",
      "Training Loss: 0.006370015066349879\n",
      "Training Loss: 0.006348927211947739\n",
      "Training Loss: 0.004381542059272761\n",
      "Training Loss: 0.0016264070487522985\n",
      "Training Loss: 0.0015353569408762268\n",
      "Training Loss: 0.0014229338430595817\n",
      "Validation Loss: 0.002259943126426131\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006349061744986102\n",
      "Training Loss: 0.006358231683261693\n",
      "Training Loss: 0.006339555932208895\n",
      "Training Loss: 0.004375919365265872\n",
      "Training Loss: 0.0016215829928114545\n",
      "Training Loss: 0.0015308918252412696\n",
      "Training Loss: 0.001417907176510198\n",
      "Validation Loss: 0.002256881053725603\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.0063412831665482375\n",
      "Training Loss: 0.006347166260238737\n",
      "Training Loss: 0.006330733645008877\n",
      "Training Loss: 0.0043704339664691364\n",
      "Training Loss: 0.001616586526943138\n",
      "Training Loss: 0.0015263205661904067\n",
      "Training Loss: 0.0014127925114735262\n",
      "Validation Loss: 0.0022538291842067156\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006333946423837915\n",
      "Training Loss: 0.0063367155799642205\n",
      "Training Loss: 0.006322379795601591\n",
      "Training Loss: 0.004365061268836143\n",
      "Training Loss: 0.001611451893404592\n",
      "Training Loss: 0.0015216635527758625\n",
      "Training Loss: 0.0014076133025082527\n",
      "Validation Loss: 0.0022507740944089395\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006326976986601948\n",
      "Training Loss: 0.006326791297178716\n",
      "Training Loss: 0.006314428539481014\n",
      "Training Loss: 0.004359782020983403\n",
      "Training Loss: 0.001606196608772734\n",
      "Training Loss: 0.0015169292502832832\n",
      "Training Loss: 0.0014023807379271603\n",
      "Validation Loss: 0.0022477101343195563\n",
      "Validation Accuracy: 0.11704119850187265\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.0063203196728136395\n",
      "Training Loss: 0.006317319378722459\n",
      "Training Loss: 0.006306821964681148\n",
      "Training Loss: 0.004354582608284545\n",
      "Training Loss: 0.0016008491881075316\n",
      "Training Loss: 0.0015121335340518272\n",
      "Training Loss: 0.0013971107461838983\n",
      "Validation Loss: 0.0022446281428266605\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.006313920817337931\n",
      "Training Loss: 0.00630823215818964\n",
      "Training Loss: 0.006299512357218191\n",
      "Training Loss: 0.004349452559981728\n",
      "Training Loss: 0.0015954269123903942\n",
      "Training Loss: 0.001507287272324902\n",
      "Training Loss: 0.001391814553498989\n",
      "Validation Loss: 0.0022415228294642912\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.006307739643380046\n",
      "Training Loss: 0.0062994744209572675\n",
      "Training Loss: 0.0062924607435707\n",
      "Training Loss: 0.004344383371571894\n",
      "Training Loss: 0.0015899513024487534\n",
      "Training Loss: 0.0015023996668605832\n",
      "Training Loss: 0.0013865026973508066\n",
      "Validation Loss: 0.002238390871763146\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.00630174535443075\n",
      "Training Loss: 0.006291001893114299\n",
      "Training Loss: 0.006285633299266919\n",
      "Training Loss: 0.004339371231471887\n",
      "Training Loss: 0.0015844337250018724\n",
      "Training Loss: 0.0014974808738043065\n",
      "Training Loss: 0.0013811856396932854\n",
      "Validation Loss: 0.002235226855211724\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006295904935104773\n",
      "Training Loss: 0.0062827738770283754\n",
      "Training Loss: 0.006279002304654569\n",
      "Training Loss: 0.0043344132466882\n",
      "Training Loss: 0.0015788913214055357\n",
      "Training Loss: 0.001492535546203726\n",
      "Training Loss: 0.0013758659383893246\n",
      "Validation Loss: 0.0022320293600990152\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.0062902031419798735\n",
      "Training Loss: 0.006274761173408479\n",
      "Training Loss: 0.006272546512773261\n",
      "Training Loss: 0.00432950249996793\n",
      "Training Loss: 0.0015733317473495844\n",
      "Training Loss: 0.0014875725585443433\n",
      "Training Loss: 0.0013705543733522063\n",
      "Validation Loss: 0.0022287993724671566\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006284616735065356\n",
      "Training Loss: 0.006266932379221543\n",
      "Training Loss: 0.006266242747660727\n",
      "Training Loss: 0.004324639292899519\n",
      "Training Loss: 0.0015677705062262248\n",
      "Training Loss: 0.0014825987689982866\n",
      "Training Loss: 0.0013652551787527046\n",
      "Validation Loss: 0.002225530926466916\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.0062791309598833325\n",
      "Training Loss: 0.006259266050765291\n",
      "Training Loss: 0.0062600784830283375\n",
      "Training Loss: 0.004319821610770305\n",
      "Training Loss: 0.0015622133264696458\n",
      "Training Loss: 0.0014776184826041571\n",
      "Training Loss: 0.0013599746270483593\n",
      "Validation Loss: 0.002222226552332992\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.0062737307464703915\n",
      "Training Loss: 0.006251740386942401\n",
      "Training Loss: 0.006254036885220558\n",
      "Training Loss: 0.004315046643459937\n",
      "Training Loss: 0.0015566691723506664\n",
      "Training Loss: 0.0014726373183657415\n",
      "Training Loss: 0.001354715937122819\n",
      "Validation Loss: 0.002218886704826361\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.00626840885845013\n",
      "Training Loss: 0.0062443386309314515\n",
      "Training Loss: 0.006248101700330153\n",
      "Training Loss: 0.004310312671514112\n",
      "Training Loss: 0.0015511502230219777\n",
      "Training Loss: 0.0014676640681136632\n",
      "Training Loss: 0.0013494870201247978\n",
      "Validation Loss: 0.002215508595320452\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.006263150720624253\n",
      "Training Loss: 0.00623704495257698\n",
      "Training Loss: 0.006242266027256847\n",
      "Training Loss: 0.004305617099817027\n",
      "Training Loss: 0.0015456563275074587\n",
      "Training Loss: 0.0014627016791928327\n",
      "Training Loss: 0.0013442908970318968\n",
      "Validation Loss: 0.0022120960070952174\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.00625794839579612\n",
      "Training Loss: 0.006229846881469711\n",
      "Training Loss: 0.006236512570176274\n",
      "Training Loss: 0.004300959436877747\n",
      "Training Loss: 0.001540202611577115\n",
      "Training Loss: 0.0014577562658814713\n",
      "Training Loss: 0.0013391329468868207\n",
      "Validation Loss: 0.0022086506014125603\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.0062527940317522734\n",
      "Training Loss: 0.006222729855217039\n",
      "Training Loss: 0.006230834887828678\n",
      "Training Loss: 0.0042963379146385705\n",
      "Training Loss: 0.0015347919915802777\n",
      "Training Loss: 0.001452833442017436\n",
      "Training Loss: 0.001334015708998777\n",
      "Validation Loss: 0.0022051710541571984\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00624767945962958\n",
      "Training Loss: 0.006215684158960357\n",
      "Training Loss: 0.006225221297936514\n",
      "Training Loss: 0.004291750046540983\n",
      "Training Loss: 0.0015294307596195721\n",
      "Training Loss: 0.001447940560610732\n",
      "Training Loss: 0.0013289458963117796\n",
      "Validation Loss: 0.002201659498574502\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006242595196235925\n",
      "Training Loss: 0.006208699042908847\n",
      "Training Loss: 0.006219665513490327\n",
      "Training Loss: 0.004287194087155512\n",
      "Training Loss: 0.0015241223731572973\n",
      "Training Loss: 0.001443077625517617\n",
      "Training Loss: 0.0013239226807490922\n",
      "Validation Loss: 0.0021981159513964482\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.00623754236381501\n",
      "Training Loss: 0.006201769874896854\n",
      "Training Loss: 0.006214159192168154\n",
      "Training Loss: 0.004282670431712177\n",
      "Training Loss: 0.0015188718296121806\n",
      "Training Loss: 0.0014382500045758207\n",
      "Training Loss: 0.0013189486168994336\n",
      "Validation Loss: 0.0021945429434027516\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006232510460540653\n",
      "Training Loss: 0.006194890157785266\n",
      "Training Loss: 0.006208694535307586\n",
      "Training Loss: 0.004278174231149024\n",
      "Training Loss: 0.0015136849661212181\n",
      "Training Loss: 0.0014334656569553771\n",
      "Training Loss: 0.0013140333296905737\n",
      "Validation Loss: 0.002190945088422564\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.0062274877587333325\n",
      "Training Loss: 0.006188042347785086\n",
      "Training Loss: 0.00620325717725791\n",
      "Training Loss: 0.004273706123640295\n",
      "Training Loss: 0.0015085740480571986\n",
      "Training Loss: 0.0014287318481365219\n",
      "Training Loss: 0.0013091792666818946\n",
      "Validation Loss: 0.002187322910488339\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.0062224751082248985\n",
      "Training Loss: 0.006181230404181406\n",
      "Training Loss: 0.0061978475592331965\n",
      "Training Loss: 0.004269262044108473\n",
      "Training Loss: 0.0015035322859330335\n",
      "Training Loss: 0.0014240486682683695\n",
      "Training Loss: 0.001304386538322433\n",
      "Validation Loss: 0.0021836793274498346\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006217464092187584\n",
      "Training Loss: 0.006174439962487668\n",
      "Training Loss: 0.006192453181138262\n",
      "Training Loss: 0.004264839780007606\n",
      "Training Loss: 0.0014985752271604723\n",
      "Training Loss: 0.0014194250163563993\n",
      "Training Loss: 0.0012996608162939083\n",
      "Validation Loss: 0.0021800171725158946\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006212449548766017\n",
      "Training Loss: 0.006167671782895923\n",
      "Training Loss: 0.0061870710254879665\n",
      "Training Loss: 0.0042604401881544615\n",
      "Training Loss: 0.0014936991300055524\n",
      "Training Loss: 0.0014148586276860442\n",
      "Training Loss: 0.0012949996148381615\n",
      "Validation Loss: 0.0021763346879568636\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006207430869108066\n",
      "Training Loss: 0.006160922882845626\n",
      "Training Loss: 0.006181692674290389\n",
      "Training Loss: 0.004256058043392841\n",
      "Training Loss: 0.0014889092103840085\n",
      "Training Loss: 0.0014103594006155617\n",
      "Training Loss: 0.0012904107407666742\n",
      "Validation Loss: 0.0021726413293565493\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.00620239956304431\n",
      "Training Loss: 0.00615418040775694\n",
      "Training Loss: 0.006176311080926098\n",
      "Training Loss: 0.00425169286208984\n",
      "Training Loss: 0.0014842149895412148\n",
      "Training Loss: 0.0014059293233731297\n",
      "Training Loss: 0.00128589481319068\n",
      "Validation Loss: 0.002168936408469314\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006197350698057562\n",
      "Training Loss: 0.006147444639354944\n",
      "Training Loss: 0.00617092122498434\n",
      "Training Loss: 0.004247343071692739\n",
      "Training Loss: 0.001479613148694625\n",
      "Training Loss: 0.0014015736987494165\n",
      "Training Loss: 0.0012814549972245005\n",
      "Validation Loss: 0.002165225079616232\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.006192283276468515\n",
      "Training Loss: 0.006140712283086032\n",
      "Training Loss: 0.006165518181514926\n",
      "Training Loss: 0.004243004829186247\n",
      "Training Loss: 0.0014751152135431766\n",
      "Training Loss: 0.0013972930701129371\n",
      "Training Loss: 0.0012770901730982586\n",
      "Validation Loss: 0.0021615035545032068\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006187191706849262\n",
      "Training Loss: 0.006133980215527118\n",
      "Training Loss: 0.006160097143147141\n",
      "Training Loss: 0.00423867854544369\n",
      "Training Loss: 0.001470718719647266\n",
      "Training Loss: 0.0013930927411274752\n",
      "Training Loss: 0.0012728065124247223\n",
      "Validation Loss: 0.00215778289285832\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006182073325617239\n",
      "Training Loss: 0.006127243036171421\n",
      "Training Loss: 0.006154652264667675\n",
      "Training Loss: 0.004234361966082361\n",
      "Training Loss: 0.0014664302210439927\n",
      "Training Loss: 0.0013889754867705052\n",
      "Training Loss: 0.0012686017966188957\n",
      "Validation Loss: 0.002154059366012177\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006176923227030784\n",
      "Training Loss: 0.006120500382967293\n",
      "Training Loss: 0.006149182092631236\n",
      "Training Loss: 0.004230053101127851\n",
      "Training Loss: 0.0014622474981297274\n",
      "Training Loss: 0.0013849388065864331\n",
      "Training Loss: 0.0012644760680268518\n",
      "Validation Loss: 0.002150338096565947\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006171745635801926\n",
      "Training Loss: 0.006113751848461107\n",
      "Training Loss: 0.006143682128749788\n",
      "Training Loss: 0.004225749709585216\n",
      "Training Loss: 0.001458179756809841\n",
      "Training Loss: 0.0013809909902920481\n",
      "Training Loss: 0.0012604356599331369\n",
      "Validation Loss: 0.0021466235594788203\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006166528468020261\n",
      "Training Loss: 0.006106991420965642\n",
      "Training Loss: 0.006138147644232959\n",
      "Training Loss: 0.004221451172561501\n",
      "Training Loss: 0.0014542204314784612\n",
      "Training Loss: 0.0013771275708131725\n",
      "Training Loss: 0.0012564777582883835\n",
      "Validation Loss: 0.0021429188291290217\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006161277076462284\n",
      "Training Loss: 0.006100217590574175\n",
      "Training Loss: 0.006132575973751954\n",
      "Training Loss: 0.004217156534432434\n",
      "Training Loss: 0.001450375800995971\n",
      "Training Loss: 0.0013733527471777051\n",
      "Training Loss: 0.001252604195906315\n",
      "Validation Loss: 0.0021392271529917157\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006155986877856776\n",
      "Training Loss: 0.006093428955646232\n",
      "Training Loss: 0.00612696650903672\n",
      "Training Loss: 0.004212862661661348\n",
      "Training Loss: 0.0014466441434342414\n",
      "Training Loss: 0.0013696645927120698\n",
      "Training Loss: 0.0012488127095275558\n",
      "Validation Loss: 0.0021355449382153826\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006150658103870228\n",
      "Training Loss: 0.0060866305918898436\n",
      "Training Loss: 0.006121319627272897\n",
      "Training Loss: 0.004208571346243844\n",
      "Training Loss: 0.0014430252112651943\n",
      "Training Loss: 0.001366062158922432\n",
      "Training Loss: 0.0012451055707060732\n",
      "Validation Loss: 0.0021318800858904745\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006145288879051805\n",
      "Training Loss: 0.006079816691344604\n",
      "Training Loss: 0.006115631848806515\n",
      "Training Loss: 0.004204279477780801\n",
      "Training Loss: 0.001439521509601036\n",
      "Training Loss: 0.0013625489837431814\n",
      "Training Loss: 0.00124148205111851\n",
      "Validation Loss: 0.002128234135740426\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006139879571273923\n",
      "Training Loss: 0.006072984884958714\n",
      "Training Loss: 0.0061099004955030975\n",
      "Training Loss: 0.004199987671963754\n",
      "Training Loss: 0.0014361325227946508\n",
      "Training Loss: 0.001359120846755104\n",
      "Training Loss: 0.001237942924926756\n",
      "Validation Loss: 0.002124605231551272\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006134424407500774\n",
      "Training Loss: 0.006066134764114395\n",
      "Training Loss: 0.006104128308361396\n",
      "Training Loss: 0.004195694902155083\n",
      "Training Loss: 0.0014328492595814168\n",
      "Training Loss: 0.0013557749865867664\n",
      "Training Loss: 0.00123448364167416\n",
      "Validation Loss: 0.002120998465566816\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006128931571729481\n",
      "Training Loss: 0.006059270643163472\n",
      "Training Loss: 0.006098312968970276\n",
      "Training Loss: 0.004191399382470991\n",
      "Training Loss: 0.0014296782466408332\n",
      "Training Loss: 0.001352512360608671\n",
      "Training Loss: 0.0012311066867550834\n",
      "Validation Loss: 0.0021174138660188624\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006123395218746736\n",
      "Training Loss: 0.0060523877071682365\n",
      "Training Loss: 0.006092456828337162\n",
      "Training Loss: 0.004187101708594127\n",
      "Training Loss: 0.0014266091358149423\n",
      "Training Loss: 0.0013493295005173422\n",
      "Training Loss: 0.0012278085788420866\n",
      "Validation Loss: 0.0021138531535760106\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006117814700119197\n",
      "Training Loss: 0.006045485705835745\n",
      "Training Loss: 0.006086558427778072\n",
      "Training Loss: 0.004182799273548881\n",
      "Training Loss: 0.0014236421047826298\n",
      "Training Loss: 0.0013462223401438677\n",
      "Training Loss: 0.0012245870423794258\n",
      "Validation Loss: 0.0021103149434082446\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006112194014713168\n",
      "Training Loss: 0.006038567303912714\n",
      "Training Loss: 0.006080618253909051\n",
      "Training Loss: 0.004178494513034821\n",
      "Training Loss: 0.0014207759968121536\n",
      "Training Loss: 0.001343189763938426\n",
      "Training Loss: 0.001221440207445994\n",
      "Validation Loss: 0.0021068000841592805\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006106532927369699\n",
      "Training Loss: 0.006031630252837203\n",
      "Training Loss: 0.00607463913038373\n",
      "Training Loss: 0.004174185468291398\n",
      "Training Loss: 0.0014180056034820154\n",
      "Training Loss: 0.001340229007473681\n",
      "Training Loss: 0.00121836750091461\n",
      "Validation Loss: 0.0021033127275940852\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006100831576623023\n",
      "Training Loss: 0.006024675039807335\n",
      "Training Loss: 0.006068618670105934\n",
      "Training Loss: 0.0041698710127093365\n",
      "Training Loss: 0.001415328327275347\n",
      "Training Loss: 0.0013373392556968611\n",
      "Training Loss: 0.001215368944904185\n",
      "Validation Loss: 0.0020998518771517235\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0060950845247134565\n",
      "Training Loss: 0.006017696048365906\n",
      "Training Loss: 0.006062555173411965\n",
      "Training Loss: 0.004165549929894041\n",
      "Training Loss: 0.0014127447856299115\n",
      "Training Loss: 0.001334519569281838\n",
      "Training Loss: 0.0012124430436233524\n",
      "Validation Loss: 0.002096417308176784\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006089296552818269\n",
      "Training Loss: 0.006010697926394641\n",
      "Training Loss: 0.006056451768963598\n",
      "Training Loss: 0.004161223821720341\n",
      "Training Loss: 0.0014102473165257833\n",
      "Training Loss: 0.0013317619811277837\n",
      "Training Loss: 0.0012095821146067465\n",
      "Validation Loss: 0.0020930075789396285\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006083470147568732\n",
      "Training Loss: 0.006003682230366394\n",
      "Training Loss: 0.006050311994040385\n",
      "Training Loss: 0.004156891724851448\n",
      "Training Loss: 0.0014078309330216143\n",
      "Training Loss: 0.0013290646204404766\n",
      "Training Loss: 0.0012067861454124794\n",
      "Validation Loss: 0.0020896271743187297\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006077607092447579\n",
      "Training Loss: 0.005996646591811441\n",
      "Training Loss: 0.006044132479000836\n",
      "Training Loss: 0.004152551137667615\n",
      "Training Loss: 0.0014054944178496954\n",
      "Training Loss: 0.001326427039966802\n",
      "Training Loss: 0.0012040554639679612\n",
      "Validation Loss: 0.0020862730950390053\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.006071703502675519\n",
      "Training Loss: 0.005989588552620262\n",
      "Training Loss: 0.006037915523629636\n",
      "Training Loss: 0.004148205768869957\n",
      "Training Loss: 0.0014032320003025233\n",
      "Training Loss: 0.00132384502205241\n",
      "Training Loss: 0.0012013848791684722\n",
      "Validation Loss: 0.0020829414914376085\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.0060657619591802355\n",
      "Training Loss: 0.005982511898037046\n",
      "Training Loss: 0.006031662877649069\n",
      "Training Loss: 0.004143850526525057\n",
      "Training Loss: 0.0014010436284297612\n",
      "Training Loss: 0.001321313967491733\n",
      "Training Loss: 0.0011987704562488944\n",
      "Validation Loss: 0.002079636810187421\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006059786445694044\n",
      "Training Loss: 0.005975417242152617\n",
      "Training Loss: 0.006025375011377037\n",
      "Training Loss: 0.00413949124966166\n",
      "Training Loss: 0.0013989226857665925\n",
      "Training Loss: 0.0013188336787425215\n",
      "Training Loss: 0.001196213457005797\n",
      "Validation Loss: 0.0020763576162624964\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.0060537747386842965\n",
      "Training Loss: 0.005968304561683908\n",
      "Training Loss: 0.006019053350901232\n",
      "Training Loss: 0.0041351214075257305\n",
      "Training Loss: 0.001396868386073038\n",
      "Training Loss: 0.0013163987339066807\n",
      "Training Loss: 0.0011937068850238575\n",
      "Validation Loss: 0.0020731048721057965\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.00604773439001292\n",
      "Training Loss: 0.00596117184672039\n",
      "Training Loss: 0.006012697356054559\n",
      "Training Loss: 0.004130745108486735\n",
      "Training Loss: 0.0013948755399906076\n",
      "Training Loss: 0.0013140098632720765\n",
      "Training Loss: 0.0011912540073535638\n",
      "Validation Loss: 0.0020698746469561603\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006041658307658508\n",
      "Training Loss: 0.005954021036159247\n",
      "Training Loss: 0.006006307697389275\n",
      "Training Loss: 0.004126360878508421\n",
      "Training Loss: 0.0013929433187877293\n",
      "Training Loss: 0.0013116617225023218\n",
      "Training Loss: 0.001188848155507003\n",
      "Validation Loss: 0.002066670710631657\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.0060355547163635495\n",
      "Training Loss: 0.005946852395427413\n",
      "Training Loss: 0.005999887067591771\n",
      "Training Loss: 0.0041219691420701565\n",
      "Training Loss: 0.0013910662528360263\n",
      "Training Loss: 0.0013093511579063489\n",
      "Training Loss: 0.0011864863237860846\n",
      "Validation Loss: 0.0020634855158659297\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.006029422590509057\n",
      "Training Loss: 0.005939671638188883\n",
      "Training Loss: 0.005993440998136066\n",
      "Training Loss: 0.004117570821181289\n",
      "Training Loss: 0.0013892442065116485\n",
      "Training Loss: 0.001307075640797848\n",
      "Training Loss: 0.001184167373139644\n",
      "Validation Loss: 0.0020603241835160995\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006023267202544957\n",
      "Training Loss: 0.0059324742667377\n",
      "Training Loss: 0.005986962654278613\n",
      "Training Loss: 0.004113160938213695\n",
      "Training Loss: 0.001387469041510485\n",
      "Training Loss: 0.0013048343197442592\n",
      "Training Loss: 0.001181891474770964\n",
      "Validation Loss: 0.002057187181859528\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.006017082207836211\n",
      "Training Loss: 0.005925258982460946\n",
      "Training Loss: 0.005980454220552929\n",
      "Training Loss: 0.004108745988924056\n",
      "Training Loss: 0.001385749266773928\n",
      "Training Loss: 0.001302626492761192\n",
      "Training Loss: 0.0011796542822412447\n",
      "Validation Loss: 0.0020540704674258383\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.006010876232758164\n",
      "Training Loss: 0.005918031142209656\n",
      "Training Loss: 0.005973921971744857\n",
      "Training Loss: 0.004104324214713415\n",
      "Training Loss: 0.0013840698554122355\n",
      "Training Loss: 0.0013004431977606146\n",
      "Training Loss: 0.001177449346141657\n",
      "Validation Loss: 0.002050971043392517\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006004654563730583\n",
      "Training Loss: 0.005910795709933154\n",
      "Training Loss: 0.005967366595868952\n",
      "Training Loss: 0.004099895711406134\n",
      "Training Loss: 0.0013824334075616207\n",
      "Training Loss: 0.0012982860560441622\n",
      "Training Loss: 0.0011752798387169605\n",
      "Validation Loss: 0.0020478937836701386\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.0059984126419294626\n",
      "Training Loss: 0.005903548417263664\n",
      "Training Loss: 0.005960788126685657\n",
      "Training Loss: 0.00409546079368738\n",
      "Training Loss: 0.001380839421035489\n",
      "Training Loss: 0.0012961535809881753\n",
      "Training Loss: 0.0011731430728832493\n",
      "Validation Loss: 0.0020448390389217084\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.005992155056446791\n",
      "Training Loss: 0.005896289343945682\n",
      "Training Loss: 0.005954187005991116\n",
      "Training Loss: 0.00409101906203432\n",
      "Training Loss: 0.001379284543218091\n",
      "Training Loss: 0.0012940436510689324\n",
      "Training Loss: 0.0011710379274882143\n",
      "Validation Loss: 0.0020417996697160083\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.005985881342785433\n",
      "Training Loss: 0.005889022297924385\n",
      "Training Loss: 0.0059475653315894305\n",
      "Training Loss: 0.004086571918742265\n",
      "Training Loss: 0.0013777692975418176\n",
      "Training Loss: 0.0012919554339168827\n",
      "Training Loss: 0.0011689618168020388\n",
      "Validation Loss: 0.0020387796840429036\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.005979597243713215\n",
      "Training Loss: 0.005881749907857738\n",
      "Training Loss: 0.005940926002222113\n",
      "Training Loss: 0.004082120668244898\n",
      "Training Loss: 0.001376289907057071\n",
      "Training Loss: 0.0012898842021240853\n",
      "Training Loss: 0.0011669130230438896\n",
      "Validation Loss: 0.002035779916430206\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.005973303869832307\n",
      "Training Loss: 0.005874473447329364\n",
      "Training Loss: 0.005934271112782881\n",
      "Training Loss: 0.004077664140349952\n",
      "Training Loss: 0.0013748434068111238\n",
      "Training Loss: 0.0012878295762493509\n",
      "Training Loss: 0.001164886707047117\n",
      "Validation Loss: 0.002032795180891438\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.005967004658887162\n",
      "Training Loss: 0.005867192248115316\n",
      "Training Loss: 0.0059276011941256\n",
      "Training Loss: 0.0040732065265183335\n",
      "Training Loss: 0.0013734303817909677\n",
      "Training Loss: 0.0012857892195461318\n",
      "Training Loss: 0.0011628881721117068\n",
      "Validation Loss: 0.002029830650278203\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.005960698729613796\n",
      "Training Loss: 0.00585990920313634\n",
      "Training Loss: 0.0059209179709432645\n",
      "Training Loss: 0.004068745020049391\n",
      "Training Loss: 0.0013720528187695891\n",
      "Training Loss: 0.001283765661501093\n",
      "Training Loss: 0.0011609133976890006\n",
      "Validation Loss: 0.0020268827202391537\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.005954390036640689\n",
      "Training Loss: 0.0058526290272129695\n",
      "Training Loss: 0.005914224542793818\n",
      "Training Loss: 0.004064283615953172\n",
      "Training Loss: 0.001370704572618706\n",
      "Training Loss: 0.0012817527996958234\n",
      "Training Loss: 0.0011589595196710434\n",
      "Validation Loss: 0.002023950777359286\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.005948083274997771\n",
      "Training Loss: 0.005845349677256309\n",
      "Training Loss: 0.005907522903289646\n",
      "Training Loss: 0.004059821158880368\n",
      "Training Loss: 0.0013693846245587337\n",
      "Training Loss: 0.0012797493798279902\n",
      "Training Loss: 0.0011570258353458484\n",
      "Validation Loss: 0.002021037982074781\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.00594177842955105\n",
      "Training Loss: 0.005838074967032298\n",
      "Training Loss: 0.005900816234643571\n",
      "Training Loss: 0.0040553597710095346\n",
      "Training Loss: 0.001368095806101337\n",
      "Training Loss: 0.0012777580460533499\n",
      "Training Loss: 0.0011551140684605344\n",
      "Validation Loss: 0.002018141420568995\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.005935479664476589\n",
      "Training Loss: 0.005830809837207198\n",
      "Training Loss: 0.005894108429783955\n",
      "Training Loss: 0.004050900386791909\n",
      "Training Loss: 0.001366833779902663\n",
      "Training Loss: 0.0012757732175668933\n",
      "Training Loss: 0.0011532183114468352\n",
      "Validation Loss: 0.0020152574340435276\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005929189029848203\n",
      "Training Loss: 0.005823553124209866\n",
      "Training Loss: 0.005887396461912431\n",
      "Training Loss: 0.004046445839485387\n",
      "Training Loss: 0.001365602194418898\n",
      "Training Loss: 0.0012737984167324613\n",
      "Training Loss: 0.0011513453158113406\n",
      "Validation Loss: 0.002012396651337164\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.0059229077585041525\n",
      "Training Loss: 0.00581630646775011\n",
      "Training Loss: 0.005880688994075171\n",
      "Training Loss: 0.004041992996644694\n",
      "Training Loss: 0.0013643944798241137\n",
      "Training Loss: 0.0012718294180376689\n",
      "Training Loss: 0.001149489024464856\n",
      "Validation Loss: 0.0020095500579471823\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.005916640114737675\n",
      "Training Loss: 0.0058090773498406635\n",
      "Training Loss: 0.005873987060040236\n",
      "Training Loss: 0.004037550049397396\n",
      "Training Loss: 0.001363216458412353\n",
      "Training Loss: 0.001269868076851708\n",
      "Training Loss: 0.0011476521660370054\n",
      "Validation Loss: 0.002006722317517156\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005910389687633142\n",
      "Training Loss: 0.005801862261723727\n",
      "Training Loss: 0.005867289921152406\n",
      "Training Loss: 0.004033113457189756\n",
      "Training Loss: 0.0013620692255790344\n",
      "Training Loss: 0.0012679165707231732\n",
      "Training Loss: 0.0011458365371800029\n",
      "Validation Loss: 0.0020039124248749035\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.00590415163198486\n",
      "Training Loss: 0.0057946621818700805\n",
      "Training Loss: 0.005860602719476447\n",
      "Training Loss: 0.00402868700955878\n",
      "Training Loss: 0.0013609471164818387\n",
      "Training Loss: 0.0012659700897347647\n",
      "Training Loss: 0.0011440378666156903\n",
      "Validation Loss: 0.0020011216035803383\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005897940358845517\n",
      "Training Loss: 0.005787490394432097\n",
      "Training Loss: 0.005853934464394115\n",
      "Training Loss: 0.004024272049719002\n",
      "Training Loss: 0.001359850664739497\n",
      "Training Loss: 0.0012640279612242012\n",
      "Training Loss: 0.0011422555756871588\n",
      "Validation Loss: 0.0019983504393071717\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.00589175374887418\n",
      "Training Loss: 0.005780342515208758\n",
      "Training Loss: 0.005847283845068887\n",
      "Training Loss: 0.00401987189106876\n",
      "Training Loss: 0.0013587818600353785\n",
      "Training Loss: 0.0012620927875832421\n",
      "Training Loss: 0.0011404949967254651\n",
      "Validation Loss: 0.001995597536815592\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.005885592579143122\n",
      "Training Loss: 0.005773223257274367\n",
      "Training Loss: 0.005840653732884675\n",
      "Training Loss: 0.00401548496884061\n",
      "Training Loss: 0.0013577381349750796\n",
      "Training Loss: 0.001260162527760258\n",
      "Training Loss: 0.0011387508860207162\n",
      "Validation Loss: 0.001992863644084942\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005879459272837267\n",
      "Training Loss: 0.0057661324145738034\n",
      "Training Loss: 0.0058340488630346955\n",
      "Training Loss: 0.004011116550100269\n",
      "Training Loss: 0.001356723361168406\n",
      "Training Loss: 0.0012582403150736355\n",
      "Training Loss: 0.0011370289805199719\n",
      "Validation Loss: 0.001990152832435894\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005873363716527819\n",
      "Training Loss: 0.0057590782281477\n",
      "Training Loss: 0.00582747467851732\n",
      "Training Loss: 0.00400676941331767\n",
      "Training Loss: 0.0013557344493892742\n",
      "Training Loss: 0.0012563244511693483\n",
      "Training Loss: 0.0011353265905199804\n",
      "Validation Loss: 0.001987463231320573\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005867304338607937\n",
      "Training Loss: 0.005752060040249489\n",
      "Training Loss: 0.005820933289942332\n",
      "Training Loss: 0.004002441692500724\n",
      "Training Loss: 0.0013547710751299746\n",
      "Training Loss: 0.0012544145836000098\n",
      "Training Loss: 0.0011336439947626786\n",
      "Validation Loss: 0.0019847968829122516\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005861284614074975\n",
      "Training Loss: 0.005745085724047385\n",
      "Training Loss: 0.005814430752070621\n",
      "Training Loss: 0.003998139215618721\n",
      "Training Loss: 0.001353834377368912\n",
      "Training Loss: 0.001252511502825655\n",
      "Training Loss: 0.0011319817124604014\n",
      "Validation Loss: 0.0019821514068555073\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005855308829923161\n",
      "Training Loss: 0.005738153550191782\n",
      "Training Loss: 0.005807969641755335\n",
      "Training Loss: 0.003993862884235568\n",
      "Training Loss: 0.001352926228864817\n",
      "Training Loss: 0.0012506191762076924\n",
      "Training Loss: 0.0011303441812924575\n",
      "Validation Loss: 0.0019795303283109285\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005849375396501273\n",
      "Training Loss: 0.005731268068775535\n",
      "Training Loss: 0.005801556301885284\n",
      "Training Loss: 0.0039896154552116056\n",
      "Training Loss: 0.001352040428610053\n",
      "Training Loss: 0.0012487318073544885\n",
      "Training Loss: 0.0011287243463448248\n",
      "Validation Loss: 0.001976933286696226\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.00584349891752936\n",
      "Training Loss: 0.005724437418393791\n",
      "Training Loss: 0.0057951943343505265\n",
      "Training Loss: 0.003985399268567562\n",
      "Training Loss: 0.0013511811484931968\n",
      "Training Loss: 0.0012468549993354828\n",
      "Training Loss: 0.0011271284312533681\n",
      "Validation Loss: 0.0019743603929765663\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005837672296911478\n",
      "Training Loss: 0.005717660792288371\n",
      "Training Loss: 0.005788888207171113\n",
      "Training Loss: 0.003981217061373173\n",
      "Training Loss: 0.001350344329039217\n",
      "Training Loss: 0.0012449849762197118\n",
      "Training Loss: 0.001125552774683456\n",
      "Validation Loss: 0.0019718100439887042\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.00583190232864581\n",
      "Training Loss: 0.005710944355814718\n",
      "Training Loss: 0.005782645359286107\n",
      "Training Loss: 0.003977067462619744\n",
      "Training Loss: 0.0013495265548408497\n",
      "Training Loss: 0.001243125382970902\n",
      "Training Loss: 0.0011240006249863654\n",
      "Validation Loss: 0.0019692872215017906\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005826191494707018\n",
      "Training Loss: 0.005704287380212918\n",
      "Training Loss: 0.005776463561342098\n",
      "Training Loss: 0.0039729554911900776\n",
      "Training Loss: 0.0013487345840258059\n",
      "Training Loss: 0.0012412770437367727\n",
      "Training Loss: 0.0011224710502574452\n",
      "Validation Loss: 0.0019667892121559023\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005820543118170462\n",
      "Training Loss: 0.005697696005809121\n",
      "Training Loss: 0.005770353583502583\n",
      "Training Loss: 0.003968883110283059\n",
      "Training Loss: 0.0013479595402895938\n",
      "Training Loss: 0.0012394402664358496\n",
      "Training Loss: 0.0011209653276455355\n",
      "Validation Loss: 0.0019643179205623856\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005814960490679369\n",
      "Training Loss: 0.005691172709921375\n",
      "Training Loss: 0.005764314184198156\n",
      "Training Loss: 0.003964852209901437\n",
      "Training Loss: 0.0013472104840911924\n",
      "Training Loss: 0.0012376198459969601\n",
      "Training Loss: 0.0011194866446749075\n",
      "Validation Loss: 0.0019618762252208767\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.00580944077228196\n",
      "Training Loss: 0.0056847150035900995\n",
      "Training Loss: 0.005758348800009116\n",
      "Training Loss: 0.003960865407425445\n",
      "Training Loss: 0.0013464774984458926\n",
      "Training Loss: 0.0012358082659920911\n",
      "Training Loss: 0.0011180271457851632\n",
      "Validation Loss: 0.0019594603517351754\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005803995339665562\n",
      "Training Loss: 0.005678334836266004\n",
      "Training Loss: 0.005752466300036758\n",
      "Training Loss: 0.003956919097108766\n",
      "Training Loss: 0.001345761727716308\n",
      "Training Loss: 0.0012340159760788083\n",
      "Training Loss: 0.0011165956498734885\n",
      "Validation Loss: 0.001957072695176124\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005798617117688991\n",
      "Training Loss: 0.005672026285901665\n",
      "Training Loss: 0.00574666463478934\n",
      "Training Loss: 0.0039530204413313185\n",
      "Training Loss: 0.0013450557080795988\n",
      "Training Loss: 0.0012322351003240329\n",
      "Training Loss: 0.0011151828512811334\n",
      "Validation Loss: 0.0019547114777029926\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005793319143704139\n",
      "Training Loss: 0.005665801564464346\n",
      "Training Loss: 0.005740955143701285\n",
      "Training Loss: 0.003949169741754304\n",
      "Training Loss: 0.00134436270192964\n",
      "Training Loss: 0.001230470091686584\n",
      "Training Loss: 0.0011137944280926605\n",
      "Validation Loss: 0.0019523737826650572\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005788090852438472\n",
      "Training Loss: 0.005659651068854145\n",
      "Training Loss: 0.00573533094371669\n",
      "Training Loss: 0.003945366401458159\n",
      "Training Loss: 0.001343678814737359\n",
      "Training Loss: 0.001228721508232411\n",
      "Training Loss: 0.0011124272221059073\n",
      "Validation Loss: 0.0019500667499840498\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.00578294277715031\n",
      "Training Loss: 0.005653586523840204\n",
      "Training Loss: 0.005729800587869249\n",
      "Training Loss: 0.003941612638300285\n",
      "Training Loss: 0.001343002672365401\n",
      "Training Loss: 0.0012269877958897269\n",
      "Training Loss: 0.0011110814011044568\n",
      "Validation Loss: 0.0019477863230002166\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005777873118640855\n",
      "Training Loss: 0.005647603396209888\n",
      "Training Loss: 0.005724361626198516\n",
      "Training Loss: 0.003937906927458244\n",
      "Training Loss: 0.0013423317567503546\n",
      "Training Loss: 0.0012252742393320658\n",
      "Training Loss: 0.0011097583226364804\n",
      "Validation Loss: 0.0019455341526746666\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005772877407725901\n",
      "Training Loss: 0.00564170153811574\n",
      "Training Loss: 0.0057190159353194756\n",
      "Training Loss: 0.003934250305537717\n",
      "Training Loss: 0.0013416629847779404\n",
      "Training Loss: 0.0012235766438971041\n",
      "Training Loss: 0.0011084530958760297\n",
      "Validation Loss: 0.001943308083034881\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005767959996592254\n",
      "Training Loss: 0.005635885532246903\n",
      "Training Loss: 0.005713764693355188\n",
      "Training Loss: 0.0039306413839949525\n",
      "Training Loss: 0.0013409938634140416\n",
      "Training Loss: 0.0012218997757736361\n",
      "Training Loss: 0.0011071707079827319\n",
      "Validation Loss: 0.0019411095749405438\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.0057631209207465875\n",
      "Training Loss: 0.005630150754586794\n",
      "Training Loss: 0.005708606635453179\n",
      "Training Loss: 0.003927083863018197\n",
      "Training Loss: 0.001340324908378534\n",
      "Training Loss: 0.0012202386977151036\n",
      "Training Loss: 0.0011059040234977146\n",
      "Validation Loss: 0.0019389331139788059\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005758356410660781\n",
      "Training Loss: 0.005624502285500057\n",
      "Training Loss: 0.00570354595722165\n",
      "Training Loss: 0.00392357455813908\n",
      "Training Loss: 0.001339647846034495\n",
      "Training Loss: 0.001218596242397325\n",
      "Training Loss: 0.0011046576334774727\n",
      "Validation Loss: 0.00193678727520635\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005753666624659672\n",
      "Training Loss: 0.005618931816425175\n",
      "Training Loss: 0.005698574826237745\n",
      "Training Loss: 0.003920112648265786\n",
      "Training Loss: 0.0013389662295230664\n",
      "Training Loss: 0.0012169743088452378\n",
      "Training Loss: 0.0011034273634140846\n",
      "Validation Loss: 0.0019346636489787129\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005749050081358291\n",
      "Training Loss: 0.005613444301998243\n",
      "Training Loss: 0.005693696472444571\n",
      "Training Loss: 0.003916699536566739\n",
      "Training Loss: 0.0013382780652318616\n",
      "Training Loss: 0.0012153697538451525\n",
      "Training Loss: 0.0011022148397023556\n",
      "Validation Loss: 0.0019325649879514397\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005744502297020517\n",
      "Training Loss: 0.005608034188044258\n",
      "Training Loss: 0.0056889052118640395\n",
      "Training Loss: 0.003913329666829668\n",
      "Training Loss: 0.0013375793861632702\n",
      "Training Loss: 0.0012137865649128798\n",
      "Training Loss: 0.0011010178124706727\n",
      "Validation Loss: 0.0019304899715518488\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.0057400244835298505\n",
      "Training Loss: 0.00560270190006122\n",
      "Training Loss: 0.005684202392003499\n",
      "Training Loss: 0.003910005264842766\n",
      "Training Loss: 0.0013368694212113042\n",
      "Training Loss: 0.001212221920650336\n",
      "Training Loss: 0.0010998364404804306\n",
      "Validation Loss: 0.0019284410652122897\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.0057356123649515214\n",
      "Training Loss: 0.005597444983432069\n",
      "Training Loss: 0.005679584293975495\n",
      "Training Loss: 0.003906726544810227\n",
      "Training Loss: 0.001336144787928788\n",
      "Training Loss: 0.0012106737862632145\n",
      "Training Loss: 0.0010986651973507833\n",
      "Validation Loss: 0.0019264122831359617\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005731267143273726\n",
      "Training Loss: 0.005592264247243292\n",
      "Training Loss: 0.005675051040598191\n",
      "Training Loss: 0.003903489538206486\n",
      "Training Loss: 0.0013354056628304533\n",
      "Training Loss: 0.001209143123778631\n",
      "Training Loss: 0.0010975083096127491\n",
      "Validation Loss: 0.0019244065239880765\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005726980358012952\n",
      "Training Loss: 0.005587151868385262\n",
      "Training Loss: 0.005670596471172757\n",
      "Training Loss: 0.0039002922416693764\n",
      "Training Loss: 0.0013346513209398837\n",
      "Training Loss: 0.001207631318757194\n",
      "Training Loss: 0.00109636514353042\n",
      "Validation Loss: 0.0019224236134181586\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005722754093003459\n",
      "Training Loss: 0.005582108141388744\n",
      "Training Loss: 0.005666215506498702\n",
      "Training Loss: 0.003897136590021546\n",
      "Training Loss: 0.0013338791319984012\n",
      "Training Loss: 0.001206136376640643\n",
      "Training Loss: 0.0010952307320258115\n",
      "Validation Loss: 0.0019204613462932868\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005718584936694242\n",
      "Training Loss: 0.005577130332239904\n",
      "Training Loss: 0.005661906387540511\n",
      "Training Loss: 0.0038940187587286346\n",
      "Training Loss: 0.0013330957588914315\n",
      "Training Loss: 0.0012046604452189058\n",
      "Training Loss: 0.001094110222911695\n",
      "Validation Loss: 0.0019185193338538678\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005714464822667651\n",
      "Training Loss: 0.005572213997947983\n",
      "Training Loss: 0.005657667174818926\n",
      "Training Loss: 0.0038909343724662904\n",
      "Training Loss: 0.0013322898821206764\n",
      "Training Loss: 0.0012031982751796021\n",
      "Training Loss: 0.0010929951134312432\n",
      "Validation Loss: 0.0019165964160342997\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005710399455274455\n",
      "Training Loss: 0.005567360509885475\n",
      "Training Loss: 0.0056534960551653055\n",
      "Training Loss: 0.0038878880768606905\n",
      "Training Loss: 0.0013314657208684366\n",
      "Training Loss: 0.0012017501936497864\n",
      "Training Loss: 0.0010918907473387663\n",
      "Validation Loss: 0.0019146925709249577\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.0057063808117527514\n",
      "Training Loss: 0.005562562099075876\n",
      "Training Loss: 0.005649383326526731\n",
      "Training Loss: 0.003884873785209493\n",
      "Training Loss: 0.0013306255397037602\n",
      "Training Loss: 0.0012003216250741388\n",
      "Training Loss: 0.0010907970798871248\n",
      "Validation Loss: 0.0019128104756422314\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.00570240581757389\n",
      "Training Loss: 0.005557816893560812\n",
      "Training Loss: 0.005645329875987954\n",
      "Training Loss: 0.0038818894352152713\n",
      "Training Loss: 0.0013297661019896622\n",
      "Training Loss: 0.001198905104829464\n",
      "Training Loss: 0.0010897084775933763\n",
      "Validation Loss: 0.0019109458429302866\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005698474597302265\n",
      "Training Loss: 0.0055531237786635755\n",
      "Training Loss: 0.005641331928782165\n",
      "Training Loss: 0.0038789374953194056\n",
      "Training Loss: 0.0013288904174987692\n",
      "Training Loss: 0.001197501893111621\n",
      "Training Loss: 0.0010886274841323028\n",
      "Validation Loss: 0.001909094059286981\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005694581952411681\n",
      "Training Loss: 0.00554848036612384\n",
      "Training Loss: 0.005637384047731757\n",
      "Training Loss: 0.003876011959146126\n",
      "Training Loss: 0.0013279916586179752\n",
      "Training Loss: 0.0011961096995946718\n",
      "Training Loss: 0.0010875489687896334\n",
      "Validation Loss: 0.0019072599209036699\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005690728341578506\n",
      "Training Loss: 0.005543882892234251\n",
      "Training Loss: 0.005633483977871947\n",
      "Training Loss: 0.003873114782909397\n",
      "Training Loss: 0.0013270783428743016\n",
      "Training Loss: 0.0011947313800192206\n",
      "Training Loss: 0.0010864789467450464\n",
      "Validation Loss: 0.001905443027575434\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005686908128554932\n",
      "Training Loss: 0.005539326930884272\n",
      "Training Loss: 0.005629630362382159\n",
      "Training Loss: 0.0038702433690923498\n",
      "Training Loss: 0.0013261441948998255\n",
      "Training Loss: 0.001193361811383511\n",
      "Training Loss: 0.0010854089672648116\n",
      "Validation Loss: 0.0019036370796310167\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005683126051444561\n",
      "Training Loss: 0.005534815563587472\n",
      "Training Loss: 0.005625819853157736\n",
      "Training Loss: 0.003867394266635529\n",
      "Training Loss: 0.0013251887350634207\n",
      "Training Loss: 0.0011920016814110567\n",
      "Training Loss: 0.0010843439350719563\n",
      "Validation Loss: 0.0019018441048007778\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005679370539146475\n",
      "Training Loss: 0.00553033796895761\n",
      "Training Loss: 0.0056220450706314295\n",
      "Training Loss: 0.003864568192511797\n",
      "Training Loss: 0.0013242174650076777\n",
      "Training Loss: 0.0011906529466068605\n",
      "Training Loss: 0.0010832831912557595\n",
      "Validation Loss: 0.0019000674900293616\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.0056756430596578865\n",
      "Training Loss: 0.0055258969700662415\n",
      "Training Loss: 0.0056183072400745005\n",
      "Training Loss: 0.0038617629327927718\n",
      "Training Loss: 0.0013232274531037548\n",
      "Training Loss: 0.0011893096084531862\n",
      "Training Loss: 0.001082221536853467\n",
      "Validation Loss: 0.0018983024821816403\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005671944493660703\n",
      "Training Loss: 0.005521484581404366\n",
      "Training Loss: 0.005614600564003922\n",
      "Training Loss: 0.0038589769699319734\n",
      "Training Loss: 0.0013222194493573624\n",
      "Training Loss: 0.0011879750431398862\n",
      "Training Loss: 0.001081161095353309\n",
      "Validation Loss: 0.0018965475671499103\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005668272023904138\n",
      "Training Loss: 0.005517108819331043\n",
      "Training Loss: 0.005610926021472551\n",
      "Training Loss: 0.003856206925047445\n",
      "Training Loss: 0.001321190169401234\n",
      "Training Loss: 0.0011866473159898305\n",
      "Training Loss: 0.0010800990740244742\n",
      "Validation Loss: 0.0018948017251010712\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005664621083415113\n",
      "Training Loss: 0.005512756751850248\n",
      "Training Loss: 0.005607276071095839\n",
      "Training Loss: 0.003853455918651889\n",
      "Training Loss: 0.001320146623038454\n",
      "Training Loss: 0.001185323741810862\n",
      "Training Loss: 0.0010790368436573773\n",
      "Validation Loss: 0.0018930672641759843\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005660994857316836\n",
      "Training Loss: 0.005508432743954472\n",
      "Training Loss: 0.005603654253063723\n",
      "Training Loss: 0.0038507200679305242\n",
      "Training Loss: 0.001319079782697372\n",
      "Training Loss: 0.0011840034009946976\n",
      "Training Loss: 0.001077970986661967\n",
      "Validation Loss: 0.0018913393125896366\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.0056573879404459146\n",
      "Training Loss: 0.005504132251953706\n",
      "Training Loss: 0.005600054378737696\n",
      "Training Loss: 0.0038479990272753638\n",
      "Training Loss: 0.0013179912669875192\n",
      "Training Loss: 0.0011826832163933433\n",
      "Training Loss: 0.0010768986082985066\n",
      "Validation Loss: 0.0018896172805806999\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005653804406756535\n",
      "Training Loss: 0.005499854763620533\n",
      "Training Loss: 0.005596476286300458\n",
      "Training Loss: 0.003845287073054351\n",
      "Training Loss: 0.0013168843580933754\n",
      "Training Loss: 0.0011813674178847578\n",
      "Training Loss: 0.00107582257463946\n",
      "Validation Loss: 0.0018878999128148538\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005650234229979106\n",
      "Training Loss: 0.0054955956409685315\n",
      "Training Loss: 0.0055929150513838975\n",
      "Training Loss: 0.003842587632170762\n",
      "Training Loss: 0.0013157594414951745\n",
      "Training Loss: 0.0011800525887520053\n",
      "Training Loss: 0.0010747405845904722\n",
      "Validation Loss: 0.0018861926591206056\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005646684098755941\n",
      "Training Loss: 0.005491352505632676\n",
      "Training Loss: 0.005589368139044382\n",
      "Training Loss: 0.0038398970970592926\n",
      "Training Loss: 0.001314610144472681\n",
      "Training Loss: 0.0011787340381852118\n",
      "Training Loss: 0.0010736492334399372\n",
      "Validation Loss: 0.001884486678691689\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005643149563693442\n",
      "Training Loss: 0.0054871280951192605\n",
      "Training Loss: 0.005585837009712122\n",
      "Training Loss: 0.0038372142318257828\n",
      "Training Loss: 0.0013134422585426365\n",
      "Training Loss: 0.0011774184777459595\n",
      "Training Loss: 0.0010725522480788641\n",
      "Validation Loss: 0.001882784933360618\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.0056396304647205395\n",
      "Training Loss: 0.005482917375047691\n",
      "Training Loss: 0.0055823174101533365\n",
      "Training Loss: 0.0038345377629593713\n",
      "Training Loss: 0.001312252513889689\n",
      "Training Loss: 0.0011760970515751978\n",
      "Training Loss: 0.0010714427891070953\n",
      "Validation Loss: 0.001881083345212032\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005636124797165394\n",
      "Training Loss: 0.005478718138183467\n",
      "Training Loss: 0.00557880850858055\n",
      "Training Loss: 0.003831865988831851\n",
      "Training Loss: 0.0013110408501233906\n",
      "Training Loss: 0.00117477295541903\n",
      "Training Loss: 0.0010703228478087113\n",
      "Validation Loss: 0.0018793843824578597\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.00563262865296565\n",
      "Training Loss: 0.005474527250044048\n",
      "Training Loss: 0.005575307891122066\n",
      "Training Loss: 0.003829199883475667\n",
      "Training Loss: 0.0013098039309261368\n",
      "Training Loss: 0.00117344206533744\n",
      "Training Loss: 0.0010691878364013974\n",
      "Validation Loss: 0.0018776824765107053\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005629149549640715\n",
      "Training Loss: 0.005470348642556928\n",
      "Training Loss: 0.005571813510032371\n",
      "Training Loss: 0.0038265347982087403\n",
      "Training Loss: 0.0013085445780598092\n",
      "Training Loss: 0.0011721058942202945\n",
      "Training Loss: 0.0010680420862627215\n",
      "Validation Loss: 0.0018759818020806917\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005625676724594087\n",
      "Training Loss: 0.0054661709954962135\n",
      "Training Loss: 0.005568323551560752\n",
      "Training Loss: 0.0038238718084903666\n",
      "Training Loss: 0.0013072637608274817\n",
      "Training Loss: 0.0011707641190150753\n",
      "Training Loss: 0.0010668796430400107\n",
      "Validation Loss: 0.0018742774801340986\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005622217893251218\n",
      "Training Loss: 0.0054620029061334206\n",
      "Training Loss: 0.00556483609136194\n",
      "Training Loss: 0.0038212087169085863\n",
      "Training Loss: 0.0013059584316215478\n",
      "Training Loss: 0.0011694127485679928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [49:44<33:19, 499.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.001065700707549695\n",
      "Validation Loss: 0.0018725685644694272\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.45763027980923654\n",
      "Training Loss: 0.32534289799630645\n",
      "Training Loss: 0.1937424825504422\n",
      "Training Loss: 0.099223629841581\n",
      "Training Loss: 0.061997582726180556\n",
      "Training Loss: 0.048250018507242205\n",
      "Training Loss: 0.049970590407028795\n",
      "Validation Loss: 0.04907304748948594\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.05022676272317767\n",
      "Training Loss: 0.0490242874994874\n",
      "Training Loss: 0.04773842881433666\n",
      "Training Loss: 0.045432146564126016\n",
      "Training Loss: 0.042778014559298756\n",
      "Training Loss: 0.03862774681299925\n",
      "Training Loss: 0.04057607508264482\n",
      "Validation Loss: 0.04002482303826327\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.04078459502197802\n",
      "Training Loss: 0.039146736972033976\n",
      "Training Loss: 0.03698856799863279\n",
      "Training Loss: 0.03395235516363755\n",
      "Training Loss: 0.029552426734007896\n",
      "Training Loss: 0.02583469880744815\n",
      "Training Loss: 0.026686989802401515\n",
      "Validation Loss: 0.026813852192645662\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.027748175775632263\n",
      "Training Loss: 0.02641397720668465\n",
      "Training Loss: 0.024186139665544032\n",
      "Training Loss: 0.02103012043633498\n",
      "Training Loss: 0.016219256604090335\n",
      "Training Loss: 0.014106819778680802\n",
      "Training Loss: 0.014341723853722215\n",
      "Validation Loss: 0.015411214403326759\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.01747031372971833\n",
      "Training Loss: 0.016786413472145795\n",
      "Training Loss: 0.015235834161285312\n",
      "Training Loss: 0.01245099923107773\n",
      "Training Loss: 0.008207046857569367\n",
      "Training Loss: 0.007362148131942376\n",
      "Training Loss: 0.007348059633513913\n",
      "Validation Loss: 0.008739199914031204\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.012576780267991125\n",
      "Training Loss: 0.012795204230351374\n",
      "Training Loss: 0.012027307401876897\n",
      "Training Loss: 0.009406790367793292\n",
      "Training Loss: 0.005699811379890889\n",
      "Training Loss: 0.005294004520401359\n",
      "Training Loss: 0.005525495783658698\n",
      "Validation Loss: 0.007126704025368314\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.011124088978394866\n",
      "Training Loss: 0.011422798455459997\n",
      "Training Loss: 0.010885907489573583\n",
      "Training Loss: 0.008290155002032406\n",
      "Training Loss: 0.004721345136640593\n",
      "Training Loss: 0.004409484879579395\n",
      "Training Loss: 0.004679695521481335\n",
      "Validation Loss: 0.006101885166504047\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.010238180536543951\n",
      "Training Loss: 0.010546384042827412\n",
      "Training Loss: 0.010161909550661222\n",
      "Training Loss: 0.007603555886307731\n",
      "Training Loss: 0.004117198441526852\n",
      "Training Loss: 0.003883807180682197\n",
      "Training Loss: 0.004137877932516858\n",
      "Validation Loss: 0.005530273928115151\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.009667187363374978\n",
      "Training Loss: 0.009989517347421498\n",
      "Training Loss: 0.009704517796635628\n",
      "Training Loss: 0.007162629922968336\n",
      "Training Loss: 0.0037339060893282292\n",
      "Training Loss: 0.003556347235571593\n",
      "Training Loss: 0.003781758978147991\n",
      "Validation Loss: 0.005199281209931545\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.009287080334033818\n",
      "Training Loss: 0.0096071422949899\n",
      "Training Loss: 0.009380169254727662\n",
      "Training Loss: 0.006842433515703306\n",
      "Training Loss: 0.0034614166323444807\n",
      "Training Loss: 0.003321317491354421\n",
      "Training Loss: 0.003513987310579978\n",
      "Validation Loss: 0.004943200956056878\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.008999740821309388\n",
      "Training Loss: 0.009309236727422104\n",
      "Training Loss: 0.009120389751624316\n",
      "Training Loss: 0.006587161174684298\n",
      "Training Loss: 0.0032448194752214476\n",
      "Training Loss: 0.0031345712911570443\n",
      "Training Loss: 0.00329479374573566\n",
      "Validation Loss: 0.004719923676579339\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.008764384550740942\n",
      "Training Loss: 0.00906034869607538\n",
      "Training Loss: 0.008898983780527487\n",
      "Training Loss: 0.006373400488810148\n",
      "Training Loss: 0.0030610453360714017\n",
      "Training Loss: 0.002977771168225445\n",
      "Training Loss: 0.003105854846944567\n",
      "Validation Loss: 0.004516234952993063\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.008559612764511258\n",
      "Training Loss: 0.00884162046480924\n",
      "Training Loss: 0.008701856123516337\n",
      "Training Loss: 0.006186762504221406\n",
      "Training Loss: 0.00289794508862542\n",
      "Training Loss: 0.0028399659795104526\n",
      "Training Loss: 0.0029373981087701395\n",
      "Validation Loss: 0.004328637237814063\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.008374575136695057\n",
      "Training Loss: 0.008643778207479046\n",
      "Training Loss: 0.008522086621960625\n",
      "Training Loss: 0.006019695087452419\n",
      "Training Loss: 0.0027500045570195654\n",
      "Training Loss: 0.0027153919110423884\n",
      "Training Loss: 0.0027848181006265805\n",
      "Validation Loss: 0.004154628426776606\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.00820473322062753\n",
      "Training Loss: 0.008462859095307067\n",
      "Training Loss: 0.008356526552233845\n",
      "Training Loss: 0.0058685926932957955\n",
      "Training Loss: 0.0026146678609075026\n",
      "Training Loss: 0.0026010155695257707\n",
      "Training Loss: 0.002645713519596029\n",
      "Validation Loss: 0.003992258447696793\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.008047920140670612\n",
      "Training Loss: 0.008296655693557113\n",
      "Training Loss: 0.008203444455284625\n",
      "Training Loss: 0.005731381462537683\n",
      "Training Loss: 0.0024905641097575427\n",
      "Training Loss: 0.002495097557984991\n",
      "Training Loss: 0.0025186670874245464\n",
      "Validation Loss: 0.0038403238746257365\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.007902819523587823\n",
      "Training Loss: 0.008143657071050256\n",
      "Training Loss: 0.008061754916561768\n",
      "Training Loss: 0.005606661452766275\n",
      "Training Loss: 0.0023768818069947882\n",
      "Training Loss: 0.0023966279198066333\n",
      "Training Loss: 0.0024027703423053024\n",
      "Validation Loss: 0.0036981882946478545\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.0077685776492580775\n",
      "Training Loss: 0.008002809670288116\n",
      "Training Loss: 0.007930757590802386\n",
      "Training Loss: 0.0054934006329858676\n",
      "Training Loss: 0.0022730798547854645\n",
      "Training Loss: 0.002305047193658538\n",
      "Training Loss: 0.0022973606377490797\n",
      "Validation Loss: 0.0035655049156756856\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.007644620772916824\n",
      "Training Loss: 0.00787334889639169\n",
      "Training Loss: 0.007809965166961775\n",
      "Training Loss: 0.00539078020316083\n",
      "Training Loss: 0.002178739029914141\n",
      "Training Loss: 0.002220060365507379\n",
      "Training Loss: 0.0022018732380820437\n",
      "Validation Loss: 0.003442030528661436\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.007530540683073923\n",
      "Training Loss: 0.00775467966333963\n",
      "Training Loss: 0.007699004254536703\n",
      "Training Loss: 0.005298089599091327\n",
      "Training Loss: 0.002093456260918174\n",
      "Training Loss: 0.0021415144158527257\n",
      "Training Loss: 0.00211574674845906\n",
      "Validation Loss: 0.00332753216169342\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.007425978435203433\n",
      "Training Loss: 0.007646270160330459\n",
      "Training Loss: 0.0075975206517614425\n",
      "Training Loss: 0.005214663699734956\n",
      "Training Loss: 0.002016798474069219\n",
      "Training Loss: 0.002069298800051911\n",
      "Training Loss: 0.0020384066391852685\n",
      "Validation Loss: 0.0032217266443115744\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.0073305780277587475\n",
      "Training Loss: 0.0075476153276395055\n",
      "Training Loss: 0.007505157385021448\n",
      "Training Loss: 0.005139864422526444\n",
      "Training Loss: 0.0019483031048730482\n",
      "Training Loss: 0.002003294152818853\n",
      "Training Loss: 0.001969258915196406\n",
      "Validation Loss: 0.003124306498798245\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.00724395961384289\n",
      "Training Loss: 0.007458204528084025\n",
      "Training Loss: 0.007421535917092115\n",
      "Training Loss: 0.005073059819405899\n",
      "Training Loss: 0.0018874600948765874\n",
      "Training Loss: 0.0019433430895151105\n",
      "Training Loss: 0.001907702975149732\n",
      "Validation Loss: 0.0030349352369119834\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.007165707321837545\n",
      "Training Loss: 0.007377526825293899\n",
      "Training Loss: 0.007346258556935936\n",
      "Training Loss: 0.005013634731003549\n",
      "Training Loss: 0.0018337273834913503\n",
      "Training Loss: 0.0018892231192148756\n",
      "Training Loss: 0.0018531251937383786\n",
      "Validation Loss: 0.002953226143522153\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.007095376587240025\n",
      "Training Loss: 0.007305068344576285\n",
      "Training Loss: 0.007278913342161104\n",
      "Training Loss: 0.00496097460767487\n",
      "Training Loss: 0.0017865116873872467\n",
      "Training Loss: 0.0018406244418292773\n",
      "Training Loss: 0.0018048876139800996\n",
      "Validation Loss: 0.0028787483701008033\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.007032489879056811\n",
      "Training Loss: 0.007240303917787969\n",
      "Training Loss: 0.007219058650080114\n",
      "Training Loss: 0.0049144680330937265\n",
      "Training Loss: 0.0017451776412781327\n",
      "Training Loss: 0.0017971602219040506\n",
      "Training Loss: 0.0017623368505155668\n",
      "Validation Loss: 0.0028110237401685143\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.006976553653366864\n",
      "Training Loss: 0.007182714975206181\n",
      "Training Loss: 0.007166244698455557\n",
      "Training Loss: 0.004873510447650915\n",
      "Training Loss: 0.0017090551659930497\n",
      "Training Loss: 0.0017583676402864513\n",
      "Training Loss: 0.0017247948188742158\n",
      "Validation Loss: 0.0027495275640926475\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.0069270512531511485\n",
      "Training Loss: 0.00713177471421659\n",
      "Training Loss: 0.007119992329971864\n",
      "Training Loss: 0.004837498447450343\n",
      "Training Loss: 0.0016774516590521672\n",
      "Training Loss: 0.0017237296533130575\n",
      "Training Loss: 0.0016915814344247338\n",
      "Validation Loss: 0.0026936784379222615\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.006883450391469524\n",
      "Training Loss: 0.0070869539584964515\n",
      "Training Loss: 0.0070798030449077485\n",
      "Training Loss: 0.004805850872944575\n",
      "Training Loss: 0.0016496828837262\n",
      "Training Loss: 0.0016927090991521255\n",
      "Training Loss: 0.0016620443380088545\n",
      "Validation Loss: 0.002642914640398047\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006845210067695007\n",
      "Training Loss: 0.00704771563061513\n",
      "Training Loss: 0.0070451597508508716\n",
      "Training Loss: 0.004778017392527545\n",
      "Training Loss: 0.0016251187743910123\n",
      "Training Loss: 0.001664790117647499\n",
      "Training Loss: 0.001635591825906886\n",
      "Validation Loss: 0.002596689967071456\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006811798956478015\n",
      "Training Loss: 0.007013539610197767\n",
      "Training Loss: 0.007015543594025076\n",
      "Training Loss: 0.004753501170926029\n",
      "Training Loss: 0.001603206284053158\n",
      "Training Loss: 0.0016395209156326019\n",
      "Training Loss: 0.0016117348521947862\n",
      "Validation Loss: 0.0025545548467132985\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006782711475389078\n",
      "Training Loss: 0.006983931981958449\n",
      "Training Loss: 0.0069904536905232816\n",
      "Training Loss: 0.0047318878726218825\n",
      "Training Loss: 0.0015835196670377627\n",
      "Training Loss: 0.0016165526905388106\n",
      "Training Loss: 0.001590118179883575\n",
      "Validation Loss: 0.002516164944384226\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.006757487547583878\n",
      "Training Loss: 0.006958433665568009\n",
      "Training Loss: 0.0069694112543947995\n",
      "Training Loss: 0.004712846992260893\n",
      "Training Loss: 0.0015657810427364893\n",
      "Training Loss: 0.001595663058833452\n",
      "Training Loss: 0.001570538028026931\n",
      "Validation Loss: 0.002481324166540852\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006735716878902167\n",
      "Training Loss: 0.006936618865001947\n",
      "Training Loss: 0.006951964270556345\n",
      "Training Loss: 0.00469613494729856\n",
      "Training Loss: 0.0015498471471073571\n",
      "Training Loss: 0.0015767586458241567\n",
      "Training Loss: 0.0015529215871356427\n",
      "Validation Loss: 0.0024499634032491735\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006717037905473262\n",
      "Training Loss: 0.006918086402583868\n",
      "Training Loss: 0.0069376746553462\n",
      "Training Loss: 0.004681564512793557\n",
      "Training Loss: 0.0015356820795568637\n",
      "Training Loss: 0.0015598348164348862\n",
      "Training Loss: 0.00153727798271575\n",
      "Validation Loss: 0.0024220731907501495\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.00670110996463336\n",
      "Training Loss: 0.006902432312490419\n",
      "Training Loss: 0.0069260958186350766\n",
      "Training Loss: 0.0046689655433146985\n",
      "Training Loss: 0.0015232934459345415\n",
      "Training Loss: 0.0015449250214442144\n",
      "Training Loss: 0.0015236318082315846\n",
      "Validation Loss: 0.0023976573133047616\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.00668758881278336\n",
      "Training Loss: 0.006889214880065993\n",
      "Training Loss: 0.006916742171160877\n",
      "Training Loss: 0.004658141814070404\n",
      "Training Loss: 0.0015126731852069498\n",
      "Training Loss: 0.0015320353157585486\n",
      "Training Loss: 0.001511959916970227\n",
      "Validation Loss: 0.0023766305955555916\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006676094786962494\n",
      "Training Loss: 0.006877956613898277\n",
      "Training Loss: 0.006909108110703528\n",
      "Training Loss: 0.004648859248045483\n",
      "Training Loss: 0.001503744538349565\n",
      "Training Loss: 0.0015210949223546776\n",
      "Training Loss: 0.0015021496471308638\n",
      "Validation Loss: 0.002358783159534925\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006666229622205719\n",
      "Training Loss: 0.0068681684683542694\n",
      "Training Loss: 0.006902691593859344\n",
      "Training Loss: 0.004640841854416067\n",
      "Training Loss: 0.0014963561610784382\n",
      "Training Loss: 0.0015119470536592416\n",
      "Training Loss: 0.0014939995025633835\n",
      "Validation Loss: 0.00234379193517519\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.0066576066392008216\n",
      "Training Loss: 0.006859401136171073\n",
      "Training Loss: 0.006897047987440601\n",
      "Training Loss: 0.00463380945759127\n",
      "Training Loss: 0.0014902954889112153\n",
      "Training Loss: 0.0015043670931481755\n",
      "Training Loss: 0.0014872513496811735\n",
      "Validation Loss: 0.0023312604207194965\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006649878153111786\n",
      "Training Loss: 0.006851275934604928\n",
      "Training Loss: 0.006891820707824081\n",
      "Training Loss: 0.004627502685616491\n",
      "Training Loss: 0.0014853195818432142\n",
      "Training Loss: 0.0014980936807114631\n",
      "Training Loss: 0.0014816290490853134\n",
      "Validation Loss: 0.002320761051031583\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006642768317833543\n",
      "Training Loss: 0.006843517138622701\n",
      "Training Loss: 0.006886760086053982\n",
      "Training Loss: 0.004621704764722381\n",
      "Training Loss: 0.0014811894467857201\n",
      "Training Loss: 0.0014928655997209716\n",
      "Training Loss: 0.0014768661873677048\n",
      "Validation Loss: 0.0023118914385136104\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.0066360798198729755\n",
      "Training Loss: 0.0068359463720116765\n",
      "Training Loss: 0.00688171467394568\n",
      "Training Loss: 0.004616252071064082\n",
      "Training Loss: 0.0014776890803477726\n",
      "Training Loss: 0.001488441505789524\n",
      "Training Loss: 0.0014727307819703127\n",
      "Validation Loss: 0.00230428972575824\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006629688508110121\n",
      "Training Loss: 0.006828470211476087\n",
      "Training Loss: 0.006876618174137547\n",
      "Training Loss: 0.004611025270132813\n",
      "Training Loss: 0.001474633177567739\n",
      "Training Loss: 0.0014846149038930888\n",
      "Training Loss: 0.0014690333676844601\n",
      "Validation Loss: 0.002297663098037215\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006623510956997052\n",
      "Training Loss: 0.006821037139743567\n",
      "Training Loss: 0.006871440366376191\n",
      "Training Loss: 0.004605945407020045\n",
      "Training Loss: 0.0014718822107533925\n",
      "Training Loss: 0.0014812210988020525\n",
      "Training Loss: 0.0014656306998222135\n",
      "Validation Loss: 0.0022917663814126746\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006617506627226249\n",
      "Training Loss: 0.006813639010069892\n",
      "Training Loss: 0.006866189858410508\n",
      "Training Loss: 0.004600957822331111\n",
      "Training Loss: 0.0014693243267538492\n",
      "Training Loss: 0.0014781281304021832\n",
      "Training Loss: 0.0014624068791454193\n",
      "Validation Loss: 0.002286409039755809\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006611650396371261\n",
      "Training Loss: 0.006806281098397449\n",
      "Training Loss: 0.006860887014772743\n",
      "Training Loss: 0.0045960286381159675\n",
      "Training Loss: 0.0014668777486804175\n",
      "Training Loss: 0.001475234198733233\n",
      "Training Loss: 0.0014592821219412144\n",
      "Validation Loss: 0.0022814440866234654\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006605932186357677\n",
      "Training Loss: 0.006798975700512529\n",
      "Training Loss: 0.006855556173250079\n",
      "Training Loss: 0.004591134137808694\n",
      "Training Loss: 0.001464479629357811\n",
      "Training Loss: 0.0014724634823505766\n",
      "Training Loss: 0.001456196915023611\n",
      "Validation Loss: 0.002276762462223702\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.0066003434092272075\n",
      "Training Loss: 0.006791734598809853\n",
      "Training Loss: 0.006850214938167482\n",
      "Training Loss: 0.004586260157739161\n",
      "Training Loss: 0.0014620902654860401\n",
      "Training Loss: 0.001469760676700389\n",
      "Training Loss: 0.0014531107027141842\n",
      "Validation Loss: 0.002272275955026467\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006594874005531892\n",
      "Training Loss: 0.006784566000569612\n",
      "Training Loss: 0.006844883820740506\n",
      "Training Loss: 0.00458139505048166\n",
      "Training Loss: 0.0014596772019285708\n",
      "Training Loss: 0.0014670826462679543\n",
      "Training Loss: 0.0014499934408377158\n",
      "Validation Loss: 0.002267921586413711\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006589519606204703\n",
      "Training Loss: 0.0067774742131587115\n",
      "Training Loss: 0.006839571038726717\n",
      "Training Loss: 0.004576530380581971\n",
      "Training Loss: 0.001457219149233424\n",
      "Training Loss: 0.0014643984235590325\n",
      "Training Loss: 0.0014468213904183357\n",
      "Validation Loss: 0.0022636437649081707\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.0065842709178105\n",
      "Training Loss: 0.00677046122495085\n",
      "Training Loss: 0.006834283326752484\n",
      "Training Loss: 0.0045716578933934215\n",
      "Training Loss: 0.0014547034235147293\n",
      "Training Loss: 0.0014616873003251386\n",
      "Training Loss: 0.001443585417146096\n",
      "Validation Loss: 0.0022594156286553637\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006579116643406451\n",
      "Training Loss: 0.006763520679669455\n",
      "Training Loss: 0.006829019732540473\n",
      "Training Loss: 0.00456677251480869\n",
      "Training Loss: 0.0014521197420981479\n",
      "Training Loss: 0.0014589329963200725\n",
      "Training Loss: 0.0014402737699856516\n",
      "Validation Loss: 0.0022552002161176183\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006574046984314919\n",
      "Training Loss: 0.006756648265291005\n",
      "Training Loss: 0.006823780761333182\n",
      "Training Loss: 0.004561867855809396\n",
      "Training Loss: 0.0014494618837488816\n",
      "Training Loss: 0.0014561230044637342\n",
      "Training Loss: 0.0014368799574731384\n",
      "Validation Loss: 0.0022509768483148754\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006569050680845976\n",
      "Training Loss: 0.006749836237868294\n",
      "Training Loss: 0.006818562248954549\n",
      "Training Loss: 0.004556936912777019\n",
      "Training Loss: 0.0014467251214227872\n",
      "Training Loss: 0.001453249411424622\n",
      "Training Loss: 0.001433399639208801\n",
      "Validation Loss: 0.0022467265626300503\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.006564117425587029\n",
      "Training Loss: 0.0067430736136157065\n",
      "Training Loss: 0.0068133568170014765\n",
      "Training Loss: 0.004551975376889459\n",
      "Training Loss: 0.0014439081589080161\n",
      "Training Loss: 0.00145030789411976\n",
      "Training Loss: 0.0014298317070642952\n",
      "Validation Loss: 0.0022424389517731783\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006559233345324173\n",
      "Training Loss: 0.006736349340062588\n",
      "Training Loss: 0.006808155389735475\n",
      "Training Loss: 0.0045469768995099\n",
      "Training Loss: 0.0014410096935898763\n",
      "Training Loss: 0.0014472917815146502\n",
      "Training Loss: 0.001426172986466554\n",
      "Validation Loss: 0.002238098412276026\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006554388498188928\n",
      "Training Loss: 0.006729651603382081\n",
      "Training Loss: 0.006802949961274863\n",
      "Training Loss: 0.004541935962479328\n",
      "Training Loss: 0.0014380276043812045\n",
      "Training Loss: 0.0014441988510952796\n",
      "Training Loss: 0.0014224215500144054\n",
      "Validation Loss: 0.0022336964190053346\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.0065495728538371625\n",
      "Training Loss: 0.006722969751572236\n",
      "Training Loss: 0.006797729769023136\n",
      "Training Loss: 0.004536845894399448\n",
      "Training Loss: 0.0014349627376213903\n",
      "Training Loss: 0.0014410275842237751\n",
      "Training Loss: 0.0014185792452917666\n",
      "Validation Loss: 0.00222922938946488\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006544773386558518\n",
      "Training Loss: 0.006716287838062271\n",
      "Training Loss: 0.0067924849747214465\n",
      "Training Loss: 0.004531704104228993\n",
      "Training Loss: 0.0014318179339170456\n",
      "Training Loss: 0.0014377766037068796\n",
      "Training Loss: 0.0014146469805564265\n",
      "Validation Loss: 0.002224686895763111\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006539977805223316\n",
      "Training Loss: 0.006709593358682469\n",
      "Training Loss: 0.006787203173153102\n",
      "Training Loss: 0.004526501546934014\n",
      "Training Loss: 0.001428591119038174\n",
      "Training Loss: 0.00143444516499585\n",
      "Training Loss: 0.0014106234014616347\n",
      "Validation Loss: 0.0022200666222379505\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.0065351776371244345\n",
      "Training Loss: 0.0067028737498912964\n",
      "Training Loss: 0.006781875747255981\n",
      "Training Loss: 0.004521234232670395\n",
      "Training Loss: 0.0014252853210928152\n",
      "Training Loss: 0.0014310326206759783\n",
      "Training Loss: 0.001406510252636508\n",
      "Validation Loss: 0.0022153623031726114\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.0065303593804128465\n",
      "Training Loss: 0.006696114902151749\n",
      "Training Loss: 0.006776489563053474\n",
      "Training Loss: 0.004515894442884019\n",
      "Training Loss: 0.0014218980692385231\n",
      "Training Loss: 0.0014275380047183716\n",
      "Training Loss: 0.0014023064183129464\n",
      "Validation Loss: 0.0022105731379392485\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006525515638059005\n",
      "Training Loss: 0.006689303838647902\n",
      "Training Loss: 0.006771033812547103\n",
      "Training Loss: 0.0045104802329296945\n",
      "Training Loss: 0.0014184336789912777\n",
      "Training Loss: 0.001423960950342007\n",
      "Training Loss: 0.001398014381484245\n",
      "Validation Loss: 0.0022056931690818695\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006520634423941374\n",
      "Training Loss: 0.006682427552295849\n",
      "Training Loss: 0.006765496472362429\n",
      "Training Loss: 0.004504982714788639\n",
      "Training Loss: 0.0014148908416245832\n",
      "Training Loss: 0.0014203014500526478\n",
      "Training Loss: 0.0013936326068505877\n",
      "Validation Loss: 0.002200717340600206\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006515706490026787\n",
      "Training Loss: 0.006675472307251767\n",
      "Training Loss: 0.006759866594802588\n",
      "Training Loss: 0.004499396574756247\n",
      "Training Loss: 0.0014112698012468173\n",
      "Training Loss: 0.0014165576393861557\n",
      "Training Loss: 0.001389161731058266\n",
      "Validation Loss: 0.002195649310747643\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.0065107220411300655\n",
      "Training Loss: 0.0066684276261366904\n",
      "Training Loss: 0.006754135059891269\n",
      "Training Loss: 0.004493717262739665\n",
      "Training Loss: 0.0014075688317097955\n",
      "Training Loss: 0.0014127286689472386\n",
      "Training Loss: 0.0013845999510522233\n",
      "Validation Loss: 0.0021904772185687366\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0065056735754478725\n",
      "Training Loss: 0.0066612798604182896\n",
      "Training Loss: 0.006748287983937189\n",
      "Training Loss: 0.004487937338053598\n",
      "Training Loss: 0.0014037921740236924\n",
      "Training Loss: 0.00140881426174019\n",
      "Training Loss: 0.0013799495738931\n",
      "Validation Loss: 0.002185204548347909\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006500548816984519\n",
      "Training Loss: 0.006654015879612416\n",
      "Training Loss: 0.006742313096765429\n",
      "Training Loss: 0.004482052088787896\n",
      "Training Loss: 0.001399937190435594\n",
      "Training Loss: 0.0014048136773635632\n",
      "Training Loss: 0.00137520913121989\n",
      "Validation Loss: 0.0021798246994018137\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006495338393142447\n",
      "Training Loss: 0.006646622214466333\n",
      "Training Loss: 0.006736199691076763\n",
      "Training Loss: 0.004476054389815545\n",
      "Training Loss: 0.001396004839334637\n",
      "Training Loss: 0.0014007258805213496\n",
      "Training Loss: 0.001370379384025\n",
      "Validation Loss: 0.002174336634129844\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006490033658919856\n",
      "Training Loss: 0.0066390845796559005\n",
      "Training Loss: 0.006729933970491402\n",
      "Training Loss: 0.004469937933317851\n",
      "Training Loss: 0.0013919938782055398\n",
      "Training Loss: 0.0013965486756933388\n",
      "Training Loss: 0.0013654561553994426\n",
      "Validation Loss: 0.0021687361059888276\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.0064846270473208276\n",
      "Training Loss: 0.006631394204450771\n",
      "Training Loss: 0.00672350637265481\n",
      "Training Loss: 0.004463696988968877\n",
      "Training Loss: 0.0013879025563073811\n",
      "Training Loss: 0.0013922797976556468\n",
      "Training Loss: 0.00136043962178519\n",
      "Validation Loss: 0.002163019266351447\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.006479107721243054\n",
      "Training Loss: 0.006623536410043016\n",
      "Training Loss: 0.006716902051120997\n",
      "Training Loss: 0.004457325485418551\n",
      "Training Loss: 0.0013837303615582641\n",
      "Training Loss: 0.001387918387699756\n",
      "Training Loss: 0.0013553283971123164\n",
      "Validation Loss: 0.002157181674812498\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006473467751638964\n",
      "Training Loss: 0.006615499616600573\n",
      "Training Loss: 0.006710109465639107\n",
      "Training Loss: 0.0044508166428568075\n",
      "Training Loss: 0.001379477299778955\n",
      "Training Loss: 0.0013834611026686617\n",
      "Training Loss: 0.001350119573980919\n",
      "Validation Loss: 0.0021512184702692735\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006467698867199942\n",
      "Training Loss: 0.006607271643588319\n",
      "Training Loss: 0.006703117277356796\n",
      "Training Loss: 0.0044441630021901804\n",
      "Training Loss: 0.0013751371289254166\n",
      "Training Loss: 0.0013789029862527969\n",
      "Training Loss: 0.0013448055210028543\n",
      "Validation Loss: 0.0021451213111497516\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00646179499453865\n",
      "Training Loss: 0.006598840925144032\n",
      "Training Loss: 0.006695912176510319\n",
      "Training Loss: 0.004437358654977288\n",
      "Training Loss: 0.0013707090070238337\n",
      "Training Loss: 0.0013742431825812674\n",
      "Training Loss: 0.001339386759646004\n",
      "Validation Loss: 0.002138887731912581\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006455744990380481\n",
      "Training Loss: 0.0065901946777012195\n",
      "Training Loss: 0.006688480247394182\n",
      "Training Loss: 0.004430396501265932\n",
      "Training Loss: 0.0013661901152227074\n",
      "Training Loss: 0.0013694764713727636\n",
      "Training Loss: 0.0013338587788894074\n",
      "Validation Loss: 0.0021325094606932287\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.006449540304020047\n",
      "Training Loss: 0.006581318591488525\n",
      "Training Loss: 0.006680807490483858\n",
      "Training Loss: 0.004423268400860252\n",
      "Training Loss: 0.0013615763333655196\n",
      "Training Loss: 0.0013645987482595956\n",
      "Training Loss: 0.001328215647590696\n",
      "Validation Loss: 0.002125977622948967\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006443173915613443\n",
      "Training Loss: 0.006572202041279524\n",
      "Training Loss: 0.006672880381811411\n",
      "Training Loss: 0.004415968465036712\n",
      "Training Loss: 0.0013568624013714724\n",
      "Training Loss: 0.0013596044971927767\n",
      "Training Loss: 0.0013224497437477113\n",
      "Validation Loss: 0.0021192858301710043\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006436640006722882\n",
      "Training Loss: 0.006562833710340783\n",
      "Training Loss: 0.006664687565644272\n",
      "Training Loss: 0.004408487997643533\n",
      "Training Loss: 0.001352042646612972\n",
      "Training Loss: 0.0013544878745597089\n",
      "Training Loss: 0.001316555078374222\n",
      "Validation Loss: 0.0021124222496357024\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0064299280405975875\n",
      "Training Loss: 0.006553199194604531\n",
      "Training Loss: 0.006656211887020618\n",
      "Training Loss: 0.004400820451701292\n",
      "Training Loss: 0.0013471118215238676\n",
      "Training Loss: 0.0013492426877201068\n",
      "Training Loss: 0.0013105230383371235\n",
      "Validation Loss: 0.0021053776985395364\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006423030305886641\n",
      "Training Loss: 0.0065432843251619485\n",
      "Training Loss: 0.00664743946865201\n",
      "Training Loss: 0.004392956296651391\n",
      "Training Loss: 0.0013420604680140968\n",
      "Training Loss: 0.0013438596524065361\n",
      "Training Loss: 0.0013043423287308542\n",
      "Validation Loss: 0.002098135675828161\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006415939833968878\n",
      "Training Loss: 0.006533077096100896\n",
      "Training Loss: 0.0066383551305625585\n",
      "Training Loss: 0.004384887550913845\n",
      "Training Loss: 0.0013368802709010196\n",
      "Training Loss: 0.0013383301696012495\n",
      "Training Loss: 0.0012980012622574577\n",
      "Validation Loss: 0.0020906821884997956\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.0064086497738026086\n",
      "Training Loss: 0.006522564082406462\n",
      "Training Loss: 0.006628943476825953\n",
      "Training Loss: 0.004376602928750799\n",
      "Training Loss: 0.001331559935133555\n",
      "Training Loss: 0.0013326419999066274\n",
      "Training Loss: 0.0012914853289112216\n",
      "Validation Loss: 0.0020829977997193247\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006401149668963626\n",
      "Training Loss: 0.006511730236234144\n",
      "Training Loss: 0.006619189034681767\n",
      "Training Loss: 0.004368092717559193\n",
      "Training Loss: 0.0013260833148524397\n",
      "Training Loss: 0.0013267803820781409\n",
      "Training Loss: 0.0012847738005802966\n",
      "Validation Loss: 0.0020750632289620917\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006393433668417856\n",
      "Training Loss: 0.006500560231506825\n",
      "Training Loss: 0.0066090727865230296\n",
      "Training Loss: 0.004359343958058161\n",
      "Training Loss: 0.001320432295833598\n",
      "Training Loss: 0.0013207283765950706\n",
      "Training Loss: 0.0012778444559808121\n",
      "Validation Loss: 0.002066842663910219\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006385490959510207\n",
      "Training Loss: 0.006489033127436415\n",
      "Training Loss: 0.00659857363672927\n",
      "Training Loss: 0.004350340674791369\n",
      "Training Loss: 0.0013145868423453068\n",
      "Training Loss: 0.001314463227681699\n",
      "Training Loss: 0.0012706685441662556\n",
      "Validation Loss: 0.0020583100536378618\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.006377313833218068\n",
      "Training Loss: 0.006477132727159187\n",
      "Training Loss: 0.006587674248148687\n",
      "Training Loss: 0.004341065224580234\n",
      "Training Loss: 0.0013085170244448818\n",
      "Training Loss: 0.001307957100943895\n",
      "Training Loss: 0.0012632090989063726\n",
      "Validation Loss: 0.002049415462730149\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006368888575816527\n",
      "Training Loss: 0.006464832942001521\n",
      "Training Loss: 0.006576345055364073\n",
      "Training Loss: 0.0043314933309011395\n",
      "Training Loss: 0.001302188019908499\n",
      "Training Loss: 0.0013011728088895324\n",
      "Training Loss: 0.0012554171693773242\n",
      "Validation Loss: 0.0020401051998520377\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.00636019878089428\n",
      "Training Loss: 0.0064521017274819315\n",
      "Training Loss: 0.006564556263037957\n",
      "Training Loss: 0.004321592044652789\n",
      "Training Loss: 0.0012955477481591516\n",
      "Training Loss: 0.0012940595297550317\n",
      "Training Loss: 0.0012472242742660455\n",
      "Validation Loss: 0.0020303036709105045\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006351231316803024\n",
      "Training Loss: 0.006438904417445883\n",
      "Training Loss: 0.006552269281819463\n",
      "Training Loss: 0.004311321223794949\n",
      "Training Loss: 0.001288533228507731\n",
      "Training Loss: 0.001286549350625137\n",
      "Training Loss: 0.001238543614090304\n",
      "Validation Loss: 0.0020199099517787533\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006341958810808137\n",
      "Training Loss: 0.006425188530702144\n",
      "Training Loss: 0.006539430756820366\n",
      "Training Loss: 0.004300619974601432\n",
      "Training Loss: 0.0012810540414648131\n",
      "Training Loss: 0.0012785491962131346\n",
      "Training Loss: 0.001229251649856451\n",
      "Validation Loss: 0.002008785996666747\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006332351764431223\n",
      "Training Loss: 0.006410887852543965\n",
      "Training Loss: 0.006525972377276048\n",
      "Training Loss: 0.004289404057490173\n",
      "Training Loss: 0.0012729836151993369\n",
      "Training Loss: 0.001269923039872083\n",
      "Training Loss: 0.0012191684154095129\n",
      "Validation Loss: 0.001996737817044663\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006322361136553809\n",
      "Training Loss: 0.006395906513789669\n",
      "Training Loss: 0.0065117962087970225\n",
      "Training Loss: 0.004277548386598937\n",
      "Training Loss: 0.0012641382368747144\n",
      "Training Loss: 0.001260476632596692\n",
      "Training Loss: 0.0012080377287929877\n",
      "Validation Loss: 0.001983489771896785\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.00631191968685016\n",
      "Training Loss: 0.006380111190956086\n",
      "Training Loss: 0.006496759590227157\n",
      "Training Loss: 0.004264867419769871\n",
      "Training Loss: 0.0012542479480907786\n",
      "Training Loss: 0.0012499152502277865\n",
      "Training Loss: 0.0011954697875626152\n",
      "Validation Loss: 0.001968623287371911\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006300924067618325\n",
      "Training Loss: 0.006363302891841159\n",
      "Training Loss: 0.006480652017053216\n",
      "Training Loss: 0.004251075047577615\n",
      "Training Loss: 0.001242901922087185\n",
      "Training Loss: 0.0012377865453163394\n",
      "Training Loss: 0.0011808680846297648\n",
      "Validation Loss: 0.0019515134708206815\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006289206611691043\n",
      "Training Loss: 0.00634517632657662\n",
      "Training Loss: 0.006463137044338509\n",
      "Training Loss: 0.004235714143724181\n",
      "Training Loss: 0.0012294548815407324\n",
      "Training Loss: 0.0012233666746760718\n",
      "Training Loss: 0.0011632816245401045\n",
      "Validation Loss: 0.0019311794569373151\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006276498108636588\n",
      "Training Loss: 0.0063252639246638865\n",
      "Training Loss: 0.00644369037472643\n",
      "Training Loss: 0.004218036909151124\n",
      "Training Loss: 0.0012128519416728524\n",
      "Training Loss: 0.0012054594571236522\n",
      "Training Loss: 0.0011411437183414818\n",
      "Validation Loss: 0.001906042691182401\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006262349081225693\n",
      "Training Loss: 0.006302833943627775\n",
      "Training Loss: 0.00642146436846815\n",
      "Training Loss: 0.00419679542472295\n",
      "Training Loss: 0.0011913407027896027\n",
      "Training Loss: 0.0011820446786441608\n",
      "Training Loss: 0.0011118572472332744\n",
      "Validation Loss: 0.0018735795758389048\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006246011506300419\n",
      "Training Loss: 0.0062767575879115614\n",
      "Training Loss: 0.006395087665878236\n",
      "Training Loss: 0.004169927148905117\n",
      "Training Loss: 0.0011620297837362159\n",
      "Training Loss: 0.0011497354684979655\n",
      "Training Loss: 0.0010712820310436655\n",
      "Validation Loss: 0.001830055587717911\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.0062263283343054355\n",
      "Training Loss: 0.0062455041648354385\n",
      "Training Loss: 0.006362551136408001\n",
      "Training Loss: 0.004134324804108473\n",
      "Training Loss: 0.0011205996746139135\n",
      "Training Loss: 0.001103468588044052\n",
      "Training Loss: 0.0010140672499983339\n",
      "Validation Loss: 0.0017715744091693158\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006202019845368341\n",
      "Training Loss: 0.006207854548702017\n",
      "Training Loss: 0.006321768781635911\n",
      "Training Loss: 0.004086896168882958\n",
      "Training Loss: 0.0010630061464325991\n",
      "Training Loss: 0.0010388076746312435\n",
      "Training Loss: 0.0009384891024092212\n",
      "Validation Loss: 0.001700099742561945\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006173188696848229\n",
      "Training Loss: 0.006165430469554849\n",
      "Training Loss: 0.0062731013959273696\n",
      "Training Loss: 0.004029181022488047\n",
      "Training Loss: 0.0009925475959607867\n",
      "Training Loss: 0.0009608782761642942\n",
      "Training Loss: 0.0008573900585906813\n",
      "Validation Loss: 0.0016322841626144632\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006142435563961044\n",
      "Training Loss: 0.006124046992626972\n",
      "Training Loss: 0.006221564225852489\n",
      "Training Loss: 0.003971122694347287\n",
      "Training Loss: 0.0009251243638573215\n",
      "Training Loss: 0.0008889802795602009\n",
      "Training Loss: 0.0007931771371659125\n",
      "Validation Loss: 0.0015866870968777003\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006110447630053386\n",
      "Training Loss: 0.006086940637906082\n",
      "Training Loss: 0.006172233099350706\n",
      "Training Loss: 0.003921633512800326\n",
      "Training Loss: 0.0008741224519326352\n",
      "Training Loss: 0.000836673806552426\n",
      "Training Loss: 0.0007515529809461441\n",
      "Validation Loss: 0.0015615239149275192\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.006076155785704032\n",
      "Training Loss: 0.006052599507966079\n",
      "Training Loss: 0.006126884212717414\n",
      "Training Loss: 0.0038813485645368927\n",
      "Training Loss: 0.0008386196686478798\n",
      "Training Loss: 0.0008012849738588556\n",
      "Training Loss: 0.000724415919976309\n",
      "Validation Loss: 0.0015467818172854348\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.0060416166787035765\n",
      "Training Loss: 0.006020466443733313\n",
      "Training Loss: 0.006086002645315602\n",
      "Training Loss: 0.0038480432147480316\n",
      "Training Loss: 0.0008127444325509714\n",
      "Training Loss: 0.0007761340551951434\n",
      "Training Loss: 0.0007049070555513027\n",
      "Validation Loss: 0.001536548753205359\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006009136717766523\n",
      "Training Loss: 0.0059908146667294205\n",
      "Training Loss: 0.006049369194079191\n",
      "Training Loss: 0.003819855929032201\n",
      "Training Loss: 0.0007924134629865875\n",
      "Training Loss: 0.0007568978131166659\n",
      "Training Loss: 0.0006896022319415351\n",
      "Validation Loss: 0.0015283433256879247\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.005979479516390711\n",
      "Training Loss: 0.005963598897797056\n",
      "Training Loss: 0.0060163641767576334\n",
      "Training Loss: 0.003795494072546717\n",
      "Training Loss: 0.0007755004311184166\n",
      "Training Loss: 0.0007412972887686919\n",
      "Training Loss: 0.0006768674500199268\n",
      "Validation Loss: 0.0015211377999429635\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.005952539119170978\n",
      "Training Loss: 0.005938540626666509\n",
      "Training Loss: 0.005986331992316991\n",
      "Training Loss: 0.00377405860745057\n",
      "Training Loss: 0.0007608791357051814\n",
      "Training Loss: 0.0007280827972863335\n",
      "Training Loss: 0.0006658436514408094\n",
      "Validation Loss: 0.0015144777272881116\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.005927968899486586\n",
      "Training Loss: 0.005915319022024051\n",
      "Training Loss: 0.005958711071871221\n",
      "Training Loss: 0.003754911104842904\n",
      "Training Loss: 0.0007479065480583813\n",
      "Training Loss: 0.0007165227141376818\n",
      "Training Loss: 0.0006560320823336952\n",
      "Validation Loss: 0.00150814037948798\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.005905409714905545\n",
      "Training Loss: 0.005893660064903088\n",
      "Training Loss: 0.005933065355056897\n",
      "Training Loss: 0.003737589532174752\n",
      "Training Loss: 0.0007361739057523665\n",
      "Training Loss: 0.000706151053236681\n",
      "Training Loss: 0.0006471126485121203\n",
      "Validation Loss: 0.0015020152981317172\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.0058845488919178025\n",
      "Training Loss: 0.005873335244250484\n",
      "Training Loss: 0.005909055214724504\n",
      "Training Loss: 0.0037217496753874004\n",
      "Training Loss: 0.000725411156017799\n",
      "Training Loss: 0.0006966674745490309\n",
      "Training Loss: 0.0006388759183028014\n",
      "Validation Loss: 0.0014960325546719635\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.005865131649188698\n",
      "Training Loss: 0.005854165447526611\n",
      "Training Loss: 0.005886418768204749\n",
      "Training Loss: 0.0037071379632106984\n",
      "Training Loss: 0.0007154276606888743\n",
      "Training Loss: 0.0006878611514548539\n",
      "Training Loss: 0.0006311702133098151\n",
      "Validation Loss: 0.0014901560280940317\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.005846956692403182\n",
      "Training Loss: 0.005836013947846368\n",
      "Training Loss: 0.005864959996542894\n",
      "Training Loss: 0.0036935593753150897\n",
      "Training Loss: 0.0007060848195396829\n",
      "Training Loss: 0.000679590260333498\n",
      "Training Loss: 0.0006238895857677562\n",
      "Validation Loss: 0.0014843581416465392\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.0058298629539785905\n",
      "Training Loss: 0.005818772889324464\n",
      "Training Loss: 0.0058445254719117655\n",
      "Training Loss: 0.003680864523157652\n",
      "Training Loss: 0.0006972762601071736\n",
      "Training Loss: 0.000671748595268582\n",
      "Training Loss: 0.0006169596414110856\n",
      "Validation Loss: 0.001478623128901602\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.0058137192786671225\n",
      "Training Loss: 0.00580235576373525\n",
      "Training Loss: 0.005824998523457907\n",
      "Training Loss: 0.003668939540730207\n",
      "Training Loss: 0.0006889263758785092\n",
      "Training Loss: 0.0006642643478699028\n",
      "Training Loss: 0.0006103201261430513\n",
      "Validation Loss: 0.0014729417275924285\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.005798426421242766\n",
      "Training Loss: 0.005786696986760944\n",
      "Training Loss: 0.0058062885032268245\n",
      "Training Loss: 0.003657691945845727\n",
      "Training Loss: 0.0006809705558407587\n",
      "Training Loss: 0.0006570799859036924\n",
      "Training Loss: 0.000603929789722315\n",
      "Validation Loss: 0.001467311522537103\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.005783898553345352\n",
      "Training Loss: 0.005771741326898336\n",
      "Training Loss: 0.005788324415916577\n",
      "Training Loss: 0.0036470489570638166\n",
      "Training Loss: 0.000673364184985985\n",
      "Training Loss: 0.0006501544057391584\n",
      "Training Loss: 0.0005977557233563857\n",
      "Validation Loss: 0.0014617297828807025\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.005770072184968739\n",
      "Training Loss: 0.00575744675763417\n",
      "Training Loss: 0.005771050687762909\n",
      "Training Loss: 0.0036369502695742994\n",
      "Training Loss: 0.0006660705288231839\n",
      "Training Loss: 0.0006434619229185045\n",
      "Training Loss: 0.0005917761639284435\n",
      "Validation Loss: 0.0014562008550677498\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0057568844989873465\n",
      "Training Loss: 0.005743771596462466\n",
      "Training Loss: 0.005754420244484209\n",
      "Training Loss: 0.0036273473593610105\n",
      "Training Loss: 0.0006590630084974691\n",
      "Training Loss: 0.0006369785374045023\n",
      "Training Loss: 0.0005859722623426933\n",
      "Validation Loss: 0.0014507254486539915\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.005744289988069795\n",
      "Training Loss: 0.005730683137080632\n",
      "Training Loss: 0.005738397227833048\n",
      "Training Loss: 0.0036181993904028788\n",
      "Training Loss: 0.0006523191697488073\n",
      "Training Loss: 0.0006306889002371463\n",
      "Training Loss: 0.0005803342099534348\n",
      "Validation Loss: 0.001445306783264297\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.0057322485197801145\n",
      "Training Loss: 0.005718151750625111\n",
      "Training Loss: 0.005722946700407192\n",
      "Training Loss: 0.003609469760704087\n",
      "Training Loss: 0.0006458173320424976\n",
      "Training Loss: 0.0006245755426061805\n",
      "Training Loss: 0.0005748484641662798\n",
      "Validation Loss: 0.0014399509948472516\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.005720720794051885\n",
      "Training Loss: 0.005706149628967978\n",
      "Training Loss: 0.005708040266763419\n",
      "Training Loss: 0.0036011263581167442\n",
      "Training Loss: 0.0006395417415478732\n",
      "Training Loss: 0.0006186313647049246\n",
      "Training Loss: 0.0005695071758964332\n",
      "Validation Loss: 0.0014346609199362935\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.005709672071970999\n",
      "Training Loss: 0.005694645696203224\n",
      "Training Loss: 0.005693652050103993\n",
      "Training Loss: 0.0035931422905559882\n",
      "Training Loss: 0.0006334828878607369\n",
      "Training Loss: 0.0006128501164494082\n",
      "Training Loss: 0.0005643065475305775\n",
      "Validation Loss: 0.0014294424051312514\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.005699070980772376\n",
      "Training Loss: 0.005683617780450731\n",
      "Training Loss: 0.005679758744663559\n",
      "Training Loss: 0.003585490288824076\n",
      "Training Loss: 0.0006276286330103175\n",
      "Training Loss: 0.0006072207422403153\n",
      "Training Loss: 0.0005592412419719039\n",
      "Validation Loss: 0.001424299352198459\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.005688889971934259\n",
      "Training Loss: 0.005673039158573374\n",
      "Training Loss: 0.005666338443988934\n",
      "Training Loss: 0.0035781466238404393\n",
      "Training Loss: 0.0006219705103285378\n",
      "Training Loss: 0.000601740401943971\n",
      "Training Loss: 0.0005543038836185588\n",
      "Validation Loss: 0.0014192347406834357\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.005679105321178213\n",
      "Training Loss: 0.005662886941572651\n",
      "Training Loss: 0.005653371238149702\n",
      "Training Loss: 0.003571092611164204\n",
      "Training Loss: 0.0006164965472999029\n",
      "Training Loss: 0.0005964027091977187\n",
      "Training Loss: 0.000549493755788717\n",
      "Validation Loss: 0.0014142514431451478\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.00566969015984796\n",
      "Training Loss: 0.005653137693298049\n",
      "Training Loss: 0.0056408363016089424\n",
      "Training Loss: 0.0035643056347180392\n",
      "Training Loss: 0.0006112044254405191\n",
      "Training Loss: 0.0005912054480359074\n",
      "Training Loss: 0.000544807244441472\n",
      "Validation Loss: 0.0014093503001610809\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.005660621629212983\n",
      "Training Loss: 0.005643768548616208\n",
      "Training Loss: 0.005628717575455085\n",
      "Training Loss: 0.003557769284961978\n",
      "Training Loss: 0.00060608098981902\n",
      "Training Loss: 0.0005861410700526903\n",
      "Training Loss: 0.0005402393195254263\n",
      "Validation Loss: 0.0014045357951926666\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.005651876907795668\n",
      "Training Loss: 0.005634755090577528\n",
      "Training Loss: 0.00561699377023615\n",
      "Training Loss: 0.003551464709089487\n",
      "Training Loss: 0.0006011256868077907\n",
      "Training Loss: 0.0005812090542167425\n",
      "Training Loss: 0.0005357869194995146\n",
      "Validation Loss: 0.0013998061572663724\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.005643439724808559\n",
      "Training Loss: 0.005626079645007848\n",
      "Training Loss: 0.005605650178040378\n",
      "Training Loss: 0.0035453776198846754\n",
      "Training Loss: 0.0005963281572621781\n",
      "Training Loss: 0.0005764035814354429\n",
      "Training Loss: 0.0005314477615320357\n",
      "Validation Loss: 0.0013951611009030516\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.005635287558543495\n",
      "Training Loss: 0.005617718678549864\n",
      "Training Loss: 0.005594669384881854\n",
      "Training Loss: 0.0035394917650410206\n",
      "Training Loss: 0.0005916828075714875\n",
      "Training Loss: 0.0005717209862268647\n",
      "Training Loss: 0.000527216385853535\n",
      "Validation Loss: 0.0013906031163332614\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.005627406013081782\n",
      "Training Loss: 0.0056096571998205036\n",
      "Training Loss: 0.005584038743982092\n",
      "Training Loss: 0.0035337932839320275\n",
      "Training Loss: 0.0005871809637028492\n",
      "Training Loss: 0.0005671568819889217\n",
      "Training Loss: 0.0005230902244875324\n",
      "Validation Loss: 0.0013861308314803507\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.0056197763048112396\n",
      "Training Loss: 0.0056018727115588265\n",
      "Training Loss: 0.005573740020045079\n",
      "Training Loss: 0.003528269262460526\n",
      "Training Loss: 0.0005828229068720247\n",
      "Training Loss: 0.0005627082630599034\n",
      "Training Loss: 0.0005190665907866787\n",
      "Validation Loss: 0.0013817426826947016\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.005612383838160895\n",
      "Training Loss: 0.005594347976730205\n",
      "Training Loss: 0.0055637610814301295\n",
      "Training Loss: 0.0035229071629146345\n",
      "Training Loss: 0.0005785957974148914\n",
      "Training Loss: 0.0005583721905713901\n",
      "Training Loss: 0.0005151404316711705\n",
      "Validation Loss: 0.0013774367144605152\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.005605213857488706\n",
      "Training Loss: 0.005587066810112447\n",
      "Training Loss: 0.0055540871125413106\n",
      "Training Loss: 0.0035176963925914605\n",
      "Training Loss: 0.0005744965018675429\n",
      "Training Loss: 0.0005541418053326197\n",
      "Training Loss: 0.0005113074908149428\n",
      "Validation Loss: 0.0013732107068204465\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.005598255349323154\n",
      "Training Loss: 0.005580015456071123\n",
      "Training Loss: 0.005544706200598739\n",
      "Training Loss: 0.0035126257133379114\n",
      "Training Loss: 0.0005705184141697828\n",
      "Training Loss: 0.000550013849788229\n",
      "Training Loss: 0.0005075646447585313\n",
      "Validation Loss: 0.0013690645987872552\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.005591492016101256\n",
      "Training Loss: 0.005573174288729206\n",
      "Training Loss: 0.005535603746538982\n",
      "Training Loss: 0.0035076858811225976\n",
      "Training Loss: 0.0005666555159405107\n",
      "Training Loss: 0.0005459865163356881\n",
      "Training Loss: 0.0005039084859890863\n",
      "Validation Loss: 0.00136499316257177\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005584913473576307\n",
      "Training Loss: 0.005566530743381009\n",
      "Training Loss: 0.005526767213013955\n",
      "Training Loss: 0.0035028656506619882\n",
      "Training Loss: 0.0005629063414380653\n",
      "Training Loss: 0.0005420579481506138\n",
      "Training Loss: 0.0005003355532971909\n",
      "Validation Loss: 0.0013609962114244124\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.00557850795565173\n",
      "Training Loss: 0.005560070403735153\n",
      "Training Loss: 0.005518189126742072\n",
      "Training Loss: 0.003498158940492431\n",
      "Training Loss: 0.0005592603050172329\n",
      "Training Loss: 0.0005382225781431771\n",
      "Training Loss: 0.0004968417298005079\n",
      "Validation Loss: 0.00135707166880868\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.005572264675283805\n",
      "Training Loss: 0.005553781609050929\n",
      "Training Loss: 0.005509854274569079\n",
      "Training Loss: 0.003493556143512251\n",
      "Training Loss: 0.0005557160560420016\n",
      "Training Loss: 0.0005344755114856525\n",
      "Training Loss: 0.0004934237732595648\n",
      "Validation Loss: 0.0013532147449916508\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.005566176786087454\n",
      "Training Loss: 0.005547652719542384\n",
      "Training Loss: 0.005501753364806064\n",
      "Training Loss: 0.0034890502696362092\n",
      "Training Loss: 0.0005522655987442704\n",
      "Training Loss: 0.0005308143372894846\n",
      "Training Loss: 0.0004900781435571844\n",
      "Validation Loss: 0.0013494248732934054\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005560229255352169\n",
      "Training Loss: 0.005541668265941553\n",
      "Training Loss: 0.005493875194224529\n",
      "Training Loss: 0.00348463442423963\n",
      "Training Loss: 0.0005489088495960459\n",
      "Training Loss: 0.00052723854805663\n",
      "Training Loss: 0.00048680120471544797\n",
      "Validation Loss: 0.0013456988276851413\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005554421105189249\n",
      "Training Loss: 0.005535822989768349\n",
      "Training Loss: 0.005486211430979893\n",
      "Training Loss: 0.0034803014996577985\n",
      "Training Loss: 0.0005456348576262826\n",
      "Training Loss: 0.0005237407970707864\n",
      "Training Loss: 0.00048358961434132654\n",
      "Validation Loss: 0.0013420356282561473\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.0055487387598259375\n",
      "Training Loss: 0.005530103312339634\n",
      "Training Loss: 0.005478749161120504\n",
      "Training Loss: 0.0034760460798133864\n",
      "Training Loss: 0.0005424423899967224\n",
      "Training Loss: 0.0005203202835764387\n",
      "Training Loss: 0.0004804398913984187\n",
      "Validation Loss: 0.0013384315144263682\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.005543176781502552\n",
      "Training Loss: 0.0055245014134561645\n",
      "Training Loss: 0.005471481849672273\n",
      "Training Loss: 0.0034718619257910178\n",
      "Training Loss: 0.000539326288853772\n",
      "Training Loss: 0.0005169721850688802\n",
      "Training Loss: 0.00047734997377119726\n",
      "Validation Loss: 0.0013348847338618024\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005537728437921032\n",
      "Training Loss: 0.00551900883205235\n",
      "Training Loss: 0.005464400956989266\n",
      "Training Loss: 0.003467745672096498\n",
      "Training Loss: 0.0005362834982952336\n",
      "Training Loss: 0.0005136964776465903\n",
      "Training Loss: 0.0004743167680499028\n",
      "Validation Loss: 0.0013313943221846818\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.005532383938552812\n",
      "Training Loss: 0.005513614704250358\n",
      "Training Loss: 0.005457496189046651\n",
      "Training Loss: 0.0034636917478201214\n",
      "Training Loss: 0.0005333119256101782\n",
      "Training Loss: 0.0005104887776178657\n",
      "Training Loss: 0.0004713389032622217\n",
      "Validation Loss: 0.0013279575928225975\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.00552713937824592\n",
      "Training Loss: 0.005508310906589031\n",
      "Training Loss: 0.005450756696518511\n",
      "Training Loss: 0.0034596958961628844\n",
      "Training Loss: 0.0005304063295625383\n",
      "Training Loss: 0.0005073460005223751\n",
      "Training Loss: 0.00046841228431730996\n",
      "Validation Loss: 0.0013245731646788902\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.005521990058477968\n",
      "Training Loss: 0.005503095126478001\n",
      "Training Loss: 0.005444177924655378\n",
      "Training Loss: 0.003455753663147334\n",
      "Training Loss: 0.000527563545256271\n",
      "Training Loss: 0.0005042685152875493\n",
      "Training Loss: 0.0004655348874439369\n",
      "Validation Loss: 0.0013212402020539645\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005516927654971368\n",
      "Training Loss: 0.005497956875478849\n",
      "Training Loss: 0.005437753027072176\n",
      "Training Loss: 0.0034518623645271875\n",
      "Training Loss: 0.0005247791202418739\n",
      "Training Loss: 0.0005012509737571236\n",
      "Training Loss: 0.0004627042881475063\n",
      "Validation Loss: 0.0013179552576699546\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.0055119521915912625\n",
      "Training Loss: 0.005492893575574271\n",
      "Training Loss: 0.005431472942582331\n",
      "Training Loss: 0.0034480195185460616\n",
      "Training Loss: 0.0005220535766420653\n",
      "Training Loss: 0.0004982923225543345\n",
      "Training Loss: 0.0004599198844152852\n",
      "Validation Loss: 0.0013147210578854048\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.005507054673507809\n",
      "Training Loss: 0.005487897660932503\n",
      "Training Loss: 0.005425329670542851\n",
      "Training Loss: 0.0034442215948365627\n",
      "Training Loss: 0.0005193825547030428\n",
      "Training Loss: 0.000495391212607501\n",
      "Training Loss: 0.00045718028966803105\n",
      "Validation Loss: 0.0013115352228081573\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005502230348647572\n",
      "Training Loss: 0.005482964377733879\n",
      "Training Loss: 0.005419316132902168\n",
      "Training Loss: 0.00344046695594443\n",
      "Training Loss: 0.0005167660438019084\n",
      "Training Loss: 0.0004925477437427617\n",
      "Training Loss: 0.0004544850053207483\n",
      "Validation Loss: 0.0013083971859490472\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005497475743177347\n",
      "Training Loss: 0.005478088390664198\n",
      "Training Loss: 0.0054134283383609726\n",
      "Training Loss: 0.003436752556081046\n",
      "Training Loss: 0.0005142027069086907\n",
      "Training Loss: 0.0004897597539820709\n",
      "Training Loss: 0.00045183245922089557\n",
      "Validation Loss: 0.0013053055581724365\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005492787091061473\n",
      "Training Loss: 0.00547326723462902\n",
      "Training Loss: 0.0054076567234005776\n",
      "Training Loss: 0.0034330779038282346\n",
      "Training Loss: 0.0005116851289494661\n",
      "Training Loss: 0.00048702133135520854\n",
      "Training Loss: 0.0004492190933888196\n",
      "Validation Loss: 0.0013022628436885314\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.0054881652997573836\n",
      "Training Loss: 0.005468500035931356\n",
      "Training Loss: 0.005401999520254322\n",
      "Training Loss: 0.003429440943946247\n",
      "Training Loss: 0.0005092155122110853\n",
      "Training Loss: 0.00048433655178087065\n",
      "Training Loss: 0.00044664759781881\n",
      "Validation Loss: 0.0012992662062364009\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005483602116582915\n",
      "Training Loss: 0.005463778453413397\n",
      "Training Loss: 0.005396446054801345\n",
      "Training Loss: 0.0034258402301202296\n",
      "Training Loss: 0.0005067952678655274\n",
      "Training Loss: 0.00048170267040404725\n",
      "Training Loss: 0.00044411472783394856\n",
      "Validation Loss: 0.0012963177596043607\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005479097771458327\n",
      "Training Loss: 0.005459106787457131\n",
      "Training Loss: 0.005390994734480046\n",
      "Training Loss: 0.003422275928533054\n",
      "Training Loss: 0.000504420801953529\n",
      "Training Loss: 0.00047912006761180237\n",
      "Training Loss: 0.0004416232907169615\n",
      "Validation Loss: 0.0012934191404741122\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005474644993664696\n",
      "Training Loss: 0.00545447567012161\n",
      "Training Loss: 0.005385638498701155\n",
      "Training Loss: 0.0034187467530864524\n",
      "Training Loss: 0.0005020931470789947\n",
      "Training Loss: 0.0004765890208000201\n",
      "Training Loss: 0.00043917284045164707\n",
      "Validation Loss: 0.0012905666891207276\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005470244075986556\n",
      "Training Loss: 0.005449884841218591\n",
      "Training Loss: 0.005380372352665291\n",
      "Training Loss: 0.0034152524198725587\n",
      "Training Loss: 0.0004998093799804338\n",
      "Training Loss: 0.0004741046331400867\n",
      "Training Loss: 0.00043675978802639294\n",
      "Validation Loss: 0.001287765700288539\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.005465894626686349\n",
      "Training Loss: 0.005445337675628253\n",
      "Training Loss: 0.005375192703213543\n",
      "Training Loss: 0.003411792430342757\n",
      "Training Loss: 0.0004975691981235287\n",
      "Training Loss: 0.00047167010638077045\n",
      "Training Loss: 0.00043438644879643105\n",
      "Validation Loss: 0.0012850147477535998\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005461594761582092\n",
      "Training Loss: 0.00544083074608352\n",
      "Training Loss: 0.0053700964036397635\n",
      "Training Loss: 0.003408366989024216\n",
      "Training Loss: 0.000495372823352227\n",
      "Training Loss: 0.0004692840286224964\n",
      "Training Loss: 0.0004320543903304497\n",
      "Validation Loss: 0.0012823149077036183\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005457338680280373\n",
      "Training Loss: 0.0054363609908614306\n",
      "Training Loss: 0.005365076105808839\n",
      "Training Loss: 0.0034049755058367737\n",
      "Training Loss: 0.0004932189064857085\n",
      "Training Loss: 0.0004669459257638664\n",
      "Training Loss: 0.0004297620866054785\n",
      "Validation Loss: 0.0012796673771017493\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005453128213412128\n",
      "Training Loss: 0.005431929773185402\n",
      "Training Loss: 0.005360129692126066\n",
      "Training Loss: 0.0034016177572630114\n",
      "Training Loss: 0.0004911085276398808\n",
      "Training Loss: 0.0004646556240913924\n",
      "Training Loss: 0.0004275090228475165\n",
      "Validation Loss: 0.0012770718518293146\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005448959232890047\n",
      "Training Loss: 0.005427534568007104\n",
      "Training Loss: 0.005355252186418511\n",
      "Training Loss: 0.0033982962438312823\n",
      "Training Loss: 0.0004890429935767315\n",
      "Training Loss: 0.00046241481053584723\n",
      "Training Loss: 0.0004252981661556987\n",
      "Validation Loss: 0.0012745300245683637\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.0054448299377691\n",
      "Training Loss: 0.005423175457399339\n",
      "Training Loss: 0.005350441911141388\n",
      "Training Loss: 0.003395008554798551\n",
      "Training Loss: 0.0004870203998143552\n",
      "Training Loss: 0.00046022298232855974\n",
      "Training Loss: 0.0004231293927296065\n",
      "Validation Loss: 0.0012720421964672398\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.0054407394852023575\n",
      "Training Loss: 0.005418851198628545\n",
      "Training Loss: 0.005345692107803188\n",
      "Training Loss: 0.0033917584677692503\n",
      "Training Loss: 0.0004850409108621534\n",
      "Training Loss: 0.00045807864134985723\n",
      "Training Loss: 0.00042100306254724276\n",
      "Validation Loss: 0.0012696085165331947\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005436685455497354\n",
      "Training Loss: 0.005414562215446494\n",
      "Training Loss: 0.005341000680346042\n",
      "Training Loss: 0.003388541405147407\n",
      "Training Loss: 0.0004831042301520938\n",
      "Training Loss: 0.0004559833119856194\n",
      "Training Loss: 0.0004189199086613371\n",
      "Validation Loss: 0.0012672299484899772\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005432665457483381\n",
      "Training Loss: 0.005410306403064169\n",
      "Training Loss: 0.005336364065296948\n",
      "Training Loss: 0.0033853611716767773\n",
      "Training Loss: 0.0004812124381714966\n",
      "Training Loss: 0.00045393570508167613\n",
      "Training Loss: 0.00041687888886372094\n",
      "Validation Loss: 0.0012649081002152176\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005428682046476752\n",
      "Training Loss: 0.005406087780138478\n",
      "Training Loss: 0.0053317821171367545\n",
      "Training Loss: 0.0033822159281407947\n",
      "Training Loss: 0.0004793607796455035\n",
      "Training Loss: 0.0004519372158392798\n",
      "Training Loss: 0.00041488197981379924\n",
      "Validation Loss: 0.0012626427701894153\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005424728950019926\n",
      "Training Loss: 0.005401898915297351\n",
      "Training Loss: 0.005327246555825695\n",
      "Training Loss: 0.0033791093873151113\n",
      "Training Loss: 0.0004775584113667719\n",
      "Training Loss: 0.0004499895362459938\n",
      "Training Loss: 0.0004129314312740462\n",
      "Validation Loss: 0.001260430284789746\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005420805280446075\n",
      "Training Loss: 0.005397744452347979\n",
      "Training Loss: 0.00532275801175274\n",
      "Training Loss: 0.003376037528796587\n",
      "Training Loss: 0.0004757968074409291\n",
      "Training Loss: 0.0004480896474342444\n",
      "Training Loss: 0.0004110246685013408\n",
      "Validation Loss: 0.0012582751846225227\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005416910581407137\n",
      "Training Loss: 0.005393621277762577\n",
      "Training Loss: 0.005318311969167553\n",
      "Training Loss: 0.00337300288498227\n",
      "Training Loss: 0.00047408053585968447\n",
      "Training Loss: 0.0004462390988555853\n",
      "Training Loss: 0.00040916397323599086\n",
      "Validation Loss: 0.0012561758754082541\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.005413043476874008\n",
      "Training Loss: 0.005389530225656926\n",
      "Training Loss: 0.005313908166717738\n",
      "Training Loss: 0.0033700051430787426\n",
      "Training Loss: 0.00047240645209967625\n",
      "Training Loss: 0.00044443610226153397\n",
      "Training Loss: 0.00040734823225648144\n",
      "Validation Loss: 0.001254132726013465\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005409201059374027\n",
      "Training Loss: 0.005385469054454006\n",
      "Training Loss: 0.0053095415187999605\n",
      "Training Loss: 0.003367044601982343\n",
      "Training Loss: 0.0004707776443683542\n",
      "Training Loss: 0.0004426833394973073\n",
      "Training Loss: 0.0004055790545316995\n",
      "Validation Loss: 0.0012521443939285985\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005405383666511625\n",
      "Training Loss: 0.005381438213516958\n",
      "Training Loss: 0.005305211183149368\n",
      "Training Loss: 0.003364120968253701\n",
      "Training Loss: 0.00046919289394281805\n",
      "Training Loss: 0.0004409785195457516\n",
      "Training Loss: 0.00040385643569607055\n",
      "Validation Loss: 0.0012502090825643895\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005401587872765959\n",
      "Training Loss: 0.005377436581766233\n",
      "Training Loss: 0.005300913631799631\n",
      "Training Loss: 0.0033612347115558805\n",
      "Training Loss: 0.00046765116167080123\n",
      "Training Loss: 0.0004393223404258606\n",
      "Training Loss: 0.0004021794448271976\n",
      "Validation Loss: 0.001248328209697044\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.005397812582086772\n",
      "Training Loss: 0.00537346288736444\n",
      "Training Loss: 0.00529664738336578\n",
      "Training Loss: 0.0033583843275846446\n",
      "Training Loss: 0.0004661532837053528\n",
      "Training Loss: 0.00043771490156359506\n",
      "Training Loss: 0.00040054938461253186\n",
      "Validation Loss: 0.0012465007438301283\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005394057070370764\n",
      "Training Loss: 0.005369516475475393\n",
      "Training Loss: 0.005292411107802764\n",
      "Training Loss: 0.0033555700506985886\n",
      "Training Loss: 0.0004646990420587827\n",
      "Training Loss: 0.0004361548182714614\n",
      "Training Loss: 0.0003989650806397549\n",
      "Validation Loss: 0.0012447239376492502\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005390321966260671\n",
      "Training Loss: 0.005365598925272934\n",
      "Training Loss: 0.005288203178206458\n",
      "Training Loss: 0.003352792492732988\n",
      "Training Loss: 0.0004632849934569094\n",
      "Training Loss: 0.0004346400749636814\n",
      "Training Loss: 0.0003974260673567187\n",
      "Validation Loss: 0.0012429979434317546\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005386601723730564\n",
      "Training Loss: 0.005361706167459488\n",
      "Training Loss: 0.005284019796526991\n",
      "Training Loss: 0.0033500501621165314\n",
      "Training Loss: 0.0004619161572190933\n",
      "Training Loss: 0.00043317422077961967\n",
      "Training Loss: 0.00039593400244484654\n",
      "Validation Loss: 0.0012413238436602796\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.0053828972956398505\n",
      "Training Loss: 0.005357837050687522\n",
      "Training Loss: 0.005279860843438655\n",
      "Training Loss: 0.003347343708810513\n",
      "Training Loss: 0.00046058825522777625\n",
      "Training Loss: 0.0004317541072668973\n",
      "Training Loss: 0.0003944877134199487\n",
      "Validation Loss: 0.0012396972874211746\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005379206856014207\n",
      "Training Loss: 0.005353991703595966\n",
      "Training Loss: 0.005275723342783749\n",
      "Training Loss: 0.0033446729138086086\n",
      "Training Loss: 0.0004593013945122948\n",
      "Training Loss: 0.0004303784263902344\n",
      "Training Loss: 0.00039308518596953946\n",
      "Validation Loss: 0.0012381186706954892\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005375530574237928\n",
      "Training Loss: 0.005350170728052035\n",
      "Training Loss: 0.005271607743925415\n",
      "Training Loss: 0.0033420360682794126\n",
      "Training Loss: 0.00045805596382706424\n",
      "Training Loss: 0.00042904816211375874\n",
      "Training Loss: 0.00039172832843178187\n",
      "Validation Loss: 0.0012365844877251888\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005371864183107391\n",
      "Training Loss: 0.005346369472681545\n",
      "Training Loss: 0.005267510887933895\n",
      "Training Loss: 0.0033394326125562655\n",
      "Training Loss: 0.00045685092285566495\n",
      "Training Loss: 0.0004277632629600703\n",
      "Training Loss: 0.00039041584946971855\n",
      "Validation Loss: 0.0012350965251802547\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005368211229797453\n",
      "Training Loss: 0.00534259072621353\n",
      "Training Loss: 0.00526343270088546\n",
      "Training Loss: 0.003336862035212107\n",
      "Training Loss: 0.00045568413646833504\n",
      "Training Loss: 0.000426519266620744\n",
      "Training Loss: 0.00038914527973247457\n",
      "Validation Loss: 0.0012336499449031337\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005364567712531425\n",
      "Training Loss: 0.005338832541019656\n",
      "Training Loss: 0.005259372253785841\n",
      "Training Loss: 0.003334324275783729\n",
      "Training Loss: 0.00045455617422703656\n",
      "Training Loss: 0.00042531858565780567\n",
      "Training Loss: 0.0003879174831672572\n",
      "Validation Loss: 0.0012322472176400767\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005360932895564474\n",
      "Training Loss: 0.005335092420573347\n",
      "Training Loss: 0.005255328745697625\n",
      "Training Loss: 0.0033318188887642465\n",
      "Training Loss: 0.00045346812934440097\n",
      "Training Loss: 0.0004241600749446661\n",
      "Training Loss: 0.0003867330485445564\n",
      "Validation Loss: 0.0012308836224936262\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005357305238139816\n",
      "Training Loss: 0.00533136899699457\n",
      "Training Loss: 0.005251297308132052\n",
      "Training Loss: 0.003329344367593876\n",
      "Training Loss: 0.0004524178261635825\n",
      "Training Loss: 0.0004230426059439196\n",
      "Training Loss: 0.0003855900433700299\n",
      "Validation Loss: 0.001229558876736965\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.00535368513839785\n",
      "Training Loss: 0.00532766361371614\n",
      "Training Loss: 0.005247279689647257\n",
      "Training Loss: 0.0033269001315784407\n",
      "Training Loss: 0.0004514017156907357\n",
      "Training Loss: 0.0004219638211361598\n",
      "Training Loss: 0.00038448688537755514\n",
      "Validation Loss: 0.001228271748825702\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.0053500721795717255\n",
      "Training Loss: 0.00532397425500676\n",
      "Training Loss: 0.005243275560205802\n",
      "Training Loss: 0.0033244851713243404\n",
      "Training Loss: 0.00045042225348879584\n",
      "Training Loss: 0.0004209238541443483\n",
      "Training Loss: 0.0003834234423266025\n",
      "Validation Loss: 0.0012270219623824504\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005346464379108511\n",
      "Training Loss: 0.005320300807361491\n",
      "Training Loss: 0.0052392844733549285\n",
      "Training Loss: 0.003322100359146134\n",
      "Training Loss: 0.00044947706221137195\n",
      "Training Loss: 0.00041992171230958774\n",
      "Training Loss: 0.00038239873108977915\n",
      "Validation Loss: 0.0012258065060327835\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.00534286224457901\n",
      "Training Loss: 0.00531664052454289\n",
      "Training Loss: 0.005235303295776248\n",
      "Training Loss: 0.0033197432047018085\n",
      "Training Loss: 0.0004485668488632655\n",
      "Training Loss: 0.00041895628011843654\n",
      "Training Loss: 0.00038141215864015974\n",
      "Validation Loss: 0.0012246215753501442\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005339263561181724\n",
      "Training Loss: 0.005312994250562042\n",
      "Training Loss: 0.005231332749244757\n",
      "Training Loss: 0.003317412129617878\n",
      "Training Loss: 0.00044768992520403115\n",
      "Training Loss: 0.0004180274472673773\n",
      "Training Loss: 0.000380462482717121\n",
      "Validation Loss: 0.0012234712266127757\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.005335669995984063\n",
      "Training Loss: 0.005309361246763729\n",
      "Training Loss: 0.005227372691733762\n",
      "Training Loss: 0.003315108875031001\n",
      "Training Loss: 0.00044684445259917993\n",
      "Training Loss: 0.00041713374095706966\n",
      "Training Loss: 0.0003795499711850425\n",
      "Validation Loss: 0.0012223515151343692\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.00533207708329428\n",
      "Training Loss: 0.0053057378350058575\n",
      "Training Loss: 0.00522342266922351\n",
      "Training Loss: 0.003312831371149514\n",
      "Training Loss: 0.00044603140035178514\n",
      "Training Loss: 0.00041627367980254346\n",
      "Training Loss: 0.0003786716798640555\n",
      "Validation Loss: 0.0012212599814777464\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005328488065861166\n",
      "Training Loss: 0.0053021274320781235\n",
      "Training Loss: 0.005219480486121028\n",
      "Training Loss: 0.003310577591400943\n",
      "Training Loss: 0.0004452486517402576\n",
      "Training Loss: 0.00041544592946593186\n",
      "Training Loss: 0.0003778278549725655\n",
      "Validation Loss: 0.0012201970767795303\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.005324900926789269\n",
      "Training Loss: 0.005298527803388424\n",
      "Training Loss: 0.0052155473141465335\n",
      "Training Loss: 0.0033083500124485\n",
      "Training Loss: 0.00044449475171859376\n",
      "Training Loss: 0.0004146505797325517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [57:58<24:53, 497.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.00037701704051869457\n",
      "Validation Loss: 0.0012191599929560941\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.4521579983830452\n",
      "Training Loss: 0.3559112686663866\n",
      "Training Loss: 0.27922606855630877\n",
      "Training Loss: 0.20584603201597929\n",
      "Training Loss: 0.14927430448122322\n",
      "Training Loss: 0.10806104439776391\n",
      "Training Loss: 0.08907722415402532\n",
      "Validation Loss: 0.07542031124401628\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.07230638660490513\n",
      "Training Loss: 0.05979133978486061\n",
      "Training Loss: 0.052811272833496334\n",
      "Training Loss: 0.04650932309217751\n",
      "Training Loss: 0.0447032243385911\n",
      "Training Loss: 0.04077784406021237\n",
      "Training Loss: 0.04178249855991453\n",
      "Validation Loss: 0.04088651310750161\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.04298550233244896\n",
      "Training Loss: 0.04142665265128016\n",
      "Training Loss: 0.03978375047445297\n",
      "Training Loss: 0.03599160574376583\n",
      "Training Loss: 0.03380967338103801\n",
      "Training Loss: 0.029695637808181345\n",
      "Training Loss: 0.028810515780933202\n",
      "Validation Loss: 0.02763422377658694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.030202847737818957\n",
      "Training Loss: 0.02809381696395576\n",
      "Training Loss: 0.02570577083621174\n",
      "Training Loss: 0.02115556589968037\n",
      "Training Loss: 0.017519272700883447\n",
      "Training Loss: 0.014387044890318066\n",
      "Training Loss: 0.012946526089217513\n",
      "Validation Loss: 0.0128492110143747\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.01648988461587578\n",
      "Training Loss: 0.01565915994811803\n",
      "Training Loss: 0.014365509119816124\n",
      "Training Loss: 0.011038989465450868\n",
      "Training Loss: 0.007528371469816193\n",
      "Training Loss: 0.006752808770397678\n",
      "Training Loss: 0.006572915429715067\n",
      "Validation Loss: 0.008331176798706978\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.012532869202550501\n",
      "Training Loss: 0.01291769165894948\n",
      "Training Loss: 0.012350393815431744\n",
      "Training Loss: 0.0095010223868303\n",
      "Training Loss: 0.0060307501314673575\n",
      "Training Loss: 0.005845034372759983\n",
      "Training Loss: 0.005939973646309227\n",
      "Validation Loss: 0.008100139863147272\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.012212785421870648\n",
      "Training Loss: 0.012660408697556705\n",
      "Training Loss: 0.01210095071233809\n",
      "Training Loss: 0.009282917850650847\n",
      "Training Loss: 0.005811320098582655\n",
      "Training Loss: 0.005657251719385386\n",
      "Training Loss: 0.005771917375968769\n",
      "Validation Loss: 0.007936015398126472\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.012033858386566863\n",
      "Training Loss: 0.01249061913928017\n",
      "Training Loss: 0.01193824916612357\n",
      "Training Loss: 0.009150834524771199\n",
      "Training Loss: 0.0056943512277212\n",
      "Training Loss: 0.005540225760778412\n",
      "Training Loss: 0.005655649475520477\n",
      "Validation Loss: 0.007790463071400195\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.01189152306644246\n",
      "Training Loss: 0.012350001411978156\n",
      "Training Loss: 0.01180054348660633\n",
      "Training Loss: 0.009028986963676289\n",
      "Training Loss: 0.0055841634958051145\n",
      "Training Loss: 0.005425714313751087\n",
      "Training Loss: 0.005519955670461058\n",
      "Validation Loss: 0.007618213925259501\n",
      "Validation Accuracy: 0.05852059925093633\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.01171075360267423\n",
      "Training Loss: 0.012149258105782792\n",
      "Training Loss: 0.011584728497546166\n",
      "Training Loss: 0.008794388139504008\n",
      "Training Loss: 0.005360903078690171\n",
      "Training Loss: 0.005174866300076246\n",
      "Training Loss: 0.005147722323890776\n",
      "Validation Loss: 0.0071785768113807125\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.011148796963971109\n",
      "Training Loss: 0.011429640614660458\n",
      "Training Loss: 0.010782716718968004\n",
      "Training Loss: 0.00783305201708572\n",
      "Training Loss: 0.004472074696677737\n",
      "Training Loss: 0.004240679746144452\n",
      "Training Loss: 0.004023198888171464\n",
      "Validation Loss: 0.006007536765264717\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.00968570297700353\n",
      "Training Loss: 0.0099539167934563\n",
      "Training Loss: 0.009594520116224884\n",
      "Training Loss: 0.0068472005391959105\n",
      "Training Loss: 0.0036284034873824565\n",
      "Training Loss: 0.0035657931049354373\n",
      "Training Loss: 0.0034983119743992575\n",
      "Validation Loss: 0.005158606585078569\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.008962838768493384\n",
      "Training Loss: 0.009274449478834868\n",
      "Training Loss: 0.009032137935282663\n",
      "Training Loss: 0.006453048126131762\n",
      "Training Loss: 0.003235901212319732\n",
      "Training Loss: 0.0032210859717451967\n",
      "Training Loss: 0.00318810353113804\n",
      "Validation Loss: 0.004645151735374125\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.008540663850726559\n",
      "Training Loss: 0.008848530981922523\n",
      "Training Loss: 0.008640655834460631\n",
      "Training Loss: 0.006162906797835603\n",
      "Training Loss: 0.0029698122589616105\n",
      "Training Loss: 0.002971245245717\n",
      "Training Loss: 0.0029531830313499086\n",
      "Validation Loss: 0.004309129897088217\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.008230090936413035\n",
      "Training Loss: 0.008522531515918672\n",
      "Training Loss: 0.008337461783085018\n",
      "Training Loss: 0.00592927656514803\n",
      "Training Loss: 0.0027725474850740285\n",
      "Training Loss: 0.0027786483279487583\n",
      "Training Loss: 0.002769310702569783\n",
      "Validation Loss: 0.004060467797129202\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.007982321220915765\n",
      "Training Loss: 0.008258026274852455\n",
      "Training Loss: 0.008094792750198395\n",
      "Training Loss: 0.005738832854258362\n",
      "Training Loss: 0.0026213046966586263\n",
      "Training Loss: 0.0026277360740641596\n",
      "Training Loss: 0.002623826881463174\n",
      "Validation Loss: 0.0038657581676948775\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.007779202851234004\n",
      "Training Loss: 0.008039962069597095\n",
      "Training Loss: 0.00789858158212155\n",
      "Training Loss: 0.005583751123049296\n",
      "Training Loss: 0.0025032412688597103\n",
      "Training Loss: 0.002508740142948227\n",
      "Training Loss: 0.002507827834924683\n",
      "Validation Loss: 0.0037096148275697\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.00761084946920164\n",
      "Training Loss: 0.00785883212229237\n",
      "Training Loss: 0.007738574623363093\n",
      "Training Loss: 0.0054570867179427295\n",
      "Training Loss: 0.0024095698416931556\n",
      "Training Loss: 0.0024141582562879193\n",
      "Training Loss: 0.0024144203099422156\n",
      "Validation Loss: 0.0035821204553340815\n",
      "Validation Accuracy: 0.10533707865168539\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.007469866063911468\n",
      "Training Loss: 0.0077068783680442725\n",
      "Training Loss: 0.007606273316778243\n",
      "Training Loss: 0.005352650608401745\n",
      "Training Loss: 0.0023341458810318726\n",
      "Training Loss: 0.002338196254131617\n",
      "Training Loss: 0.0023385011206846686\n",
      "Validation Loss: 0.003475986995336631\n",
      "Validation Accuracy: 0.11704119850187265\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.007350268864538521\n",
      "Training Loss: 0.007577640803065151\n",
      "Training Loss: 0.0074947849626187234\n",
      "Training Loss: 0.00526539817743469\n",
      "Training Loss: 0.0022727968796971256\n",
      "Training Loss: 0.0022765611250360964\n",
      "Training Loss: 0.0022764497046591716\n",
      "Validation Loss: 0.0033859181915905172\n",
      "Validation Accuracy: 0.11704119850187265\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.007247347146039829\n",
      "Training Loss: 0.007465986895840615\n",
      "Training Loss: 0.007398813184117899\n",
      "Training Loss: 0.0051914779779326636\n",
      "Training Loss: 0.0022227027683402413\n",
      "Training Loss: 0.0022261371328204404\n",
      "Training Loss: 0.002225649768806761\n",
      "Validation Loss: 0.0033082298494959144\n",
      "Validation Accuracy: 0.11704119850187265\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.007157562173670157\n",
      "Training Loss: 0.007368060451699421\n",
      "Training Loss: 0.007314524323446676\n",
      "Training Loss: 0.005128054660744965\n",
      "Training Loss: 0.002181846610474167\n",
      "Training Loss: 0.00218463786666689\n",
      "Training Loss: 0.0021840774624433835\n",
      "Validation Loss: 0.003240401829786473\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.007078373237745837\n",
      "Training Loss: 0.007281128612812608\n",
      "Training Loss: 0.007239306771662086\n",
      "Training Loss: 0.005073073384555755\n",
      "Training Loss: 0.002148651844909182\n",
      "Training Loss: 0.0021503349531121784\n",
      "Training Loss: 0.002150046403403394\n",
      "Validation Loss: 0.00318068287083321\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.007008031092118472\n",
      "Training Loss: 0.00720337038859725\n",
      "Training Loss: 0.007171513687353581\n",
      "Training Loss: 0.005025054982979782\n",
      "Training Loss: 0.0021217896409507376\n",
      "Training Loss: 0.00212186581302376\n",
      "Training Loss: 0.002122095192462439\n",
      "Validation Loss: 0.003127818851847697\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.0069453710736706855\n",
      "Training Loss: 0.007133637906517834\n",
      "Training Loss: 0.007110180858289823\n",
      "Training Loss: 0.004982930280966684\n",
      "Training Loss: 0.0021000806454685517\n",
      "Training Loss: 0.0020981187790312106\n",
      "Training Loss: 0.002098940880241571\n",
      "Validation Loss: 0.003080864150464284\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.006889618629356846\n",
      "Training Loss: 0.007071225952822715\n",
      "Training Loss: 0.007054776115110144\n",
      "Training Loss: 0.004945897635479923\n",
      "Training Loss: 0.0020824552231351844\n",
      "Training Loss: 0.002078159332522773\n",
      "Training Loss: 0.002079464609269053\n",
      "Validation Loss: 0.0030390482911690094\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.006840203104075044\n",
      "Training Loss: 0.007015646859072149\n",
      "Training Loss: 0.007004971868591383\n",
      "Training Loss: 0.004913308813120239\n",
      "Training Loss: 0.002067956084065372\n",
      "Training Loss: 0.002061185958882561\n",
      "Training Loss: 0.002062709056335734\n",
      "Validation Loss: 0.003001707611140736\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.006796657150844112\n",
      "Training Loss: 0.006966503168223425\n",
      "Training Loss: 0.0069605197745841\n",
      "Training Loss: 0.0048846185005095326\n",
      "Training Loss: 0.002055747096746927\n",
      "Training Loss: 0.002046520825024345\n",
      "Training Loss: 0.0020478866272605955\n",
      "Validation Loss: 0.0029682583749609955\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.0067585322470404205\n",
      "Training Loss: 0.006923395215999335\n",
      "Training Loss: 0.00692115739453584\n",
      "Training Loss: 0.0048593418023665435\n",
      "Training Loss: 0.0020451263661379927\n",
      "Training Loss: 0.0020335927896667273\n",
      "Training Loss: 0.0020343662097002378\n",
      "Validation Loss: 0.0029381746699822515\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006725379542913288\n",
      "Training Loss: 0.006885887800017372\n",
      "Training Loss: 0.0068865819391794505\n",
      "Training Loss: 0.004837043380539399\n",
      "Training Loss: 0.0020355360045505224\n",
      "Training Loss: 0.0020219460752559826\n",
      "Training Loss: 0.0020216745484503917\n",
      "Validation Loss: 0.002911000937107455\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006696740324841812\n",
      "Training Loss: 0.006853508477797731\n",
      "Training Loss: 0.006856444459408521\n",
      "Training Loss: 0.004817335472180276\n",
      "Training Loss: 0.0020265565426961984\n",
      "Training Loss: 0.0020112218514259437\n",
      "Training Loss: 0.002009477363753831\n",
      "Validation Loss: 0.0028863400925031066\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006672145458869636\n",
      "Training Loss: 0.006825750434072689\n",
      "Training Loss: 0.006830355426063761\n",
      "Training Loss: 0.00479987205733778\n",
      "Training Loss: 0.0020178935847070534\n",
      "Training Loss: 0.002001157137347036\n",
      "Training Loss: 0.0019975557520228904\n",
      "Validation Loss: 0.002863856840748793\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.006651132765691728\n",
      "Training Loss: 0.006802096761530266\n",
      "Training Loss: 0.006807904604356736\n",
      "Training Loss: 0.004784345947555267\n",
      "Training Loss: 0.002009350631124107\n",
      "Training Loss: 0.0019915576199127827\n",
      "Training Loss: 0.00198577954331995\n",
      "Validation Loss: 0.002843267572973942\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006633249496808276\n",
      "Training Loss: 0.006782033911440522\n",
      "Training Loss: 0.006788674570852891\n",
      "Training Loss: 0.004770487528439844\n",
      "Training Loss: 0.0020008152024820446\n",
      "Training Loss: 0.001982292842149036\n",
      "Training Loss: 0.0019740846479544415\n",
      "Validation Loss: 0.002824341000492897\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006618067407980561\n",
      "Training Loss: 0.006765067690284923\n",
      "Training Loss: 0.006772257649572566\n",
      "Training Loss: 0.004758057290891884\n",
      "Training Loss: 0.001992226751608541\n",
      "Training Loss: 0.001973272057948634\n",
      "Training Loss: 0.0019624537011259237\n",
      "Validation Loss: 0.0028068732794231215\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.006605184581130743\n",
      "Training Loss: 0.006750734015367925\n",
      "Training Loss: 0.006758263250812888\n",
      "Training Loss: 0.00474684266388067\n",
      "Training Loss: 0.0019835648640582804\n",
      "Training Loss: 0.0019644359820813408\n",
      "Training Loss: 0.0019508900499204175\n",
      "Validation Loss: 0.0027906916898671774\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006594232290517539\n",
      "Training Loss: 0.006738608296727761\n",
      "Training Loss: 0.006746330143650994\n",
      "Training Loss: 0.004736655603264808\n",
      "Training Loss: 0.001974828484089812\n",
      "Training Loss: 0.001955744933075039\n",
      "Training Loss: 0.0019394120771903545\n",
      "Validation Loss: 0.0027756516493365087\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006584880437003449\n",
      "Training Loss: 0.0067283146281261\n",
      "Training Loss: 0.006736131712095812\n",
      "Training Loss: 0.00472733046481153\n",
      "Training Loss: 0.0019660293610650114\n",
      "Training Loss: 0.0019471714808605611\n",
      "Training Loss: 0.001928036938625155\n",
      "Validation Loss: 0.002761615455836888\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006576842939248309\n",
      "Training Loss: 0.006719526664819568\n",
      "Training Loss: 0.00672737916582264\n",
      "Training Loss: 0.004718721829412971\n",
      "Training Loss: 0.001957182895130245\n",
      "Training Loss: 0.0019386930794280488\n",
      "Training Loss: 0.0019167767860926688\n",
      "Validation Loss: 0.002748464636721784\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006569875170243904\n",
      "Training Loss: 0.006711967319715768\n",
      "Training Loss: 0.006719822935992852\n",
      "Training Loss: 0.004710704177487059\n",
      "Training Loss: 0.0019483036558085587\n",
      "Training Loss: 0.0019302938067994545\n",
      "Training Loss: 0.0019056427299801726\n",
      "Validation Loss: 0.002736093187665441\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006563768807100132\n",
      "Training Loss: 0.006705402648076415\n",
      "Training Loss: 0.006713247889420018\n",
      "Training Loss: 0.004703170413267799\n",
      "Training Loss: 0.0019394051647395826\n",
      "Training Loss: 0.001921961627958808\n",
      "Training Loss: 0.00189463819944649\n",
      "Validation Loss: 0.0027243989256641407\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006558356489986181\n",
      "Training Loss: 0.006699647521600127\n",
      "Training Loss: 0.006707478011958301\n",
      "Training Loss: 0.004696031675266567\n",
      "Training Loss: 0.001930500626040157\n",
      "Training Loss: 0.001913684018800268\n",
      "Training Loss: 0.0018837608853937127\n",
      "Validation Loss: 0.0027132978535750847\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006553502053720877\n",
      "Training Loss: 0.006694547852966934\n",
      "Training Loss: 0.006702364007942378\n",
      "Training Loss: 0.0046892127624596465\n",
      "Training Loss: 0.001921598761837231\n",
      "Training Loss: 0.0019054518235498109\n",
      "Training Loss: 0.001873005693196319\n",
      "Validation Loss: 0.0027027098961873853\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006549096171511337\n",
      "Training Loss: 0.006689982113894075\n",
      "Training Loss: 0.006697785358410329\n",
      "Training Loss: 0.004682654088246636\n",
      "Training Loss: 0.001912706663715653\n",
      "Training Loss: 0.00189725611358881\n",
      "Training Loss: 0.0018623681274766567\n",
      "Validation Loss: 0.0026925642431185194\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.006545052484143526\n",
      "Training Loss: 0.006685851973015815\n",
      "Training Loss: 0.006693640381563455\n",
      "Training Loss: 0.00467630479754007\n",
      "Training Loss: 0.0019038345277658665\n",
      "Training Loss: 0.001889093926147325\n",
      "Training Loss: 0.001851839779119473\n",
      "Validation Loss: 0.0026828010398885827\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006541301630204543\n",
      "Training Loss: 0.006682078486774117\n",
      "Training Loss: 0.006689845493528992\n",
      "Training Loss: 0.004670126499986509\n",
      "Training Loss: 0.001894987364212284\n",
      "Training Loss: 0.0018809577042702586\n",
      "Training Loss: 0.0018414129063603468\n",
      "Validation Loss: 0.002673364616647077\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006537788204150274\n",
      "Training Loss: 0.006678597561549395\n",
      "Training Loss: 0.006686333115212619\n",
      "Training Loss: 0.004664085200929548\n",
      "Training Loss: 0.0018861703072616364\n",
      "Training Loss: 0.001872843478777213\n",
      "Training Loss: 0.0018310803893837146\n",
      "Validation Loss: 0.0026642063505945847\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.0065344695351086555\n",
      "Training Loss: 0.006675358169013634\n",
      "Training Loss: 0.0066830468992702665\n",
      "Training Loss: 0.004658156051373225\n",
      "Training Loss: 0.0018773873693135102\n",
      "Training Loss: 0.0018647490088187623\n",
      "Training Loss: 0.0018208365203463473\n",
      "Validation Loss: 0.002655284055674358\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.0065313080826308574\n",
      "Training Loss: 0.006672315504401922\n",
      "Training Loss: 0.006679939089808613\n",
      "Training Loss: 0.004652317118889186\n",
      "Training Loss: 0.0018686405742482748\n",
      "Training Loss: 0.001856667580868816\n",
      "Training Loss: 0.0018106717932096217\n",
      "Validation Loss: 0.0026465599026839677\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006528276724275202\n",
      "Training Loss: 0.006669437454547733\n",
      "Training Loss: 0.006676972013665363\n",
      "Training Loss: 0.004646550086836215\n",
      "Training Loss: 0.0018599322831141763\n",
      "Training Loss: 0.0018485985802544746\n",
      "Training Loss: 0.001800581491261255\n",
      "Validation Loss: 0.002638006439299082\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.00652535074390471\n",
      "Training Loss: 0.006666690348647535\n",
      "Training Loss: 0.006674110741587355\n",
      "Training Loss: 0.004640842372173211\n",
      "Training Loss: 0.0018512679140258114\n",
      "Training Loss: 0.0018405404352233746\n",
      "Training Loss: 0.001790563168906374\n",
      "Validation Loss: 0.002629595744481994\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006522509031929076\n",
      "Training Loss: 0.006664049500832334\n",
      "Training Loss: 0.006671328428201378\n",
      "Training Loss: 0.0046351814323134025\n",
      "Training Loss: 0.0018426467385143043\n",
      "Training Loss: 0.0018324904210749082\n",
      "Training Loss: 0.0017806123447371646\n",
      "Validation Loss: 0.002621306520768996\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006519735793117434\n",
      "Training Loss: 0.006661491078557446\n",
      "Training Loss: 0.006668599490076303\n",
      "Training Loss: 0.0046295576979173345\n",
      "Training Loss: 0.0018340719031402842\n",
      "Training Loss: 0.001824448726256378\n",
      "Training Loss: 0.001770725883252453\n",
      "Validation Loss: 0.00261312188292428\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.006517015805002302\n",
      "Training Loss: 0.006658998784841969\n",
      "Training Loss: 0.006665908106369898\n",
      "Training Loss: 0.004623963819321944\n",
      "Training Loss: 0.0018255415577732492\n",
      "Training Loss: 0.0018164140358567238\n",
      "Training Loss: 0.001760901835077675\n",
      "Validation Loss: 0.002605025394477843\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006514338473789394\n",
      "Training Loss: 0.006656554249348119\n",
      "Training Loss: 0.006663234516745433\n",
      "Training Loss: 0.00461839478892216\n",
      "Training Loss: 0.0018170626225764864\n",
      "Training Loss: 0.0018083882551582064\n",
      "Training Loss: 0.0017511428432771935\n",
      "Validation Loss: 0.0025970082631298036\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.006511693226639181\n",
      "Training Loss: 0.0066541452589444815\n",
      "Training Loss: 0.006660569161176681\n",
      "Training Loss: 0.004612845728916\n",
      "Training Loss: 0.001808633337932406\n",
      "Training Loss: 0.0018003727357427124\n",
      "Training Loss: 0.001741447165113641\n",
      "Validation Loss: 0.002589067301193203\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.006509072720073164\n",
      "Training Loss: 0.006651757905492559\n",
      "Training Loss: 0.0066578979475889356\n",
      "Training Loss: 0.0046073138998326615\n",
      "Training Loss: 0.0018002579617314041\n",
      "Training Loss: 0.0017923711470211857\n",
      "Training Loss: 0.0017318185629846994\n",
      "Validation Loss: 0.0025811925143908628\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.006506470259046182\n",
      "Training Loss: 0.006649383447365835\n",
      "Training Loss: 0.006655214048805647\n",
      "Training Loss: 0.004601798360235989\n",
      "Training Loss: 0.0017919411775073968\n",
      "Training Loss: 0.0017843885472393595\n",
      "Training Loss: 0.0017222639216925018\n",
      "Validation Loss: 0.002573389615357787\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.006503879743395374\n",
      "Training Loss: 0.006647011282620951\n",
      "Training Loss: 0.006652509841369465\n",
      "Training Loss: 0.0045962995591253275\n",
      "Training Loss: 0.001783687065326376\n",
      "Training Loss: 0.0017764310906932224\n",
      "Training Loss: 0.0017127873800927773\n",
      "Validation Loss: 0.002565655614029658\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.006501299127703533\n",
      "Training Loss: 0.006644637573044747\n",
      "Training Loss: 0.0066497831168817354\n",
      "Training Loss: 0.004590818508513621\n",
      "Training Loss: 0.0017755005127401092\n",
      "Training Loss: 0.0017685049977444578\n",
      "Training Loss: 0.0017033983756846282\n",
      "Validation Loss: 0.0025579971465158314\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.006498722744872794\n",
      "Training Loss: 0.006642253677127883\n",
      "Training Loss: 0.006647028365987353\n",
      "Training Loss: 0.004585358347612783\n",
      "Training Loss: 0.001767389693268342\n",
      "Training Loss: 0.0017606197120039724\n",
      "Training Loss: 0.0016941038587538059\n",
      "Validation Loss: 0.002550419651199488\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.006496152989566326\n",
      "Training Loss: 0.006639859116403386\n",
      "Training Loss: 0.006644247332005761\n",
      "Training Loss: 0.004579922816119506\n",
      "Training Loss: 0.0017593603726709262\n",
      "Training Loss: 0.0017527846647135447\n",
      "Training Loss: 0.001684916582889855\n",
      "Validation Loss: 0.0025429367567395113\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.006493585580028593\n",
      "Training Loss: 0.006637447128305211\n",
      "Training Loss: 0.006641438589431345\n",
      "Training Loss: 0.004574516618231428\n",
      "Training Loss: 0.0017514228698564693\n",
      "Training Loss: 0.0017450127622578293\n",
      "Training Loss: 0.0016758481317083352\n",
      "Validation Loss: 0.0025355513960825912\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.006491021384717897\n",
      "Training Loss: 0.006635017762891948\n",
      "Training Loss: 0.006638604188919999\n",
      "Training Loss: 0.004569145833011134\n",
      "Training Loss: 0.0017435852531343698\n",
      "Training Loss: 0.0017373136802052614\n",
      "Training Loss: 0.001666910071944585\n",
      "Validation Loss: 0.002528282500997932\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.006488461853004992\n",
      "Training Loss: 0.006632570209912956\n",
      "Training Loss: 0.006635746925603599\n",
      "Training Loss: 0.004563816844529356\n",
      "Training Loss: 0.0017358570353826507\n",
      "Training Loss: 0.0017297012642666233\n",
      "Training Loss: 0.0016581184284586924\n",
      "Validation Loss: 0.002521140853964204\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.006485906519228593\n",
      "Training Loss: 0.006630103719653562\n",
      "Training Loss: 0.006632871039328165\n",
      "Training Loss: 0.004558536063195788\n",
      "Training Loss: 0.0017282476495893207\n",
      "Training Loss: 0.0017221860293648206\n",
      "Training Loss: 0.00164948303179699\n",
      "Validation Loss: 0.002514139791620958\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.006483357957331464\n",
      "Training Loss: 0.006627619741484523\n",
      "Training Loss: 0.0066299795178929345\n",
      "Training Loss: 0.004553312596835894\n",
      "Training Loss: 0.0017207702535961288\n",
      "Training Loss: 0.001714785859512631\n",
      "Training Loss: 0.0016410227947926614\n",
      "Validation Loss: 0.002507290727029858\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.00648081601364538\n",
      "Training Loss: 0.006625117459334433\n",
      "Training Loss: 0.006627077588345856\n",
      "Training Loss: 0.004548152286442928\n",
      "Training Loss: 0.0017134339390031527\n",
      "Training Loss: 0.0017075094042229466\n",
      "Training Loss: 0.0016327475919388234\n",
      "Validation Loss: 0.002500608793855732\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.006478282354073599\n",
      "Training Loss: 0.0066226000187452885\n",
      "Training Loss: 0.006624170721624978\n",
      "Training Loss: 0.0045430634205695245\n",
      "Training Loss: 0.0017062472287216223\n",
      "Training Loss: 0.0017003714898601175\n",
      "Training Loss: 0.0016246697309543379\n",
      "Validation Loss: 0.0024941069675018237\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.006475760148605332\n",
      "Training Loss: 0.006620069891214371\n",
      "Training Loss: 0.006621265634894371\n",
      "Training Loss: 0.004538052849311498\n",
      "Training Loss: 0.0016992187943833414\n",
      "Training Loss: 0.0016933815079391934\n",
      "Training Loss: 0.0016168008063687012\n",
      "Validation Loss: 0.0024877967564075182\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.006473250001436099\n",
      "Training Loss: 0.006617527089547366\n",
      "Training Loss: 0.006618365845642984\n",
      "Training Loss: 0.00453312742181879\n",
      "Training Loss: 0.0016923582875460852\n",
      "Training Loss: 0.0016865505346504506\n",
      "Training Loss: 0.0016091508806857747\n",
      "Validation Loss: 0.0024816866162710926\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.006470754484180361\n",
      "Training Loss: 0.00661497799330391\n",
      "Training Loss: 0.00661547951051034\n",
      "Training Loss: 0.004528292927425355\n",
      "Training Loss: 0.0016856694749731104\n",
      "Training Loss: 0.0016798857002868317\n",
      "Training Loss: 0.0016017246486444493\n",
      "Validation Loss: 0.002475783036919729\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.00646827524760738\n",
      "Training Loss: 0.006612424569902941\n",
      "Training Loss: 0.006612611809396185\n",
      "Training Loss: 0.004523555210762424\n",
      "Training Loss: 0.0016791576425021048\n",
      "Training Loss: 0.001673394082026789\n",
      "Training Loss: 0.001594529899157351\n",
      "Validation Loss: 0.0024700927238977665\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.006465814786497504\n",
      "Training Loss: 0.006609868537634611\n",
      "Training Loss: 0.006609768318012357\n",
      "Training Loss: 0.004518917946406873\n",
      "Training Loss: 0.0016728257828799542\n",
      "Training Loss: 0.0016670819050341379\n",
      "Training Loss: 0.0015875693220004906\n",
      "Validation Loss: 0.0024646197130453636\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.006463373081060126\n",
      "Training Loss: 0.006607313402928412\n",
      "Training Loss: 0.006606952890870162\n",
      "Training Loss: 0.004514385905640666\n",
      "Training Loss: 0.0016666777615319006\n",
      "Training Loss: 0.0016609511530259625\n",
      "Training Loss: 0.0015808433838537894\n",
      "Validation Loss: 0.002459363493464943\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.00646095268195495\n",
      "Training Loss: 0.006604762288043275\n",
      "Training Loss: 0.006604171353392303\n",
      "Training Loss: 0.004509960344803403\n",
      "Training Loss: 0.0016607125666632783\n",
      "Training Loss: 0.0016550044203177095\n",
      "Training Loss: 0.0015743538223614451\n",
      "Validation Loss: 0.002454324386061661\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.006458554478595033\n",
      "Training Loss: 0.0066022183513268825\n",
      "Training Loss: 0.006601427221321501\n",
      "Training Loss: 0.004505644551318255\n",
      "Training Loss: 0.0016549307345121633\n",
      "Training Loss: 0.0016492410446517169\n",
      "Training Loss: 0.0015680964328930714\n",
      "Validation Loss: 0.0024494984523351355\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.006456181381363422\n",
      "Training Loss: 0.006599684157408774\n",
      "Training Loss: 0.00659872530202847\n",
      "Training Loss: 0.004501438396328012\n",
      "Training Loss: 0.0016493269320926628\n",
      "Training Loss: 0.0016436595426057466\n",
      "Training Loss: 0.001562067810446024\n",
      "Validation Loss: 0.002444884954116304\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.006453833313426003\n",
      "Training Loss: 0.006597162144025788\n",
      "Training Loss: 0.006596067367354408\n",
      "Training Loss: 0.004497342288595973\n",
      "Training Loss: 0.0016438991898030508\n",
      "Training Loss: 0.001638255279540317\n",
      "Training Loss: 0.0015562611519999337\n",
      "Validation Loss: 0.002440473965908948\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.006451511969789863\n",
      "Training Loss: 0.0065946548467036336\n",
      "Training Loss: 0.0065934561943868175\n",
      "Training Loss: 0.00449335531804536\n",
      "Training Loss: 0.0016386436711763963\n",
      "Training Loss: 0.001633026082854485\n",
      "Training Loss: 0.001550670336582698\n",
      "Validation Loss: 0.002436257916684036\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.006449218688067049\n",
      "Training Loss: 0.006592166232876479\n",
      "Training Loss: 0.006590897224959917\n",
      "Training Loss: 0.004489476245580591\n",
      "Training Loss: 0.0016335495028761216\n",
      "Training Loss: 0.0016279636582476086\n",
      "Training Loss: 0.001545284760359209\n",
      "Validation Loss: 0.002432226809875321\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.006446956089930609\n",
      "Training Loss: 0.006589698558673263\n",
      "Training Loss: 0.006588391413679347\n",
      "Training Loss: 0.004485702812744421\n",
      "Training Loss: 0.00162861199627514\n",
      "Training Loss: 0.0016230626878677868\n",
      "Training Loss: 0.0015400960169790778\n",
      "Validation Loss: 0.002428371809254669\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.006444724936736748\n",
      "Training Loss: 0.00658725511864759\n",
      "Training Loss: 0.006585939437500201\n",
      "Training Loss: 0.004482033051826875\n",
      "Training Loss: 0.0016238258573866916\n",
      "Training Loss: 0.001618316997482907\n",
      "Training Loss: 0.0015350960637442767\n",
      "Validation Loss: 0.0024246799923943476\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.006442524632439018\n",
      "Training Loss: 0.0065848349907901135\n",
      "Training Loss: 0.006583542754524388\n",
      "Training Loss: 0.0044784638874261875\n",
      "Training Loss: 0.0016191813693149015\n",
      "Training Loss: 0.001613716763094999\n",
      "Training Loss: 0.0015302731067640706\n",
      "Validation Loss: 0.0024211459917631976\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.006440356660168618\n",
      "Training Loss: 0.006582441021455452\n",
      "Training Loss: 0.006581200342625379\n",
      "Training Loss: 0.004474991773240618\n",
      "Training Loss: 0.0016146715806098656\n",
      "Training Loss: 0.0016092572892375755\n",
      "Training Loss: 0.0015256182529265062\n",
      "Validation Loss: 0.0024177550342786808\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.006438222288852558\n",
      "Training Loss: 0.0065800753433723005\n",
      "Training Loss: 0.006578914797282778\n",
      "Training Loss: 0.004471613177156541\n",
      "Training Loss: 0.001610288714437047\n",
      "Training Loss: 0.0016049286324414424\n",
      "Training Loss: 0.0015211203988292254\n",
      "Validation Loss: 0.0024144951383189093\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.006436121008591727\n",
      "Training Loss: 0.006577739015920087\n",
      "Training Loss: 0.006576685970067046\n",
      "Training Loss: 0.004468325473499135\n",
      "Training Loss: 0.0016060226934496314\n",
      "Training Loss: 0.0016007224858913104\n",
      "Training Loss: 0.0015167663581087253\n",
      "Validation Loss: 0.0024113579906526717\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.00643405562848784\n",
      "Training Loss: 0.006575434507103637\n",
      "Training Loss: 0.006574513243394904\n",
      "Training Loss: 0.00446512341615744\n",
      "Training Loss: 0.0016018662181159016\n",
      "Training Loss: 0.0015966312537784688\n",
      "Training Loss: 0.0015125497766712214\n",
      "Validation Loss: 0.002408331336060274\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.006432024686364457\n",
      "Training Loss: 0.006573162100976333\n",
      "Training Loss: 0.0065723964292556045\n",
      "Training Loss: 0.004462003721055225\n",
      "Training Loss: 0.0015978135289333294\n",
      "Training Loss: 0.0015926485999079886\n",
      "Training Loss: 0.0015084611529891845\n",
      "Validation Loss: 0.0024054052020651587\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.0064300266897771505\n",
      "Training Loss: 0.006570919300429523\n",
      "Training Loss: 0.006570332277333364\n",
      "Training Loss: 0.004458963225260959\n",
      "Training Loss: 0.0015938589726283681\n",
      "Training Loss: 0.0015887669145013205\n",
      "Training Loss: 0.00150449147564359\n",
      "Validation Loss: 0.0024025661925120762\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.006428062034538016\n",
      "Training Loss: 0.00656870826962404\n",
      "Training Loss: 0.00656832123931963\n",
      "Training Loss: 0.004455997148834285\n",
      "Training Loss: 0.001589991189102875\n",
      "Training Loss: 0.001584977974271169\n",
      "Training Loss: 0.0015006294303748292\n",
      "Validation Loss: 0.0023998143631016157\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.006426132551860064\n",
      "Training Loss: 0.006566530495183543\n",
      "Training Loss: 0.006566361517761834\n",
      "Training Loss: 0.004453102045008563\n",
      "Training Loss: 0.0015862079127691687\n",
      "Training Loss: 0.0015812766237650066\n",
      "Training Loss: 0.0014968703339400236\n",
      "Validation Loss: 0.0023971306872318677\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.006424233459401876\n",
      "Training Loss: 0.006564383759396151\n",
      "Training Loss: 0.0065644522453658285\n",
      "Training Loss: 0.004450274357805028\n",
      "Training Loss: 0.00158250094318646\n",
      "Training Loss: 0.001577655512810452\n",
      "Training Loss: 0.0014932034559024033\n",
      "Validation Loss: 0.002394506425142449\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.006422367934137582\n",
      "Training Loss: 0.00656226952909492\n",
      "Training Loss: 0.00656259095587302\n",
      "Training Loss: 0.004447509261663072\n",
      "Training Loss: 0.001578865071060136\n",
      "Training Loss: 0.001574109728244366\n",
      "Training Loss: 0.0014896234557090792\n",
      "Validation Loss: 0.0023919432741207895\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.006420533685013652\n",
      "Training Loss: 0.006560185805428773\n",
      "Training Loss: 0.0065607757831458\n",
      "Training Loss: 0.00444480529113207\n",
      "Training Loss: 0.0015752955163770821\n",
      "Training Loss: 0.0015706330224929844\n",
      "Training Loss: 0.0014861229970119893\n",
      "Validation Loss: 0.002389428620326964\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.006418730538571254\n",
      "Training Loss: 0.006558134943479672\n",
      "Training Loss: 0.006559007001924328\n",
      "Training Loss: 0.004442156720760977\n",
      "Training Loss: 0.0015717849601060152\n",
      "Training Loss: 0.0015672190152690745\n",
      "Training Loss: 0.0014826946490211412\n",
      "Validation Loss: 0.0023869551353728614\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.006416957576293498\n",
      "Training Loss: 0.006556115431012585\n",
      "Training Loss: 0.006557281184359454\n",
      "Training Loss: 0.004439562727784505\n",
      "Training Loss: 0.0015683313785120846\n",
      "Training Loss: 0.00156386511225719\n",
      "Training Loss: 0.0014793346227088477\n",
      "Validation Loss: 0.0023845172444034073\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.006415212629362941\n",
      "Training Loss: 0.00655412414111197\n",
      "Training Loss: 0.006555595679674298\n",
      "Training Loss: 0.004437018362659728\n",
      "Training Loss: 0.0015649326640414074\n",
      "Training Loss: 0.0015605673713434954\n",
      "Training Loss: 0.0014760396945348476\n",
      "Validation Loss: 0.0023821118365788363\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.006413494042353704\n",
      "Training Loss: 0.006552162193693221\n",
      "Training Loss: 0.006553949498338625\n",
      "Training Loss: 0.004434520294889808\n",
      "Training Loss: 0.001561579599074321\n",
      "Training Loss: 0.001557318793347804\n",
      "Training Loss: 0.0014727995691646357\n",
      "Validation Loss: 0.0023797360928607095\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.006411802863003686\n",
      "Training Loss: 0.0065502283989917484\n",
      "Training Loss: 0.0065523402573307975\n",
      "Training Loss: 0.0044320672890171406\n",
      "Training Loss: 0.0015582726687716785\n",
      "Training Loss: 0.0015541167215269525\n",
      "Training Loss: 0.0014696141736931167\n",
      "Validation Loss: 0.0023773793383564256\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.006410136098274961\n",
      "Training Loss: 0.006548322371672839\n",
      "Training Loss: 0.0065507670945953575\n",
      "Training Loss: 0.004429655925778206\n",
      "Training Loss: 0.001555007553397445\n",
      "Training Loss: 0.0015509588431450538\n",
      "Training Loss: 0.0014664776794961654\n",
      "Validation Loss: 0.0023750372525097015\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.006408493013586849\n",
      "Training Loss: 0.006546442183898762\n",
      "Training Loss: 0.006549227084033191\n",
      "Training Loss: 0.004427283249533502\n",
      "Training Loss: 0.0015517793357139452\n",
      "Training Loss: 0.0015478389406052883\n",
      "Training Loss: 0.0014633855018473696\n",
      "Validation Loss: 0.0023727100623740153\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.006406873631058261\n",
      "Training Loss: 0.006544588833348825\n",
      "Training Loss: 0.00654771918838378\n",
      "Training Loss: 0.0044249463973392265\n",
      "Training Loss: 0.0015485880343476311\n",
      "Training Loss: 0.0015447567220689962\n",
      "Training Loss: 0.0014603348828677554\n",
      "Validation Loss: 0.0023703936862188987\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.006405275852885097\n",
      "Training Loss: 0.0065427591872867195\n",
      "Training Loss: 0.006546240999014117\n",
      "Training Loss: 0.00442264395285747\n",
      "Training Loss: 0.0015454284389852546\n",
      "Training Loss: 0.0015417081276973476\n",
      "Training Loss: 0.001457323418726446\n",
      "Validation Loss: 0.002368081868791862\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.006403696851339191\n",
      "Training Loss: 0.006540953036164865\n",
      "Training Loss: 0.006544791971682571\n",
      "Training Loss: 0.0044203734904294835\n",
      "Training Loss: 0.0015422994957771151\n",
      "Training Loss: 0.0015386884468171047\n",
      "Training Loss: 0.001454345199308591\n",
      "Validation Loss: 0.0023657708088274387\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.0064021379221230745\n",
      "Training Loss: 0.006539170691976324\n",
      "Training Loss: 0.0065433689818019045\n",
      "Training Loss: 0.004418131943093612\n",
      "Training Loss: 0.0015391989632917102\n",
      "Training Loss: 0.001535698049410712\n",
      "Training Loss: 0.0014514003900694661\n",
      "Validation Loss: 0.00236346965430266\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.006400597556494176\n",
      "Training Loss: 0.006537409201264381\n",
      "Training Loss: 0.006541970359394327\n",
      "Training Loss: 0.004415918089653133\n",
      "Training Loss: 0.0015361226024106145\n",
      "Training Loss: 0.0015327322701341472\n",
      "Training Loss: 0.0014484849672589918\n",
      "Validation Loss: 0.0023611626633153427\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.006399072541389614\n",
      "Training Loss: 0.006535668217111379\n",
      "Training Loss: 0.006540596235427074\n",
      "Training Loss: 0.004413729322259315\n",
      "Training Loss: 0.0015330694449949079\n",
      "Training Loss: 0.0015297885487234452\n",
      "Training Loss: 0.001445594059478026\n",
      "Validation Loss: 0.002358852837157125\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.006397565426304937\n",
      "Training Loss: 0.006533949176082387\n",
      "Training Loss: 0.006539245536550879\n",
      "Training Loss: 0.004411563968751579\n",
      "Training Loss: 0.0015300360045512208\n",
      "Training Loss: 0.001526865007035667\n",
      "Training Loss: 0.0014427277589857113\n",
      "Validation Loss: 0.002356538174748909\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.006396072199568153\n",
      "Training Loss: 0.006532248920993879\n",
      "Training Loss: 0.006537915167864412\n",
      "Training Loss: 0.0044094201561529186\n",
      "Training Loss: 0.0015270222397521138\n",
      "Training Loss: 0.0015239604170346865\n",
      "Training Loss: 0.0014398835458268877\n",
      "Validation Loss: 0.002354215644605193\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.006394592334982008\n",
      "Training Loss: 0.00653056551120244\n",
      "Training Loss: 0.006536603962886147\n",
      "Training Loss: 0.0044072966589010325\n",
      "Training Loss: 0.0015240244280721527\n",
      "Training Loss: 0.0015210705167555715\n",
      "Training Loss: 0.0014370577558293007\n",
      "Validation Loss: 0.00235188174511006\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.0063931250246241685\n",
      "Training Loss: 0.006528900947887451\n",
      "Training Loss: 0.006535312506603077\n",
      "Training Loss: 0.004405190665711416\n",
      "Training Loss: 0.0015210402235970833\n",
      "Training Loss: 0.0015181940006732475\n",
      "Training Loss: 0.0014342478632170242\n",
      "Validation Loss: 0.0023495362799489097\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.006391670388402417\n",
      "Training Loss: 0.006527252302039415\n",
      "Training Loss: 0.006534037256496959\n",
      "Training Loss: 0.004403101046482334\n",
      "Training Loss: 0.0015180686343228445\n",
      "Training Loss: 0.0015153287474095123\n",
      "Training Loss: 0.0014314532571006566\n",
      "Validation Loss: 0.002347178732010986\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.006390225972281769\n",
      "Training Loss: 0.006525619415333495\n",
      "Training Loss: 0.00653277926845476\n",
      "Training Loss: 0.004401026758278022\n",
      "Training Loss: 0.0015151077923655975\n",
      "Training Loss: 0.0015124727528018412\n",
      "Training Loss: 0.0014286699144577142\n",
      "Validation Loss: 0.002344803749134569\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.006388791564386338\n",
      "Training Loss: 0.006524001156212762\n",
      "Training Loss: 0.006531536856782622\n",
      "Training Loss: 0.004398965164000401\n",
      "Training Loss: 0.0015121538774110377\n",
      "Training Loss: 0.0015096230070048477\n",
      "Training Loss: 0.0014258952584350483\n",
      "Validation Loss: 0.0023424138013301383\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.006387367902789265\n",
      "Training Loss: 0.006522400238318369\n",
      "Training Loss: 0.006530311221722513\n",
      "Training Loss: 0.004396916011173744\n",
      "Training Loss: 0.0015092052142426837\n",
      "Training Loss: 0.0015067777810327243\n",
      "Training Loss: 0.00142312901240075\n",
      "Validation Loss: 0.00234000298291818\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.006385950994445011\n",
      "Training Loss: 0.006520810736110434\n",
      "Training Loss: 0.006529097058228217\n",
      "Training Loss: 0.004394876940641552\n",
      "Training Loss: 0.0015062607404252048\n",
      "Training Loss: 0.0015039350671577267\n",
      "Training Loss: 0.0014203677365730982\n",
      "Validation Loss: 0.002337577927282973\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.006384542461019009\n",
      "Training Loss: 0.006519234082661569\n",
      "Training Loss: 0.006527896435000002\n",
      "Training Loss: 0.004392846411356004\n",
      "Training Loss: 0.0015033210716501343\n",
      "Training Loss: 0.001501094901104807\n",
      "Training Loss: 0.0014176110023981891\n",
      "Validation Loss: 0.0023351238401953744\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.0063831389090046285\n",
      "Training Loss: 0.006517668765736744\n",
      "Training Loss: 0.0065267074276926\n",
      "Training Loss: 0.004390823763096705\n",
      "Training Loss: 0.001500380301295081\n",
      "Training Loss: 0.001498252674791729\n",
      "Training Loss: 0.0014148557171574795\n",
      "Validation Loss: 0.0023326504696945375\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.006381741398945451\n",
      "Training Loss: 0.006516114418627694\n",
      "Training Loss: 0.006525529332575389\n",
      "Training Loss: 0.004388807241484756\n",
      "Training Loss: 0.0014974370624986477\n",
      "Training Loss: 0.0014954070300882449\n",
      "Training Loss: 0.0014120987005298957\n",
      "Validation Loss: 0.002330148266562983\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.0063803508365526795\n",
      "Training Loss: 0.006514572373125702\n",
      "Training Loss: 0.006524363490170799\n",
      "Training Loss: 0.004386796077451436\n",
      "Training Loss: 0.0014944899077818264\n",
      "Training Loss: 0.0014925557019887493\n",
      "Training Loss: 0.001409339298843406\n",
      "Validation Loss: 0.002327621957938595\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.0063789627864025535\n",
      "Training Loss: 0.006513038369594142\n",
      "Training Loss: 0.0065232069371268155\n",
      "Training Loss: 0.0043847880059911404\n",
      "Training Loss: 0.0014915370485687164\n",
      "Training Loss: 0.001489697697761585\n",
      "Training Loss: 0.001406575523869833\n",
      "Validation Loss: 0.002325066813086386\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.006377579165855423\n",
      "Training Loss: 0.0065115147980395706\n",
      "Training Loss: 0.00652206068043597\n",
      "Training Loss: 0.004382783025357639\n",
      "Training Loss: 0.0014885779609903694\n",
      "Training Loss: 0.0014868311824830017\n",
      "Training Loss: 0.0014038051496027036\n",
      "Validation Loss: 0.002322477984202028\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.0063761971739586445\n",
      "Training Loss: 0.006509997873799875\n",
      "Training Loss: 0.006520921610062942\n",
      "Training Loss: 0.004380778928025393\n",
      "Training Loss: 0.0014856093552953098\n",
      "Training Loss: 0.0014839529173332267\n",
      "Training Loss: 0.0014010264110402204\n",
      "Validation Loss: 0.0023198646928396507\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.006374818739714101\n",
      "Training Loss: 0.006508489499101416\n",
      "Training Loss: 0.00651979208283592\n",
      "Training Loss: 0.004378775105142268\n",
      "Training Loss: 0.0014826292250654661\n",
      "Training Loss: 0.0014810618748742855\n",
      "Training Loss: 0.001398236842796905\n",
      "Validation Loss: 0.0023172129779898873\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.006373441924806684\n",
      "Training Loss: 0.006506989786867052\n",
      "Training Loss: 0.006518671175581403\n",
      "Training Loss: 0.004376770030648913\n",
      "Training Loss: 0.0014796328288502992\n",
      "Training Loss: 0.0014781532851338853\n",
      "Training Loss: 0.001395431520504644\n",
      "Validation Loss: 0.002314525799414192\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.0063720674614887686\n",
      "Training Loss: 0.006505496955942363\n",
      "Training Loss: 0.006517557764891535\n",
      "Training Loss: 0.004374763226660434\n",
      "Training Loss: 0.0014766218158183619\n",
      "Training Loss: 0.0014752285503345775\n",
      "Training Loss: 0.001392613370117033\n",
      "Validation Loss: 0.0023118002696201722\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.006370691705960781\n",
      "Training Loss: 0.006504008780466392\n",
      "Training Loss: 0.0065164523746352645\n",
      "Training Loss: 0.004372752878407482\n",
      "Training Loss: 0.0014735919138183816\n",
      "Training Loss: 0.0014722847044322408\n",
      "Training Loss: 0.0013897778575483245\n",
      "Validation Loss: 0.0023090378691970758\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.006369317130884155\n",
      "Training Loss: 0.006502525338437409\n",
      "Training Loss: 0.006515352785936557\n",
      "Training Loss: 0.004370738034194801\n",
      "Training Loss: 0.0014705435995711015\n",
      "Training Loss: 0.0014693186749354936\n",
      "Training Loss: 0.0013869224175869022\n",
      "Validation Loss: 0.0023062332130642433\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.006367941548814997\n",
      "Training Loss: 0.006501048140926287\n",
      "Training Loss: 0.006514261451084167\n",
      "Training Loss: 0.004368717648467282\n",
      "Training Loss: 0.0014674713829299435\n",
      "Training Loss: 0.0014663285431015538\n",
      "Training Loss: 0.001384044302685652\n",
      "Validation Loss: 0.002303384298616532\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.006366565816570073\n",
      "Training Loss: 0.006499574478948489\n",
      "Training Loss: 0.006513176445150748\n",
      "Training Loss: 0.00436669012036873\n",
      "Training Loss: 0.0014643738041922915\n",
      "Training Loss: 0.0014633113614399918\n",
      "Training Loss: 0.0013811418965633494\n",
      "Validation Loss: 0.002300492321618062\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.0063651898864191024\n",
      "Training Loss: 0.006498106149956584\n",
      "Training Loss: 0.0065120981330983345\n",
      "Training Loss: 0.004364655109238811\n",
      "Training Loss: 0.0014612497384950984\n",
      "Training Loss: 0.0014602663148980356\n",
      "Training Loss: 0.0013782126411388163\n",
      "Validation Loss: 0.0022975527923397216\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.006363811373012141\n",
      "Training Loss: 0.006496640868717805\n",
      "Training Loss: 0.006511026146472432\n",
      "Training Loss: 0.0043626115635561295\n",
      "Training Loss: 0.00145809690860915\n",
      "Training Loss: 0.0014571918929868843\n",
      "Training Loss: 0.0013752559226122684\n",
      "Validation Loss: 0.002294563087839668\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.006362430840963498\n",
      "Training Loss: 0.0064951766561716796\n",
      "Training Loss: 0.0065099594759522005\n",
      "Training Loss: 0.004360559020133223\n",
      "Training Loss: 0.0014549140082090162\n",
      "Training Loss: 0.0014540845951705706\n",
      "Training Loss: 0.0013722695197793656\n",
      "Validation Loss: 0.00229152286323206\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.006361046436941251\n",
      "Training Loss: 0.006493713804520667\n",
      "Training Loss: 0.006508897899184376\n",
      "Training Loss: 0.004358494763728231\n",
      "Training Loss: 0.0014516982318309602\n",
      "Training Loss: 0.0014509414666827069\n",
      "Training Loss: 0.0013692486769286915\n",
      "Validation Loss: 0.0022884326201492106\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.006359659776790067\n",
      "Training Loss: 0.006492253356846049\n",
      "Training Loss: 0.006507842586725019\n",
      "Training Loss: 0.004356419331597863\n",
      "Training Loss: 0.0014484453751356342\n",
      "Training Loss: 0.0014477605537103954\n",
      "Training Loss: 0.0013661908732319717\n",
      "Validation Loss: 0.002285281917837201\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.0063582714030053465\n",
      "Training Loss: 0.006490795469144359\n",
      "Training Loss: 0.006506794093875215\n",
      "Training Loss: 0.004354330451606075\n",
      "Training Loss: 0.0014451531779195647\n",
      "Training Loss: 0.0014445388296007878\n",
      "Training Loss: 0.0013630936504341663\n",
      "Validation Loss: 0.0022820686247402083\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.006356878669466823\n",
      "Training Loss: 0.006489336265949532\n",
      "Training Loss: 0.006505751402000897\n",
      "Training Loss: 0.004352227761701215\n",
      "Training Loss: 0.0014418202699744143\n",
      "Training Loss: 0.001441274910539505\n",
      "Training Loss: 0.0013599562688614243\n",
      "Validation Loss: 0.0022787941302118775\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.0063554819545242934\n",
      "Training Loss: 0.006487878138432279\n",
      "Training Loss: 0.00650471422879491\n",
      "Training Loss: 0.004350110147061059\n",
      "Training Loss: 0.0014384444698225707\n",
      "Training Loss: 0.0014379651371564251\n",
      "Training Loss: 0.001356774489977397\n",
      "Validation Loss: 0.0022754594682557125\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.006354081748286262\n",
      "Training Loss: 0.006486419662833214\n",
      "Training Loss: 0.00650368400500156\n",
      "Training Loss: 0.0043479766171367375\n",
      "Training Loss: 0.0014350217839819378\n",
      "Training Loss: 0.0014346076179208467\n",
      "Training Loss: 0.0013535441338899546\n",
      "Validation Loss: 0.0022720537288594043\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.006352678035618738\n",
      "Training Loss: 0.006484960201196373\n",
      "Training Loss: 0.006502660003025085\n",
      "Training Loss: 0.00434582635294646\n",
      "Training Loss: 0.0014315515138150659\n",
      "Training Loss: 0.001431199143626145\n",
      "Training Loss: 0.0013502652302850038\n",
      "Validation Loss: 0.002268583547696297\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.006351269654696807\n",
      "Training Loss: 0.006483500277390704\n",
      "Training Loss: 0.006501642449875362\n",
      "Training Loss: 0.004343658653961029\n",
      "Training Loss: 0.0014280292026523965\n",
      "Training Loss: 0.0014277380206476663\n",
      "Training Loss: 0.001346933166787494\n",
      "Validation Loss: 0.0022650369332252204\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.006349856789456681\n",
      "Training Loss: 0.0064820371975656595\n",
      "Training Loss: 0.00650063076580409\n",
      "Training Loss: 0.004341471482912311\n",
      "Training Loss: 0.0014244536113983486\n",
      "Training Loss: 0.0014242196443956345\n",
      "Training Loss: 0.001343544525152538\n",
      "Validation Loss: 0.0022614164729620077\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.006348440170986578\n",
      "Training Loss: 0.0064805730909574776\n",
      "Training Loss: 0.006499626892618835\n",
      "Training Loss: 0.004339264750014991\n",
      "Training Loss: 0.0014208208536729216\n",
      "Training Loss: 0.0014206418517278508\n",
      "Training Loss: 0.0013400961151637603\n",
      "Validation Loss: 0.002257714639914881\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.0063470196549315\n",
      "Training Loss: 0.006479108256753534\n",
      "Training Loss: 0.0064986317028524355\n",
      "Training Loss: 0.004337036707729567\n",
      "Training Loss: 0.0014171271773375338\n",
      "Training Loss: 0.0014170019863377092\n",
      "Training Loss: 0.0013365856026939583\n",
      "Validation Loss: 0.0022539310187819914\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.006345593346050009\n",
      "Training Loss: 0.006477639439981431\n",
      "Training Loss: 0.00649764385598246\n",
      "Training Loss: 0.004334786897088634\n",
      "Training Loss: 0.0014133713978662853\n",
      "Training Loss: 0.0014132969870843226\n",
      "Training Loss: 0.0013330077852879184\n",
      "Validation Loss: 0.0022500562781340716\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.006344164962647483\n",
      "Training Loss: 0.006476169492816553\n",
      "Training Loss: 0.006496666159946472\n",
      "Training Loss: 0.004332514473499032\n",
      "Training Loss: 0.0014095489873579936\n",
      "Training Loss: 0.0014095223649928813\n",
      "Training Loss: 0.0013293594363494776\n",
      "Validation Loss: 0.0022460955915857483\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.006342731805052609\n",
      "Training Loss: 0.006474696175428107\n",
      "Training Loss: 0.006495696236379444\n",
      "Training Loss: 0.004330218222312396\n",
      "Training Loss: 0.001405659437077702\n",
      "Training Loss: 0.0014056775594508508\n",
      "Training Loss: 0.0013256405577703845\n",
      "Validation Loss: 0.0022420405517271213\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.006341294209705666\n",
      "Training Loss: 0.006473218771861866\n",
      "Training Loss: 0.006494736518943682\n",
      "Training Loss: 0.004327897415059852\n",
      "Training Loss: 0.0014016988170624245\n",
      "Training Loss: 0.0014017580837389688\n",
      "Training Loss: 0.0013218432322901208\n",
      "Validation Loss: 0.0022378880940447476\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.006339854169636965\n",
      "Training Loss: 0.006471739517292008\n",
      "Training Loss: 0.006493787731742486\n",
      "Training Loss: 0.004325551745423582\n",
      "Training Loss: 0.0013976653624558821\n",
      "Training Loss: 0.0013977609333960571\n",
      "Training Loss: 0.0013179656534339302\n",
      "Validation Loss: 0.002233635276108666\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.006338411084143445\n",
      "Training Loss: 0.006470257859909907\n",
      "Training Loss: 0.006492850953363814\n",
      "Training Loss: 0.004323179969505872\n",
      "Training Loss: 0.0013935532425239216\n",
      "Training Loss: 0.0013936816033674403\n",
      "Training Loss: 0.0013140032638330012\n",
      "Validation Loss: 0.0022292791019683906\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.006336965948576107\n",
      "Training Loss: 0.006468773406231776\n",
      "Training Loss: 0.00649192798009608\n",
      "Training Loss: 0.004320780247071525\n",
      "Training Loss: 0.0013893596117850392\n",
      "Training Loss: 0.0013895170555770163\n",
      "Training Loss: 0.0013099500286625698\n",
      "Validation Loss: 0.0022248084416433347\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.006335519900312647\n",
      "Training Loss: 0.006467289186548442\n",
      "Training Loss: 0.006491020155954175\n",
      "Training Loss: 0.004318354326969711\n",
      "Training Loss: 0.0013850825739064022\n",
      "Training Loss: 0.0013852646764280507\n",
      "Training Loss: 0.0013058043527416885\n",
      "Validation Loss: 0.0022202231601876473\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.006334073985926807\n",
      "Training Loss: 0.006465802396414802\n",
      "Training Loss: 0.0064901288348482924\n",
      "Training Loss: 0.004315900015935768\n",
      "Training Loss: 0.0013807195871049771\n",
      "Training Loss: 0.0013809211539046373\n",
      "Training Loss: 0.0013015617123164703\n",
      "Validation Loss: 0.002215522318648537\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.0063326276035513725\n",
      "Training Loss: 0.006464315477060154\n",
      "Training Loss: 0.006489253837498836\n",
      "Training Loss: 0.004313415306969545\n",
      "Training Loss: 0.0013762674525787588\n",
      "Training Loss: 0.0013764817485935055\n",
      "Training Loss: 0.001297216444509104\n",
      "Validation Loss: 0.002210700668308235\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.006331183854490519\n",
      "Training Loss: 0.006462830265518278\n",
      "Training Loss: 0.00648840079200454\n",
      "Training Loss: 0.004310902703291503\n",
      "Training Loss: 0.0013717218054807745\n",
      "Training Loss: 0.001371944409693242\n",
      "Training Loss: 0.0012927660350396763\n",
      "Validation Loss: 0.002205751284417149\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.006329742472153157\n",
      "Training Loss: 0.006461344349663705\n",
      "Training Loss: 0.006487567735021003\n",
      "Training Loss: 0.004308360028662719\n",
      "Training Loss: 0.0013670824593282305\n",
      "Training Loss: 0.0013673062145971927\n",
      "Training Loss: 0.001288207867910387\n",
      "Validation Loss: 0.0022006729741542415\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.006328304911730811\n",
      "Training Loss: 0.006459861050825566\n",
      "Training Loss: 0.006486757989623584\n",
      "Training Loss: 0.004305787545599742\n",
      "Training Loss: 0.0013623452682077187\n",
      "Training Loss: 0.00136256374331424\n",
      "Training Loss: 0.0012835334513511044\n",
      "Validation Loss: 0.002195462505686296\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.0063268748647533355\n",
      "Training Loss: 0.006458383384160698\n",
      "Training Loss: 0.006485976189142093\n",
      "Training Loss: 0.004303184795571724\n",
      "Training Loss: 0.001357507704597083\n",
      "Training Loss: 0.0013577144021110143\n",
      "Training Loss: 0.0012787414879130666\n",
      "Validation Loss: 0.0021901152141162016\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.006325453107710928\n",
      "Training Loss: 0.006456911563873291\n",
      "Training Loss: 0.006485223021009005\n",
      "Training Loss: 0.0043005513680691365\n",
      "Training Loss: 0.0013525667010981124\n",
      "Training Loss: 0.0013527539442293346\n",
      "Training Loss: 0.0012738268901011907\n",
      "Validation Loss: 0.002184623007271055\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.006324042225023732\n",
      "Training Loss: 0.006455447500338778\n",
      "Training Loss: 0.006484501555096358\n",
      "Training Loss: 0.004297888843284454\n",
      "Training Loss: 0.0013475231941993116\n",
      "Training Loss: 0.0013476826969417743\n",
      "Training Loss: 0.0012687870324589311\n",
      "Validation Loss: 0.0021789964111452076\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.006322645224863663\n",
      "Training Loss: 0.006453993158647791\n",
      "Training Loss: 0.0064838147512637076\n",
      "Training Loss: 0.004295195836748462\n",
      "Training Loss: 0.0013423724445601692\n",
      "Training Loss: 0.001342497283621924\n",
      "Training Loss: 0.0012636196437233592\n",
      "Validation Loss: 0.002173226664177655\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.0063212629803456365\n",
      "Training Loss: 0.006452550239628181\n",
      "Training Loss: 0.00648316468053963\n",
      "Training Loss: 0.004292472943488974\n",
      "Training Loss: 0.0013371164532145485\n",
      "Training Loss: 0.0013371964224643306\n",
      "Training Loss: 0.001258320212655235\n",
      "Validation Loss: 0.0021673112744482912\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.0063199010700918734\n",
      "Training Loss: 0.006451122573344037\n",
      "Training Loss: 0.006482555271941237\n",
      "Training Loss: 0.004289721565437503\n",
      "Training Loss: 0.0013317535493115428\n",
      "Training Loss: 0.0013317818396171787\n",
      "Training Loss: 0.0012528901830955875\n",
      "Validation Loss: 0.0021612507663710803\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.006318557949271053\n",
      "Training Loss: 0.006449708018917591\n",
      "Training Loss: 0.006481986814760603\n",
      "Training Loss: 0.0042869411583524196\n",
      "Training Loss: 0.0013262862135889008\n",
      "Training Loss: 0.0013262519903946667\n",
      "Training Loss: 0.0012473266629967838\n",
      "Validation Loss: 0.002155052896280344\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.006317240166245028\n",
      "Training Loss: 0.006448311005951837\n",
      "Training Loss: 0.00648146364314016\n",
      "Training Loss: 0.004284133704932174\n",
      "Training Loss: 0.0013207148580113425\n",
      "Training Loss: 0.0013206101099785884\n",
      "Training Loss: 0.0012416314722213427\n",
      "Validation Loss: 0.0021487150987878805\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.006315948945702985\n",
      "Training Loss: 0.006446931292302906\n",
      "Training Loss: 0.006480984815279953\n",
      "Training Loss: 0.004281298732094001\n",
      "Training Loss: 0.0013150426019274163\n",
      "Training Loss: 0.0013148559436376673\n",
      "Training Loss: 0.0012358035852957983\n",
      "Validation Loss: 0.0021422415046755587\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.006314687561243772\n",
      "Training Loss: 0.006445571574149653\n",
      "Training Loss: 0.00648055334109813\n",
      "Training Loss: 0.004278438034671126\n",
      "Training Loss: 0.00130927065765718\n",
      "Training Loss: 0.0013089943520026282\n",
      "Training Loss: 0.0012298455600102897\n",
      "Validation Loss: 0.0021356415216748216\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.006313459675293415\n",
      "Training Loss: 0.006444230165798217\n",
      "Training Loss: 0.0064801687799626966\n",
      "Training Loss: 0.004275550233141984\n",
      "Training Loss: 0.0013034053953015245\n",
      "Training Loss: 0.0013030302531842608\n",
      "Training Loss: 0.0012237624221597798\n",
      "Validation Loss: 0.0021289153826163127\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.006312264512525871\n",
      "Training Loss: 0.006442905615549535\n",
      "Training Loss: 0.006479827386792749\n",
      "Training Loss: 0.004272637439571554\n",
      "Training Loss: 0.0012974541395669803\n",
      "Training Loss: 0.0012969702668488025\n",
      "Training Loss: 0.0012175591888080816\n",
      "Validation Loss: 0.0021220774168309088\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.006311105405911803\n",
      "Training Loss: 0.006441595400683581\n",
      "Training Loss: 0.006479527752962895\n",
      "Training Loss: 0.004269698223361047\n",
      "Training Loss: 0.00129142266465351\n",
      "Training Loss: 0.0012908208072622074\n",
      "Training Loss: 0.0012112406802771147\n",
      "Validation Loss: 0.002115138226397133\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.006309980992227793\n",
      "Training Loss: 0.00644029542338103\n",
      "Training Loss: 0.006479263875517063\n",
      "Training Loss: 0.004266732493852033\n",
      "Training Loss: 0.001285317837027833\n",
      "Training Loss: 0.001284589964125189\n",
      "Training Loss: 0.001204814765806077\n",
      "Validation Loss: 0.0021080994427645377\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.006308891237713396\n",
      "Training Loss: 0.006438999538077041\n",
      "Training Loss: 0.006479028186295182\n",
      "Training Loss: 0.004263738128065598\n",
      "Training Loss: 0.0012791512596595567\n",
      "Training Loss: 0.0012782858426362508\n",
      "Training Loss: 0.0011982889968203381\n",
      "Validation Loss: 0.002100979117611214\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.006307833507889882\n",
      "Training Loss: 0.006437700246460736\n",
      "Training Loss: 0.006478811582201161\n",
      "Training Loss: 0.004260712845862145\n",
      "Training Loss: 0.001272927572572371\n",
      "Training Loss: 0.0012719182563887444\n",
      "Training Loss: 0.0011916686520271468\n",
      "Validation Loss: 0.0020937787811228245\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.006306804225314409\n",
      "Training Loss: 0.006436386596178636\n",
      "Training Loss: 0.00647860253462568\n",
      "Training Loss: 0.00425765154272085\n",
      "Training Loss: 0.0012666579129290768\n",
      "Training Loss: 0.0012654936272156192\n",
      "Training Loss: 0.0011849617451662197\n",
      "Validation Loss: 0.0020865107005094054\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.006305799405090511\n",
      "Training Loss: 0.006435047672130168\n",
      "Training Loss: 0.006478386654052883\n",
      "Training Loss: 0.004254549911129288\n",
      "Training Loss: 0.0012603480031248182\n",
      "Training Loss: 0.001259018307682709\n",
      "Training Loss: 0.0011781719836289994\n",
      "Validation Loss: 0.002079171924402742\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.006304810571018607\n",
      "Training Loss: 0.006433669502148405\n",
      "Training Loss: 0.006478147569578141\n",
      "Training Loss: 0.004251399328932166\n",
      "Training Loss: 0.0012540043698390946\n",
      "Training Loss: 0.0012524994443811012\n",
      "Training Loss: 0.0011713036929722875\n",
      "Validation Loss: 0.002071770331765654\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.006303830366814509\n",
      "Training Loss: 0.0064322354819159955\n",
      "Training Loss: 0.006477866432396695\n",
      "Training Loss: 0.0042481914839299865\n",
      "Training Loss: 0.0012476317018445115\n",
      "Training Loss: 0.0012459350338758668\n",
      "Training Loss: 0.0011643530173023463\n",
      "Validation Loss: 0.0020642980715401176\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.006302850909996778\n",
      "Training Loss: 0.006430730589199811\n",
      "Training Loss: 0.0064775259583257135\n",
      "Training Loss: 0.004244913850416196\n",
      "Training Loss: 0.001241227661666926\n",
      "Training Loss: 0.0012393239756056573\n",
      "Training Loss: 0.0011573126321309246\n",
      "Validation Loss: 0.0020567371074174797\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.0063018625800032166\n",
      "Training Loss: 0.00642913603107445\n",
      "Training Loss: 0.006477105139056221\n",
      "Training Loss: 0.004241550622391514\n",
      "Training Loss: 0.0012347840346046723\n",
      "Training Loss: 0.0012326543421659153\n",
      "Training Loss: 0.001150165750659653\n",
      "Validation Loss: 0.0020490720288650122\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.006300859345356002\n",
      "Training Loss: 0.00642743724398315\n",
      "Training Loss: 0.006476584076299332\n",
      "Training Loss: 0.004238084970565979\n",
      "Training Loss: 0.001228290756844217\n",
      "Training Loss: 0.0012259117264329688\n",
      "Training Loss: 0.0011428908468951705\n",
      "Validation Loss: 0.0020412659432350096\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.006299828727496788\n",
      "Training Loss: 0.006425613979808986\n",
      "Training Loss: 0.006475942449760623\n",
      "Training Loss: 0.004234493669791846\n",
      "Training Loss: 0.00122172490999219\n",
      "Training Loss: 0.0012190664017543895\n",
      "Training Loss: 0.0011354498763830633\n",
      "Validation Loss: 0.0020332764399243163\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.006298767774133011\n",
      "Training Loss: 0.006423652339726686\n",
      "Training Loss: 0.006475162385613658\n",
      "Training Loss: 0.004230748830013909\n",
      "Training Loss: 0.001215053671039641\n",
      "Training Loss: 0.0012120784643047955\n",
      "Training Loss: 0.0011277908798365387\n",
      "Validation Loss: 0.0020250440615513588\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.00629767359001562\n",
      "Training Loss: 0.006421534979017452\n",
      "Training Loss: 0.006474222172982991\n",
      "Training Loss: 0.004226816389273153\n",
      "Training Loss: 0.00120823471879703\n",
      "Training Loss: 0.001204895050614141\n",
      "Training Loss: 0.0011198461832827888\n",
      "Validation Loss: 0.0020164892037986856\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.00629654020536691\n",
      "Training Loss: 0.0064192445133812726\n",
      "Training Loss: 0.00647310302068945\n",
      "Training Loss: 0.0042226502182893455\n",
      "Training Loss: 0.001201202182128327\n",
      "Training Loss: 0.0011974378579179756\n",
      "Training Loss: 0.0011115138199238573\n",
      "Validation Loss: 0.002007506520937333\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.00629537244560197\n",
      "Training Loss: 0.0064167655899655075\n",
      "Training Loss: 0.006471782636363059\n",
      "Training Loss: 0.004218193078559125\n",
      "Training Loss: 0.0011938711283437443\n",
      "Training Loss: 0.0011896005924063502\n",
      "Training Loss: 0.0011026657326874557\n",
      "Validation Loss: 0.0019979541892384627\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.006294175735674799\n",
      "Training Loss: 0.006414079474052414\n",
      "Training Loss: 0.0064702379034133626\n",
      "Training Loss: 0.004213366061885608\n",
      "Training Loss: 0.0011861206250614487\n",
      "Training Loss: 0.0011812367290986003\n",
      "Training Loss: 0.0010931116106075933\n",
      "Validation Loss: 0.001987635286844053\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.006292962834704667\n",
      "Training Loss: 0.006411166517063975\n",
      "Training Loss: 0.006468440932221711\n",
      "Training Loss: 0.00420806041409378\n",
      "Training Loss: 0.0011777797242393717\n",
      "Training Loss: 0.0011721388425939949\n",
      "Training Loss: 0.0010825912334257737\n",
      "Validation Loss: 0.001976289648509476\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.00629175468115136\n",
      "Training Loss: 0.006407999703660607\n",
      "Training Loss: 0.0064663523342460396\n",
      "Training Loss: 0.004202122160349973\n",
      "Training Loss: 0.001168605355778709\n",
      "Training Loss: 0.0011620087659684941\n",
      "Training Loss: 0.0010707299527712166\n",
      "Validation Loss: 0.0019635526707120353\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.006290581935318187\n",
      "Training Loss: 0.006404544892720878\n",
      "Training Loss: 0.006463919642847031\n",
      "Training Loss: 0.0041953292634570975\n",
      "Training Loss: 0.001158246367413085\n",
      "Training Loss: 0.0011504056481498992\n",
      "Training Loss: 0.0010569685472728452\n",
      "Validation Loss: 0.0019488738923772214\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.006289492798969149\n",
      "Training Loss: 0.006400754100177437\n",
      "Training Loss: 0.006461060957517475\n",
      "Training Loss: 0.004187344561651116\n",
      "Training Loss: 0.0011461634625447915\n",
      "Training Loss: 0.001136656577582471\n",
      "Training Loss: 0.0010404632201243657\n",
      "Validation Loss: 0.0019314776408317768\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.006288558481028304\n",
      "Training Loss: 0.006396552725927904\n",
      "Training Loss: 0.006457649852382019\n",
      "Training Loss: 0.004177654797531432\n",
      "Training Loss: 0.0011315278442634736\n",
      "Training Loss: 0.0011197077539691236\n",
      "Training Loss: 0.0010199012864904944\n",
      "Validation Loss: 0.0019101888143125232\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.006287897796137258\n",
      "Training Loss: 0.006391846219776199\n",
      "Training Loss: 0.006453487337566912\n",
      "Training Loss: 0.004165462174423738\n",
      "Training Loss: 0.00111302469696966\n",
      "Training Loss: 0.0010978816286660732\n",
      "Training Loss: 0.0009932631564151962\n",
      "Validation Loss: 0.0018833110423772579\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.006287704254500568\n",
      "Training Loss: 0.006386513733305037\n",
      "Training Loss: 0.0064482568390667435\n",
      "Training Loss: 0.004149570230438258\n",
      "Training Loss: 0.0010886282133287751\n",
      "Training Loss: 0.0010685937737434869\n",
      "Training Loss: 0.0009576734971778933\n",
      "Validation Loss: 0.0018487396728062535\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.006288313633995131\n",
      "Training Loss: 0.006380460924701765\n",
      "Training Loss: 0.006441477180924266\n",
      "Training Loss: 0.004128428366748267\n",
      "Training Loss: 0.001055605391447898\n",
      "Training Loss: 0.0010284775832406013\n",
      "Training Loss: 0.0009101936964725609\n",
      "Validation Loss: 0.0018051926960382953\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.006290252709295601\n",
      "Training Loss: 0.006373759090201929\n",
      "Training Loss: 0.00643251474481076\n",
      "Training Loss: 0.004100921332646976\n",
      "Training Loss: 0.001011845718603581\n",
      "Training Loss: 0.0009755164288071682\n",
      "Training Loss: 0.0008514180940255756\n",
      "Validation Loss: 0.0017556064759013993\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.006293782765278593\n",
      "Training Loss: 0.0063665324659086765\n",
      "Training Loss: 0.006420613244408742\n",
      "Training Loss: 0.004068321072336403\n",
      "Training Loss: 0.0009596762082219357\n",
      "Training Loss: 0.000914298854622757\n",
      "Training Loss: 0.0007899867587548215\n",
      "Validation Loss: 0.001708336945085556\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.006296843932941556\n",
      "Training Loss: 0.006357712103053927\n",
      "Training Loss: 0.006404831124236807\n",
      "Training Loss: 0.004034642971382709\n",
      "Training Loss: 0.0009073451651784126\n",
      "Training Loss: 0.0008562114770029438\n",
      "Training Loss: 0.0007366883433860494\n",
      "Validation Loss: 0.001668941678838152\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.0062946942611597475\n",
      "Training Loss: 0.0063448570040054615\n",
      "Training Loss: 0.006384719299385324\n",
      "Training Loss: 0.004003014270165295\n",
      "Training Loss: 0.000861814277159283\n",
      "Training Loss: 0.000808443871756026\n",
      "Training Loss: 0.0006943240428518038\n",
      "Validation Loss: 0.001636639292232434\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0062852739833761\n",
      "Training Loss: 0.006327238797675818\n",
      "Training Loss: 0.006361234298674389\n",
      "Training Loss: 0.003974118322657887\n",
      "Training Loss: 0.0008240920671232743\n",
      "Training Loss: 0.0007703667014357052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [1:05:44<16:15, 487.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0006605005991150392\n",
      "Validation Loss: 0.001609329780450785\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "Training Loss: 0.20113282516598702\n",
      "Training Loss: 0.147364467009902\n",
      "Training Loss: 0.10770275969058275\n",
      "Training Loss: 0.0747669352311641\n",
      "Training Loss: 0.056236245995387436\n",
      "Training Loss: 0.04669303343631327\n",
      "Training Loss: 0.04848110700026154\n",
      "Validation Loss: 0.04719619547132026\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.048720621392130854\n",
      "Training Loss: 0.04699020383879542\n",
      "Training Loss: 0.044552516834810374\n",
      "Training Loss: 0.04079993470804766\n",
      "Training Loss: 0.03757865143939853\n",
      "Training Loss: 0.03248941037338227\n",
      "Training Loss: 0.032763432320207356\n",
      "Validation Loss: 0.03247170602337698\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.034317732909694317\n",
      "Training Loss: 0.03259577163495123\n",
      "Training Loss: 0.029844468189403416\n",
      "Training Loss: 0.02563934669364244\n",
      "Training Loss: 0.02141150784678757\n",
      "Training Loss: 0.018049650469329208\n",
      "Training Loss: 0.01767894845921546\n",
      "Validation Loss: 0.019348959960845694\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.0217029791790992\n",
      "Training Loss: 0.02082800227217376\n",
      "Training Loss: 0.018949986868537962\n",
      "Training Loss: 0.01527233398403041\n",
      "Training Loss: 0.011248845111113041\n",
      "Training Loss: 0.009841501037590206\n",
      "Training Loss: 0.009802405401133\n",
      "Validation Loss: 0.012429698416614409\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.015494217881932854\n",
      "Training Loss: 0.015410360749810934\n",
      "Training Loss: 0.014307909442577512\n",
      "Training Loss: 0.011099745763640385\n",
      "Training Loss: 0.00729769313824363\n",
      "Training Loss: 0.006701333830133081\n",
      "Training Loss: 0.006811103699728847\n",
      "Validation Loss: 0.00914011673726253\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.012798844021745026\n",
      "Training Loss: 0.012862260185647756\n",
      "Training Loss: 0.012007346774917096\n",
      "Training Loss: 0.008971966377284844\n",
      "Training Loss: 0.005160891864215955\n",
      "Training Loss: 0.0047992166469339285\n",
      "Training Loss: 0.004871820046682842\n",
      "Validation Loss: 0.006588495733680322\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.010767542658140884\n",
      "Training Loss: 0.010816760141169652\n",
      "Training Loss: 0.010279218115611001\n",
      "Training Loss: 0.007439299739489797\n",
      "Training Loss: 0.003738817027187906\n",
      "Training Loss: 0.003601076562772505\n",
      "Training Loss: 0.003663066928857006\n",
      "Validation Loss: 0.0051830566561456475\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.009473291847389192\n",
      "Training Loss: 0.009492704762378708\n",
      "Training Loss: 0.009144301610067487\n",
      "Training Loss: 0.006432467571576126\n",
      "Training Loss: 0.0028868810198036956\n",
      "Training Loss: 0.002892330391332507\n",
      "Training Loss: 0.002880819950660225\n",
      "Validation Loss: 0.0042469583269494696\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.008486084036994725\n",
      "Training Loss: 0.008525294854771347\n",
      "Training Loss: 0.008310007688123733\n",
      "Training Loss: 0.005759166071366053\n",
      "Training Loss: 0.0023988319089403376\n",
      "Training Loss: 0.002468370745773427\n",
      "Training Loss: 0.0024140377176809125\n",
      "Validation Loss: 0.003656647906922962\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.007824108492350206\n",
      "Training Loss: 0.007889272749889643\n",
      "Training Loss: 0.007761885228101164\n",
      "Training Loss: 0.005348906483995961\n",
      "Training Loss: 0.0021286829256860072\n",
      "Training Loss: 0.0022106651973444968\n",
      "Training Loss: 0.002140208612254355\n",
      "Validation Loss: 0.0032889852481231456\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.00739303755806759\n",
      "Training Loss: 0.0074728656560182575\n",
      "Training Loss: 0.007400738593423739\n",
      "Training Loss: 0.005088886883604573\n",
      "Training Loss: 0.001965314804110676\n",
      "Training Loss: 0.002038651596812997\n",
      "Training Loss: 0.001966881779662799\n",
      "Validation Loss: 0.0030389537654157855\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.007104534324025735\n",
      "Training Loss: 0.0071943911525886505\n",
      "Training Loss: 0.0071606055519077925\n",
      "Training Loss: 0.00491632927354658\n",
      "Training Loss: 0.0018562831361487043\n",
      "Training Loss: 0.001914339358190773\n",
      "Training Loss: 0.0018475921871140598\n",
      "Validation Loss: 0.0028561425283254727\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.006909307573223487\n",
      "Training Loss: 0.007007816837867722\n",
      "Training Loss: 0.007002128203166649\n",
      "Training Loss: 0.00479835605568951\n",
      "Training Loss: 0.001776752292207675\n",
      "Training Loss: 0.0018193463750503724\n",
      "Training Loss: 0.0017591259749315214\n",
      "Validation Loss: 0.00271705553872591\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.006777660581283271\n",
      "Training Loss: 0.006883814390748739\n",
      "Training Loss: 0.006898786254459992\n",
      "Training Loss: 0.004715797344833845\n",
      "Training Loss: 0.0017146900850639212\n",
      "Training Loss: 0.0017437884764513\n",
      "Training Loss: 0.0016896063974127173\n",
      "Validation Loss: 0.002608899410146076\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.0066899369435850535\n",
      "Training Loss: 0.006802287148311734\n",
      "Training Loss: 0.006832259906223044\n",
      "Training Loss: 0.0046566843491746114\n",
      "Training Loss: 0.0016641945604351348\n",
      "Training Loss: 0.0016820783915318315\n",
      "Training Loss: 0.001632933675427921\n",
      "Validation Loss: 0.002523605466174129\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.0066324257431551814\n",
      "Training Loss: 0.0067491811944637445\n",
      "Training Loss: 0.006789933539694175\n",
      "Training Loss: 0.004613356643676525\n",
      "Training Loss: 0.0016223502314824145\n",
      "Training Loss: 0.001630993901999318\n",
      "Training Loss: 0.0015859409507538657\n",
      "Validation Loss: 0.0024557751025708063\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.006595368179259822\n",
      "Training Loss: 0.0067146720259916035\n",
      "Training Loss: 0.006763154532527551\n",
      "Training Loss: 0.004580828507969272\n",
      "Training Loss: 0.0015876285335980355\n",
      "Training Loss: 0.0015885853966028662\n",
      "Training Loss: 0.001546856977220159\n",
      "Validation Loss: 0.0024016359376229264\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.006571788343135268\n",
      "Training Loss: 0.006691938429139554\n",
      "Training Loss: 0.006746005525346845\n",
      "Training Loss: 0.004555770355800633\n",
      "Training Loss: 0.0015590620177681558\n",
      "Training Loss: 0.0015535274196008687\n",
      "Training Loss: 0.001514495906230877\n",
      "Validation Loss: 0.0023584004947418617\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.006556716475170105\n",
      "Training Loss: 0.006676288350718096\n",
      "Training Loss: 0.0067344567063264546\n",
      "Training Loss: 0.004535877384914784\n",
      "Training Loss: 0.0015358374777133576\n",
      "Training Loss: 0.001524757508741459\n",
      "Training Loss: 0.0014878682000562548\n",
      "Validation Loss: 0.0023238600442301105\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.006546652248362079\n",
      "Training Loss: 0.006664533378789202\n",
      "Training Loss: 0.0067257919756229965\n",
      "Training Loss: 0.004519500906899339\n",
      "Training Loss: 0.0015171621953777503\n",
      "Training Loss: 0.0015013187487056712\n",
      "Training Loss: 0.001466050263188663\n",
      "Validation Loss: 0.00229620010788685\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.006539198255632072\n",
      "Training Loss: 0.0066545649932231755\n",
      "Training Loss: 0.006718240779591724\n",
      "Training Loss: 0.004505452808007249\n",
      "Training Loss: 0.0015022478614991996\n",
      "Training Loss: 0.0014823181121755625\n",
      "Training Loss: 0.0014481734488072107\n",
      "Validation Loss: 0.002273918853938398\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.00653278133017011\n",
      "Training Loss: 0.006645044786855578\n",
      "Training Loss: 0.0067107040085829794\n",
      "Training Loss: 0.004492886197040207\n",
      "Training Loss: 0.0014903536161000376\n",
      "Training Loss: 0.0014669357777165714\n",
      "Training Loss: 0.001433445366055821\n",
      "Validation Loss: 0.002255788127881255\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.006526433504186571\n",
      "Training Loss: 0.006635177045827731\n",
      "Training Loss: 0.006702558506513015\n",
      "Training Loss: 0.004481209988589399\n",
      "Training Loss: 0.0014808171731419861\n",
      "Training Loss: 0.0014544395131088094\n",
      "Training Loss: 0.0014211756002623587\n",
      "Validation Loss: 0.0022408119924405717\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.006519612950505689\n",
      "Training Loss: 0.006624539045151323\n",
      "Training Loss: 0.00669350226293318\n",
      "Training Loss: 0.0044700243357510775\n",
      "Training Loss: 0.0014730704131943639\n",
      "Training Loss: 0.001444195443255012\n",
      "Training Loss: 0.001410777573400992\n",
      "Validation Loss: 0.0022281995105290302\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.006512061600806192\n",
      "Training Loss: 0.006612950871931389\n",
      "Training Loss: 0.006683438936015591\n",
      "Training Loss: 0.004459068129508523\n",
      "Training Loss: 0.0014666439747088589\n",
      "Training Loss: 0.0014356678027979796\n",
      "Training Loss: 0.001401765898699523\n",
      "Validation Loss: 0.002217316942586832\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.0065036970633082095\n",
      "Training Loss: 0.006600381751777604\n",
      "Training Loss: 0.006672394463093951\n",
      "Training Loss: 0.004448173502314603\n",
      "Training Loss: 0.0014611576050810981\n",
      "Training Loss: 0.0014284107527782908\n",
      "Training Loss: 0.0013937428827921394\n",
      "Validation Loss: 0.0022076665208420336\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.006494538774713874\n",
      "Training Loss: 0.0065868827875237915\n",
      "Training Loss: 0.006660456083482131\n",
      "Training Loss: 0.004437237102392828\n",
      "Training Loss: 0.0014563051126606297\n",
      "Training Loss: 0.001422058628013474\n",
      "Training Loss: 0.0013863853767543332\n",
      "Validation Loss: 0.0021988478538162537\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.006484655989333987\n",
      "Training Loss: 0.006572546784300357\n",
      "Training Loss: 0.006647738225292414\n",
      "Training Loss: 0.004426190742815379\n",
      "Training Loss: 0.0014518381230300293\n",
      "Training Loss: 0.0014163012058997992\n",
      "Training Loss: 0.0013794220790441613\n",
      "Validation Loss: 0.00219053631008137\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.006474141598446294\n",
      "Training Loss: 0.006557480975752697\n",
      "Training Loss: 0.00663436051341705\n",
      "Training Loss: 0.004414989878569031\n",
      "Training Loss: 0.0014475472080812323\n",
      "Training Loss: 0.0014108764143747976\n",
      "Training Loss: 0.001372619738758658\n",
      "Validation Loss: 0.0021824552330396868\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006463092467747629\n",
      "Training Loss: 0.00654179115081206\n",
      "Training Loss: 0.006620431984774768\n",
      "Training Loss: 0.004403599002544069\n",
      "Training Loss: 0.0014432538558321539\n",
      "Training Loss: 0.0014055508938326966\n",
      "Training Loss: 0.0013657691881962819\n",
      "Validation Loss: 0.002174362649302668\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006451600641012192\n",
      "Training Loss: 0.006525571568636224\n",
      "Training Loss: 0.006606047520181164\n",
      "Training Loss: 0.004391981239241432\n",
      "Training Loss: 0.0014387900177098346\n",
      "Training Loss: 0.0014001042227027938\n",
      "Training Loss: 0.001358665631414624\n",
      "Validation Loss: 0.002166027322615155\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006439750420395285\n",
      "Training Loss: 0.006508905496448278\n",
      "Training Loss: 0.006591280589345843\n",
      "Training Loss: 0.004380091006460134\n",
      "Training Loss: 0.001433988681383198\n",
      "Training Loss: 0.0013943120837939206\n",
      "Training Loss: 0.0013510973519441905\n",
      "Validation Loss: 0.0021572123258876046\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.0064276170323137195\n",
      "Training Loss: 0.006491858675144613\n",
      "Training Loss: 0.006576181588461623\n",
      "Training Loss: 0.004367865973763401\n",
      "Training Loss: 0.0014286655838077421\n",
      "Training Loss: 0.001387922140347655\n",
      "Training Loss: 0.0013428144666977459\n",
      "Validation Loss: 0.0021476461334622287\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.006415265233954415\n",
      "Training Loss: 0.006474482514895499\n",
      "Training Loss: 0.006560776600381359\n",
      "Training Loss: 0.004355212450100225\n",
      "Training Loss: 0.001422595199837815\n",
      "Training Loss: 0.001380625931196846\n",
      "Training Loss: 0.0013335006324632559\n",
      "Validation Loss: 0.002136992035221748\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.0064027558045927436\n",
      "Training Loss: 0.006456811181269586\n",
      "Training Loss: 0.00654505898244679\n",
      "Training Loss: 0.004341987815278117\n",
      "Training Loss: 0.0014154801111726555\n",
      "Training Loss: 0.0013720145819388562\n",
      "Training Loss: 0.00132272613060195\n",
      "Validation Loss: 0.0021247924603622105\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.006390128338243812\n",
      "Training Loss: 0.006438849557889625\n",
      "Training Loss: 0.006528965083416552\n",
      "Training Loss: 0.004327966926648514\n",
      "Training Loss: 0.0014069018549344038\n",
      "Training Loss: 0.0013615070039668354\n",
      "Training Loss: 0.0013098616497154581\n",
      "Validation Loss: 0.0021103893124153105\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006377404549857602\n",
      "Training Loss: 0.006420565067091957\n",
      "Training Loss: 0.006512347690295428\n",
      "Training Loss: 0.004312783207788016\n",
      "Training Loss: 0.0013962236024963205\n",
      "Training Loss: 0.001348215031903237\n",
      "Training Loss: 0.0012939308604836698\n",
      "Validation Loss: 0.002092778167015025\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006364555477630347\n",
      "Training Loss: 0.0064018506510183214\n",
      "Training Loss: 0.006494901181431487\n",
      "Training Loss: 0.004295823825304979\n",
      "Training Loss: 0.001382433318794938\n",
      "Training Loss: 0.0013307195825473172\n",
      "Training Loss: 0.0012733527217642403\n",
      "Validation Loss: 0.002070368579290162\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006351451253285631\n",
      "Training Loss: 0.0063824496651068326\n",
      "Training Loss: 0.006476012300699949\n",
      "Training Loss: 0.004276031717454316\n",
      "Training Loss: 0.0013638643335434609\n",
      "Training Loss: 0.0013066811577300541\n",
      "Training Loss: 0.0012455310987570555\n",
      "Validation Loss: 0.002040704190037789\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.0063377585145644845\n",
      "Training Loss: 0.006361806261120364\n",
      "Training Loss: 0.0064544523577205835\n",
      "Training Loss: 0.004251621649164008\n",
      "Training Loss: 0.001337873346201377\n",
      "Training Loss: 0.0012724191071174574\n",
      "Training Loss: 0.0012065736462682252\n",
      "Validation Loss: 0.0020006817963592574\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.00632285397965461\n",
      "Training Loss: 0.006338827706640586\n",
      "Training Loss: 0.006427955427207053\n",
      "Training Loss: 0.004220100234306301\n",
      "Training Loss: 0.0013012951976270415\n",
      "Training Loss: 0.0012238891151355347\n",
      "Training Loss: 0.0011534360634686892\n",
      "Validation Loss: 0.001950392836494692\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006306270703207701\n",
      "Training Loss: 0.006311789231840521\n",
      "Training Loss: 0.006393524170853197\n",
      "Training Loss: 0.0041805311824282395\n",
      "Training Loss: 0.0012551596088451334\n",
      "Training Loss: 0.0011642402930010576\n",
      "Training Loss: 0.0010941995171015152\n",
      "Validation Loss: 0.0019035869439774321\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006287998739862815\n",
      "Training Loss: 0.006278068695683032\n",
      "Training Loss: 0.00635033862781711\n",
      "Training Loss: 0.004138090951382765\n",
      "Training Loss: 0.0012118109707080294\n",
      "Training Loss: 0.0011122786327905487\n",
      "Training Loss: 0.001050209823079058\n",
      "Validation Loss: 0.0018769332023742104\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.006263756534317508\n",
      "Training Loss: 0.006234498941339552\n",
      "Training Loss: 0.006302290527382865\n",
      "Training Loss: 0.004098861137463245\n",
      "Training Loss: 0.0011807151731045452\n",
      "Training Loss: 0.0010783062521659303\n",
      "Training Loss: 0.0010238144603499676\n",
      "Validation Loss: 0.00186288616088505\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.0062321372143924235\n",
      "Training Loss: 0.006185218020109459\n",
      "Training Loss: 0.006255609567742795\n",
      "Training Loss: 0.004063709448964801\n",
      "Training Loss: 0.0011572513754072133\n",
      "Training Loss: 0.0010539817593235056\n",
      "Training Loss: 0.0010044350910175127\n",
      "Validation Loss: 0.0018527530963673895\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.0062000315741170195\n",
      "Training Loss: 0.0061374730756506325\n",
      "Training Loss: 0.0062137008283752945\n",
      "Training Loss: 0.004032290170580381\n",
      "Training Loss: 0.0011365501128602774\n",
      "Training Loss: 0.0010329877136973664\n",
      "Training Loss: 0.0009873196887201629\n",
      "Validation Loss: 0.001843849520551299\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006171105788089335\n",
      "Training Loss: 0.0060941213765181605\n",
      "Training Loss: 0.006176834021462127\n",
      "Training Loss: 0.004004278332504327\n",
      "Training Loss: 0.001117034212948056\n",
      "Training Loss: 0.0010134986117918744\n",
      "Training Loss: 0.0009711399433581391\n",
      "Validation Loss: 0.001835137000808855\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006145806645508856\n",
      "Training Loss: 0.006055520614609122\n",
      "Training Loss: 0.006144302519969642\n",
      "Training Loss: 0.003979285049426835\n",
      "Training Loss: 0.0010983268696872983\n",
      "Training Loss: 0.0009950499852129724\n",
      "Training Loss: 0.0009554699451109627\n",
      "Validation Loss: 0.0018262303274513784\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.006123711492400616\n",
      "Training Loss: 0.006021287197945639\n",
      "Training Loss: 0.006115281154634431\n",
      "Training Loss: 0.00395688011587481\n",
      "Training Loss: 0.001080321334884502\n",
      "Training Loss: 0.0009774589640437626\n",
      "Training Loss: 0.0009401528903981671\n",
      "Validation Loss: 0.0018170404804727473\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.006104227194446139\n",
      "Training Loss: 0.005990825390908867\n",
      "Training Loss: 0.006089020177023485\n",
      "Training Loss: 0.0039366284442076\n",
      "Training Loss: 0.001062964941811515\n",
      "Training Loss: 0.0009606092768080998\n",
      "Training Loss: 0.0009251298452727497\n",
      "Validation Loss: 0.001807583249082496\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.006086775207659229\n",
      "Training Loss: 0.005963516301708296\n",
      "Training Loss: 0.006064880475169048\n",
      "Training Loss: 0.003918121893948409\n",
      "Training Loss: 0.0010462097641720902\n",
      "Training Loss: 0.0009444090902979951\n",
      "Training Loss: 0.0009103665462316712\n",
      "Validation Loss: 0.0017978961353647183\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.006070847469964065\n",
      "Training Loss: 0.005938787384657189\n",
      "Training Loss: 0.006042337820399552\n",
      "Training Loss: 0.0039009997886023484\n",
      "Training Loss: 0.0010299988109909463\n",
      "Training Loss: 0.0009287736464466434\n",
      "Training Loss: 0.0008958347991574556\n",
      "Validation Loss: 0.0017880051896457425\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.006056015786598436\n",
      "Training Loss: 0.005916137060848996\n",
      "Training Loss: 0.006020959373563528\n",
      "Training Loss: 0.003884951929721865\n",
      "Training Loss: 0.0010142746508063282\n",
      "Training Loss: 0.0009136284945998341\n",
      "Training Loss: 0.000881503438722575\n",
      "Validation Loss: 0.0017779361632563634\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.00604193554376252\n",
      "Training Loss: 0.005895135794999078\n",
      "Training Loss: 0.006000390365952626\n",
      "Training Loss: 0.0038697157420392613\n",
      "Training Loss: 0.000998975379334297\n",
      "Training Loss: 0.0008989074885903392\n",
      "Training Loss: 0.0008673421023559058\n",
      "Validation Loss: 0.0017676907326644182\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.006028312899870798\n",
      "Training Loss: 0.005875416877679527\n",
      "Training Loss: 0.005980337646324187\n",
      "Training Loss: 0.0038550734923046546\n",
      "Training Loss: 0.0009840441241976805\n",
      "Training Loss: 0.0008845526310324204\n",
      "Training Loss: 0.0008533157868805575\n",
      "Validation Loss: 0.001757275570277299\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.0060149255656870085\n",
      "Training Loss: 0.005856677681440488\n",
      "Training Loss: 0.005960559267550707\n",
      "Training Loss: 0.0038408448105474234\n",
      "Training Loss: 0.0009694251423934475\n",
      "Training Loss: 0.0008705160113458987\n",
      "Training Loss: 0.0008393956276995596\n",
      "Validation Loss: 0.0017466828517427439\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.00600157447042875\n",
      "Training Loss: 0.005838655759580433\n",
      "Training Loss: 0.005940847699530422\n",
      "Training Loss: 0.0038268786740809444\n",
      "Training Loss: 0.000955070950294612\n",
      "Training Loss: 0.0008567577961366623\n",
      "Training Loss: 0.0008255576727970037\n",
      "Validation Loss: 0.0017359120188558245\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.005988107525045052\n",
      "Training Loss: 0.005821135630831122\n",
      "Training Loss: 0.005921028552111238\n",
      "Training Loss: 0.003813051838442334\n",
      "Training Loss: 0.0009409373630478512\n",
      "Training Loss: 0.0008432438748423011\n",
      "Training Loss: 0.0008117831018171273\n",
      "Validation Loss: 0.0017249554900142054\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.005974385190056637\n",
      "Training Loss: 0.005803933091228828\n",
      "Training Loss: 0.005900948601774872\n",
      "Training Loss: 0.0037992620288423494\n",
      "Training Loss: 0.0009269876472535543\n",
      "Training Loss: 0.0008299508644267916\n",
      "Training Loss: 0.0007980593140382553\n",
      "Validation Loss: 0.0017138107534592154\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.005960295299300924\n",
      "Training Loss: 0.005786892614560201\n",
      "Training Loss: 0.00588047789176926\n",
      "Training Loss: 0.003785427792390692\n",
      "Training Loss: 0.0009131907304981723\n",
      "Training Loss: 0.0008168628900602925\n",
      "Training Loss: 0.0007843839285487775\n",
      "Validation Loss: 0.001702481289224155\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.005945736685534939\n",
      "Training Loss: 0.005769884316250682\n",
      "Training Loss: 0.005859503200044855\n",
      "Training Loss: 0.003771485743491212\n",
      "Training Loss: 0.0008995286488061538\n",
      "Training Loss: 0.0008039707176794764\n",
      "Training Loss: 0.0007707651312375674\n",
      "Validation Loss: 0.001690977916303736\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.005930624888278544\n",
      "Training Loss: 0.005752803937066346\n",
      "Training Loss: 0.005837929054396227\n",
      "Training Loss: 0.00375738837843528\n",
      "Training Loss: 0.0008859870727610541\n",
      "Training Loss: 0.0007912768724781927\n",
      "Training Loss: 0.0007572231855010614\n",
      "Validation Loss: 0.0016793223490985634\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.005914886752143502\n",
      "Training Loss: 0.005735570518299937\n",
      "Training Loss: 0.005815681248204783\n",
      "Training Loss: 0.003743106285983231\n",
      "Training Loss: 0.0008725631327251903\n",
      "Training Loss: 0.0007787911799096036\n",
      "Training Loss: 0.0007437923402903834\n",
      "Validation Loss: 0.0016675565352755833\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.005898466621292755\n",
      "Training Loss: 0.005718121613608673\n",
      "Training Loss: 0.005792708327062428\n",
      "Training Loss: 0.0037286335391399916\n",
      "Training Loss: 0.0008592686148767826\n",
      "Training Loss: 0.0007665404758881777\n",
      "Training Loss: 0.0007305265559261897\n",
      "Validation Loss: 0.0016557436234976972\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.005881328553659842\n",
      "Training Loss: 0.005700428253039718\n",
      "Training Loss: 0.005768990161595866\n",
      "Training Loss: 0.0037139835601556117\n",
      "Training Loss: 0.0008461266155063641\n",
      "Training Loss: 0.0007545551247312688\n",
      "Training Loss: 0.0007174986555764918\n",
      "Validation Loss: 0.0016439771235001374\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.005863464685389772\n",
      "Training Loss: 0.005682480741525069\n",
      "Training Loss: 0.005744541585445404\n",
      "Training Loss: 0.00369919723860221\n",
      "Training Loss: 0.0008331818645092426\n",
      "Training Loss: 0.0007428875115874689\n",
      "Training Loss: 0.0007048053336620796\n",
      "Validation Loss: 0.0016323881348497356\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.005844897944480181\n",
      "Training Loss: 0.005664299755590036\n",
      "Training Loss: 0.005719424632843584\n",
      "Training Loss: 0.0036843449254229197\n",
      "Training Loss: 0.0008205008445656859\n",
      "Training Loss: 0.0007316079844895284\n",
      "Training Loss: 0.0006925682527071331\n",
      "Validation Loss: 0.0016211410690088537\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.0058256874908693135\n",
      "Training Loss: 0.005645935991196893\n",
      "Training Loss: 0.005693755886750296\n",
      "Training Loss: 0.003669526475132443\n",
      "Training Loss: 0.0008081678784947144\n",
      "Training Loss: 0.0007207930058939383\n",
      "Training Loss: 0.000680923724139575\n",
      "Validation Loss: 0.0016104359945373031\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.0058059512544423346\n",
      "Training Loss: 0.005627468737657182\n",
      "Training Loss: 0.005667712711729109\n",
      "Training Loss: 0.0036548708862392232\n",
      "Training Loss: 0.0007962879912520294\n",
      "Training Loss: 0.0007105323681753362\n",
      "Training Loss: 0.00067001779905695\n",
      "Validation Loss: 0.0016004945643547895\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.005785859532188624\n",
      "Training Loss: 0.005609006437589414\n",
      "Training Loss: 0.005641538447234779\n",
      "Training Loss: 0.0036405299929901957\n",
      "Training Loss: 0.0007849856202665251\n",
      "Training Loss: 0.000700919618102489\n",
      "Training Loss: 0.0006599960654421011\n",
      "Validation Loss: 0.0015915392768091421\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.005765628010849468\n",
      "Training Loss: 0.005590669472585432\n",
      "Training Loss: 0.005615526498295367\n",
      "Training Loss: 0.0036266650819743516\n",
      "Training Loss: 0.0007743889794073766\n",
      "Training Loss: 0.0006920402245305013\n",
      "Training Loss: 0.0006509766540693817\n",
      "Validation Loss: 0.0015837577199855513\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.005745517135364935\n",
      "Training Loss: 0.005572592097450979\n",
      "Training Loss: 0.005590008909348398\n",
      "Training Loss: 0.0036134279347606935\n",
      "Training Loss: 0.0007646152764937142\n",
      "Training Loss: 0.00068395406131458\n",
      "Training Loss: 0.0006430310988798738\n",
      "Validation Loss: 0.001577274255696091\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.005725811849115417\n",
      "Training Loss: 0.005554912218358367\n",
      "Training Loss: 0.005565324185881764\n",
      "Training Loss: 0.003600943931232905\n",
      "Training Loss: 0.0007557531794009265\n",
      "Training Loss: 0.0006766916553897317\n",
      "Training Loss: 0.0006361728991760173\n",
      "Validation Loss: 0.0015721211180135379\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.0057067758141784\n",
      "Training Loss: 0.005537752525415271\n",
      "Training Loss: 0.005541779361665249\n",
      "Training Loss: 0.0035892928519751875\n",
      "Training Loss: 0.0007478502234880579\n",
      "Training Loss: 0.0006702391018916387\n",
      "Training Loss: 0.0006303509577264777\n",
      "Validation Loss: 0.0015682356692270692\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.0056886349897831675\n",
      "Training Loss: 0.005521218987414613\n",
      "Training Loss: 0.005519617268582806\n",
      "Training Loss: 0.0035785022798518183\n",
      "Training Loss: 0.0007408985093934462\n",
      "Training Loss: 0.0006645471906085731\n",
      "Training Loss: 0.0006254533871106105\n",
      "Validation Loss: 0.0015654746536339882\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.005671552991843782\n",
      "Training Loss: 0.005505394451902248\n",
      "Training Loss: 0.0054989977797959\n",
      "Training Loss: 0.0035685528710018843\n",
      "Training Loss: 0.0007348416965396609\n",
      "Training Loss: 0.0006595323688816279\n",
      "Training Loss: 0.0006213392377685523\n",
      "Validation Loss: 0.0015636458100222596\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.0056556135352002455\n",
      "Training Loss: 0.005490328444284387\n",
      "Training Loss: 0.005479988952865824\n",
      "Training Loss: 0.0035593923517444636\n",
      "Training Loss: 0.0007295844941108953\n",
      "Training Loss: 0.0006550933216931299\n",
      "Training Loss: 0.000617850399503368\n",
      "Validation Loss: 0.0015625437420325472\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.005640833386569284\n",
      "Training Loss: 0.005476047359406948\n",
      "Training Loss: 0.005462580132298171\n",
      "Training Loss: 0.0035509457348962313\n",
      "Training Loss: 0.0007250082994869444\n",
      "Training Loss: 0.0006511256044905167\n",
      "Training Loss: 0.0006148342143569608\n",
      "Validation Loss: 0.0015619765521416942\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.005627180964220315\n",
      "Training Loss: 0.005462554662954062\n",
      "Training Loss: 0.005446701630717143\n",
      "Training Loss: 0.003543134351930348\n",
      "Training Loss: 0.0007209947139199357\n",
      "Training Loss: 0.0006475286967179272\n",
      "Training Loss: 0.0006121621638158104\n",
      "Validation Loss: 0.0015617760088188358\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.005614580193068832\n",
      "Training Loss: 0.00544983032217715\n",
      "Training Loss: 0.005432243520626798\n",
      "Training Loss: 0.003535880561976228\n",
      "Training Loss: 0.0007174230882083066\n",
      "Training Loss: 0.0006442095273087034\n",
      "Training Loss: 0.0006097209767176537\n",
      "Validation Loss: 0.0015618088051296548\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "INFO: Validation loss did not improve in epoch 80\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.0056029503309400755\n",
      "Training Loss: 0.005437846870627254\n",
      "Training Loss: 0.005419073831290006\n",
      "Training Loss: 0.0035291138049797157\n",
      "Training Loss: 0.0007141915909596719\n",
      "Training Loss: 0.0006410925230011344\n",
      "Training Loss: 0.0006074275902210502\n",
      "Validation Loss: 0.0015619723654957313\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "INFO: Validation loss did not improve in epoch 81\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.0055921908421441915\n",
      "Training Loss: 0.005426556904567405\n",
      "Training Loss: 0.005407052362570539\n",
      "Training Loss: 0.003522772251890274\n",
      "Training Loss: 0.0007112194315413944\n",
      "Training Loss: 0.0006381204278295627\n",
      "Training Loss: 0.0006052195977827068\n",
      "Validation Loss: 0.0015621895474348607\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "INFO: Validation loss did not improve in epoch 82\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.00558220901642926\n",
      "Training Loss: 0.0054159170505590735\n",
      "Training Loss: 0.0053960418479982765\n",
      "Training Loss: 0.003516799997887574\n",
      "Training Loss: 0.0007084308893536218\n",
      "Training Loss: 0.0006352420110488311\n",
      "Training Loss: 0.0006030495020240778\n",
      "Validation Loss: 0.0015623999486175063\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "INFO: Validation loss did not improve in epoch 83\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.005572907130699605\n",
      "Training Loss: 0.005405870339018293\n",
      "Training Loss: 0.005385911173652858\n",
      "Training Loss: 0.003511149078112794\n",
      "Training Loss: 0.0007057815518055577\n",
      "Training Loss: 0.0006324271935591241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [1:08:40<06:30, 390.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0006008896566345357\n",
      "Validation Loss: 0.0015625660448319414\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "INFO: Validation loss did not improve in epoch 84\n",
      "Early stopping after 84 epochs\n",
      "Epoch: 1\n",
      "Training Loss: 0.08290100384503603\n",
      "Training Loss: 0.07200247367843986\n",
      "Training Loss: 0.06685264399275184\n",
      "Training Loss: 0.06284182167612017\n",
      "Training Loss: 0.06161526652984321\n",
      "Training Loss: 0.05607767396606505\n",
      "Training Loss: 0.06032164014875889\n",
      "Validation Loss: 0.05885862873009082\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "Training Loss: 0.05963106416165829\n",
      "Training Loss: 0.05775717373937368\n",
      "Training Loss: 0.0555050079524517\n",
      "Training Loss: 0.05144787055440247\n",
      "Training Loss: 0.04783427219837904\n",
      "Training Loss: 0.04099573766812682\n",
      "Training Loss: 0.04083247072063387\n",
      "Validation Loss: 0.038619783138402836\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "Training Loss: 0.039723276123404504\n",
      "Training Loss: 0.036766360700130465\n",
      "Training Loss: 0.032978397011756894\n",
      "Training Loss: 0.027943903491832317\n",
      "Training Loss: 0.022683791294693948\n",
      "Training Loss: 0.018886227151378988\n",
      "Training Loss: 0.01806659787427634\n",
      "Validation Loss: 0.018262528580672732\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "Training Loss: 0.021430464033037423\n",
      "Training Loss: 0.020506040155887605\n",
      "Training Loss: 0.01840517332777381\n",
      "Training Loss: 0.01469463546294719\n",
      "Training Loss: 0.010079523713793606\n",
      "Training Loss: 0.00874063586234115\n",
      "Training Loss: 0.008322672605281696\n",
      "Validation Loss: 0.009632503809599199\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "Training Loss: 0.013540377279277892\n",
      "Training Loss: 0.01315242080250755\n",
      "Training Loss: 0.012020258978009225\n",
      "Training Loss: 0.008665390280075371\n",
      "Training Loss: 0.004706093287677504\n",
      "Training Loss: 0.004331295901210979\n",
      "Training Loss: 0.0041904266539495435\n",
      "Validation Loss: 0.0057465309176457985\n",
      "Validation Accuracy: 0.11704119850187265\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "Training Loss: 0.009572147376602516\n",
      "Training Loss: 0.009336153932381422\n",
      "Training Loss: 0.008993694515665993\n",
      "Training Loss: 0.006368662918393966\n",
      "Training Loss: 0.0033088397578103466\n",
      "Training Loss: 0.003202031747205183\n",
      "Training Loss: 0.0032198302441975102\n",
      "Validation Loss: 0.004661226786162063\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "Training Loss: 0.008222668456146493\n",
      "Training Loss: 0.008112311613513157\n",
      "Training Loss: 0.008049676734954119\n",
      "Training Loss: 0.005718368013622239\n",
      "Training Loss: 0.0028696666666655803\n",
      "Training Loss: 0.0027814339255564846\n",
      "Training Loss: 0.002793027220031945\n",
      "Validation Loss: 0.004084429395828075\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "Training Loss: 0.007588351335143671\n",
      "Training Loss: 0.007513674795627594\n",
      "Training Loss: 0.0075316947163082655\n",
      "Training Loss: 0.0053350571008922995\n",
      "Training Loss: 0.002563127351750154\n",
      "Training Loss: 0.00249631784754456\n",
      "Training Loss: 0.0024941858461534137\n",
      "Validation Loss: 0.003663478727663487\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "Training Loss: 0.007187781793763861\n",
      "Training Loss: 0.007145083883078769\n",
      "Training Loss: 0.007197518448811025\n",
      "Training Loss: 0.005077248746965779\n",
      "Training Loss: 0.002334603440540377\n",
      "Training Loss: 0.002285381808615057\n",
      "Training Loss: 0.002272931297920877\n",
      "Validation Loss: 0.0033493604430796277\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "Training Loss: 0.006920347326667979\n",
      "Training Loss: 0.00691050321678631\n",
      "Training Loss: 0.006980236684903503\n",
      "Training Loss: 0.004902131457056385\n",
      "Training Loss: 0.002164128782460466\n",
      "Training Loss: 0.002126919231086504\n",
      "Training Loss: 0.0021076389857626054\n",
      "Validation Loss: 0.00311507872113529\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Training Loss: 0.006744720285059885\n",
      "Training Loss: 0.006765361821744591\n",
      "Training Loss: 0.006843251659302041\n",
      "Training Loss: 0.0047843617358012125\n",
      "Training Loss: 0.0020373513498634565\n",
      "Training Loss: 0.002007663184631383\n",
      "Training Loss: 0.0019836697439313865\n",
      "Validation Loss: 0.0029401933038145068\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "Training Loss: 0.006633886138442904\n",
      "Training Loss: 0.0066789358237292614\n",
      "Training Loss: 0.006759094431763515\n",
      "Training Loss: 0.004705144944455242\n",
      "Training Loss: 0.001943230304896133\n",
      "Training Loss: 0.001917953248921549\n",
      "Training Loss: 0.0018902730924310162\n",
      "Validation Loss: 0.002808903679051105\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 13\n",
      "Training Loss: 0.006566046938532963\n",
      "Training Loss: 0.00662751181749627\n",
      "Training Loss: 0.00670654131565243\n",
      "Training Loss: 0.00465035840854398\n",
      "Training Loss: 0.0018731269463023636\n",
      "Training Loss: 0.0018502418755088\n",
      "Training Loss: 0.0018192883640585932\n",
      "Validation Loss: 0.002709034130858843\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 14\n",
      "Training Loss: 0.00652404593070969\n",
      "Training Loss: 0.006594313430832699\n",
      "Training Loss: 0.006670999927446246\n",
      "Training Loss: 0.004610312043660088\n",
      "Training Loss: 0.0018203930802701508\n",
      "Training Loss: 0.001798630251432769\n",
      "Training Loss: 0.001764561596501153\n",
      "Validation Loss: 0.002631511399151813\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 15\n",
      "Training Loss: 0.006496179207460955\n",
      "Training Loss: 0.006569343986921012\n",
      "Training Loss: 0.006643912702566012\n",
      "Training Loss: 0.004579077911039349\n",
      "Training Loss: 0.001780150797130773\n",
      "Training Loss: 0.0017587112552428152\n",
      "Training Loss: 0.0017216202791314572\n",
      "Validation Loss: 0.0025699088509736952\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 16\n",
      "Training Loss: 0.006475565689615905\n",
      "Training Loss: 0.006547580992337316\n",
      "Training Loss: 0.0066208994470071045\n",
      "Training Loss: 0.00455330401964602\n",
      "Training Loss: 0.00174892125884071\n",
      "Training Loss: 0.0017273067124187946\n",
      "Training Loss: 0.0016872894405969418\n",
      "Validation Loss: 0.0025198419791147503\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 17\n",
      "Training Loss: 0.006458575553260744\n",
      "Training Loss: 0.006526854374678806\n",
      "Training Loss: 0.006599904393078759\n",
      "Training Loss: 0.004531117837759666\n",
      "Training Loss: 0.0017242527335474733\n",
      "Training Loss: 0.0017021593398385449\n",
      "Training Loss: 0.0016593212209409104\n",
      "Validation Loss: 0.0024783593142049367\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 18\n",
      "Training Loss: 0.00644338836427778\n",
      "Training Loss: 0.006506323857465759\n",
      "Training Loss: 0.006579982300754636\n",
      "Training Loss: 0.004511430390994065\n",
      "Training Loss: 0.001704407643264858\n",
      "Training Loss: 0.00168166171584744\n",
      "Training Loss: 0.0016361144605616573\n",
      "Validation Loss: 0.002443448219654126\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 19\n",
      "Training Loss: 0.0064290948293637485\n",
      "Training Loss: 0.006485713057918474\n",
      "Training Loss: 0.006560687249293551\n",
      "Training Loss: 0.004493557234964101\n",
      "Training Loss: 0.0016881428554188461\n",
      "Training Loss: 0.0016646537207998335\n",
      "Training Loss: 0.0016164993034908548\n",
      "Validation Loss: 0.002413682021660214\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 20\n",
      "Training Loss: 0.006415227851830423\n",
      "Training Loss: 0.00646496209083125\n",
      "Training Loss: 0.0065417910250835124\n",
      "Training Loss: 0.004477031023416202\n",
      "Training Loss: 0.0016745516886294354\n",
      "Training Loss: 0.0016502906290406826\n",
      "Training Loss: 0.0015996176493354142\n",
      "Validation Loss: 0.002388013185508556\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Training Loss: 0.006401519929058849\n",
      "Training Loss: 0.006444070303114131\n",
      "Training Loss: 0.006523152785375714\n",
      "Training Loss: 0.004461509515240322\n",
      "Training Loss: 0.0016629738334449939\n",
      "Training Loss: 0.0016379513034917182\n",
      "Training Loss: 0.0015848265633394477\n",
      "Validation Loss: 0.0023656465896459344\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 22\n",
      "Training Loss: 0.006387802680255845\n",
      "Training Loss: 0.006423049957957119\n",
      "Training Loss: 0.006504680503858253\n",
      "Training Loss: 0.004446732099459041\n",
      "Training Loss: 0.0016529069241369143\n",
      "Training Loss: 0.0016271676400356227\n",
      "Training Loss: 0.0015716381605307107\n",
      "Validation Loss: 0.0023459676390013237\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 23\n",
      "Training Loss: 0.0063739640696439896\n",
      "Training Loss: 0.006401921302312985\n",
      "Training Loss: 0.006486305261496454\n",
      "Training Loss: 0.004432494747306919\n",
      "Training Loss: 0.0016439821910171305\n",
      "Training Loss: 0.0016175936214858665\n",
      "Training Loss: 0.0015596881516103168\n",
      "Validation Loss: 0.002328491900207768\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 24\n",
      "Training Loss: 0.006359910579631105\n",
      "Training Loss: 0.006380690860096365\n",
      "Training Loss: 0.006467979758745059\n",
      "Training Loss: 0.004418640301155392\n",
      "Training Loss: 0.0016359103060676716\n",
      "Training Loss: 0.0016089603185537272\n",
      "Training Loss: 0.0015486923113348895\n",
      "Validation Loss: 0.002312830601790955\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 25\n",
      "Training Loss: 0.006345583323854953\n",
      "Training Loss: 0.006359384068055078\n",
      "Training Loss: 0.006449676799820736\n",
      "Training Loss: 0.004405044410377741\n",
      "Training Loss: 0.0016284709324827418\n",
      "Training Loss: 0.001601070403630729\n",
      "Training Loss: 0.0015384453277511056\n",
      "Validation Loss: 0.002298676427902849\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 26\n",
      "Training Loss: 0.006330928885145113\n",
      "Training Loss: 0.006338019773829728\n",
      "Training Loss: 0.0064313857012894\n",
      "Training Loss: 0.004391622248804196\n",
      "Training Loss: 0.0016215001081582158\n",
      "Training Loss: 0.0015937664952070918\n",
      "Training Loss: 0.0015287895734945778\n",
      "Validation Loss: 0.00228578342763142\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 27\n",
      "Training Loss: 0.006315923222573474\n",
      "Training Loss: 0.006316642489982769\n",
      "Training Loss: 0.006413122587837279\n",
      "Training Loss: 0.0043783167343644895\n",
      "Training Loss: 0.0016148612179677003\n",
      "Training Loss: 0.0015869266491790768\n",
      "Training Loss: 0.0015196109679527581\n",
      "Validation Loss: 0.0022739553101376493\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 28\n",
      "Training Loss: 0.006300561173120513\n",
      "Training Loss: 0.006295306595275179\n",
      "Training Loss: 0.006394916267599911\n",
      "Training Loss: 0.004365097545960452\n",
      "Training Loss: 0.0016084596274595242\n",
      "Training Loss: 0.0015804582643613686\n",
      "Training Loss: 0.0015108291912474669\n",
      "Validation Loss: 0.002263033486142308\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 29\n",
      "Training Loss: 0.006284862103639171\n",
      "Training Loss: 0.006274088786449283\n",
      "Training Loss: 0.006376818707212806\n",
      "Training Loss: 0.0043519586659385824\n",
      "Training Loss: 0.0016022144653834404\n",
      "Training Loss: 0.0015742789061914664\n",
      "Training Loss: 0.0015023828213452362\n",
      "Validation Loss: 0.0022528838998052093\n",
      "Validation Accuracy: 0.175561797752809\n",
      "**************************************************\n",
      "Epoch: 30\n",
      "Training Loss: 0.006268874143715948\n",
      "Training Loss: 0.006253081938484684\n",
      "Training Loss: 0.006358889110852033\n",
      "Training Loss: 0.0043389161569939464\n",
      "Training Loss: 0.0015960769887897186\n",
      "Training Loss: 0.0015683289554726798\n",
      "Training Loss: 0.0014942355240054894\n",
      "Validation Loss: 0.002243399500152731\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Training Loss: 0.006252658467274159\n",
      "Training Loss: 0.006232381908921525\n",
      "Training Loss: 0.006341195675777272\n",
      "Training Loss: 0.004326000091095921\n",
      "Training Loss: 0.0015900058808620088\n",
      "Training Loss: 0.0015625528145756108\n",
      "Training Loss: 0.001486355062952498\n",
      "Validation Loss: 0.0022344826975722243\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 32\n",
      "Training Loss: 0.006236305134370923\n",
      "Training Loss: 0.006212093427311629\n",
      "Training Loss: 0.006323806392028928\n",
      "Training Loss: 0.004313251662242692\n",
      "Training Loss: 0.0015839768522710074\n",
      "Training Loss: 0.0015569081930152605\n",
      "Training Loss: 0.0014787182103464147\n",
      "Validation Loss: 0.0022260502048036474\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 33\n",
      "Training Loss: 0.006219912897795439\n",
      "Training Loss: 0.006192315247608349\n",
      "Training Loss: 0.00630678178044036\n",
      "Training Loss: 0.004300713949196506\n",
      "Training Loss: 0.0015779729843779934\n",
      "Training Loss: 0.001551359560835408\n",
      "Training Loss: 0.0014713092707825126\n",
      "Validation Loss: 0.0022180342634425636\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "Training Loss: 0.0062035817035939545\n",
      "Training Loss: 0.006173126399517059\n",
      "Training Loss: 0.006290169220883399\n",
      "Training Loss: 0.004288430922024418\n",
      "Training Loss: 0.0015719941836141516\n",
      "Training Loss: 0.0015458840312930987\n",
      "Training Loss: 0.0014641140371531947\n",
      "Validation Loss: 0.0022103710098201686\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 35\n",
      "Training Loss: 0.006187408919213339\n",
      "Training Loss: 0.006154583066236227\n",
      "Training Loss: 0.00627399893826805\n",
      "Training Loss: 0.004276439205277711\n",
      "Training Loss: 0.0015660419150663075\n",
      "Training Loss: 0.0015404698348720559\n",
      "Training Loss: 0.0014571269106090766\n",
      "Validation Loss: 0.0022030119099200806\n",
      "Validation Accuracy: 0.16385767790262173\n",
      "**************************************************\n",
      "Epoch: 36\n",
      "Training Loss: 0.0061714719503652305\n",
      "Training Loss: 0.0061367148300632834\n",
      "Training Loss: 0.006258284233044833\n",
      "Training Loss: 0.004264764876716072\n",
      "Training Loss: 0.0015601218827941922\n",
      "Training Loss: 0.0015351095183723372\n",
      "Training Loss: 0.001450337631322327\n",
      "Validation Loss: 0.002195912961911738\n",
      "Validation Accuracy: 0.15215355805243447\n",
      "**************************************************\n",
      "Epoch: 37\n",
      "Training Loss: 0.006155843359883875\n",
      "Training Loss: 0.006119535398902372\n",
      "Training Loss: 0.0062430247687734666\n",
      "Training Loss: 0.004253428690863075\n",
      "Training Loss: 0.0015542504514451138\n",
      "Training Loss: 0.0015298055302991997\n",
      "Training Loss: 0.0014437434769206448\n",
      "Validation Loss: 0.0021890398473103883\n",
      "Validation Accuracy: 0.1404494382022472\n",
      "**************************************************\n",
      "Epoch: 38\n",
      "Training Loss: 0.006140563080552965\n",
      "Training Loss: 0.006103024136973545\n",
      "Training Loss: 0.006228203169303015\n",
      "Training Loss: 0.004242435412743361\n",
      "Training Loss: 0.0015484368820034434\n",
      "Training Loss: 0.001524567294618464\n",
      "Training Loss: 0.001437341037672013\n",
      "Validation Loss: 0.002182366049855909\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 39\n",
      "Training Loss: 0.006125662541016937\n",
      "Training Loss: 0.006087160732131451\n",
      "Training Loss: 0.006213801328558474\n",
      "Training Loss: 0.004231785833253525\n",
      "Training Loss: 0.0015426932819536886\n",
      "Training Loss: 0.00151940262709104\n",
      "Training Loss: 0.0014311266206641449\n",
      "Validation Loss: 0.002175869589943128\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 40\n",
      "Training Loss: 0.006111151098739356\n",
      "Training Loss: 0.006071900088572875\n",
      "Training Loss: 0.006199789911042899\n",
      "Training Loss: 0.004221471331256907\n",
      "Training Loss: 0.0015370336880732793\n",
      "Training Loss: 0.0015143278856703545\n",
      "Training Loss: 0.0014251027232239722\n",
      "Validation Loss: 0.002169534996106579\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Training Loss: 0.006097024786286056\n",
      "Training Loss: 0.006057197072077542\n",
      "Training Loss: 0.006186138510238379\n",
      "Training Loss: 0.004211479945806787\n",
      "Training Loss: 0.0015314689232036472\n",
      "Training Loss: 0.0015093553611950482\n",
      "Training Loss: 0.001419266981392866\n",
      "Validation Loss: 0.0021633506716143562\n",
      "Validation Accuracy: 0.12874531835205993\n",
      "**************************************************\n",
      "Epoch: 42\n",
      "Training Loss: 0.006083275609416888\n",
      "Training Loss: 0.0060430080851074305\n",
      "Training Loss: 0.006172819808125496\n",
      "Training Loss: 0.00420179538166849\n",
      "Training Loss: 0.0015260090847732499\n",
      "Training Loss: 0.0015044968965958105\n",
      "Training Loss: 0.001413618653823505\n",
      "Validation Loss: 0.0021573060836049133\n",
      "Validation Accuracy: 0.11704119850187265\n",
      "**************************************************\n",
      "Epoch: 43\n",
      "Training Loss: 0.006069886966142804\n",
      "Training Loss: 0.006029288766440004\n",
      "Training Loss: 0.0061598050442989915\n",
      "Training Loss: 0.004192399831372313\n",
      "Training Loss: 0.0015206597960786895\n",
      "Training Loss: 0.0014997593644511652\n",
      "Training Loss: 0.0014081509554671357\n",
      "Validation Loss: 0.0021513882353648974\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 44\n",
      "Training Loss: 0.0060568357142619786\n",
      "Training Loss: 0.006015994701301679\n",
      "Training Loss: 0.006147069451399148\n",
      "Training Loss: 0.004183272964583012\n",
      "Training Loss: 0.001515424783283379\n",
      "Training Loss: 0.0014951538742752745\n",
      "Training Loss: 0.00140286327994545\n",
      "Validation Loss: 0.0021455956845106523\n",
      "Validation Accuracy: 0.09363295880149813\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "Training Loss: 0.00604410030762665\n",
      "Training Loss: 0.006003086643759161\n",
      "Training Loss: 0.006134586570551619\n",
      "Training Loss: 0.004174400570773287\n",
      "Training Loss: 0.0015103142989391926\n",
      "Training Loss: 0.0014906854776199907\n",
      "Training Loss: 0.0013977518808678724\n",
      "Validation Loss: 0.002139921919196366\n",
      "Validation Accuracy: 0.08192883895131087\n",
      "**************************************************\n",
      "Epoch: 46\n",
      "Training Loss: 0.006031661129090935\n",
      "Training Loss: 0.005990537927718833\n",
      "Training Loss: 0.006122343498282135\n",
      "Training Loss: 0.004165760940377367\n",
      "Training Loss: 0.001505321472795913\n",
      "Training Loss: 0.0014863522485393332\n",
      "Training Loss: 0.0013928058105375386\n",
      "Validation Loss: 0.002134358374339141\n",
      "Validation Accuracy: 0.0702247191011236\n",
      "**************************************************\n",
      "Epoch: 47\n",
      "Training Loss: 0.006019495131913573\n",
      "Training Loss: 0.005978314406238496\n",
      "Training Loss: 0.006110318539431319\n",
      "Training Loss: 0.0041573393462749665\n",
      "Training Loss: 0.001500453739281511\n",
      "Training Loss: 0.001482162803367828\n",
      "Training Loss: 0.0013880253507522866\n",
      "Validation Loss: 0.0021289056089614285\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 48\n",
      "Training Loss: 0.006007580385776237\n",
      "Training Loss: 0.005966389479581267\n",
      "Training Loss: 0.0060984945518430325\n",
      "Training Loss: 0.004149118426867062\n",
      "Training Loss: 0.0014957093670091126\n",
      "Training Loss: 0.001478111406613607\n",
      "Training Loss: 0.0013833980688650626\n",
      "Validation Loss: 0.0021235556300286526\n",
      "Validation Accuracy: 0.04681647940074907\n",
      "**************************************************\n",
      "Epoch: 49\n",
      "Training Loss: 0.00599589966237545\n",
      "Training Loss: 0.005954745172057301\n",
      "Training Loss: 0.006086860001087189\n",
      "Training Loss: 0.004141084834554931\n",
      "Training Loss: 0.0014910826772393192\n",
      "Training Loss: 0.0014741949307790492\n",
      "Training Loss: 0.001378916335888789\n",
      "Validation Loss: 0.0021183078444003884\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 50\n",
      "Training Loss: 0.005984437405131758\n",
      "Training Loss: 0.005943361432291567\n",
      "Training Loss: 0.006075403076829388\n",
      "Training Loss: 0.004133223915559938\n",
      "Training Loss: 0.0014865781521075404\n",
      "Training Loss: 0.001470415418225457\n",
      "Training Loss: 0.0013745787529478549\n",
      "Validation Loss: 0.002113160471401826\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Training Loss: 0.00597317406325601\n",
      "Training Loss: 0.0059322155197151\n",
      "Training Loss: 0.006064108343562111\n",
      "Training Loss: 0.004125522230897332\n",
      "Training Loss: 0.0014821919007226825\n",
      "Training Loss: 0.0014667663221916882\n",
      "Training Loss: 0.001370370556251146\n",
      "Validation Loss: 0.002108105196875227\n",
      "Validation Accuracy: 0.0351123595505618\n",
      "**************************************************\n",
      "Epoch: 52\n",
      "Training Loss: 0.005962099207099527\n",
      "Training Loss: 0.005921300374902785\n",
      "Training Loss: 0.006052970788441598\n",
      "Training Loss: 0.004117969124054071\n",
      "Training Loss: 0.0014779145730426536\n",
      "Training Loss: 0.001463241092133103\n",
      "Training Loss: 0.0013662847387604414\n",
      "Validation Loss: 0.002103141548891625\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 53\n",
      "Training Loss: 0.005951199689880013\n",
      "Training Loss: 0.005910597912734374\n",
      "Training Loss: 0.006041979568544775\n",
      "Training Loss: 0.0041105546490871345\n",
      "Training Loss: 0.0014737525476084556\n",
      "Training Loss: 0.0014598397732333978\n",
      "Training Loss: 0.001362318039391539\n",
      "Validation Loss: 0.0020982698339132647\n",
      "Validation Accuracy: 0.023408239700374533\n",
      "**************************************************\n",
      "Epoch: 54\n",
      "Training Loss: 0.005940461570862681\n",
      "Training Loss: 0.0059000963671132925\n",
      "Training Loss: 0.006031125445733778\n",
      "Training Loss: 0.004103268468315946\n",
      "Training Loss: 0.001469693259568885\n",
      "Training Loss: 0.001456551782466704\n",
      "Training Loss: 0.0013584585052012698\n",
      "Validation Loss: 0.002093483851952234\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 55\n",
      "Training Loss: 0.005929876226000488\n",
      "Training Loss: 0.005889787379419431\n",
      "Training Loss: 0.006020401390269398\n",
      "Training Loss: 0.004096099450835027\n",
      "Training Loss: 0.001465737841645023\n",
      "Training Loss: 0.001453374634802458\n",
      "Training Loss: 0.0013547013849165524\n",
      "Validation Loss: 0.002088786497095614\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 56\n",
      "Training Loss: 0.005919437279226258\n",
      "Training Loss: 0.0058796585199888795\n",
      "Training Loss: 0.0060098001966252925\n",
      "Training Loss: 0.004089042201085249\n",
      "Training Loss: 0.0014618836365116294\n",
      "Training Loss: 0.0014503035117377295\n",
      "Training Loss: 0.0013510393757314887\n",
      "Validation Loss: 0.0020841705451622667\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 57\n",
      "Training Loss: 0.005909129114588723\n",
      "Training Loss: 0.005869701640913263\n",
      "Training Loss: 0.005999315991648473\n",
      "Training Loss: 0.004082087530841818\n",
      "Training Loss: 0.0014581208704476012\n",
      "Training Loss: 0.0014473303095292067\n",
      "Training Loss: 0.001347464914797456\n",
      "Validation Loss: 0.0020796389106228557\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 58\n",
      "Training Loss: 0.005898953917203471\n",
      "Training Loss: 0.00585991115658544\n",
      "Training Loss: 0.005988943690317683\n",
      "Training Loss: 0.004075230371381622\n",
      "Training Loss: 0.001454448341319221\n",
      "Training Loss: 0.0014444492977054323\n",
      "Training Loss: 0.0013439684523473261\n",
      "Validation Loss: 0.0020751845683904245\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 59\n",
      "Training Loss: 0.005888901221333072\n",
      "Training Loss: 0.00585027878289111\n",
      "Training Loss: 0.005978676779195666\n",
      "Training Loss: 0.004068463793519186\n",
      "Training Loss: 0.001450862482379307\n",
      "Training Loss: 0.0014416571733454474\n",
      "Training Loss: 0.001340549391170498\n",
      "Validation Loss: 0.0020708122460388716\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 60\n",
      "Training Loss: 0.005878964455332607\n",
      "Training Loss: 0.005840793924871832\n",
      "Training Loss: 0.005968509121448733\n",
      "Training Loss: 0.004061783542565536\n",
      "Training Loss: 0.00144735888010473\n",
      "Training Loss: 0.0014389475308416878\n",
      "Training Loss: 0.0013371993999317057\n",
      "Validation Loss: 0.0020665169328042176\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Training Loss: 0.005869137396803126\n",
      "Training Loss: 0.0058314555429387835\n",
      "Training Loss: 0.005958440367830917\n",
      "Training Loss: 0.0040551845381560266\n",
      "Training Loss: 0.001443928917287849\n",
      "Training Loss: 0.0014363129351841052\n",
      "Training Loss: 0.0013339109785010805\n",
      "Validation Loss: 0.0020622985977840815\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 62\n",
      "Training Loss: 0.005859418791951612\n",
      "Training Loss: 0.00582225542049855\n",
      "Training Loss: 0.005948463127133437\n",
      "Training Loss: 0.004048661341366824\n",
      "Training Loss: 0.001440569164333283\n",
      "Training Loss: 0.0014337488239834783\n",
      "Training Loss: 0.0013306805110914865\n",
      "Validation Loss: 0.0020581563253523364\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 63\n",
      "Training Loss: 0.005849802875891328\n",
      "Training Loss: 0.005813189891632646\n",
      "Training Loss: 0.005938575788168237\n",
      "Training Loss: 0.004042212520289468\n",
      "Training Loss: 0.0014372819414711557\n",
      "Training Loss: 0.0014312537107616663\n",
      "Training Loss: 0.0013275048463401618\n",
      "Validation Loss: 0.002054091733987164\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 64\n",
      "Training Loss: 0.0058402837789617475\n",
      "Training Loss: 0.005804251959780231\n",
      "Training Loss: 0.005928774868953042\n",
      "Training Loss: 0.004035834510141285\n",
      "Training Loss: 0.0014340540389093804\n",
      "Training Loss: 0.0014288173118256963\n",
      "Training Loss: 0.0013243744632200104\n",
      "Validation Loss: 0.002050102363542866\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 65\n",
      "Training Loss: 0.005830863860901445\n",
      "Training Loss: 0.005795438977656886\n",
      "Training Loss: 0.0059190569672500715\n",
      "Training Loss: 0.004029523644130677\n",
      "Training Loss: 0.0014308850385714323\n",
      "Training Loss: 0.0014264402935077669\n",
      "Training Loss: 0.001321291242929874\n",
      "Validation Loss: 0.0020461890350853716\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 66\n",
      "Training Loss: 0.005821535001741722\n",
      "Training Loss: 0.005786744891665876\n",
      "Training Loss: 0.005909418951487169\n",
      "Training Loss: 0.0040232796462078115\n",
      "Training Loss: 0.0014277717730146833\n",
      "Training Loss: 0.0014241128198773368\n",
      "Training Loss: 0.0013182441856770311\n",
      "Validation Loss: 0.002042350291842612\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 67\n",
      "Training Loss: 0.005812298941891641\n",
      "Training Loss: 0.005778169016120955\n",
      "Training Loss: 0.005899861988145858\n",
      "Training Loss: 0.004017097782780183\n",
      "Training Loss: 0.0014247044977673796\n",
      "Training Loss: 0.001421830534163746\n",
      "Training Loss: 0.001315232302295044\n",
      "Validation Loss: 0.002038585265713817\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 68\n",
      "Training Loss: 0.005803152332082391\n",
      "Training Loss: 0.005769708582665772\n",
      "Training Loss: 0.0058903812133939935\n",
      "Training Loss: 0.004010979867889546\n",
      "Training Loss: 0.0014216854185360717\n",
      "Training Loss: 0.0014195929635752692\n",
      "Training Loss: 0.0013122531212866307\n",
      "Validation Loss: 0.0020348985724356933\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 69\n",
      "Training Loss: 0.005794089059345424\n",
      "Training Loss: 0.005761356113944203\n",
      "Training Loss: 0.00588097921514418\n",
      "Training Loss: 0.004004922229942167\n",
      "Training Loss: 0.0014187045572907663\n",
      "Training Loss: 0.0014173913688136962\n",
      "Training Loss: 0.0013092984115792205\n",
      "Validation Loss: 0.0020312823819878165\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "Training Loss: 0.005785114463651553\n",
      "Training Loss: 0.005753112433012575\n",
      "Training Loss: 0.005871650152257644\n",
      "Training Loss: 0.003998924107872881\n",
      "Training Loss: 0.0014157618484750855\n",
      "Training Loss: 0.0014152252281201072\n",
      "Training Loss: 0.0013063697535108077\n",
      "Validation Loss: 0.0020277464357601506\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Training Loss: 0.005776220753323287\n",
      "Training Loss: 0.005744972198735923\n",
      "Training Loss: 0.005862395037547685\n",
      "Training Loss: 0.003992985311633675\n",
      "Training Loss: 0.0014128498449281323\n",
      "Training Loss: 0.0014130857333657333\n",
      "Training Loss: 0.0013034573338518386\n",
      "Validation Loss: 0.0020242802379522055\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 72\n",
      "Training Loss: 0.005767413114663214\n",
      "Training Loss: 0.005736937235342339\n",
      "Training Loss: 0.005853214638773352\n",
      "Training Loss: 0.003987104306434048\n",
      "Training Loss: 0.0014099658378836466\n",
      "Training Loss: 0.001410972467274405\n",
      "Training Loss: 0.001300562301694299\n",
      "Validation Loss: 0.00202088851057872\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 73\n",
      "Training Loss: 0.005758685455657542\n",
      "Training Loss: 0.0057290020491927865\n",
      "Training Loss: 0.005844106522272341\n",
      "Training Loss: 0.003981281720189145\n",
      "Training Loss: 0.0014071048767800676\n",
      "Training Loss: 0.0014088801485922886\n",
      "Training Loss: 0.00129767964790517\n",
      "Validation Loss: 0.00201757341053874\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 74\n",
      "Training Loss: 0.005750038797268644\n",
      "Training Loss: 0.005721162663539872\n",
      "Training Loss: 0.005835069275926799\n",
      "Training Loss: 0.003975514334306354\n",
      "Training Loss: 0.0014042669687478338\n",
      "Training Loss: 0.0014068051085632761\n",
      "Training Loss: 0.001294806469741161\n",
      "Validation Loss: 0.00201433055307315\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 75\n",
      "Training Loss: 0.005741468399064615\n",
      "Training Loss: 0.005713418541126884\n",
      "Training Loss: 0.005826103977160529\n",
      "Training Loss: 0.003969803846848663\n",
      "Training Loss: 0.0014014426406356506\n",
      "Training Loss: 0.0014047417999972821\n",
      "Training Loss: 0.001291937414498534\n",
      "Validation Loss: 0.0020111589859307774\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 76\n",
      "Training Loss: 0.0057329784019384536\n",
      "Training Loss: 0.005705770323984325\n",
      "Training Loss: 0.005817208263324574\n",
      "Training Loss: 0.003964149033708964\n",
      "Training Loss: 0.001398632162818103\n",
      "Training Loss: 0.0014026896705763648\n",
      "Training Loss: 0.0012890717112168204\n",
      "Validation Loss: 0.0020080621857658265\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 77\n",
      "Training Loss: 0.005724563947878778\n",
      "Training Loss: 0.0056982126837829125\n",
      "Training Loss: 0.0058083851280389355\n",
      "Training Loss: 0.003958549504604889\n",
      "Training Loss: 0.0013958314238698222\n",
      "Training Loss: 0.0014006426579726394\n",
      "Training Loss: 0.001286204981006449\n",
      "Validation Loss: 0.0020050366412611045\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 78\n",
      "Training Loss: 0.00571622506598942\n",
      "Training Loss: 0.0056907432083971796\n",
      "Training Loss: 0.005799629552057013\n",
      "Training Loss: 0.003953002561902394\n",
      "Training Loss: 0.001393036084700725\n",
      "Training Loss: 0.0013985984023747733\n",
      "Training Loss: 0.0012833344805403613\n",
      "Validation Loss: 0.0020020793927096577\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 79\n",
      "Training Loss: 0.00570796083076857\n",
      "Training Loss: 0.0056833582220133395\n",
      "Training Loss: 0.005790941181476228\n",
      "Training Loss: 0.003947508806450059\n",
      "Training Loss: 0.0013902438012883067\n",
      "Training Loss: 0.0013965541058860254\n",
      "Training Loss: 0.0012804573553876252\n",
      "Validation Loss: 0.001999190040560586\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 80\n",
      "Training Loss: 0.005699767993064597\n",
      "Training Loss: 0.005676059070974588\n",
      "Training Loss: 0.00578232274390757\n",
      "Training Loss: 0.0039420663054625035\n",
      "Training Loss: 0.001387450599577278\n",
      "Training Loss: 0.0013945023006090196\n",
      "Training Loss: 0.0012775668391259388\n",
      "Validation Loss: 0.001996364388303144\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Training Loss: 0.005691648598294705\n",
      "Training Loss: 0.005668841715669259\n",
      "Training Loss: 0.005773769268998876\n",
      "Training Loss: 0.003936673918069573\n",
      "Training Loss: 0.001384652712586103\n",
      "Training Loss: 0.0013924444928852609\n",
      "Training Loss: 0.001274664673182997\n",
      "Validation Loss: 0.0019936049108529525\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 82\n",
      "Training Loss: 0.00568359816330485\n",
      "Training Loss: 0.005661703433142975\n",
      "Training Loss: 0.005765282430220395\n",
      "Training Loss: 0.003931331215280807\n",
      "Training Loss: 0.0013818523199006449\n",
      "Training Loss: 0.0013903781574481399\n",
      "Training Loss: 0.001271747359278379\n",
      "Validation Loss: 0.0019909064031479887\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 83\n",
      "Training Loss: 0.005675613722996786\n",
      "Training Loss: 0.005654639663407579\n",
      "Training Loss: 0.00575685910123866\n",
      "Training Loss: 0.003926035837794189\n",
      "Training Loss: 0.0013790417281416012\n",
      "Training Loss: 0.0013882961274066475\n",
      "Training Loss: 0.0012688071834418223\n",
      "Validation Loss: 0.0019882644981421417\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 84\n",
      "Training Loss: 0.005667699493933469\n",
      "Training Loss: 0.005647651446633972\n",
      "Training Loss: 0.005748499406035989\n",
      "Training Loss: 0.003920785977243213\n",
      "Training Loss: 0.0013762185364612379\n",
      "Training Loss: 0.0013861974925384857\n",
      "Training Loss: 0.0012658462049876106\n",
      "Validation Loss: 0.0019856787891448075\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 85\n",
      "Training Loss: 0.005659846456255764\n",
      "Training Loss: 0.005640732788015157\n",
      "Training Loss: 0.005740199976717122\n",
      "Training Loss: 0.003915580613102066\n",
      "Training Loss: 0.0013733860877255212\n",
      "Training Loss: 0.00138408109152806\n",
      "Training Loss: 0.0012628618277813076\n",
      "Validation Loss: 0.001983147894323556\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 86\n",
      "Training Loss: 0.005652054285164922\n",
      "Training Loss: 0.005633879955857992\n",
      "Training Loss: 0.0057319581159390506\n",
      "Training Loss: 0.003910418704908807\n",
      "Training Loss: 0.0013705428218963788\n",
      "Training Loss: 0.001381946082256036\n",
      "Training Loss: 0.0012598547742527443\n",
      "Validation Loss: 0.0019806688759355607\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 87\n",
      "Training Loss: 0.00564431773382239\n",
      "Training Loss: 0.005627087689936161\n",
      "Training Loss: 0.005723771302145905\n",
      "Training Loss: 0.0039052952849306165\n",
      "Training Loss: 0.0013676829051109962\n",
      "Training Loss: 0.0013797877057368168\n",
      "Training Loss: 0.001256816265595262\n",
      "Validation Loss: 0.001978233094939313\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 88\n",
      "Training Loss: 0.005636642051395029\n",
      "Training Loss: 0.005620359065942466\n",
      "Training Loss: 0.0057156422268599275\n",
      "Training Loss: 0.0039002099630306474\n",
      "Training Loss: 0.0013648044897854562\n",
      "Training Loss: 0.0013776040220545838\n",
      "Training Loss: 0.0012537469784729182\n",
      "Validation Loss: 0.001975837661117343\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 89\n",
      "Training Loss: 0.005629019860643894\n",
      "Training Loss: 0.00561368934053462\n",
      "Training Loss: 0.0057075649406760935\n",
      "Training Loss: 0.003895161354303127\n",
      "Training Loss: 0.0013619118711358168\n",
      "Training Loss: 0.0013753945947246394\n",
      "Training Loss: 0.0012506470696098404\n",
      "Validation Loss: 0.001973483217151989\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 90\n",
      "Training Loss: 0.005621449556201696\n",
      "Training Loss: 0.005607073864084668\n",
      "Training Loss: 0.0056995383102912455\n",
      "Training Loss: 0.0038901459651242477\n",
      "Training Loss: 0.0013589985651196912\n",
      "Training Loss: 0.0013731562588509404\n",
      "Training Loss: 0.0012475116825225996\n",
      "Validation Loss: 0.0019711616377334007\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Training Loss: 0.005613928413949907\n",
      "Training Loss: 0.005600508026545868\n",
      "Training Loss: 0.005691557855461724\n",
      "Training Loss: 0.003885162168298848\n",
      "Training Loss: 0.001356070183828706\n",
      "Training Loss: 0.0013708925124956296\n",
      "Training Loss: 0.0012443475071631837\n",
      "Validation Loss: 0.00196887241229101\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 92\n",
      "Training Loss: 0.0056064482498914\n",
      "Training Loss: 0.005593988340697251\n",
      "Training Loss: 0.005683622658252716\n",
      "Training Loss: 0.003880207357869949\n",
      "Training Loss: 0.001353121769570862\n",
      "Training Loss: 0.0013685957541019888\n",
      "Training Loss: 0.001241143551596906\n",
      "Validation Loss: 0.001966606964896109\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 93\n",
      "Training Loss: 0.0055990149779245255\n",
      "Training Loss: 0.00558751531294547\n",
      "Training Loss: 0.005675730583025142\n",
      "Training Loss: 0.0038752796121116263\n",
      "Training Loss: 0.0013501556475966935\n",
      "Training Loss: 0.0013662698036205256\n",
      "Training Loss: 0.0012379066738503752\n",
      "Validation Loss: 0.001964365265354835\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 94\n",
      "Training Loss: 0.005591618691105396\n",
      "Training Loss: 0.005581080842530355\n",
      "Training Loss: 0.0056678772589657455\n",
      "Training Loss: 0.0038703748698753772\n",
      "Training Loss: 0.0013471708879660583\n",
      "Training Loss: 0.0013639132541720755\n",
      "Training Loss: 0.0012346332947345217\n",
      "Validation Loss: 0.0019621420319214785\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 95\n",
      "Training Loss: 0.00558426029747352\n",
      "Training Loss: 0.005574685850879177\n",
      "Training Loss: 0.005660061877570115\n",
      "Training Loss: 0.0038654924924776423\n",
      "Training Loss: 0.0013441712479834679\n",
      "Training Loss: 0.0013615263019892154\n",
      "Training Loss: 0.0012313243536482333\n",
      "Validation Loss: 0.0019599307175422235\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 96\n",
      "Training Loss: 0.0055769351462367925\n",
      "Training Loss: 0.005568327535293065\n",
      "Training Loss: 0.005652284273528494\n",
      "Training Loss: 0.003860629265254829\n",
      "Training Loss: 0.0013411504069517832\n",
      "Training Loss: 0.0013591045342764118\n",
      "Training Loss: 0.0012279779706295812\n",
      "Validation Loss: 0.001957725482048425\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 97\n",
      "Training Loss: 0.0055696420010644945\n",
      "Training Loss: 0.0055620023899246\n",
      "Training Loss: 0.005644537372281775\n",
      "Training Loss: 0.0038557837977714372\n",
      "Training Loss: 0.0013381161090364913\n",
      "Training Loss: 0.0013566537306178362\n",
      "Training Loss: 0.0012246005146153039\n",
      "Validation Loss: 0.0019555274225563886\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 98\n",
      "Training Loss: 0.005562376513844356\n",
      "Training Loss: 0.005555709062609822\n",
      "Training Loss: 0.005636823523091152\n",
      "Training Loss: 0.0038509533205069602\n",
      "Training Loss: 0.0013350667678605531\n",
      "Training Loss: 0.0013541707721014973\n",
      "Training Loss: 0.0012211883893905905\n",
      "Validation Loss: 0.0019533291845172576\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 99\n",
      "Training Loss: 0.005555136821931228\n",
      "Training Loss: 0.005549443964846432\n",
      "Training Loss: 0.005629138206131756\n",
      "Training Loss: 0.003846137624932453\n",
      "Training Loss: 0.0013320035090146122\n",
      "Training Loss: 0.0013516562586301005\n",
      "Training Loss: 0.001217743235247326\n",
      "Validation Loss: 0.0019511234924094298\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 100\n",
      "Training Loss: 0.005547920510871336\n",
      "Training Loss: 0.005543208392919041\n",
      "Training Loss: 0.005621480561094358\n",
      "Training Loss: 0.0038413320227118674\n",
      "Training Loss: 0.0013289263620390557\n",
      "Training Loss: 0.001349113144169678\n",
      "Training Loss: 0.0012142689331813017\n",
      "Validation Loss: 0.001948915901140411\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Training Loss: 0.005540723844897002\n",
      "Training Loss: 0.005536997197195887\n",
      "Training Loss: 0.005613850661902688\n",
      "Training Loss: 0.003836537451570621\n",
      "Training Loss: 0.0013258359450264833\n",
      "Training Loss: 0.0013465367479511769\n",
      "Training Loss: 0.0012107624744385247\n",
      "Validation Loss: 0.001946691472923962\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 102\n",
      "Training Loss: 0.005533548835664988\n",
      "Training Loss: 0.005530814921366982\n",
      "Training Loss: 0.005606246532988734\n",
      "Training Loss: 0.0038317520401324147\n",
      "Training Loss: 0.0013227382277545986\n",
      "Training Loss: 0.0013439347262465162\n",
      "Training Loss: 0.0012072300534055102\n",
      "Validation Loss: 0.0019444547776064805\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 103\n",
      "Training Loss: 0.005526393089676276\n",
      "Training Loss: 0.005524658870417625\n",
      "Training Loss: 0.00559866932220757\n",
      "Training Loss: 0.0038269743870478126\n",
      "Training Loss: 0.0013196282797434834\n",
      "Training Loss: 0.001341299053019611\n",
      "Training Loss: 0.0012036706017534017\n",
      "Validation Loss: 0.0019421959437572918\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 104\n",
      "Training Loss: 0.005519252542871982\n",
      "Training Loss: 0.005518527061212808\n",
      "Training Loss: 0.005591114861890674\n",
      "Training Loss: 0.0038222043089626823\n",
      "Training Loss: 0.0013165149367705452\n",
      "Training Loss: 0.001338639737732592\n",
      "Training Loss: 0.0012000891086063347\n",
      "Validation Loss: 0.0019399153762139478\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 105\n",
      "Training Loss: 0.00551212886464782\n",
      "Training Loss: 0.005512423647451215\n",
      "Training Loss: 0.005583586294087581\n",
      "Training Loss: 0.0038174410440842623\n",
      "Training Loss: 0.0013133948684844654\n",
      "Training Loss: 0.0013359522508835653\n",
      "Training Loss: 0.0011964870983501895\n",
      "Validation Loss: 0.0019376095177689463\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 106\n",
      "Training Loss: 0.005505019670818001\n",
      "Training Loss: 0.0055063471890753135\n",
      "Training Loss: 0.005576082782936282\n",
      "Training Loss: 0.0038126841065241023\n",
      "Training Loss: 0.0013102737739973235\n",
      "Training Loss: 0.0013332424906548112\n",
      "Training Loss: 0.0011928694675589213\n",
      "Validation Loss: 0.001935279017317803\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 107\n",
      "Training Loss: 0.005497924662195146\n",
      "Training Loss: 0.005500296176178381\n",
      "Training Loss: 0.005568602909916081\n",
      "Training Loss: 0.003807933321368182\n",
      "Training Loss: 0.0013071502854290883\n",
      "Training Loss: 0.0013305075123207645\n",
      "Training Loss: 0.0011892350522975904\n",
      "Validation Loss: 0.0019329164162136907\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 108\n",
      "Training Loss: 0.0054908496874850245\n",
      "Training Loss: 0.005494281035498716\n",
      "Training Loss: 0.005561153569724411\n",
      "Training Loss: 0.0038031889223202595\n",
      "Training Loss: 0.0013040249247569591\n",
      "Training Loss: 0.0013277483967976877\n",
      "Training Loss: 0.0011855877805646742\n",
      "Validation Loss: 0.0019305225429892513\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 109\n",
      "Training Loss: 0.005483790867729112\n",
      "Training Loss: 0.005488296332769096\n",
      "Training Loss: 0.005553730551037006\n",
      "Training Loss: 0.003798453471827088\n",
      "Training Loss: 0.0013009057137242054\n",
      "Training Loss: 0.0013249705170164817\n",
      "Training Loss: 0.0011819323171948781\n",
      "Validation Loss: 0.0019280963237458414\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 110\n",
      "Training Loss: 0.005476748842047527\n",
      "Training Loss: 0.005482349024969153\n",
      "Training Loss: 0.005546340367873199\n",
      "Training Loss: 0.003793725239665946\n",
      "Training Loss: 0.0012977876316290348\n",
      "Training Loss: 0.001322172399959527\n",
      "Training Loss: 0.0011782711472915252\n",
      "Validation Loss: 0.0019256376956976698\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Training Loss: 0.00546972832060419\n",
      "Training Loss: 0.005476436862372793\n",
      "Training Loss: 0.005538979332195595\n",
      "Training Loss: 0.003789009082247503\n",
      "Training Loss: 0.0012946822331286966\n",
      "Training Loss: 0.0013193590826995206\n",
      "Training Loss: 0.001174610867528827\n",
      "Validation Loss: 0.0019231469893421671\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 112\n",
      "Training Loss: 0.005462726156692952\n",
      "Training Loss: 0.005470566769363358\n",
      "Training Loss: 0.005531654559890739\n",
      "Training Loss: 0.0037843048300419467\n",
      "Training Loss: 0.0012915826535027008\n",
      "Training Loss: 0.001316527545932331\n",
      "Training Loss: 0.0011709494522801833\n",
      "Validation Loss: 0.0019206207402334774\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 113\n",
      "Training Loss: 0.005455753389978782\n",
      "Training Loss: 0.00546474355389364\n",
      "Training Loss: 0.005524367660982534\n",
      "Training Loss: 0.0037796146387699993\n",
      "Training Loss: 0.0012884958062204532\n",
      "Training Loss: 0.0013136829713039334\n",
      "Training Loss: 0.0011672944972815457\n",
      "Validation Loss: 0.0019180646046519992\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 114\n",
      "Training Loss: 0.0054488053440582005\n",
      "Training Loss: 0.005458968004095368\n",
      "Training Loss: 0.005517120995209552\n",
      "Training Loss: 0.003774942186428234\n",
      "Training Loss: 0.001285422292567091\n",
      "Training Loss: 0.0013108239680877888\n",
      "Training Loss: 0.0011636454582912847\n",
      "Validation Loss: 0.001915474895567525\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 115\n",
      "Training Loss: 0.005441892463713884\n",
      "Training Loss: 0.005453247195109725\n",
      "Training Loss: 0.005509921755292453\n",
      "Training Loss: 0.003770290579122957\n",
      "Training Loss: 0.0012823655958345626\n",
      "Training Loss: 0.0013079567910608604\n",
      "Training Loss: 0.0011600086449470837\n",
      "Validation Loss: 0.001912857472551254\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 116\n",
      "Training Loss: 0.005435016324045136\n",
      "Training Loss: 0.005447582284105011\n",
      "Training Loss: 0.005502767590223811\n",
      "Training Loss: 0.003765660652134102\n",
      "Training Loss: 0.0012793288977991325\n",
      "Training Loss: 0.0013050825788377552\n",
      "Training Loss: 0.0011563893596758135\n",
      "Validation Loss: 0.0019102148349355028\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 117\n",
      "Training Loss: 0.005428174036787823\n",
      "Training Loss: 0.0054419744963524865\n",
      "Training Loss: 0.005495663160108961\n",
      "Training Loss: 0.0037610576176666656\n",
      "Training Loss: 0.0012763143061602022\n",
      "Training Loss: 0.0013022041240765248\n",
      "Training Loss: 0.0011527882925292943\n",
      "Validation Loss: 0.0019075474792294386\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 118\n",
      "Training Loss: 0.005421380581101403\n",
      "Training Loss: 0.005436437051976099\n",
      "Training Loss: 0.005488615522626788\n",
      "Training Loss: 0.003756484268087661\n",
      "Training Loss: 0.001273320987529587\n",
      "Training Loss: 0.001299317999437335\n",
      "Training Loss: 0.0011492095764697296\n",
      "Validation Loss: 0.001904858790851662\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 119\n",
      "Training Loss: 0.005414637072244659\n",
      "Training Loss: 0.005430967193096876\n",
      "Training Loss: 0.005481628171983175\n",
      "Training Loss: 0.003751945475960383\n",
      "Training Loss: 0.0012703547519049606\n",
      "Training Loss: 0.0012964333975833142\n",
      "Training Loss: 0.0011456569063011558\n",
      "Validation Loss: 0.001902153670255004\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 120\n",
      "Training Loss: 0.005407944218022749\n",
      "Training Loss: 0.005425566048943438\n",
      "Training Loss: 0.005474700513295829\n",
      "Training Loss: 0.003747445223707473\n",
      "Training Loss: 0.0012674203453934751\n",
      "Training Loss: 0.0012935528889647684\n",
      "Training Loss: 0.0011421368969604373\n",
      "Validation Loss: 0.0018994384689417002\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Training Loss: 0.005401309594744817\n",
      "Training Loss: 0.005420239595114253\n",
      "Training Loss: 0.005467839060584083\n",
      "Training Loss: 0.0037429873118526302\n",
      "Training Loss: 0.0012645174891804346\n",
      "Training Loss: 0.0012906767992535606\n",
      "Training Loss: 0.0011386493490135763\n",
      "Validation Loss: 0.0018967169257184343\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 122\n",
      "Training Loss: 0.005394739008042961\n",
      "Training Loss: 0.005414993135491386\n",
      "Training Loss: 0.005461049894220196\n",
      "Training Loss: 0.0037385749942041003\n",
      "Training Loss: 0.0012616470603097696\n",
      "Training Loss: 0.0012878065616678215\n",
      "Training Loss: 0.0011351978663151384\n",
      "Validation Loss: 0.0018939924589227468\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 123\n",
      "Training Loss: 0.005388237403822132\n",
      "Training Loss: 0.005409829124691896\n",
      "Training Loss: 0.005454333340167068\n",
      "Training Loss: 0.00373421432566829\n",
      "Training Loss: 0.0012588111299555748\n",
      "Training Loss: 0.0012849446877953596\n",
      "Training Loss: 0.0011317847048485418\n",
      "Validation Loss: 0.0018912684717407443\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 124\n",
      "Training Loss: 0.005381808616220951\n",
      "Training Loss: 0.005404749572044239\n",
      "Training Loss: 0.0054476942692417655\n",
      "Training Loss: 0.003729906751541421\n",
      "Training Loss: 0.0012560127064352854\n",
      "Training Loss: 0.0012820933618786512\n",
      "Training Loss: 0.001128412674734136\n",
      "Validation Loss: 0.0018885532570200579\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 125\n",
      "Training Loss: 0.0053754579013912005\n",
      "Training Loss: 0.005399754612590187\n",
      "Training Loss: 0.005441135470173322\n",
      "Training Loss: 0.00372565834200941\n",
      "Training Loss: 0.0012532548945455345\n",
      "Training Loss: 0.0012792601156252203\n",
      "Training Loss: 0.0011250879402359714\n",
      "Validation Loss: 0.0018858524131604335\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 126\n",
      "Training Loss: 0.005369187102769501\n",
      "Training Loss: 0.005394847606075928\n",
      "Training Loss: 0.005434660994797014\n",
      "Training Loss: 0.003721474110498093\n",
      "Training Loss: 0.0012505395081825554\n",
      "Training Loss: 0.0012764452894043644\n",
      "Training Loss: 0.0011218113356881076\n",
      "Validation Loss: 0.0018831714139710624\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 127\n",
      "Training Loss: 0.005363001068471931\n",
      "Training Loss: 0.0053900274843908845\n",
      "Training Loss: 0.005428270020638592\n",
      "Training Loss: 0.003717353740212275\n",
      "Training Loss: 0.0012478680092317517\n",
      "Training Loss: 0.0012736504791973856\n",
      "Training Loss: 0.0011185847117303638\n",
      "Validation Loss: 0.001880516934911424\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 128\n",
      "Training Loss: 0.005356905889348127\n",
      "Training Loss: 0.005385295238229446\n",
      "Training Loss: 0.0054219687182921915\n",
      "Training Loss: 0.0037133030414406676\n",
      "Training Loss: 0.0012452399292669724\n",
      "Training Loss: 0.0012708771554025588\n",
      "Training Loss: 0.0011154084611916914\n",
      "Validation Loss: 0.0018778899841512856\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 129\n",
      "Training Loss: 0.005350903646904044\n",
      "Training Loss: 0.0053806551219895485\n",
      "Training Loss: 0.005415759078459814\n",
      "Training Loss: 0.0037093261402333156\n",
      "Training Loss: 0.001242657419788884\n",
      "Training Loss: 0.0012681294586218428\n",
      "Training Loss: 0.0011122875408182153\n",
      "Validation Loss: 0.0018753031362894546\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 130\n",
      "Training Loss: 0.005344992548343725\n",
      "Training Loss: 0.00537609925144352\n",
      "Training Loss: 0.0054096390429185705\n",
      "Training Loss: 0.003705422715720488\n",
      "Training Loss: 0.0012401229144597891\n",
      "Training Loss: 0.0012654103092791047\n",
      "Training Loss: 0.0011092221520812018\n",
      "Validation Loss: 0.0018727555639923234\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Training Loss: 0.005339181353338063\n",
      "Training Loss: 0.0053716346790315585\n",
      "Training Loss: 0.005403614231036045\n",
      "Training Loss: 0.00370159792189952\n",
      "Training Loss: 0.001237636102741817\n",
      "Training Loss: 0.0012627222273295046\n",
      "Training Loss: 0.0011062148044584318\n",
      "Validation Loss: 0.0018702524728124285\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 132\n",
      "Training Loss: 0.005333465799922123\n",
      "Training Loss: 0.0053672521305270495\n",
      "Training Loss: 0.00539768177550286\n",
      "Training Loss: 0.00369785266200779\n",
      "Training Loss: 0.0012351995115750468\n",
      "Training Loss: 0.0012600679510069313\n",
      "Training Loss: 0.0011032667770632543\n",
      "Validation Loss: 0.001867801125409005\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 133\n",
      "Training Loss: 0.005327851573238149\n",
      "Training Loss: 0.0053629566344898195\n",
      "Training Loss: 0.005391846551210619\n",
      "Training Loss: 0.0036941896569624077\n",
      "Training Loss: 0.0012328096271085086\n",
      "Training Loss: 0.0012574461792246438\n",
      "Training Loss: 0.0011003755511774216\n",
      "Validation Loss: 0.00186540443292812\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 134\n",
      "Training Loss: 0.005322340473067015\n",
      "Training Loss: 0.005358745073899627\n",
      "Training Loss: 0.005386109238606877\n",
      "Training Loss: 0.0036906089304829946\n",
      "Training Loss: 0.0012304676056373864\n",
      "Training Loss: 0.0012548615784180584\n",
      "Training Loss: 0.0010975440951733618\n",
      "Validation Loss: 0.001863064263138069\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 135\n",
      "Training Loss: 0.005316927457461134\n",
      "Training Loss: 0.0053546117810765285\n",
      "Training Loss: 0.005380464479676448\n",
      "Training Loss: 0.0036871121493459212\n",
      "Training Loss: 0.001228180053876713\n",
      "Training Loss: 0.00125231831902056\n",
      "Training Loss: 0.0010947764856246067\n",
      "Validation Loss: 0.0018607891259116752\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 136\n",
      "Training Loss: 0.005311615833779797\n",
      "Training Loss: 0.00535055436193943\n",
      "Training Loss: 0.005374915185966529\n",
      "Training Loss: 0.003683698532258859\n",
      "Training Loss: 0.0012259403768985066\n",
      "Training Loss: 0.001249813916365383\n",
      "Training Loss: 0.0010920693638036027\n",
      "Validation Loss: 0.0018585768596082576\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 137\n",
      "Training Loss: 0.005306405324954539\n",
      "Training Loss: 0.005346573343849741\n",
      "Training Loss: 0.005369461476220749\n",
      "Training Loss: 0.0036803705856436864\n",
      "Training Loss: 0.0012237488075334112\n",
      "Training Loss: 0.0012473516260070028\n",
      "Training Loss: 0.001089423035591608\n",
      "Validation Loss: 0.0018564318203191145\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 138\n",
      "Training Loss: 0.0053012967872200535\n",
      "Training Loss: 0.0053426628286251795\n",
      "Training Loss: 0.005364102554158308\n",
      "Training Loss: 0.003677126123366179\n",
      "Training Loss: 0.0012216059400816449\n",
      "Training Loss: 0.001244932268455159\n",
      "Training Loss: 0.0010868360630411189\n",
      "Validation Loss: 0.00185435293687068\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 139\n",
      "Training Loss: 0.005296291502891109\n",
      "Training Loss: 0.005338823951897212\n",
      "Training Loss: 0.005358840242261067\n",
      "Training Loss: 0.0036739673938427585\n",
      "Training Loss: 0.0012195119286479894\n",
      "Training Loss: 0.0012425576524401549\n",
      "Training Loss: 0.0010843112833390479\n",
      "Validation Loss: 0.0018523458475578395\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 140\n",
      "Training Loss: 0.005291382704745047\n",
      "Training Loss: 0.00533504948427435\n",
      "Training Loss: 0.005353667404851876\n",
      "Training Loss: 0.0036708901412202976\n",
      "Training Loss: 0.0012174666023929603\n",
      "Training Loss: 0.00124023163531092\n",
      "Training Loss: 0.0010818496794672682\n",
      "Validation Loss: 0.001850412671074048\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Training Loss: 0.005286570216994733\n",
      "Training Loss: 0.005331335168448277\n",
      "Training Loss: 0.005348586798645556\n",
      "Training Loss: 0.0036678970526554623\n",
      "Training Loss: 0.001215470187162282\n",
      "Training Loss: 0.001237952103765565\n",
      "Training Loss: 0.001079446957592154\n",
      "Validation Loss: 0.0018485495031458998\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 142\n",
      "Training Loss: 0.005281854150234721\n",
      "Training Loss: 0.0053276802983600645\n",
      "Training Loss: 0.005343596675083972\n",
      "Training Loss: 0.003664983141643461\n",
      "Training Loss: 0.0012135193399444688\n",
      "Training Loss: 0.0012357182509003905\n",
      "Training Loss: 0.0010771054786164313\n",
      "Validation Loss: 0.0018467613035239182\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 143\n",
      "Training Loss: 0.00527723359875381\n",
      "Training Loss: 0.005324082875740714\n",
      "Training Loss: 0.005338696778053418\n",
      "Training Loss: 0.0036621505094808528\n",
      "Training Loss: 0.0012116134112875442\n",
      "Training Loss: 0.0012335336297837785\n",
      "Training Loss: 0.001074821401562076\n",
      "Validation Loss: 0.0018450453767791908\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 144\n",
      "Training Loss: 0.005272706522955559\n",
      "Training Loss: 0.005320537847583182\n",
      "Training Loss: 0.005333883885759861\n",
      "Training Loss: 0.003659394758142298\n",
      "Training Loss: 0.0012097534169151914\n",
      "Training Loss: 0.0012314002004859504\n",
      "Training Loss: 0.0010725985896715428\n",
      "Validation Loss: 0.0018434006897802377\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 145\n",
      "Training Loss: 0.005268268273212016\n",
      "Training Loss: 0.005317041730158963\n",
      "Training Loss: 0.005329158331733197\n",
      "Training Loss: 0.0036567167942121157\n",
      "Training Loss: 0.0012079374221502803\n",
      "Training Loss: 0.001229314537922619\n",
      "Training Loss: 0.0010704315974726342\n",
      "Validation Loss: 0.001841828530398808\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 146\n",
      "Training Loss: 0.005263919312274084\n",
      "Training Loss: 0.005313591518788598\n",
      "Training Loss: 0.005324515094980597\n",
      "Training Loss: 0.0036541118477180137\n",
      "Training Loss: 0.0012061635176360142\n",
      "Training Loss: 0.001227277810612577\n",
      "Training Loss: 0.0010683200908533762\n",
      "Validation Loss: 0.0018403259884698616\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 147\n",
      "Training Loss: 0.00525966145156417\n",
      "Training Loss: 0.005310189162264578\n",
      "Training Loss: 0.0053199577808845785\n",
      "Training Loss: 0.003651580730074784\n",
      "Training Loss: 0.00120443052452174\n",
      "Training Loss: 0.0012252909976086812\n",
      "Training Loss: 0.001066265191329876\n",
      "Validation Loss: 0.00183889294943113\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 148\n",
      "Training Loss: 0.005255485629895702\n",
      "Training Loss: 0.005306825001607649\n",
      "Training Loss: 0.005315480895224028\n",
      "Training Loss: 0.0036491194984409957\n",
      "Training Loss: 0.001202737223211443\n",
      "Training Loss: 0.001223349692809279\n",
      "Training Loss: 0.0010642609710339458\n",
      "Validation Loss: 0.0018375252741075047\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 149\n",
      "Training Loss: 0.005251395616796799\n",
      "Training Loss: 0.005303502753959037\n",
      "Training Loss: 0.005311084575951099\n",
      "Training Loss: 0.003646724849531893\n",
      "Training Loss: 0.0012010847587953322\n",
      "Training Loss: 0.0012214629427035107\n",
      "Training Loss: 0.0010623139995732346\n",
      "Validation Loss: 0.0018362255715974345\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 150\n",
      "Training Loss: 0.005247383945970796\n",
      "Training Loss: 0.005300216166069731\n",
      "Training Loss: 0.005306766112917103\n",
      "Training Loss: 0.003644395900919335\n",
      "Training Loss: 0.0011994692613370717\n",
      "Training Loss: 0.0012196216566371732\n",
      "Training Loss: 0.0010604162170784548\n",
      "Validation Loss: 0.0018349887066553888\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Training Loss: 0.0052434502600226555\n",
      "Training Loss: 0.005296963677974418\n",
      "Training Loss: 0.005302522290148773\n",
      "Training Loss: 0.0036421302580856717\n",
      "Training Loss: 0.0011978947192255874\n",
      "Training Loss: 0.001217830802925164\n",
      "Training Loss: 0.0010585715449997223\n",
      "Validation Loss: 0.0018338153469254182\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 152\n",
      "Training Loss: 0.005239592875586823\n",
      "Training Loss: 0.005293742454960011\n",
      "Training Loss: 0.0052983549243072045\n",
      "Training Loss: 0.003639925789466361\n",
      "Training Loss: 0.001196353344275849\n",
      "Training Loss: 0.0012160866725753295\n",
      "Training Loss: 0.0010567738734243903\n",
      "Validation Loss: 0.001832697944814158\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 153\n",
      "Training Loss: 0.005235811471357011\n",
      "Training Loss: 0.005290554396924563\n",
      "Training Loss: 0.005294260498485528\n",
      "Training Loss: 0.0036377792981511447\n",
      "Training Loss: 0.00119484647657373\n",
      "Training Loss: 0.001214389009255683\n",
      "Training Loss: 0.0010550241790770088\n",
      "Validation Loss: 0.001831641453531944\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 154\n",
      "Training Loss: 0.0052321017446229235\n",
      "Training Loss: 0.005287392755853944\n",
      "Training Loss: 0.0052902364492183555\n",
      "Training Loss: 0.0036356872381293217\n",
      "Training Loss: 0.001193374340655282\n",
      "Training Loss: 0.0012127356958808377\n",
      "Training Loss: 0.0010533199430210515\n",
      "Validation Loss: 0.0018306341956845704\n",
      "Validation Accuracy: 0.011704119850187267\n",
      "**************************************************\n",
      "Epoch: 155\n",
      "Training Loss: 0.005228463489911519\n",
      "Training Loss: 0.005284262053901329\n",
      "Training Loss: 0.005286284191533923\n",
      "Training Loss: 0.003633648281102069\n",
      "Training Loss: 0.0011919317714637145\n",
      "Training Loss: 0.0012111266964348034\n",
      "Training Loss: 0.001051657247444382\n",
      "Validation Loss: 0.0018296791667865122\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 156\n",
      "Training Loss: 0.005224894433049485\n",
      "Training Loss: 0.005281157073331997\n",
      "Training Loss: 0.005282400622963906\n",
      "Training Loss: 0.0036316613615781533\n",
      "Training Loss: 0.0011905224743532017\n",
      "Training Loss: 0.0012095637316815555\n",
      "Training Loss: 0.0010500397071882617\n",
      "Validation Loss: 0.001828774403582749\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 157\n",
      "Training Loss: 0.005221388885984197\n",
      "Training Loss: 0.005278076847316697\n",
      "Training Loss: 0.005278583208564669\n",
      "Training Loss: 0.0036297233258665075\n",
      "Training Loss: 0.0011891393703990615\n",
      "Training Loss: 0.0012080373252683786\n",
      "Training Loss: 0.0010484592730063014\n",
      "Validation Loss: 0.0018279154125290613\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 158\n",
      "Training Loss: 0.005217950941878371\n",
      "Training Loss: 0.005275021069101058\n",
      "Training Loss: 0.00527483022713568\n",
      "Training Loss: 0.0036278314895753284\n",
      "Training Loss: 0.0011877876122889574\n",
      "Training Loss: 0.0012065580075432082\n",
      "Training Loss: 0.0010469213756732643\n",
      "Validation Loss: 0.0018271011234329113\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 159\n",
      "Training Loss: 0.005214572818367742\n",
      "Training Loss: 0.005271986537845805\n",
      "Training Loss: 0.005271143328282051\n",
      "Training Loss: 0.0036259832841460593\n",
      "Training Loss: 0.0011864601897832471\n",
      "Training Loss: 0.001205114960175706\n",
      "Training Loss: 0.001045420949376421\n",
      "Validation Loss: 0.001826330299763016\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 160\n",
      "Training Loss: 0.005211254310561344\n",
      "Training Loss: 0.00526897516916506\n",
      "Training Loss: 0.00526751669938676\n",
      "Training Loss: 0.0036241758726828265\n",
      "Training Loss: 0.0011851602551178075\n",
      "Training Loss: 0.0012037113416590729\n",
      "Training Loss: 0.0010439538363425527\n",
      "Validation Loss: 0.001825593045563001\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Training Loss: 0.005207996499957517\n",
      "Training Loss: 0.005265988082974218\n",
      "Training Loss: 0.0052639532467583195\n",
      "Training Loss: 0.0036224088331800884\n",
      "Training Loss: 0.0011838828469626606\n",
      "Training Loss: 0.0012023416798911057\n",
      "Training Loss: 0.0010425214594579303\n",
      "Validation Loss: 0.0018248927633649555\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 162\n",
      "Training Loss: 0.005204796380130574\n",
      "Training Loss: 0.005263020837446675\n",
      "Training Loss: 0.005260447976761498\n",
      "Training Loss: 0.0036206794156169054\n",
      "Training Loss: 0.00118262896372471\n",
      "Training Loss: 0.0012010079876927193\n",
      "Training Loss: 0.0010411201017268468\n",
      "Validation Loss: 0.0018242250160882703\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 163\n",
      "Training Loss: 0.00520165283116512\n",
      "Training Loss: 0.005260075143305585\n",
      "Training Loss: 0.005257000964484178\n",
      "Training Loss: 0.0036189846036722884\n",
      "Training Loss: 0.0011813967241323553\n",
      "Training Loss: 0.0011997077125124633\n",
      "Training Loss: 0.0010397498779639136\n",
      "Validation Loss: 0.001823590613190043\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 164\n",
      "Training Loss: 0.005198561875731684\n",
      "Training Loss: 0.005257148087839596\n",
      "Training Loss: 0.00525361105101183\n",
      "Training Loss: 0.003617324365914101\n",
      "Training Loss: 0.0011801832712080794\n",
      "Training Loss: 0.0011984397754713427\n",
      "Training Loss: 0.001038408610620536\n",
      "Validation Loss: 0.001822984975122599\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 165\n",
      "Training Loss: 0.005195522763533518\n",
      "Training Loss: 0.005254242499941029\n",
      "Training Loss: 0.005250276717706583\n",
      "Training Loss: 0.0036156950240547304\n",
      "Training Loss: 0.001178992666391423\n",
      "Training Loss: 0.0011972020597022493\n",
      "Training Loss: 0.001037095069186762\n",
      "Validation Loss: 0.0018224054139782413\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 166\n",
      "Training Loss: 0.005192534564412199\n",
      "Training Loss: 0.005251353037892841\n",
      "Training Loss: 0.005246995277120732\n",
      "Training Loss: 0.003614095474185888\n",
      "Training Loss: 0.001177819524455117\n",
      "Training Loss: 0.0011959950605523772\n",
      "Training Loss: 0.001035808504529996\n",
      "Validation Loss: 0.0018218488725467343\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 167\n",
      "Training Loss: 0.005189595283009112\n",
      "Training Loss: 0.005248484878102318\n",
      "Training Loss: 0.005243766126804985\n",
      "Training Loss: 0.0036125230410834776\n",
      "Training Loss: 0.0011766628186160233\n",
      "Training Loss: 0.0011948128016956616\n",
      "Training Loss: 0.0010345457697985693\n",
      "Validation Loss: 0.0018213142728963204\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 168\n",
      "Training Loss: 0.005186700184131041\n",
      "Training Loss: 0.005245634184102527\n",
      "Training Loss: 0.005240586401196196\n",
      "Training Loss: 0.003610979537625099\n",
      "Training Loss: 0.0011755252489820123\n",
      "Training Loss: 0.0011936606079689226\n",
      "Training Loss: 0.001033306987665128\n",
      "Validation Loss: 0.001820804567158236\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 169\n",
      "Training Loss: 0.0051838527503423396\n",
      "Training Loss: 0.0052428014483302835\n",
      "Training Loss: 0.005237458515330218\n",
      "Training Loss: 0.0036094595248869154\n",
      "Training Loss: 0.0011743984582426492\n",
      "Training Loss: 0.0011925276819965802\n",
      "Training Loss: 0.0010320885863620787\n",
      "Validation Loss: 0.0018203102558204199\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 170\n",
      "Training Loss: 0.005181052857660688\n",
      "Training Loss: 0.0052399903885088864\n",
      "Training Loss: 0.005234379210160114\n",
      "Training Loss: 0.0036079629784217106\n",
      "Training Loss: 0.0011732883314834908\n",
      "Training Loss: 0.0011914193267875816\n",
      "Training Loss: 0.001030890400579665\n",
      "Validation Loss: 0.0018198299461501311\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Training Loss: 0.005178295116056688\n",
      "Training Loss: 0.0052371973189292475\n",
      "Training Loss: 0.005231349069508724\n",
      "Training Loss: 0.0036064904619706795\n",
      "Training Loss: 0.0011721892813511658\n",
      "Training Loss: 0.0011903291968337725\n",
      "Training Loss: 0.0010297082459146622\n",
      "Validation Loss: 0.0018193672798322827\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 172\n",
      "Training Loss: 0.005175581341027282\n",
      "Training Loss: 0.005234421755303629\n",
      "Training Loss: 0.005228362528141588\n",
      "Training Loss: 0.003605036480730632\n",
      "Training Loss: 0.0011711049359291793\n",
      "Training Loss: 0.0011892624726169742\n",
      "Training Loss: 0.0010285474380361847\n",
      "Validation Loss: 0.0018189172334585352\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 173\n",
      "Training Loss: 0.005172905993531458\n",
      "Training Loss: 0.005231664682505652\n",
      "Training Loss: 0.005225421114591882\n",
      "Training Loss: 0.003603601431241259\n",
      "Training Loss: 0.0011700320431555155\n",
      "Training Loss: 0.0011882135132327675\n",
      "Training Loss: 0.0010274029390711804\n",
      "Validation Loss: 0.0018184826175650854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 174\n",
      "Training Loss: 0.005170271957176738\n",
      "Training Loss: 0.00522892418783158\n",
      "Training Loss: 0.005222522909753025\n",
      "Training Loss: 0.0036021855259605216\n",
      "Training Loss: 0.0011689698460395448\n",
      "Training Loss: 0.0011871829489246012\n",
      "Training Loss: 0.0010262729301757645\n",
      "Validation Loss: 0.001818056042613078\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 175\n",
      "Training Loss: 0.005167677521239966\n",
      "Training Loss: 0.005226201086770743\n",
      "Training Loss: 0.005219668152858503\n",
      "Training Loss: 0.0036007871839683504\n",
      "Training Loss: 0.001167917512502754\n",
      "Training Loss: 0.0011861668032361195\n",
      "Training Loss: 0.0010251578195311596\n",
      "Validation Loss: 0.0018176385492552072\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 176\n",
      "Training Loss: 0.00516511662339326\n",
      "Training Loss: 0.005223494495730847\n",
      "Training Loss: 0.005216852831654251\n",
      "Training Loss: 0.0035994043068785686\n",
      "Training Loss: 0.001166873948823195\n",
      "Training Loss: 0.0011851665527501608\n",
      "Training Loss: 0.0010240555615746417\n",
      "Validation Loss: 0.0018172297129243585\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 177\n",
      "Training Loss: 0.005162596029113047\n",
      "Training Loss: 0.0052208099170820785\n",
      "Training Loss: 0.005214080220321193\n",
      "Training Loss: 0.0035980377801752184\n",
      "Training Loss: 0.001165836938744178\n",
      "Training Loss: 0.0011841764728887938\n",
      "Training Loss: 0.0010229620595055167\n",
      "Validation Loss: 0.0018168253547682911\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 178\n",
      "Training Loss: 0.005160112597513944\n",
      "Training Loss: 0.005218141774530522\n",
      "Training Loss: 0.005211346057476476\n",
      "Training Loss: 0.0035966839622415137\n",
      "Training Loss: 0.0011648098831938114\n",
      "Training Loss: 0.0011831995264219586\n",
      "Training Loss: 0.0010218826953496318\n",
      "Validation Loss: 0.001816427968233154\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 179\n",
      "Training Loss: 0.005157663049758412\n",
      "Training Loss: 0.005215491947601549\n",
      "Training Loss: 0.005208651194116101\n",
      "Training Loss: 0.0035953438529395497\n",
      "Training Loss: 0.0011637882575450931\n",
      "Training Loss: 0.00118223415556713\n",
      "Training Loss: 0.0010208130773389711\n",
      "Validation Loss: 0.0018160397437364491\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 180\n",
      "Training Loss: 0.00515524699119851\n",
      "Training Loss: 0.005212856018333696\n",
      "Training Loss: 0.005205990918329917\n",
      "Training Loss: 0.003594017286086455\n",
      "Training Loss: 0.0011627753863285761\n",
      "Training Loss: 0.0011812790276599116\n",
      "Training Loss: 0.001019751248677494\n",
      "Validation Loss: 0.0018156509976598612\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Training Loss: 0.005152864512638189\n",
      "Training Loss: 0.0052102400519652295\n",
      "Training Loss: 0.005203368716174737\n",
      "Training Loss: 0.0035926999103685375\n",
      "Training Loss: 0.001161764720309293\n",
      "Training Loss: 0.0011803333705756812\n",
      "Training Loss: 0.001018699185369769\n",
      "Validation Loss: 0.001815267198253423\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 182\n",
      "Training Loss: 0.005150515213026665\n",
      "Training Loss: 0.005207640158478171\n",
      "Training Loss: 0.005200780380400829\n",
      "Training Loss: 0.003591396195697598\n",
      "Training Loss: 0.0011607630521757529\n",
      "Training Loss: 0.00117939554649638\n",
      "Training Loss: 0.0010176545164722484\n",
      "Validation Loss: 0.0018148865858799244\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 183\n",
      "Training Loss: 0.005148196374066174\n",
      "Training Loss: 0.00520505691296421\n",
      "Training Loss: 0.005198226284701377\n",
      "Training Loss: 0.003590100935252849\n",
      "Training Loss: 0.0011597642516426277\n",
      "Training Loss: 0.0011784644651925192\n",
      "Training Loss: 0.0010166174353798852\n",
      "Validation Loss: 0.001814509177652854\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 184\n",
      "Training Loss: 0.005145909003331326\n",
      "Training Loss: 0.005202491098898463\n",
      "Training Loss: 0.0051957061875145885\n",
      "Training Loss: 0.003588814479735447\n",
      "Training Loss: 0.0011587697155482602\n",
      "Training Loss: 0.0011775354853307363\n",
      "Training Loss: 0.001015581368119456\n",
      "Validation Loss: 0.0018141293684142722\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 185\n",
      "Training Loss: 0.005143655318534002\n",
      "Training Loss: 0.00519994449801743\n",
      "Training Loss: 0.005193220396758989\n",
      "Training Loss: 0.003587537935527507\n",
      "Training Loss: 0.0011577749170828611\n",
      "Training Loss: 0.001176612429699162\n",
      "Training Loss: 0.0010145515801559667\n",
      "Validation Loss: 0.001813752551420072\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 186\n",
      "Training Loss: 0.005141433015814982\n",
      "Training Loss: 0.005197413754067384\n",
      "Training Loss: 0.00519076565396972\n",
      "Training Loss: 0.0035862699020071886\n",
      "Training Loss: 0.0011567877784545999\n",
      "Training Loss: 0.001175695734564215\n",
      "Training Loss: 0.0010135291157348546\n",
      "Validation Loss: 0.0018133767237381011\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 187\n",
      "Training Loss: 0.005139234198140912\n",
      "Training Loss: 0.005194897197652608\n",
      "Training Loss: 0.005188340146560222\n",
      "Training Loss: 0.003585008147492772\n",
      "Training Loss: 0.001155803234578343\n",
      "Training Loss: 0.0011747831427783239\n",
      "Training Loss: 0.0010125106273335405\n",
      "Validation Loss: 0.0018129996139615468\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 188\n",
      "Training Loss: 0.005137064521550201\n",
      "Training Loss: 0.0051923982222797345\n",
      "Training Loss: 0.005185946597484872\n",
      "Training Loss: 0.003583755080908304\n",
      "Training Loss: 0.0011548183312697802\n",
      "Training Loss: 0.001173868580517592\n",
      "Training Loss: 0.001011492273391923\n",
      "Validation Loss: 0.0018126197709320009\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 189\n",
      "Training Loss: 0.005134927147300914\n",
      "Training Loss: 0.005189916362287477\n",
      "Training Loss: 0.005183580738375894\n",
      "Training Loss: 0.003582507157261716\n",
      "Training Loss: 0.0011538372641371098\n",
      "Training Loss: 0.0011729592498159035\n",
      "Training Loss: 0.0010104784117720555\n",
      "Validation Loss: 0.0018122394758665564\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 190\n",
      "Training Loss: 0.005132812824449502\n",
      "Training Loss: 0.005187446902855299\n",
      "Training Loss: 0.005181244736886584\n",
      "Training Loss: 0.003581264087551972\n",
      "Training Loss: 0.0011528563460160512\n",
      "Training Loss: 0.0011720480637450238\n",
      "Training Loss: 0.0010094653438136446\n",
      "Validation Loss: 0.001811856992236663\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Training Loss: 0.005130729216034524\n",
      "Training Loss: 0.0051849984505679455\n",
      "Training Loss: 0.005178937966702506\n",
      "Training Loss: 0.0035800281015690417\n",
      "Training Loss: 0.0011518749347305857\n",
      "Training Loss: 0.0011711369411204942\n",
      "Training Loss: 0.0010084529979212675\n",
      "Validation Loss: 0.0018114740810965708\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 192\n",
      "Training Loss: 0.005128672039718367\n",
      "Training Loss: 0.005182562812697142\n",
      "Training Loss: 0.005176657440024428\n",
      "Training Loss: 0.0035787974395498166\n",
      "Training Loss: 0.0011508950391726103\n",
      "Training Loss: 0.0011702248462825081\n",
      "Training Loss: 0.0010074408259242774\n",
      "Validation Loss: 0.0018110837114186547\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 193\n",
      "Training Loss: 0.005126642482355237\n",
      "Training Loss: 0.005180146650527604\n",
      "Training Loss: 0.005174405233119615\n",
      "Training Loss: 0.003577571127389092\n",
      "Training Loss: 0.0011499116441700608\n",
      "Training Loss: 0.0011693093911162578\n",
      "Training Loss: 0.0010064262970990968\n",
      "Validation Loss: 0.001810692197317083\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 194\n",
      "Training Loss: 0.005124639649875462\n",
      "Training Loss: 0.005177745134569704\n",
      "Training Loss: 0.00517217892804183\n",
      "Training Loss: 0.003576348664792022\n",
      "Training Loss: 0.0011489311426703354\n",
      "Training Loss: 0.0011683935982000548\n",
      "Training Loss: 0.0010054140274587554\n",
      "Validation Loss: 0.001810297666009418\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 195\n",
      "Training Loss: 0.00512266049394384\n",
      "Training Loss: 0.005175356582039967\n",
      "Training Loss: 0.00516997650382109\n",
      "Training Loss: 0.003575131314137252\n",
      "Training Loss: 0.0011479494458762928\n",
      "Training Loss: 0.001167473543464439\n",
      "Training Loss: 0.0010043996742751915\n",
      "Validation Loss: 0.0018098999610761954\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 196\n",
      "Training Loss: 0.005120706950547174\n",
      "Training Loss: 0.005172984229284338\n",
      "Training Loss: 0.005167800445924513\n",
      "Training Loss: 0.0035739166905113962\n",
      "Training Loss: 0.0011469654114625882\n",
      "Training Loss: 0.0011665503702533897\n",
      "Training Loss: 0.0010033831950568128\n",
      "Validation Loss: 0.0018094972809140086\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 197\n",
      "Training Loss: 0.00511878069548402\n",
      "Training Loss: 0.00517062715953216\n",
      "Training Loss: 0.005165651369024999\n",
      "Training Loss: 0.003572705221886281\n",
      "Training Loss: 0.0011459773941896855\n",
      "Training Loss: 0.0011656209139619023\n",
      "Training Loss: 0.0010023612549412064\n",
      "Validation Loss: 0.0018090879949453297\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 198\n",
      "Training Loss: 0.005116880423156544\n",
      "Training Loss: 0.005168288833810947\n",
      "Training Loss: 0.005163526873802766\n",
      "Training Loss: 0.003571496002405183\n",
      "Training Loss: 0.001144987896841485\n",
      "Training Loss: 0.0011646864452632144\n",
      "Training Loss: 0.0010013384452031459\n",
      "Validation Loss: 0.0018086749413931456\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 199\n",
      "Training Loss: 0.005115003202226944\n",
      "Training Loss: 0.005165961929014884\n",
      "Training Loss: 0.005161425718688406\n",
      "Training Loss: 0.0035702895163558426\n",
      "Training Loss: 0.001143996890023118\n",
      "Training Loss: 0.0011637457068718504\n",
      "Training Loss: 0.0010003130992117804\n",
      "Validation Loss: 0.0018082573945147477\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n",
      "Epoch: 200\n",
      "Training Loss: 0.0051131508313119415\n",
      "Training Loss: 0.005163650112808682\n",
      "Training Loss: 0.005159346308209934\n",
      "Training Loss: 0.0035690865262586156\n",
      "Training Loss: 0.0011430039844708517\n",
      "Training Loss: 0.0011628002079669387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:15:36<00:00, 453.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0009992819309991318\n",
      "Validation Loss: 0.001807834395082765\n",
      "Validation Accuracy: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictive performance\n",
    "predictive_results = predictive_evaluation(data_real_numpy, data_syn_numpy, hyperparameters, include_baseline=True, verbose=True)\n",
    "\n",
    "# save results\n",
    "bidirectionality = \"bi\" if hyperparameters[\"bidirectional\"] else 'no_bi'\n",
    "predictive_results.to_csv(DATA_FOLDER / f\"results_{syn_data_type}_{hyperparameters['num_epochs']}_{hyperparameters['num_evaluation_runs']}_{bidirectionality}.csv\", index=False)\n",
    "\n",
    "# split in mse and mae results\n",
    "mse_results = predictive_results.loc[predictive_results['Metric'] == 'MSE']\n",
    "mae_results = predictive_results.loc[predictive_results['Metric'] == 'MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x15ceb72b050>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJEAAAK9CAYAAACU8P3FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAACD1klEQVR4nOzdeXhTZeL28fskabpBW5bSRUtZxLKIA8IAdRuXQkFcUBxFQVEQFMENlxneYVxx11FRkZ8Mgg6bG6KCoIAI47CI7CAWVCggtKAlDYW2aZPz/sGQMbaQBtImbb+f68o15jknOffpMrR3n/McwzRNUwAAAAAAAMAJWEIdAAAAAAAAAOGPEgkAAAAAAAB+USIBAAAAAADAL0okAAAAAAAA+EWJBAAAAAAAAL8okQAAAAAAAOAXJRIAAAAAAAD8okQCAAAAAACAX5RIAAAAAAAA8IsSCQCAOuTRRx9VixYtQh2j3jEMQ48++miNHKu+fo5P5WPcokUL3XLLLUHNAwBAfUSJBABAFUydOlWGYcgwDH399dcVtpumqbS0NBmGocsvv7zS93A4HIqKipJhGNq6dWul+9xyyy3e4/z+ERUVFdRzCrbPPvusxooUVM0bb7yhP//5z2revLkMwzhhkeJwODR8+HAlJiYqNjZWF198sdauXXvC9//t98WJHvWx9Drm9x+LuLg4/elPf9K8efNCHQ0AgIDZQh0AAIDaJCoqSjNmzND555/vM7506VLt2bNHkZGRx33t+++/L8MwlJycrOnTp2vcuHGV7hcZGal//vOfFcatVuupha9mn332mV5//XWKpDDy7LPP6tChQ+rWrZv27dt33P08Ho/69u2rDRs26MEHH1TTpk01YcIEXXTRRVqzZo3atGlT6esuvPBC/etf//IZu+2229StWzcNHz7cO9agQYNTPpfi4mLZbCf3o2tOTo4sltD97bRnz566+eabZZqmcnNz9cYbb+iKK67Q/PnzlZ2dHbJcAAAEihIJAIAAXHbZZXr//fc1fvx4n19oZ8yYoS5duuiXX3457munTZumyy67TOnp6ZoxY8ZxSySbzaZBgwYFPTtqh5KSEtnt9qCUHkuXLvXOQjpRkfPBBx9o+fLlev/993XttddKkq677jqdeeaZeuSRRzRjxoxKX9eqVSu1atXKZ+yOO+5Qq1atTvg1XF5eLo/HI7vdXuVzOZWZeCcqd2vCmWee6fPx6N+/v9q3b69XXnmFEgkAUKtwORsAAAG44YYb9Ouvv2rhwoXeMZfLpQ8++EA33njjcV+3a9cu/fvf/9aAAQM0YMAA7dixQ8uXL6+JyJVyuVx6+OGH1aVLF8XHxys2NlYXXHCBlixZ4rPfV199JcMw9NVXX/mM79y5U4ZhaOrUqZKOXob3+uuvS/K9fOeYw4cP6/7771daWpoiIyOVkZGhF154QaZpVsg2bdo0denSRdHR0WrcuLEGDBig3bt3++xz0UUX6ayzztJ3332niy++WDExMTrttNP03HPPVXi/kpISPfroozrzzDMVFRWllJQUXXPNNfrxxx8DzldaWqr77rtPiYmJatiwoa688krt2bOn0o/xzz//rCFDhigpKUmRkZHq0KGD3nrrrUo/vrNmzdLYsWN12mmnKSYmRk6ns9L3DFR6errP5+F4PvjgAyUlJemaa67xjiUmJuq6667Txx9/rNLS0pPOcOxr5YUXXtDLL7+s1q1bKzIyUt99912Vvw6limsiPfroozIMQz/88INuueUWJSQkKD4+XrfeequOHDni89rfr4l07DK8//znPxo9erT3Er6rr75aBw4c8Hmtx+PRo48+qtTUVMXExOjiiy/Wd999d0rrLLVr105Nmzb1+Ro8lmnnzp0++1b2PRjI1/+rr76qDh06KCYmRo0aNVLXrl2PWwoCAOAPM5EAAAhAixYtlJmZqZkzZ6pPnz6SpPnz56uwsFADBgzQ+PHjK33dzJkzFRsbq8svv1zR0dFq3bq1pk+frnPPPbfS/Sub0WS32xUXFxeU83A6nfrnP/+pG264QcOGDdOhQ4c0efJkZWdn65tvvlGnTp0Cer/bb79de/fu1cKFCytc3mSapq688kotWbJEQ4cOVadOnfT555/rwQcf1M8//6yXXnrJu++TTz6pv//977ruuut022236cCBA3r11Vd14YUXat26dUpISPDue/DgQfXu3VvXXHONrrvuOn3wwQf6y1/+oo4dO3o/N263W5dffrkWL16sAQMG6J577tGhQ4e0cOFCbd68Wa1btw4o32233aZp06bpxhtv1Lnnnqsvv/xSffv2rfDxyM/PV48ePWQYhkaNGqXExETNnz9fQ4cOldPp1L333uuz/xNPPCG73a4HHnhApaWlAc3QCYZ169bpnHPOqTD7qVu3bnrzzTe1bds2dezY8ZSOMWXKFJWUlGj48OGKjIxU48aNg/J1eN1116lly5Z6+umntXbtWv3zn/9Us2bN9Oyzz/p97V133aVGjRrpkUce0c6dO/Xyyy9r1KhRevfdd737jBkzRs8995yuuOIKZWdna8OGDcrOzlZJSclJfywKCwt18OBBtW7d+qTfoypf/5MmTdLdd9+ta6+9Vvfcc49KSkq0ceNGrVq16oSlNwAAx2UCAAC/pkyZYkoyV69ebb722mtmw4YNzSNHjpimaZp//vOfzYsvvtg0TdNMT083+/btW+H1HTt2NAcOHOh9/v/+3/8zmzZtapaVlfnsN3jwYFNSpY/s7Gy/OR955BEzPT3d737l5eVmaWmpz9jBgwfNpKQkc8iQId6xJUuWmJLMJUuW+Oy7Y8cOU5I5ZcoU79jIkSPNyn60mDNnjinJHDdunM/4tddeaxqGYf7www+maZrmzp07TavVaj755JM++23atMm02Ww+43/6059MSeY777zjHSstLTWTk5PN/v37e8feeustU5L5j3/8o0Iuj8cTUL7169ebksw777zTZ78bb7zRlGQ+8sgj3rGhQ4eaKSkp5i+//OKz74ABA8z4+Hjv186xj2+rVq28Y/5U9XP8e7GxsebgwYOPu+23n/dj5s2bZ0oyFyxYcNLHOfa1EhcXZ+7fv99n36p+HZqmWeFj/Mgjj5iSKux39dVXm02aNPEZS09P98l07Ps5KyvL+3VgmqZ53333mVar1XQ4HKZpmmZeXp5ps9nMfv36+bzfo48+ako67sfz97mHDh1qHjhwwNy/f7/57bffmr179zYlmc8//3yFTDt27PB5fWXfg1X9+r/qqqvMDh06+M0IAEBVcTkbAAABuu6661RcXKy5c+fq0KFDmjt37gn/qr9x40Zt2rRJN9xwg3fshhtu0C+//KLPP/+8wv5RUVFauHBhhcczzzwTtHOwWq3e2S4ej0cFBQUqLy9X165d/d6RK1CfffaZrFar7r77bp/x+++/X6Zpav78+ZKk2bNny+Px6LrrrtMvv/zifSQnJ6tNmzYVLnFq0KCBzzozdrtd3bp1008//eQd+/DDD9W0aVPdddddFXIdu8yrqvk+++wzSaqw3+9nFZmmqQ8//FBXXHGFTNP0OZfs7GwVFhZW+BgPHjxY0dHRlX8Aa0BxcXGl6wYdW4eouLj4lI/Rv39/JSYm+owF4+vwjjvu8Hl+wQUX6Ndff63SJYHDhw/3udzvggsukNvtVm5uriRp8eLFKi8v15133unzusq+nk5k8uTJSkxMVLNmzdS1a1ctXrxYDz30kEaPHh3Q+/xWVb7+ExIStGfPHq1evfqkjwMAwG9xORsAAAFKTExUVlaWZsyYoSNHjsjtdnsXI67MtGnTFBsbq1atWumHH36QdPSX8xYtWmj69OkVLoeyWq3Kysqq1nOQpLffflsvvviivv/+e5WVlXnHW7ZsGdTj5ObmKjU1VQ0bNvQZb9eunXe7JG3fvl2maR73TmARERE+z08//fQK6/00atRIGzdu9D7/8ccflZGRccK7elU1X25uriwWS4VLkDIyMnyeHzhwQA6HQ2+++abefPPNSo+5f/9+n+fB/pgHKjo6utJ1j45dshWMgut453iqX4fNmzf3ed6oUSNJRy/38nf554leK/3vc3/GGWf47Ne4cWPvvlVx1VVXadSoUXK5XFq9erWeeuopHTly5JQWT6/K1/9f/vIXLVq0SN26ddMZZ5yhXr166cYbb9R555130scFANRvlEgAAJyEG2+8UcOGDVNeXp769Onjs1bPb5mmqZkzZ+rw4cNq3759he379+9XUVFRUG6BHohp06bplltuUb9+/fTggw+qWbNmslqtevrpp30W+z3eosxutzvomTwejwzD0Pz582W1Wits//3HqLJ9JFW6WHdN8ng8kqRBgwZp8ODBle5z9tln+zwP5SwkSUpJSdG+ffsqjB8bS01NPeVjVHaOVf06PJFT+Tqoqa+h008/3VsMX3bZZWratKlGjRqliy++2LuYeaDfa1XJ3q5dO+Xk5Gju3LlasGCBPvzwQ02YMEEPP/ywHnvssVM5JQBAPUWJBADASbj66qt1++23a+XKlT6L8P7e0qVLtWfPHj3++OPemS3HHDx4UMOHD9ecOXNOeDv06vDBBx+oVatWmj17ts8vr4888ojPfsdmWzgcDp/xYzM0fut4vwSnp6dr0aJFOnTokM9sn++//967XZJ3keuWLVvqzDPPDPykKtG6dWutWrVKZWVlFWYyBZovPT1dHo/HO7vpmJycHJ/3O3bnNrfbXSMzyoKhU6dO+ve//y2Px+MzO2bVqlWKiYkJ2ufj96r6dRgqxz73P/zwg8/MqF9//dU7W+lk3H777XrppZc0duxYXX311TIMI6DvtUDExsbq+uuv1/XXXy+Xy6VrrrlGTz75pMaMGeO9XBEAgKpiTSQAAE5CgwYN9MYbb+jRRx/VFVdccdz9jl3K9uCDD+raa6/1eQwbNkxt2rTR9OnTazD5UcdmMfx21sKqVau0YsUKn/3S09NltVq1bNkyn/EJEyZUeM/Y2FhJFX8Jvuyyy+R2u/Xaa6/5jL/00ksyDMN7J6lrrrlGVqtVjz32WIWZIKZp6tdffw3gDI/q37+/fvnllwrHPvaegeQ79r+/vwPfyy+/7PPcarWqf//++vDDD7V58+YKx/39LeTDwbXXXqv8/HzNnj3bO/bLL7/o/fff1xVXXFHpeknBUNWvw1C59NJLZbPZ9MYbb/iMV/b1FAibzab7779fW7du1ccffyxJ3sskf/u95na7j3tJZFX8/nvGbrerffv2Mk3T59JBAACqiplIAACcpONdqnRMaWmpPvzwQ/Xs2fO4f/G/8sor9corr2j//v1q1qyZJKm8vFzTpk2rdP+rr77aW9acissvv1yzZ8/W1Vdfrb59+2rHjh2aOHGi2rdvr6KiIu9+8fHx+vOf/6xXX31VhmGodevWmjt3boU1fSSpS5cuko4uPJ2dnS2r1aoBAwboiiuu0MUXX6y//e1v2rlzp/7whz/oiy++0Mcff6x7773X+8tz69atNW7cOI0ZM0Y7d+5Uv3791LBhQ+3YsUMfffSRhg8frgceeCCg87z55pv1zjvvaPTo0frmm290wQUX6PDhw1q0aJHuvPNOXXXVVVXO16lTJ91www2aMGGCCgsLde6552rx4sXeda5+65lnntGSJUvUvXt3DRs2TO3bt1dBQYHWrl2rRYsWqaCgIKDzOFmffvqpNmzYIEkqKyvTxo0bNW7cOElHv/aOXVZ37bXXqkePHrr11lv13XffqWnTppowYYLcbne1XvZU1a/DUElKStI999yjF198UVdeeaV69+6tDRs2aP78+WratOlxZ99VxS233KKHH35Yzz77rPr166cOHTqoR48eGjNmjAoKCtS4cWPNmjVL5eXlJ32MXr16KTk5Weedd56SkpK0detWvfbaa+rbt2+FNcAAAKgKSiQAAKrJvHnz5HA4TjhT6YorrtCLL76oWbNmee/6VVpaqptuuqnS/Xfs2BGUEumWW25RXl6e/u///k+ff/652rdvr2nTpun999/XV1995bPvq6++qrKyMk2cOFGRkZG67rrr9Pzzz+uss87y2e+aa67RXXfdpVmzZmnatGkyTVMDBgyQxWLRJ598oocffljvvvuupkyZohYtWuj555/X/fff7/Mef/3rX3XmmWfqpZde8pYXaWlp6tWrl6688sqAz9Nqteqzzz7Tk08+qRkzZujDDz9UkyZNdP7556tjx46SFFC+t956S4mJiZo+fbrmzJmjSy65RPPmzVNaWprPfklJSfrmm2/0+OOPa/bs2ZowYYKaNGmiDh066Nlnnw34PE7Whx9+qLffftv7fN26dVq3bp2ko+v0HCuRjn2cHnzwQY0fP17FxcX64x//qKlTp1ZYODyYAvk6DJVnn31WMTExmjRpkhYtWqTMzEx98cUXOv/880/pcrDo6GiNGjVKjz76qL766itddNFFmj59um6//XY988wzSkhI0NChQ3XxxRerZ8+eJ3WM22+/XdOnT9c//vEPFRUV6fTTT9fdd9+tsWPHnnRuAED9ZpihXn0SAAAEzaOPPqqpU6dq586doY6CasLnOPQcDocaNWqkcePG6W9/+1uo4wAAUGNYEwkAAAA4juLi4gpjx9bBuuiii2o2DAAAIcblbAAAAMBxvPvuu5o6daouu+wyNWjQQF9//bVmzpypXr166bzzzgt1PAAAahQlEgAAAHAcZ599tmw2m5577jk5nU7vYtvHFigHAKA+YU0kAAAAAAAA+MWaSAAAAAAAAPCLEgkAAAAAAAB+sSZSFXg8Hu3du1cNGzaUYRihjgMAAAAAABAUpmnq0KFDSk1NlcVy4rlGlEhVsHfvXqWlpYU6BgAAAAAAQLXYvXu3Tj/99BPuQ4lUBQ0bNpR09AMaFxcX4jQAAAAAAADB4XQ6lZaW5u0+ToQSqQqOXcIWFxdHiQQAAAAAAOqcqizfE9KFtZctW6YrrrhCqampMgxDc+bM8dlumqYefvhhpaSkKDo6WllZWdq+fbvPPgUFBRo4cKDi4uKUkJCgoUOHqqioyGefjRs36oILLlBUVJTS0tL03HPPVfepAQAAAAAA1CkhLZEOHz6sP/zhD3r99dcr3f7cc89p/PjxmjhxolatWqXY2FhlZ2erpKTEu8/AgQO1ZcsWLVy4UHPnztWyZcs0fPhw73an06levXopPT1da9as0fPPP69HH31Ub775ZrWfHwAAAAAAQF1hmKZphjqEdHTa1EcffaR+/fpJOjoLKTU1Vffff78eeOABSVJhYaGSkpI0depUDRgwQFu3blX79u21evVqde3aVZK0YMECXXbZZdqzZ49SU1P1xhtv6G9/+5vy8vJkt9slSX/96181Z84cff/991XK5nQ6FR8fr8LCQi5nAwAAAAAAdUYgnUfYrom0Y8cO5eXlKSsryzsWHx+v7t27a8WKFRowYIBWrFihhIQEb4EkSVlZWbJYLFq1apWuvvpqrVixQhdeeKG3QJKk7OxsPfvsszp48KAaNWpU4dilpaUqLS31Pnc6ndV0lgAAAAAAoKpM01R5ebncbneoo9QqERERslqtp/w+YVsi5eXlSZKSkpJ8xpOSkrzb8vLy1KxZM5/tNptNjRs39tmnZcuWFd7j2LbKSqSnn35ajz32WHBOBAAAAAAAnDKXy6V9+/bpyJEjoY5S6xiGodNPP10NGjQ4pfcJ2xIplMaMGaPRo0d7nx+73R0AAAAAAKh5Ho9HO3bskNVqVWpqqux2e5XuJoajs7cOHDigPXv2qE2bNqc0IylsS6Tk5GRJUn5+vlJSUrzj+fn56tSpk3ef/fv3+7yuvLxcBQUF3tcnJycrPz/fZ59jz4/t83uRkZGKjIwMynkAAAAAAIBT43K55PF4lJaWppiYmFDHqXUSExO1c+dOlZWVnVKJFNK7s51Iy5YtlZycrMWLF3vHnE6nVq1apczMTElSZmamHA6H1qxZ493nyy+/lMfjUffu3b37LFu2TGVlZd59Fi5cqIyMjEovZQMAAAAAAOHJYgnbGiOsBWvWVkg/+kVFRVq/fr3Wr18v6ehi2uvXr9euXbtkGIbuvfdejRs3Tp988ok2bdqkm2++Wampqd47uLVr1069e/fWsGHD9M033+g///mPRo0apQEDBig1NVWSdOONN8put2vo0KHasmWL3n33Xb3yyis+l6sBAAAAAADgxEJ6Odu3336riy++2Pv8WLEzePBgTZ06VQ899JAOHz6s4cOHy+Fw6Pzzz9eCBQsUFRXlfc306dM1atQoXXrppbJYLOrfv7/Gjx/v3R4fH68vvvhCI0eOVJcuXdS0aVM9/PDDGj58eM2dKAAAAAAAQC1nmKZphjpEuHM6nYqPj1dhYaHi4uJCHQcAAAAAgHqlpKREO3bsUMuWLX0mlqBqTvTxC6Tz4GJCAAAAAACAanLLLbfIMAzdcccdFbaNHDlShmHolltu8RlfsWKFrFar+vbtW+E1O3fulGEYlT5WrlxZXachKYzvzgYAAAAAABBs69ev1/z587Vv3z6lpKSoT58+3rvAV5e0tDTNmjVLL730kqKjoyUdnR00Y8YMNW/evML+kydP1l133aXJkydr79693nWff2vRokXq0KGDz1iTJk2q5wT+i5lIAAAAAACgXli/fr0mTpyo3NxcuVwu5ebmauLEid4bflWXc845R2lpaZo9e7Z3bPbs2WrevLk6d+7ss29RUZHeffddjRgxQn379tXUqVMrfc8mTZooOTnZ5xEREVGdp0GJBAAAAAAA6of58+dXOr5gwYJqP/aQIUM0ZcoU7/O33npLt956a4X93nvvPbVt21YZGRkaNGiQ3nrrLYXLctaUSAAAAAAAoF7Yt29fpeN79+6t9mMPGjRIX3/9tXJzc5Wbm6v//Oc/GjRoUIX9Jk+e7B3v3bu3CgsLtXTp0gr7nXvuuWrQoIHPo7qxJhIAAAAAAKgXUlJSlJubW2G8sjWHgi0xMdF7eZppmurbt6+aNm3qs09OTo6++eYbffTRR5Ikm82m66+/XpMnT9ZFF13ks++7776rdu3aVXvu36JEAgAAAAAA9UKfPn00ceLESsdrwpAhQzRq1ChJ0uuvv15h++TJk1VeXu5TapmmqcjISL322muKj4/3jqelpemMM86o/tC/weVsAAAAAACgXujUqZPuuOMOtWjRQna7XS1atNCIESP0hz/8oUaO37t3b7lcLpWVlSk7O9tnW3l5ud555x29+OKLWr9+vfexYcMGpaamaubMmTWS8USYiQQAAAAAAOqNTp06qVOnTiE5ttVq1datW73//Vtz587VwYMHNXToUJ8ZR5LUv39/TZ48WXfccYd37Ndff1VeXp7PfgkJCYqKiqqm9MxEAgAAAAAAqDFxcXGKi4urMD558mRlZWVVKJCkoyXSt99+q40bN3rHsrKylJKS4vOYM2dOdUZnJhIAAABQG3377bdavXq13G63zj77bJ177rmy2fjxHgDCzdSpU0+4vSrFT7du3WSapvf5b/+7JvGvDAAAAFDLvPvuu1qyZIn3+ebNm7Vx40aNHDlShmGEMBkAoC7jcjYAAACgFtm/f79PgXTM5s2b9f3334cgEQCgvmAmEgAAAOqckpIS5ebmhjpGtVi/fr2Kiooq3bZ06VJZLLX778Tp6enVuigsAODkUSIBAACgzsnNzdWwYcNCHaNalJWVHbdE2r59u2bMmFHDiYJr0qRJysjICHUMAEAlKJEAAABQ56Snp2vSpEmhjlEtduzYob/+9a9q3bq1oqOjveNRUVEaOXKkYmJiQpju1KWnp4c6AoAwFqoFpWu7YH3cKJEAAABQ50RFRdXp2SyxsbFq06aNHA6HJCk5OVk33XSTWrduHdpgAFBNIiIiJElHjhzxKdBRNS6XS5JktVpP6X0okQAAAIBaxmq16tZbb1WTJk3kdruVlJQU6kgAUK2sVqsSEhK0f/9+SVJMTAx3o6wij8ejAwcOKCYmRjbbqdVAlEgAAABALdW0adNQRwCAGpOcnCxJ3iIJVWexWNS8efNTLt4okQAAAAAAQNgzDEMpKSlq1qyZysrKQh2nVrHb7UG5eyclEgAAAAAAqDWsVuspr+2Dk3PqNRQAAAAAAADqPEokAAAAAAAA+EWJBAAAAAAAAL8okQAAAIBa5IcfftDhw4c1Y8YMLVq0SC6XK9SRAAD1BAtrAwAAALXEggULNHPmTLlcLv3444/Kz8/XmjVrNHr0aEVERIQ6HgCgjmMmEgAAAFALHD58WPPmzaswvmPHDn377bchSAQAqG8okQAAAIBaIDc3V2VlZZVuy8nJqeE0AID6iBIJAAAAqAUaNmx43G3x8fE1mAQAUF9RIgEAAAC1QFpamlq2bFlh3Gq16txzzw1BIgBAfUOJBAAAANQSt99+u1q1auV93rhxY91+++1KSkoKYSoAQH3B3dkAAACAWiIhIUEDBw7Ul19+qTvuuEPnnXeeDMMIdSwAQD3BTCQAAACglrFYLEpMTKRAAgDUKEokAAAAAAAA+EWJBAAAAAAAAL8okQAAAAAAAOAXJRIAAAAAAAD8okQCAAAAAACAX5RIAAAAAAAA8IsSCQAAAAAAAH5RIgEAAAAAAMAvSiQAAAAAAAD4RYkEAAAAAAAAvyiRAAAAAAAA4BclEgAAAAAAAPyiRAIAAAAAAIBflEgAAAAAAADwixIJAAAAAAAAflEiAQAAAAAAwC9KJAAAAAAAAPhFiQQAAAAAAAC/KJEAAACAOqagoEC7du1SWVlZqKMAAOoQW6gDAAAAAAiOw4cP6+2339bGjRslSbGxsbrmmmt03nnnhTgZAKAuYCYSAAAAUEe888473gJJOloq/etf/9L27dtDmAoAUFdQIgEAAAB1gMPh0IYNGyrd9vXXX9dwGgBAXUSJBAAAANQBRUVFx93mdDprMAkAoK6iRAIAAADqgOTkZMXHx1e6LSMjo4bTAADqIkokAAAAoA6w2Wzq379/hfGUlBRdeOGFIUgEAKhruDsbAAAAUEd069ZNiYmJ+ve//y2n06mMjAydd955iomJCXU0AEAdQIkEAAAA1CEtW7ZUy5YtQx0DAFAHcTkbAAAAAAAA/KJEAgAAAAAAgF+USAAAAAAAAPCLEgkAAAAAAAB+USIBAAAAAADAL0okAAAAAAAA+EWJBAAAAAAAAL/CvkQ6dOiQ7r33XqWnpys6OlrnnnuuVq9e7d1umqYefvhhpaSkKDo6WllZWdq+fbvPexQUFGjgwIGKi4tTQkKChg4dqqKiopo+FQAAAAAAgFor7Euk2267TQsXLtS//vUvbdq0Sb169VJWVpZ+/vlnSdJzzz2n8ePHa+LEiVq1apViY2OVnZ2tkpIS73sMHDhQW7Zs0cKFCzV37lwtW7ZMw4cPD9UpAQAAAAAA1DqGaZpmqEMcT3FxsRo2bKiPP/5Yffv29Y536dJFffr00RNPPKHU1FTdf//9euCBByRJhYWFSkpK0tSpUzVgwABt3bpV7du31+rVq9W1a1dJ0oIFC3TZZZdpz549Sk1NrXDc0tJSlZaWep87nU6lpaWpsLBQcXFx1XzWAAAAwPHl5ORo2LBhmjRpkjIyMkIdBwBQyzmdTsXHx1ep8wjrmUjl5eVyu92KioryGY+OjtbXX3+tHTt2KC8vT1lZWd5t8fHx6t69u1asWCFJWrFihRISErwFkiRlZWXJYrFo1apVlR736aefVnx8vPeRlpZWDWcHAAAAAABQe4R1idSwYUNlZmbqiSee0N69e+V2uzVt2jStWLFC+/btU15eniQpKSnJ53VJSUnebXl5eWrWrJnPdpvNpsaNG3v3+b0xY8aosLDQ+9i9e3c1nB0AAAAAAEDtEdYlkiT961//kmmaOu200xQZGanx48frhhtukMVSfdEjIyMVFxfn8wAAAAAAAKjPbKEO4E/r1q21dOlSHT58WE6nUykpKbr++uvVqlUrJScnS5Ly8/OVkpLifU1+fr46deokSUpOTtb+/ft93rO8vFwFBQXe1wMAANRn+fn5cjgcoY6BKsrNzfX5X4S/hISECldPAEBtFPYl0jGxsbGKjY3VwYMH9fnnn+u5555Ty5YtlZycrMWLF3tLI6fTqVWrVmnEiBGSpMzMTDkcDq1Zs0ZdunSRJH355ZfyeDzq3r17qE4HAAAgLOTn52vgoIFylbpCHQUBGjduXKgjoIrskXZNnzadIglArRf2JdLnn38u0zSVkZGhH374QQ8++KDatm2rW2+9VYZh6N5779W4cePUpk0btWzZUn//+9+Vmpqqfv36SZLatWun3r17a9iwYZo4caLKyso0atQoDRgwoNI7swEAANQnDodDrlKXPN08MuPC9qa9QK1lOA25vnHJ4XBQIgGo9cK+RCosLNSYMWO0Z88eNW7cWP3799eTTz6piIgISdJDDz2kw4cPa/jw4XI4HDr//PO1YMECnzu6TZ8+XaNGjdKll14qi8Wi/v37a/z48aE6JQAAgLBjxplSo1CnAOoeU5SzAOoOwzRN/l/ND6fTqfj4eBUWFrLINgAAqFNycnI0bNgwubPclEhAdTgoWRdZNWnSJGVkZIQ6DQBUEEjnEfZ3ZwMAAAAAAEDoUSIBAAAAAADAL0okAAAAAAAA+EWJBAAAAAAAAL8okQAAAAAAAOAXJRIAAAAAAAD8okQCAAAAAACAX5RIAAAAAAAA8IsSCQAAAAAAAH5RIgEAAAAAAMAvSiQAAAAAAAD4RYkEAAAAAAAAvyiRAAAAAAAA4BclEgAAAAAAAPyiRAIAAAAAAIBflEgAAAAAAADwixIJAAAAAAAAflEiAQAAAAAAwC9KJAAAAAAAAPhFiQQAAAAAAAC/KJEAAAAAAADgFyUSAAAAAAAA/KJEAgAAAAAAgF+USAAAAAAAAPCLEgkAAAAAAAB+USIBAAAAAADAL0okAAAAAAAA+EWJBAAAAAAAAL8okQAAAAAAAOAXJRIAAAAAAAD8okQCAAAAAACAX5RIAAAAAAAA8IsSCQAAAAAAAH5RIgEAAAAAAMAvSiQAAAAAAAD4RYkEAAAAAAAAvyiRAAAAAAAA4BclEgAAAAAAAPyiRAIAAAAAAIBflEgAAAAAAADwixIJAAAAAAAAflEiAQAAAAAAwC9KJAAAAAAAAPhFiQQAAAAAAAC/KJEAAAAAAADgFyUSAAAAAAAA/KJEAgAAAAAAgF+2UAcAAAAAUDVmmamyvWXyFHtkTbDKlmSTYRihjgUAqCcokQAAAIBawO1068jKI/K4PN4xW2ObYrrHyLBSJAEAqh+XswEAAAC1QMnmEp8CSZLKC8rl2uEKUSIAQH1DiQQAAACEOY/Lo/KC8kq3leWV1XAaAEB9RYkEAAAAhDnDMGSo8kvWDAuXsgEAagYlEgAAABDmjAhDtmaVL2cakRpRw2kAAPUVJRIAAABQC0R1jJK1odVnzH6aXRHplEgAgJrB3dkAAACAWsASbVHshbFy/+KWp9gjayNrhVIJAIDqRIkEAAAA1BKGYciWyI/wAIDQ4HI2AAAAAAAA+EWJBAAAAAAAAL8okQAAAAAAAOAXJRIAAAAAAAD8okQCAAAAAACAX5RIAAAAAAAA8IsSCQAAAAAAAH5RIgEAAAAAAMAvSiQAAAAAAAD4RYkEAAAAAAAAv8K6RHK73fr73/+uli1bKjo6Wq1bt9YTTzwh0zS9+5imqYcfflgpKSmKjo5WVlaWtm/f7vM+BQUFGjhwoOLi4pSQkKChQ4eqqKiopk8HAAAAAACg1grrEunZZ5/VG2+8oddee01bt27Vs88+q+eee06vvvqqd5/nnntO48eP18SJE7Vq1SrFxsYqOztbJSUl3n0GDhyoLVu2aOHChZo7d66WLVum4cOHh+KUAAAAAAAAaiVbqAOcyPLly3XVVVepb9++kqQWLVpo5syZ+uabbyQdnYX08ssva+zYsbrqqqskSe+8846SkpI0Z84cDRgwQFu3btWCBQu0evVqde3aVZL06quv6rLLLtMLL7yg1NTU0JwcAAAAAABALRLWM5HOPfdcLV68WNu2bZMkbdiwQV9//bX69OkjSdqxY4fy8vKUlZXlfU18fLy6d++uFStWSJJWrFihhIQEb4EkSVlZWbJYLFq1alWlxy0tLZXT6fR5AAAAAAAA1GdhPRPpr3/9q5xOp9q2bSur1Sq3260nn3xSAwcOlCTl5eVJkpKSknxel5SU5N2Wl5enZs2a+Wy32Wxq3Lixd5/fe/rpp/XYY48F+3QAAAAAAABqrbCeifTee+9p+vTpmjFjhtauXau3335bL7zwgt5+++1qPe6YMWNUWFjofezevbtajwcAAAAAABDuwnom0oMPPqi//vWvGjBggCSpY8eOys3N1dNPP63BgwcrOTlZkpSfn6+UlBTv6/Lz89WpUydJUnJysvbv3+/zvuXl5SooKPC+/vciIyMVGRlZDWcEAAAAAABQO4X1TKQjR47IYvGNaLVa5fF4JEktW7ZUcnKyFi9e7N3udDq1atUqZWZmSpIyMzPlcDi0Zs0a7z5ffvmlPB6PunfvXgNnAQAAAAAAUPuF9UykK664Qk8++aSaN2+uDh06aN26dfrHP/6hIUOGSJIMw9C9996rcePGqU2bNmrZsqX+/ve/KzU1Vf369ZMktWvXTr1799awYcM0ceJElZWVadSoURowYAB3ZgMAAAAAAKiisC6RXn31Vf3973/XnXfeqf379ys1NVW33367Hn74Ye8+Dz30kA4fPqzhw4fL4XDo/PPP14IFCxQVFeXdZ/r06Ro1apQuvfRSWSwW9e/fX+PHjw/FKQEAAAAAANRKhmmaZqhDhDun06n4+HgVFhYqLi4u1HEAAACCJicnR8OGDZM7yy01CnUaoA46KFkXWTVp0iRlZGSEOg0AVBBI5xHWayIBAAAAAAAgPFAiAQAAAAAAwC9KJAAAAAAAAPhFiQQAAAAAAAC/KJEAAAAAAADgFyUSAAAAAAAA/KJEAgAAAAAAgF+USAAAAAAAAPCLEgkAAAAAAAB+USIBAAAAAADAL0okAAAAAAAA+EWJBAAAAAAAAL8okQAAAAAAAOAXJRIAAAAAAAD8okQCAAAAAACAX5RIAAAAAAAA8IsSCQAAAAAAAH7ZQh0AAAAAQHhxO9xyH3LL0sAiWyN+ZQAAHMW/CAAAAAAkSabb1JFvj6j8QLl3zNbEppg/xsiwGSFMBgAIB1zOBgAAAECSVLqt1KdAkqTyX8tV+n1piBIBAMIJJRIAAAAASVLZz2UBjQMA6hdKJAAAAABHeSofNj1mzeYAAIQlSiQAAAAAkiRbcuVLph5vHABQv1AiAQAAAJAkRWZEytrA6jNmibEoqm1UiBIBAMIJf1IAAAAAIEmyRFoUe0GsyvaVyXPII0sDiyJSI2RYuTMbAIASCQAAAMBvGFZD9tPtoY4BAAhDXM4GAAAAAAAAvyiRAAAAAAAA4BclEgAAAAAAAPyiRAIAAAAAAIBflEgAAAAAAADwixIJAAAAAAAAflEiAQAAAAAAwC9KJAAAAAAAAPhFiQQAAAAAAAC/KJEAAAAAAADgFyUSAAAAAAAA/KJEAgAAAAAAgF+USAAAAAAAAPCLEgkAAAAAAAB+USIBAAAAAADAL0okAAAAAAAA+EWJBAAAAAAAAL8okQAAAAAAAOAXJRIAAAAAAAD8okQCAAAAAACAX5RIAAAAAAAA8IsSCQAAAAAAAH5RIgEAAAAAAMAvSiQAAAAAAAD4RYkEAAAAAAAAvyiRAAAAAAAA4BclEgAAAAAAAPyiRAIAAAAAAIBflEgAAAAAAADwixIJAAAAAAAAflEiAQAAAAAAwC9KJAAAAAAAAPhFiQQAAAAAAAC/KJEAAAAAAADgly3UAQAAABAGnKEOANRRfG8BqEMokQAAACDrN9ZQRwAAAGGOEgkAAAByd3NLcaFOAdRBTkpaAHUHJRIAAACOFkiNQh0CAACEMxbWBgAAAAAAgF+USAAAAAAAAPCLEgkAAAAAAAB+hX2J1KJFCxmGUeExcuRISVJJSYlGjhypJk2aqEGDBurfv7/y8/N93mPXrl3q27evYmJi1KxZMz344IMqLy8PxekAAAAAAADUSmFfIq1evVr79u3zPhYuXChJ+vOf/yxJuu+++/Tpp5/q/fff19KlS7V3715dc8013te73W717dtXLpdLy5cv19tvv62pU6fq4YcfDsn5AAAAAAAA1EYBlUhut1vLli2Tw+GopjgVJSYmKjk52fuYO3euWrdurT/96U8qLCzU5MmT9Y9//EOXXHKJunTpoilTpmj58uVauXKlJOmLL77Qd999p2nTpqlTp07q06ePnnjiCb3++utyuVw1dh4AAAAAAAC1WUAlktVqVa9evXTw4MHqynNCLpdL06ZN05AhQ2QYhtasWaOysjJlZWV592nbtq2aN2+uFStWSJJWrFihjh07KikpybtPdna2nE6ntmzZUulxSktL5XQ6fR4AAAAAAAD1WcCXs5111ln66aefqiOLX3PmzJHD4dAtt9wiScrLy5PdbldCQoLPfklJScrLy/Pu89sC6dj2Y9sq8/TTTys+Pt77SEtLC+6JAAAAAAAA1DIBl0jjxo3TAw88oLlz52rfvn01OmNn8uTJ6tOnj1JTU6v1OGPGjFFhYaH3sXv37mo9HgAAAAAAQLizBfqCyy67TJJ05ZVXyjAM77hpmjIMQ263O3jpfiM3N1eLFi3S7NmzvWPJyclyuVxyOBw+s5Hy8/OVnJzs3eebb77xea9jd287ts/vRUZGKjIyMshnAAAAAAAAUHsFXCItWbKkOnL4NWXKFDVr1kx9+/b1jnXp0kURERFavHix+vfvL0nKycnRrl27lJmZKUnKzMzUk08+qf3796tZs2aSpIULFyouLk7t27ev+RMBAAAAAACohQIukf70pz9VR44T8ng8mjJligYPHiyb7X+R4+PjNXToUI0ePVqNGzdWXFyc7rrrLmVmZqpHjx6SpF69eql9+/a66aab9NxzzykvL09jx47VyJEjmW0EAAAAAABQRQGXSJLkcDg0efJkbd26VZLUoUMHDRkyRPHx8UENd8yiRYu0a9cuDRkypMK2l156SRaLRf3791dpaamys7M1YcIE73ar1aq5c+dqxIgRyszMVGxsrAYPHqzHH3+8WrICAAAAAADURYZpmmYgL/j222+VnZ2t6OhodevWTZK0evVqFRcX64svvtA555xTLUFDyel0Kj4+XoWFhYqLiwt1HAAAgKDJycnRsGHD5M5yS41CnQaogw5K1kVWTZo0SRkZGaFOAwAVBNJ5BDwT6b777tOVV16pSZMmeS8tKy8v12233aZ7771Xy5YtO7nUAAAAAAAACFsBl0jffvutT4EkSTabTQ899JC6du0a1HAAAAAAAAAID5ZAXxAXF6ddu3ZVGN+9e7caNmwYlFAAAAAAAAAILwGXSNdff72GDh2qd999V7t379bu3bs1a9Ys3XbbbbrhhhuqIyMAAAAAAABCLODL2V544QUZhqGbb75Z5eXlkqSIiAiNGDFCzzzzTNADAgAAAAAAIPQCKpHcbrdWrlypRx99VE8//bR+/PFHSVLr1q0VExNTLQEBAAAAAAAQegGVSFarVb169dLWrVvVsmVLdezYsbpyAQAAAAAAIIwEvCbSWWedpZ9++qk6sgAAAAAAACBMBVwijRs3Tg888IDmzp2rffv2yel0+jwAAAAAAABQ9wS8sPZll10mSbryyitlGIZ33DRNGYYht9sdvHQAAAAAAAAICwGXSEuWLKmOHAAAAAAAAAhjAZVIZWVlevzxxzVx4kS1adOmujIBAAAAAAAgzAS0JlJERIQ2btxYXVkAAAAAAAAQpgJeWHvQoEGaPHlydWQBAAAAAABAmAp4TaTy8nK99dZbWrRokbp06aLY2Fif7f/4xz+CFg4AAAAAAADhIeASafPmzTrnnHMkSdu2bfPZ9tu7tQEAAAAAAKDu4O5sAAAAAAAA8CvgNZFOZP/+/cF8OwAAAAAAAISJKpdIMTExOnDggPd53759tW/fPu/z/Px8paSkBDcdAAAAUEu4nW6V/Vwmt9Md6igAAFSLKl/OVlJSItM0vc+XLVum4uJin31+ux0AAACoD0y3qeK1xSrLL/OORTSLUHSXaBlW1gwFANQdQb2cjYW1AQAAUN+Ubiv1KZAkqWx/mUq3lYYoEQAA1SOoJRIAAABQ35TtKQtoHACA2qrKJZJhGD4zjX7/HAAAAKiPTE/lSzocbxwAgNqqymsimaapM88801scFRUVqXPnzrJYLN7tAAAAQH0T0SxCrp9dlY4DAFCXVLlEmjJlSnXmAAAAAGqlyLaRKi8ol6fY4x2zRFsU2TYyhKkAAAi+KpdIgwcPrs4cAAAAQK1kibaowZ8aqOznMnkOeWRpaFHEaREybCz9AACoW6pcIgEAAAConGEzZE+3hzoGAADViruzAQAAAAAAwC9KJAAAAAAAAPhFiQQAAAAAAAC/TrpEcrlcysnJUXl5eTDzAAAAAAAAIAwFXCIdOXJEQ4cOVUxMjDp06KBdu3ZJku666y4988wzQQ8IAAAAAACA0Au4RBozZow2bNigr776SlFRUd7xrKwsvfvuu0ENBwAAANQ1pseU+6Bb7iJ3qKMAABAQW6AvmDNnjt5991316NFDhmF4xzt06KAff/wxqOEAAACAuqRsX5lKNpfIU+qRJNkSbIo+J1qWGJYqBQCEv4D/tTpw4ICaNWtWYfzw4cM+pRIAAACA/3EXuVW8tthbIElSuaNcR749EsJUAABUXcAlUteuXTVv3jzv82PF0T//+U9lZmYGLxkAAABQh5TtLpNpmhXG3U633A4ubQMAhL+AL2d76qmn1KdPH3333XcqLy/XK6+8ou+++07Lly/X0qVLqyMjAAAAUOuZrooFUlW2AQAQLgKeiXT++edr/fr1Ki8vV8eOHfXFF1+oWbNmWrFihbp06VIdGQEAAIBaz9a08r/fGlZD1kbWGk4DAEDgAp6JJEmtW7fWpEmTgp0FAAAAqLNsKTbZdtlU/mu5z3hkRqSMCNYWBQCEv4BnIlmtVu3fv7/C+K+//iqrlb+gAAAAAJUxLIZiusUo+uxoRSRHyJ5mV2xmrCJbRYY6GgAAVRLwTKTKFgOUpNLSUtnt9lMOBAAAANRVhtWQvbld9ub83AwAqH2qXCKNHz9e0tG7sf3zn/9UgwYNvNvcbreWLVumtm3bBj8hAAAAAAAAQq7KJdJLL70k6ehMpIkTJ/pcuma329WiRQtNnDgx+AkBAAAAAAAQclUukXbs2CFJuvjiizV79mw1atSo2kIBAAAAAAAgvAS8JtKSJUuqIwcAAAAAAADCWMAl0pAhQ064/a233jrpMAAAAAAAAAhPAZdIBw8e9HleVlamzZs3y+Fw6JJLLglaMAAAAAAAAISPgEukjz76qMKYx+PRiBEj1Lp166CEAgAAAAAAQHixBOVNLBaNHj3aewc3AAAAAAAA1C1BKZEk6ccff1R5eXmw3g4AAAAAAABhJODL2UaPHu3z3DRN7du3T/PmzdPgwYODFgwAAAAAAADhI+ASad26dT7PLRaLEhMT9eKLL/q9cxsAAAAAAABqp4BLpCVLllRHDgAAAAAAAISxoK2JBAAAAAAAgLqrSjOROnfuLMMwqvSGa9euPaVAAAAAAAAACD9VKpH69etXzTEAAAAAAAAQzqpUIj3yyCPVnQMAAAAAAABhLOCFtY9Zs2aNtm7dKknq0KGDOnfuHLRQAAAAAAAACC8Bl0j79+/XgAED9NVXXykhIUGS5HA4dPHFF2vWrFlKTEwMdkYAAAAAAACEWMB3Z7vrrrt06NAhbdmyRQUFBSooKNDmzZvldDp19913V0dGAAAAAAAAhFjAM5EWLFigRYsWqV27dt6x9u3b6/XXX1evXr2CGg4AAAAAAADhIeCZSB6PRxERERXGIyIi5PF4ghIKAAAAAAAA4SXgEumSSy7RPffco71793rHfv75Z91333269NJLgxoOAAAAAAAA4SHgEum1116T0+lUixYt1Lp1a7Vu3VotW7aU0+nUq6++Wh0ZAQAAAAAAEGIBr4mUlpamtWvXatGiRfr+++8lSe3atVNWVlbQwwEAAAAAACA8BFwiSZJhGOrZs6d69uwpSXI4HMHMBAAAAAAAgDAT8OVszz77rN59913v8+uuu05NmjTRaaedpg0bNgQ1HAAAAAAAAMJDwCXSxIkTlZaWJklauHChFi5cqPnz56tPnz568MEHgx7w559/1qBBg9SkSRNFR0erY8eO+vbbb73bTdPUww8/rJSUFEVHRysrK0vbt2/3eY+CggINHDhQcXFxSkhI0NChQ1VUVBT0rAAAAAAAAHVVwCVSXl6et0SaO3eurrvuOvXq1UsPPfSQVq9eHdRwBw8e1HnnnaeIiAjNnz9f3333nV588UU1atTIu89zzz2n8ePHa+LEiVq1apViY2OVnZ2tkpIS7z4DBw7Uli1btHDhQs2dO1fLli3T8OHDg5oVAAAAAACgLgt4TaRGjRpp9+7dSktL04IFCzRu3DhJR2cEud3uoIZ79tlnlZaWpilTpnjHWrZs6f1v0zT18ssva+zYsbrqqqskSe+8846SkpI0Z84cDRgwQFu3btWCBQu0evVqde3aVZL06quv6rLLLtMLL7yg1NTUoGYGAAAAAACoiwKeiXTNNdfoxhtvVM+ePfXrr7+qT58+kqR169bpjDPOCGq4Tz75RF27dtWf//xnNWvWTJ07d9akSZO823fs2KG8vDyfO8PFx8ere/fuWrFihSRpxYoVSkhI8BZIkpSVlSWLxaJVq1ZVetzS0lI5nU6fBwAAAAAAQH0WcIn00ksvadSoUWrfvr0WLlyoBg0aSJL27dunO++8M6jhfvrpJ73xxhtq06aNPv/8c40YMUJ333233n77bUlHL62TpKSkJJ/XJSUlebfl5eWpWbNmPtttNpsaN27s3ef3nn76acXHx3sfxy7fAwAAAAAAqK8CvpwtIiJCDzzwQIXx++67LyiBfsvj8ahr16566qmnJEmdO3fW5s2bNXHiRA0ePDjoxztmzJgxGj16tPe50+mkSAIAAAAAAPVawDORJCknJ0ejRo3SpZdeqksvvVSjRo1STk5OsLMpJSVF7du39xlr166ddu3aJUlKTk6WJOXn5/vsk5+f792WnJys/fv3+2wvLy9XQUGBd5/fi4yMVFxcnM8DAAAAAACgPgt4JtKHH36oAQMGqGvXrsrMzJQkrVy5UmeddZZmzZql/v37By3ceeedV6Gc2rZtm9LT0yUdXWQ7OTlZixcvVqdOnSQdnTW0atUqjRgxQpKUmZkph8OhNWvWqEuXLpKkL7/8Uh6PR927dw9aVgAAgNrMcBoyZYY6BlDnGE4j1BEAIGgCLpEeeughjRkzRo8//rjP+COPPKKHHnooqCXSfffdp3PPPVdPPfWUrrvuOn3zzTd688039eabb0qSDMPQvffeq3HjxqlNmzZq2bKl/v73vys1NVX9+vWTdHTmUu/evTVs2DBNnDhRZWVlGjVqlAYMGMCd2QAAQL2XkJAge6Rdrm9coY4C1Fn2SLsSEhJCHQMATplhmmZAf3KKiYnRxo0bK9yJbfv27frDH/6gI0eOBDXg3LlzNWbMGG3fvl0tW7bU6NGjNWzYMO920zT1yCOP6M0335TD4dD555+vCRMm6Mwzz/TuU1BQoFGjRunTTz+VxWJR//79NX78eO+i4P44nU7Fx8ersLCQS9sAAECdk5+fL4fDEeoYqKLc3FyNGzdOY8eO9c7QR3hLSEiocDMgAAgXgXQeAZdIl112mf785z/r1ltv9RmfMmWKZs2apc8//zzwxGGOEgkAAADhIicnR8OGDdOkSZOUkZER6jgAgFoukM6jSpezffLJJ97/vvLKK/WXv/xFa9asUY8ePSQdXRPp/fff12OPPXYKsQEAAAAAABCuqjQTyWKp2k3cDMOQ2+0+5VDhhplIAAAACBfMRAIABFPQZyJ5PJ6gBAMAAAAAAEDtVLUpRlXgcDj02muvBevtAAAAAAAAEEZOuURavHixbrzxRqWkpOiRRx4JRiYAAAAAAACEmZMqkXbv3q3HH39cLVu2VK9evWQYhj766CPl5eUFOx8AAAAAAADCQJVLpLKyMr3//vvKzs5WRkaG1q9fr+eff14Wi0V/+9vf1Lt3b0VERFRnVgAAAAAAAIRIlRbWlqTTTjtNbdu21aBBgzRr1iw1atRIknTDDTdUWzgAAACgLti8ebM+++wz7du3T8nJyerTp4/OPvvsUMcCACAgVZ6JVF5eLsMwZBiGrFZrdWYCAAAA6oyNGzfqtdde008//aTi4mLt2LFDEyZM0IYNG0IdDQCAgFS5RNq7d6+GDx+umTNnKjk5Wf3799dHH30kwzCqMx8AAABQq82fP7/S8c8++6yGkwAAcGqqXCJFRUVp4MCB+vLLL7Vp0ya1a9dOd999t8rLy/Xkk09q4cKFcrvd1ZkVAAAAqHX27t0b0DgAAOHqpO7O1rp1a40bN065ubmaN2+eSktLdfnllyspKSnY+QAAAIBaLTk5udJxfnYGANQ2J1UieV9ssahPnz764IMPtGfPHv2///f/gpULAAAAqBP69OlT6Xjv3r1rOAkAAKfmlEqk30pMTNTo0aOD9XYAAABAndCpUycNHz5caWlpslqtOv3003Xbbbepa9euoY4GAEBAbKEOAAAAANR155xzjs4555xQxwAA4JQEbSYSAAAAAAAA6i5KJAAAAAAAAPhFiQQAAAAAAAC/Al4Tye12a+rUqVq8eLH2798vj8fjs/3LL78MWjgAAAAAAACEh4BLpHvuuUdTp05V3759ddZZZ8kwjOrIBQAAAAAAgDAScIk0a9Ysvffee7rsssuqIw8AAAAAAADCUMBrItntdp1xxhnVkQUAAAAAAABhKuAS6f7779crr7wi0zSrIw8AAAAAAADCUMCXs3399ddasmSJ5s+frw4dOigiIsJn++zZs4MWDgAAAAAAAOEh4BIpISFBV199dXVkAQAAAAAAQJgKuESaMmVKdeQAAAAAAABAGAt4TSQAAAAAAADUPwHPRJKkDz74QO+995527doll8vls23t2rVBCQYAAAAAAIDwEfBMpPHjx+vWW29VUlKS1q1bp27duqlJkyb66aef1KdPn+rICAAAAAAAgBALuESaMGGC3nzzTb366quy2+166KGHtHDhQt19990qLCysjowAAAAAAAAIsYBLpF27duncc8+VJEVHR+vQoUOSpJtuukkzZ84MbjoAAAAAAACEhYBLpOTkZBUUFEiSmjdvrpUrV0qSduzYIdM0g5sOAAAAAAAAYSHgEumSSy7RJ598Ikm69dZbdd9996lnz566/vrrdfXVVwc9IAAAAAAAAEIv4Luzvfnmm/J4PJKkkSNHqkmTJlq+fLmuvPJK3X777UEPCAAAAAAAgNALuESyWCyyWP43gWnAgAEaMGBAUEMBAAAAAAAgvAR8OZsk/fvf/9agQYOUmZmpn3/+WZL0r3/9S19//XVQwwEAAAAAACA8BFwiffjhh8rOzlZ0dLTWrVun0tJSSVJhYaGeeuqpoAcEAAAAAABA6AVcIo0bN04TJ07UpEmTFBER4R0/77zztHbt2qCGAwAAAAAAQHgIuETKycnRhRdeWGE8Pj5eDocjGJkAAAAAAAAQZgIukZKTk/XDDz9UGP/666/VqlWroIQCAAAAAABAeAm4RBo2bJjuuecerVq1SoZhaO/evZo+fboeeOABjRgxojoyAgAAAAAAIMRsgb7gr3/9qzwejy699FIdOXJEF154oSIjI/XAAw/orrvuqo6MAAAAAAAACLGASyTDMPS3v/1NDz74oH744QcVFRWpffv2atCgQXXkAwAAAAAAQBgIuEQ6xm63q3379sHMAgAAAAAAgDBV5RJpyJAhVdrvrbfeOukwAAAAAAAACE9VLpGmTp2q9PR0de7cWaZpVmcmAAAAAAAAhJkql0gjRozQzJkztWPHDt16660aNGiQGjduXJ3ZAAAAAAAAECYsVd3x9ddf1759+/TQQw/p008/VVpamq677jp9/vnnzEwCAAAAAACo46pcIklSZGSkbrjhBi1cuFDfffedOnTooDvvvFMtWrRQUVFRdWUEAAAAAABAiAVUIvm80GKRYRgyTVNutzuYmQAAAAAAABBmAiqRSktLNXPmTPXs2VNnnnmmNm3apNdee027du1SgwYNqisjAAAAAAAAQqzKC2vfeeedmjVrltLS0jRkyBDNnDlTTZs2rc5sAAAAAAAACBNVLpEmTpyo5s2bq1WrVlq6dKmWLl1a6X6zZ88OWjgAAAAAAACEhyqXSDfffLMMw6jOLAAAAAAAAAhTVS6Rpk6dWo0xAAAAAAAAEM5O+u5sAAAAAAAAqD8okQAAAAAAAOAXJRIAAAAAAAD8okQCAAAAAACAX5RIAAAAAAAA8IsSCQAAAAAAAH5RIgEAAAAAAMAvSiQAAAAAAAD4RYkEAAAAAAAAvyiRAAAAAAAA4BclEgAAAAAAAPwK6xLp0UcflWEYPo+2bdt6t5eUlGjkyJFq0qSJGjRooP79+ys/P9/nPXbt2qW+ffsqJiZGzZo104MPPqjy8vKaPhUAAAAAAIBazRbqAP506NBBixYt8j632f4X+b777tO8efP0/vvvKz4+XqNGjdI111yj//znP5Ikt9utvn37Kjk5WcuXL9e+fft08803KyIiQk899VSNnwsAAAAAAEBtFfYlks1mU3JycoXxwsJCTZ48WTNmzNAll1wiSZoyZYratWunlStXqkePHvriiy/03XffadGiRUpKSlKnTp30xBNP6C9/+YseffRR2e32mj4dAAAAAACAWimsL2eTpO3btys1NVWtWrXSwIEDtWvXLknSmjVrVFZWpqysLO++bdu2VfPmzbVixQpJ0ooVK9SxY0clJSV598nOzpbT6dSWLVuOe8zS0lI5nU6fBwAAAAAAQH0W1iVS9+7dNXXqVC1YsEBvvPGGduzYoQsuuECHDh1SXl6e7Ha7EhISfF6TlJSkvLw8SVJeXp5PgXRs+7Ftx/P0008rPj7e+0hLSwvuiQEAAAAAANQyYX05W58+fbz/ffbZZ6t79+5KT0/Xe++9p+jo6Go77pgxYzR69Gjvc6fTSZEEAAAAAADqtbAukX4vISFBZ555pn744Qf17NlTLpdLDofDZzZSfn6+dw2l5ORkffPNNz7vcezubZWts3RMZGSkIiMjg38CAAAAQAgVFhZq6dKl2r17t5o1a6Y//elPatasWahjAQBqibC+nO33ioqK9OOPPyolJUVdunRRRESEFi9e7N2ek5OjXbt2KTMzU5KUmZmpTZs2af/+/d59Fi5cqLi4OLVv377G8wMAAAChcuDAAY0bN06fffaZNm3apMWLF2vcuHH66aefQh0NAFBLhHWJ9MADD2jp0qXauXOnli9frquvvlpWq1U33HCD4uPjNXToUI0ePVpLlizRmjVrdOuttyozM1M9evSQJPXq1Uvt27fXTTfdpA0bNujzzz/X2LFjNXLkSGYaAQAAoF6ZN2+eDh065DPmcrn00UcfhSgRAKC2CevL2fbs2aMbbrhBv/76qxITE3X++edr5cqVSkxMlCS99NJLslgs6t+/v0pLS5Wdna0JEyZ4X2+1WjV37lyNGDFCmZmZio2N1eDBg/X444+H6pQAAACAkMjJyal0fPv27XK73bJarTWcCABQ24R1iTRr1qwTbo+KitLrr7+u119//bj7pKen67PPPgt2NAAAAKBWadCggQ4ePFhhPCYmhgIJAFAlYX05GwAAAIDguOCCCwIaBwDg9yiRAAAAgHrgwgsvVO/evRURESFJslgsOvfcc3XFFVeEOBkAoLYI68vZAAAAAARPv3791KtXL+Xn56tJkyaKi4sLdSQAQC1CiQQAAADUIzExMWrZsmWoYwAAaiEuZwMAAAAAAIBflEgAAAAAAADwixIJAAAAAAAAfrEmEgAAAFCDTNPUwoUL9dVXX+nQoUM644wz1K9fP6Wnp4c6GgAAJ8RMJAAAAKAGzZ49W7Nnz1ZBQYHKysq0detWvfjii8rPzw91NAAAToiZSAAAAKhzSkpKlJubG+oYFZSWlmru3LkqKyursG3WrFnq3bu33/c4dl7heH7BkJ6erqioqFDHAABUwjBN0wx1iHDndDoVHx+vwsJCxcXFhToOAAAA/MjJydGwYcNCHaMCt9stp9NZ6baIiAg1aNCghhOFn0mTJikjIyPUMQCg3gik82AmEgAAAOqc9PR0TZo0KdQxKigtLdVLL71U6Uykbt26KTs7OwSpwgtrQwFA+KJEAgAAQJ0TFRUVtrNZrrrqKi1YsMBnLCoqSjfccIMSExNDlAoAAP8okQAAAIAadNVVVykuLk5Lly6V0+lUmzZtdMUVV1AgAQDCHmsiVQFrIgEAAAAAgLookM7DUkOZAAAAAAAAUItRIgEAAAAAAMAvSiQAAAAAAAD4RYkEAAAAAAAAvyiRAAAAAAAA4BclEgAAAAAAAPyiRAIAAAAAAIBflEgAAAAAAADwixIJAAAAAAAAflEiAQAAAAAAwC9KJAAAAAAAAPhFiQQAAAAAAAC/KJEAAAAAAADgFyUSAAAAAAAA/KJEAgAAAAAAgF+USAAAAAAAAPCLEgkAAAAAAAB+USIBAAAAAADAL0okAAAAAAAA+EWJBAAAAAAAAL8okQAAAAAAAOAXJRIAAAAAAAD8okQCAAAAAACAX5RIAAAAAAAA8IsSCQAAAAAAAH5RIgEAAAAAAMAvSiQAAAAAAAD4RYkEAAAAAAAAvyiRAAAAAAAA4BclEgAAAAAAAPyiRAIAAAAAAIBflEgAAAAAAADwixIJAAAAAAAAflEiAQAAAAAAwC9KJAAAAAAAAPhFiQQAAAAAAAC/KJEAAAAAAADgFyUSAAAAAAAA/KJEAgAAAAAAgF+USAAAAAAAAPCLEgkAAAAAAAB+USIBAAAAAADAL0okAAAAAAAA+EWJBAAAAAAAAL8okQAAAAAAAOAXJRIAAAAAAAD8okQCAAAAAACAX5RIAAAAAAAA8IsSCQAAAAAAAH5RIgEAAAAAAMAvSiQAAAAAAAD4RYkEAAAAAAAAv2pVifTMM8/IMAzde++93rGSkhKNHDlSTZo0UYMGDdS/f3/l5+f7vG7Xrl3q27evYmJi1KxZMz344IMqLy+v4fQAAAAAAAC1V60pkVavXq3/+7//09lnn+0zft999+nTTz/V+++/r6VLl2rv3r265pprvNvdbrf69u0rl8ul5cuX6+2339bUqVP18MMP1/QpAAAAAAAA1Fq1okQqKirSwIEDNWnSJDVq1Mg7XlhYqMmTJ+sf//iHLrnkEnXp0kVTpkzR8uXLtXLlSknSF198oe+++07Tpk1Tp06d1KdPHz3xxBN6/fXX5XK5QnVKAAAAAAAAtUqtKJFGjhypvn37Kisry2d8zZo1Kisr8xlv27atmjdvrhUrVkiSVqxYoY4dOyopKcm7T3Z2tpxOp7Zs2VLp8UpLS+V0On0eAAAAAAAA9Zkt1AH8mTVrltauXavVq1dX2JaXlye73a6EhASf8aSkJOXl5Xn3+W2BdGz7sW2Vefrpp/XYY48FIT0AAAAAAEDdENYzkXbv3q177rlH06dPV1RUVI0dd8yYMSosLPQ+du/eXWPHBgAAAAAACEdhXSKtWbNG+/fv1znnnCObzSabzaalS5dq/PjxstlsSkpKksvlksPh8Hldfn6+kpOTJUnJyckV7tZ27PmxfX4vMjJScXFxPg8AAAAAAID6LKxLpEsvvVSbNm3S+vXrvY+uXbtq4MCB3v+OiIjQ4sWLva/JycnRrl27lJmZKUnKzMzUpk2btH//fu8+CxcuVFxcnNq3b1/j5wQAAAAAAFAbhfWaSA0bNtRZZ53lMxYbG6smTZp4x4cOHarRo0ercePGiouL01133aXMzEz16NFDktSrVy+1b99eN910k5577jnl5eVp7NixGjlypCIjI2v8nAAAAAAAAGqjsC6RquKll16SxWJR//79VVpaquzsbE2YMMG73Wq1au7cuRoxYoQyMzMVGxurwYMH6/HHHw9hagAAAAAAgNrFME3TDHWIcOd0OhUfH6/CwkLWRwIAAAAAAHVGIJ1HWK+JBAAAAAAAgPBAiQQAAAAAAAC/av2aSECweDwebdu2TSUlJTrzzDMVExMT6kgAAAAAAIQNSiRA0u7du/XGG2+ooKBAkmS323XttdfqwgsvDHEyAAAAAADCAyUS6j3TNPV///d/3gJJklwul2bMmKGWLVsqLS1NkrRt2zZt3LhRdrtd3bt3V1JSUqgiAwAAAABQ4yiRUGNKSkqUm5sb6hgV5ObmaufOnZVu+/jjj9WzZ0/NmzdPa9eu9Y6///77uvzyy9WpU6eaCRlC6enpioqKCnUMAAAAAECIGaZpmqEOEe4Cud0dji8nJ0fDhg0LdYwKysrKVFRUVOm2yMhIRUREVLrdMAzFx8fLMIzqjhhSkyZNUkZGRqhjAAAAAACqQSCdBzORUGPS09M1adKkUMeowOVy6eWXX1ZpaWmFbQMGDNCOHTu0atWqSl977bXXKiYmRuPGjdPYsWOVnp5e3XFrXF08JwAAAABA4CiRUGOioqLCdkbLsGHD9Pbbb+u3E/O6du2qvn376pNPPtGWLVsqfd0ZZ5yhiIgISUfLlnA9PwAAAAAAThUlEiCpR48eatGihVauXKnS0lJ16NBBHTp0kGEY6tatm+bPn1/hNfHx8Wrbtq1+/PHHECQGAAAAAKBmUSIB/5WcnKx+/fpVGE9JSdGgQYP07rvvqqysTJIUFxen22+/XTYb30IAAAAAgPqB34CBKjj//PPVuXNnff/997Lb7WrXrh0FEgAAAACgXuG3YKCKYmNj1aVLl1DHAAAAAAAgJCyhDgAAAAAAAIDwR4kEAAAAAAAAvyiRAAAAAAAA4BclEgAAAAAAAPyiRAIAAAAAAIBflEjAKThy5Ij27Nkjj8cT6igAAAAAAFQrW6gDALXVnDlztHjxYh08eFBOp1Mff/yx7rvvPkVERIQ6GgAAAAAAQcdMJOAkfP3111qwYIHKysokSaZpauPGjfr4449VUlKizZs366effpJpmiFOCgAAAABAcDATCTgJ//73vysdnzNnjpYtWyaXyyVJSklJ0R133KGkpKSajAcAAACgHjBNU3v37pXNZuN3DtQISqQwlZ+fL4fDEeoYOI49e/aoqKhIklRcXCxJcjgcOnDggFq3bi2L5egkv+3bt+uZZ57RHXfcEbKsqFxCQgL/0AIAAKDW2rZtm/71r3/pwIEDkqT09HQNGTKkRn/GLS8v16pVq7RlyxZFR0fr3HPPVevWrWvs+Kh5hsn1Nn45nU7Fx8ersLBQcXFx1X68/Px8DRw4SC5XabUfCyfnyJEjKi31/fy43W6ZpimbrWI327Bhw0rHETp2e6SmT59GkQQAAIBa59ChQ/rb3/7mvQLimKZNm+qxxx6T1Wqt9gzl5eV69dVXlZOT4zN+/fXX6+KLL6724yN4Auk8+K02DDkcDrlcpSppfZHM6IRQx0ElTFexzO0rZZaX/G+spEiGLVIeW8WFtUtadZO1YZOajIgTMIod0o9fyeFwUCIBAADAR0lJiXJzc0Md44RWrlypgoKCCuNFRUVasGCBzjjjDB04cED/+c9/tHfvXsXFxal79+5q06ZN0DJs2rRJa9asqTD+zjvvqEmTJoqMjAzasaoiPT1dUVFRNXrM+ogSKYyZ0QnyxDYNdQxUJlayd05Uef6PMo8clBHZQDZ7tMp2rdfvp/YZVruU2FoeK99u4YI7CgAAAOB4cnNzNWzYsFDHOKHi4mKVlJRUum379u2y2Ww6dOiQz41+5syZo9jYWNnt9qBkOHz4cIWZUMfcfvvtNX7X6kmTJikjI6NGj1kf8VstcJKMiChFnN7B+9w0TXmc++V27P3tXrKld5ZBgQQAAADUCunp6Zo0aVKoY5zQjz/+qBkzZlQYNwxDo0aN0tKlS7Vx40afbcXFxfrhhx/02GOPqUWLFqecYeHChVq5cmWl24YOHarU1NRTPkYg0tPTa/R49RW/2QJBYhiGIs48X9aDP8vt2CfDFiFr0xayxCSEOhoAAACAKoqKigr7GS1nnnmmdu7cWaEoysrKUvfu3fX555+rQYMGFV7n8XiUmJgYlPOLi4vTd999J4/H4zN++umnsyZSHUaJBASRYRiyNj5d1sanhzoKAAAAgDrKMAzdfvvtWrVqldavXy+bzaZu3bqpU6dOkqQmTZpo7969lb4uWOsGpaSkaMiQIZo1a5b3ztUtW7bUbbfdFpT3R3iiREKdZJqm5C6XrDYZhhHqOAAAAECdk5+fL4fDEeoY9VqTJk106aWXep8fu1Na69attXLlSp81kYqLixUZGak9e/YE7e5tDRs21JAhQ7Rv3z5FRkYqMTFRv/zyi3755ZegvH99l5CQEHY3AqJEQp1imqbc+75X+b5tMstLZEQ1kO20s2RryvWxAAAAQLDk5+dr0MCBKj3OwsoIPZfLpZKSErndblksFtntdkVFRWncuHGhjoYqirTbNW369LAqkiiRUKe49+WobPf/rgs2S4pU9uNKGdYIWRvV7MJuAAAAQF3lcDhU6nLpWkmJoQ6Dytntkt0ul2nKJslSxSs0Sj0eGYYhO1d0hNQBSR+4XHI4HJRIQHUwTVPledsq3Vael1NrSyR3Yb7Kf94iz+ECGZGxsiVnyNasVahjAQAAAEqUlCrKhlNRbprySNVX2lTxfQvcbi09ckQ/l5dLklpEROhPMTGKMQzluFzaUVYmqyG1tUeqRURE9WTFb5j+dwkBSqQwZhQ7ZAl1iFrE9LilksLK/wk7XCDL4dp3Xa778EGV/fCNJM/R8yorVvkPv8godiiiafMQp6u9jGJHqCMAAACgniv2eLSsuFg/lrnkMaVUm00XxkSrqbXmf00vM019XHRIhz3/Ky52lpXJccipOKtVu8rKveM/uMrUJSpKmdHRNZ4ToUeJFMaifvwq1BFqnfLDTrnd7grjdrtd0ZvnnNx7lpfL9d9rvSMiIhRRg617UVGRLGVlFcbNov2K2hfHouEAAABALTXv8GHllf+vnNlbXq45RUUa1DBOUZbjTyco8XhULqnBCfYJ1HaXy6dA+l8mt/a63YoxfI+1trREHSMjg5oBtQMlUhgraX2RzOiEUMeoVQxHnjy56+Uz9c+wSm16qDg6LuD3K8v/SWV5OVJkrCSpWJItPk32tLOCktcf19alMl1HKox7JBW3z5JhZRrpyTCKHZS0AAAAQXBAUrhedhPOfikv187yin8sLvGYWu4qVduoqEq2efTNkSPa898/MjeyWtU1JkaJtlP/tX6Xx62SSj6Ph0yPbDJkMX63zZQ2lpephd1+ysdG5Q6EOsBxUCKFMTM6QZ7YpqGOUatYYpsqokHTo3dnKy2SJbaRbCntpNgEeQJ8L9NVLNcvuZLV9/8YywrzZTm9oywNmgQv+HEYcUnyHPy54rg9Rp6GycxEOkn8vQQAACA4Pgh1gFrK5fHo8HG27fN4tKSS8UOHD6v8NzOXdrrdWl9UpLi4OFlOcUZQmc2mokrGPYYhwzAqXTLkF8MQf9KufyiRUOdY45NljU8+5fdxF+ZJZuXVk/vg3hopkWyp7eR27KuQw3ZaewokAAAAhBx3Zzs5h6xWfXqcbZk2m1r+buzX8nJ9/psCycs0dbbLpbMqmbkUCNNm02KbTft/d4yUyEjtLy+X2/SdidTQatXlNhtLqlejAwrPkpYSCTiOE10qVlOXkVkaNJG97Z9U/vMWmYcPyoiMlTUlQ7amLWrk+AAAAMCJcHe2k2S1aU9kpLaWunyGm1qt6hFhl/V3H9MSj6mo43ycIzyeU/8cGIZubNBQG0pL9WOZSxZJbex2dbRH6ufyci05ckROz9E/bCfZrOoZE6sEg/n91Ss8LxOlRAKOw5KQIsMWJbO8xHeDYZG1Bu+MZo1rJmtcsxo7HgAAAIDqd0l0jBKtVuW4XCozpZYRETonKlLWSq44SLLZZBiSWUmvkBKENZEkyWYY6hIVpS6/m9WUFhGhm+Li9KvHLZsMJVitQTkeaidKJOA4DItV9ozz5dq+3Lu4tWGLVETLP8qwx4Q4HQAAAIDazDAMnR0ZpbMj/V+KFmux6A+RkVpfUuoznmiz6oyI6l/c2jAMNbVSH4ASCTghS4Mmiux0uTyHDkimR5aGiTIsNO8AAAAAatb50TFKtNr0vatUZaapFhEROjsySjbWSkUNokQC/DAM46QuJ3P/ulvl+dtluo7I0jBRttR2skTHVUNCAAAAAPVBht2uDHv1zzwCjocSKYwZxQ5uRV5LlR3YqbK9W73PPUcOyvXLTkW1yZQlkkvhQs0odoQ6AgAAAADUOpRIYSghIUF2e6T041ehjoKTYJqmXE6nLP+9e8FvGRvyFR1DiRQO7PZIJSQkhDoGAAAAANQalEhhKCkpSdOnT5PD4Qh1FFRBbm6uxo0bp7Fjxyo9PV0FBQV6/fXXK9339NNP16233lql93W73dq8ebN++OEHRUZG6g9/+IPS0tKCGb1eS0hIUFJSUqhjAAAAAECtQYkUppKSkvgFt5ZJT09XRkaGSktL1bhxY7lcrgr7ZGRkKCMjw+97lZeX67XXXtP333/vHdu+fbuuv/56XXzxxUHNDQAAAABAVVAioV5xu90qKSlRTEyMjGq6i0FkZKQuuOACLV682GfcarXqkksuqdJ7rFu3zqdAOuajjz5Sjx49FB0dHZSsAAAAwKk4IEkyQ5yibjBNs9p+R0HtcyDUAY6DEgn1gmma+uyzz/Tll1/q8OHDSkxM1JVXXqk//vGP1XK8/v37y263a9myZTp8+LDS0tLUr18/paenV+n1W7durXTc5XLpp59+UocOHYIZFwAAAAhIQkKCIu12fVDJ7HtUnWmaKi4ulsvlkmmaioiIUHR0tKxWa6ijIQxE2u1ht44rJRLqhc8++0yffvqp9/mBAwc0efJkxcTEVEshY7FYdNVVV+nKK69UWVmZ7AHehjPmBItvx8bGnmo8AAAA4JQkJSVp2vTprON6it59911t27bNZywmJkZ33HHHKf/cv3v3buXn56tJkyZq0aKFdu3a5bOWK8JfOK7jSomEGlNSUqLc3NwaP67H49GcOXNUXFxcYdt7772nG2+88ZTe/9g5BfPckpOTdeTIEXl+d4e3pKQklZaWKicnJ2jH8ic9PV1RUVE1djwAAADUDqzjemry8/O1d+9eNWjQoMK2AwcO6Jxzzjmp93W5XJowYYLP8hjp6enq3bu397+rsk4rUBlKJNSY3NxcDRs2rMaPa5rmcf9CsnnzZi1ZsiQoxxk3blxQ3ucYl8ul4uJib5Fks9m0Y8cOrV69OqjH8WfSpEn8IwMAAAAEWX5+/klt8+ezzz6rsL5qbm5uhTVbgZNBiYQak56erkmTJtX4cU3T1IQJE1RQUFBh21lnnaWrr766xjNVldvt1t69exUVFaXExMSQZGCqKwAAABB8p5122nG3paWlnfT7Hu+Pzlu2bDnp9wSOoURCjYmKigrZjJabb75Z//znP33GIiMjdfPNNys1NTUkmaqqffv2oY4AAAAAIMiaNGmic889V8uXL68w3qNHj5N+398vieFvHAgEJRLqha5duyo2NlaLFi3SL7/8ohYtWig7OzvsCyQAAAAAddegQYOUnJys5cuXq7S0VGeddZb69u2r6Ojok37Pzp0768svv6ww3rZtW+3YseNU4gKUSKg/2rVrp3bt2oU6BgAAAABIOnpX5169eqlXr15Be8++fftq27Zt2rNnj3csMTFRWVlZmj9/ftCOg/qJEgkAAAAAgDoiNjZWY8aM0caNG/Xzzz8rKSlJnTp10k8//RTqaKgDKJEAAAAAAKhDrFarOnfurM6dO4c6CuoYS6gDAAAAAAAAIPxRIgEAAAAAAMAvSiQAAAAAAAD4RYkEAAAAAAAAvyiRAAAAAAAA4Bd3ZwMAoBZZt26dvvjiCx04cEBpaWnq27evzjjjjFDHAgAAIVBaWqq1a9fq4MGDOuOMM3TmmWeGOhLqOEokAABqAY/Ho2XLlmnmzJkyDEOStHXrVm3btk2jR49W69atQ5wQAADUpD179mj8+PFyOp3esbPPPlvDhw+Xzcav+qgeXM4GAECY++qrrzRmzBg99thj2rp1qw4cOODd5na7NX/+/BCmAwAAoTB9+nSfAkmSNm7cqP/85z8hSoT6gHoSABDWSkpKlJubG+oYIbNhwwZ98sknMk1Thw8fliTt3r1bpaWlSkhIkCR99913ysnJCWHKyqWnpysqKirUMQAACEht+NnD6XRq06ZNlW5btGiRkpOTK4wfO6dwP7eTxc8dNcMwTdMMdYjjeeONN/TGG29o586dkqQOHTro4YcfVp8+fSQd/ea+//77NWvWLJWWlio7O1sTJkxQUlKS9z127dqlESNGaMmSJWrQoIEGDx6sp59+OqDpfU6nU/Hx8SosLFRcXFxQzxEAcGI5OTkaNmxYqGOEjNPplNvtliSVl5fr2D/bhmF4/y2LiIhQgwYNQpbxeCZNmqSMjIxQxwAAICC14WcPj8ejwsLCSreF688F1Y2fO05eIJ1HWJdIn376qaxWq9q0aSPTNPX222/r+eef17p169ShQweNGDFC8+bN09SpUxUfH69Ro0bJYrF4p++53W516tRJycnJev7557Vv3z7dfPPNGjZsmJ566qkq56BEAoDQqQ1/DTwZubm5GjdunMaOHav09PTj7vf888+rpKREklRYWKj8/HxJR0ukNm3ayDAM3XjjjWrVqlWN5A4EfxEEANRGteVnj6lTp2r37t0Vxi+//HJ17tw5BIlCi587Tl6dKZEq07hxYz3//PO69tprlZiYqBkzZujaa6+VJH3//fdq166dVqxYoR49emj+/Pm6/PLLtXfvXu/spIkTJ+ovf/mLDhw4ILvdXqVjUiIBAAoKCrR27VqZpqnOnTuradOmp/R+x/7K6e+vZq+88oq2bt3qff7rr7/qwIEDioiI0EUXXaTLL79cf/jDH04pCwAAqH3y8/P18ssv6+DBg96xP/7xj7r11ltlsbD8MaoukM6j1qyJ5Ha79f777+vw4cPKzMzUmjVrVFZWpqysLO8+bdu2VfPmzb0l0ooVK9SxY0efy9uys7M1YsQIbdmy5bjtbGlpqUpLS73Pf79YGQCgfvn66681ffp076VkH374oa677jpdcsklQTvGvn37tHDhQu3Zs0fNmjVTVlaWWrRoocsvv1zbt29XeXm5JKlJkyZq1qyZ7rrrLrVt2zZoxwcAALVLUlKSnnjiCW3cuFEOh0NnnHGGmjdvHupYqOPCvkTatGmTMjMzVVJSogYNGuijjz5S+/bttX79etntdu+iosckJSUpLy9PkpSXl+dTIB3bfmzb8Tz99NN67LHHgnsiAIBayeFwaObMmfr9xN333ntPZ5999inPSJKOLpT9/PPPy+VySTq6nt+6deu8RdGDDz6oL774Qnv37lVycrJ69uwZlpevAQCAmmWz2XTOOeeEOgbqkbAvkTIyMrR+/XoVFhbqgw8+0ODBg7V06dJqPeaYMWM0evRo73On06m0tLRqPSYABEN+fr4cDkeoY9Qpa9asOe7ClfPmzVOPHj1O6n1/e4eUVatWqaCgoMI+U6ZM0ZAhQyRJF154oXe8rKwsLO/GVh8kJCRU+AMVAABAfRH2JZLdbtcZZ5whSerSpYtWr16tV155Rddff71cLpccDofPbKT8/Hzv7QyTk5P1zTff+LzfsQVJK7vl4TGRkZGKjIwM8pkAQPXKz8/XoIEDVfrf2SwIjtLSUh05cqTSbdu2bdPkyZNP6f3HjRunwsJCeTyeCts2bNig5cuXyzCMUzoGgifSbte06dMpkgAAQL0U9iXS73k8HpWWlqpLly6KiIjQ4sWL1b9/f0lHFyndtWuXMjMzJUmZmZl68skntX//fjVr1kyStHDhQsXFxal9+/YhOwcAqA4Oh0OlLpdGdDis1Fh3qOPUGUfKTL29vlzu392GwpB08x9K1TDy1Eu797aUaf/hive5iI0wdGvnQ6f8/giOvYetemPL0e81SiQAAFAfhXWJNGbMGPXp00fNmzfXoUOHNGPGDH311Vf6/PPPFR8fr6FDh2r06NFq3Lix4uLidNdddykzM9N7aUGvXr3Uvn173XTTTXruueeUl5ensWPHauTIkcw0AlBnpca61TKOEimYBp1l0ewct9z/nSxkNaQrzrTq7MSKs4dORnZLQx9+X/G9era08LkEAABA2AjrEmn//v26+eabtW/fPsXHx+vss8/W559/rp49e0qSXnrpJVksFvXv31+lpaXKzs7WhAkTvK+3Wq2aO3euRowYoczMTMXGxmrw4MF6/PHHQ3VKAIBa6Owki1o3MrT1V1OmKbVraqiBPXiXmHVKtqi4XFqa69HhMlNRNkOZpxs6P43L2AAAABA+wrpE8rfORFRUlF5//XW9/vrrx90nPT1dn332WbCjAQDqmVi7oa4p1VfqZJ5uUbdUQ0UuKdYu2SwUSAAAAAgvYV0iAQBQn1gthuKjQp0CAAAAqJwl1AEAAAAAAAAQ/iiRAAAAAAAA4BclEgAAAAAAAPyiRAIAAAAAAIBflEgAAAAAAADwi7uzAQBQTZylpn46aCrKJrVpbMhqMUIdCQAAADhplEgAAFSDZbs8WrzDLY959HlDu6GBHa06rSFFEgAAAGonLmcDACDIcgtNLfzpfwWSJB1ymXp3i1umaR7/hQAAAEAYo0QCACDINuZ7Kh0/WGJql7OGwwAAAABBwuVsAAAEWXnlHZIkye0xJQV+SZuz1NT6fFNFLlMt4g21bWrIYnBpHAAAAGoOJRIA1DF7DzPJNNTiow0dKS+rMB5lM+Q2bNrhDKz8+dnp0afbyn9TTpk6raFFV2RYZWOx7hrD9xYAAKjvKJEAoI55Y0uDUEeo90zT1JEjR+RyubxjhmEoNjZWj30bEfB7HTp0SG637z/Z3x+UVvwSpcjIyKBkBgAAAPyhRAKAOmZEhyKlxp7geir45TFN/ew0VeaRTmtoKNJ2crN9fnYa2lXokd1q6MwmhhpGFksqDug9Dhabmr7JVem25vFFujKj9KSyIXB7D1soaQEAQL1GiQQAdUxqrEct49yhjlFr/XzI1IwtbjlLj95FLcIi9TnDqj+mBn4pU8s4SacfK6BOrthrZDcVY6v8jm6J0SafawAAANQYLu4HAOC/PKapmZv/VyBJUplH+mSbW/lFlRc51S0hylCL+IozoUrdpo6UmZq+ya2vdrp1uCw0+QAAAFB/UCIBAPBfOx2mCksrL2M27A/dJYLXtLMqMeZ/RdKRMlNFLim30NT3v3q0eKdHE771Lb8AAACAYONyNgCoY/YetoY6Qq2102noSHnlZdHeIot2OEP3sb08w6q9h47OPlq5xy2VmjpS/r/tR8qlj7ZJF6bz+a8ufG8BAID6jhIJAOqIhIQERdrtemNLqJPUXqZpqrCwUKZZcUbPnrIGWrQ/sDurVQePx6PCwsJKt213WrUwP66GE9UvkXa7EhISQh0DAAAgJCiRAKCOSEpK0rTp0+VwOEIdpVbbsGGDPv30U58i6eyzz9aVV14pwzi5u7RVJjc3V+PGjdPYsWOVnp5e5deVl5frhRdeUFlZWYVtzZs31+DBg4OWERUlJCQoKSkp1DEAAABCghIJAOqQpKQkfsE9RRkZGbrwwgu1cuVKuVwudezYUe3atau246WnpysjIyOg1/Ts2VPLli2rMN6vX7+A3wsAAACoKkokAAB+Jzk5Wf369Qt1jOO69tprVVJSotWrV8s0TdntdvXu3VvdunULdTQAAADUYZRIAADUMna7XUOGDNE111yjgoICpaSkKDo6OtSxAAAAUMdRIgEA4IfD4VB0dLQiIyNDHcVHQkICizwDAACgxlAiAQBwHFu2bNH777+vvLw82Ww2de3aVTfccEPYlUkAAABATaBEAgCgEvv27dOECRPkdrslHb0r2rHFtocPHx7idAAAAEDNo0QCAKASy5Yt8xZIv7V27VoVFBSocePGNZbF5XLp66+/1ubNmxUZGanMzEydffbZNXZ8AAAAQKJEAgCgUr/++utxtzkcjhorkcrLy/Xqq69q+/bt3rF169apb9++uuKKK2okAwAAACBJllAHAAAgHLVq1arS8cjISJ122mk1lmPdunU+BdIxCxYs0KFDh2osBwAAAMBMJABAWCspKVFubm6NHzcpKUkRERE6ePCgz3j37t21c+fOU37/Y+fk79y+/vprFRUVVbrtyy+/VNu2bU85S3VJT09XVFRUqGMAAAAgSAzTNM1Qhwh3TqdT8fHxKiwsVFxcXKjjAEC9kpOTo2HDhoXk2B6PR6WlpSovL5dhGLLb7bLb7TWaobi4WCUlJZVua9iwoWy28P170KRJk5SRkRHqGAAAADiBQDqP8P3JEwAAHZ3NMmnSpFDHCBmHw6E33nhD5eXlPuPJyckhK9eqKj09PdQRAAAAEESUSACAsBYVFVXvZ7PExcVpxowZOnDggCQpIyNDgwcPrtE7xAEAAABczlYFXM4GAAg10zSVl5enyMhIyiMAAAAEDZezAQBQxxiGoZSUlFDHAAAAQD1mCXUAAAAAAAAAhD9KJAAAAAAAAPhFiQQAAAAAAAC/KJEAAAAAAADgFyUSAAAAAAAA/KJEAgAAAAAAgF+USAAAAAAAAPCLEgkAAAAAAAB+USIBAAAAAADAL0okAAAAAAAA+EWJBAAAAAAAAL8okQAAAAAAAOAXJRIAAAAAAAD8okQCAAAAAACAX5RIAAAAAAAA8IsSCQAAAAAAAH5RIgEAAAAAAMAvSiQAAAAAAAD4RYkEAAAAAAAAvyiRAAAAAAAA4BclEgAAAAAAAPyiRAIAAAAAAIBftlAHqA1M05QkOf9/e/cflWV9/3H8dQkDbn7KLT/uQkAD1BtEZzLLWrtdngaWTrc685QaSNl0WraZOtcgjYnkcW7LOTNtQKbpcZjTbEezIoymOUOngiaooSc7/bIpKiLe1/cPv1zbHSqot6L0fJxzn3Nf1+fX++IcPue+3vfn+tzHjrVxJAAAAAAAAN7TlOtoyn1cDEmkVjh+/LgkKTY2to0jAQAAAAAA8L7jx48rLCzsonUMszWppm85t9utTz75RCEhITIMo63DwXXm2LFjio2N1aFDhxQaGtrW4QC4QTB3ALhczB8ALgdzBy7ENE0dP35cN998szp0uPiuR6xEaoUOHTqoc+fObR0GrnOhoaFMxgAuGXMHgMvF/AHgcjB34HxaWoHUhI21AQAAAAAA0CKSSAAAAAAAAGgRSSTgCvn7++uZZ56Rv79/W4cC4AbC3AHgcjF/ALgczB3wBjbWBgAAAAAAQItYiQQAAAAAAIAWkUQCAAAAAABAi0giAQAAAAAAoEUkkdAuDRgwQE8++WSbjZ+VlaVhw4ZdN/EAAAAA+HY5ePCgDMPQ9u3bL1intLRUhmHo66+/bvNYcGMgiQRcA6tWrVJeXl5bhwHAiwzDuOhr+vTp1gemppfdbpfL5dKmTZskSV26dLloH1lZWZKkd999V3fffbfsdrsCAwOVlJSkzMxMNTQ0tOFfAMDlaM3cIUmvvfaabr/9doWFhSkkJEQpKSnWF1IDBgy4aB8DBgyQ5DnHBAYGKjU1VYsXL26bCwdwXbrjjjt05MgRhYWFtXUouEH4tnUAwLeB3W5v6xAAeNmRI0es9ytWrFBubq727t1rnQsODtYXX3whSdq4caNSUlL0xRdfaObMmRo8eLA++ugjbd26VWfPnpUkvf/++7r//vu1d+9ehYaGSpJsNpsqKyuVkZGhxx9/XM8//7xsNpv27dunkpISqy2AG0dr5o633npLw4cP18yZM/XjH/9YhmGosrJSb775pqRzX041JZEPHTqkfv36WfOMJPn5+Vn9PfvssxozZoxOnjyplStXasyYMYqJidGgQYOuxeUCuM75+fnJ4XC0dRi4gbASCe1WY2OjJkyYoLCwMEVERCgnJ0emaUqSlixZorS0NIWEhMjhcOihhx7SZ599ZrU9evSoRowYocjISNlsNiUlJamwsNAqP3TokH72s5+pY8eOstvtGjp0qA4ePHjBWL75OFuXLl2Un5+v7OxshYSEKC4uTi+++KJHm0sdA8C15XA4rFdYWJgMw/A4FxwcbNXt1KmTHA6Hevbsqd/85jc6duyYtmzZosjISKt+U7I5KirKo98NGzbI4XBo9uzZ6tmzpxISEpSRkaFFixbJZrO11eUDuEytmTvWrl2rO++8U5MnT1b37t3VrVs3DRs2TPPnz5d07suppvqRkZGS/jvP/O98Isn6rHPLLbdo6tSpstvtVjIKwLXldrs1e/ZsJSYmyt/fX3FxcZo5c6YkaefOnbr77rtls9nUqVMnPfbYY6qrq7PaNm2XkZ+fr+joaHXs2FHPPvusGhsbNXnyZNntdnXu3NnjnqXJnj17dMcddyggIEA9e/bUu+++a5V983G2oqIidezYUevXr5fT6VRwcLAyMjI8EuCStHjxYjmdTgUEBKhHjx76y1/+4lH+wQcfqE+fPgoICFBaWpoqKiq89WdEGyOJhHaruLhYvr6++uCDD/SnP/1Jc+fOtZZwnzlzRnl5edqxY4dWr16tgwcPWo+NSFJOTo4qKyv1j3/8Q1VVVVqwYIEiIiKstunp6QoJCdGmTZtUXl5uTa6X8mjJ73//e2tC/cUvfqFx48ZZ30R6awwA15dTp07p5ZdfluS5UuBiHA6Hjhw5orKysqsZGoDriMPh0O7du7Vr1y6v9el2u1VSUqKjR4+2ev4B4F3Tpk1TQUGBda+xbNkyRUdH68SJE0pPT1d4eLi2bt2qlStXauPGjZowYYJH+7fffluffPKJysrKNHfuXD3zzDMaPHiwwsPDtWXLFo0dO1Y///nPdfjwYY92kydP1qRJk1RRUaH+/ftryJAh+vLLLy8Y58mTJzVnzhwtWbJEZWVlqq2t1VNPPWWVL126VLm5uZo5c6aqqqqUn5+vnJwcFRcXS5Lq6uo0ePBgJScna9u2bZo+fbpHe9zgTKAdcrlcptPpNN1ut3Vu6tSpptPpPG/9rVu3mpLM48ePm6ZpmkOGDDFHjx593rpLliwxu3fv7tH36dOnTZvNZq5fv940TdPMzMw0hw4d6hHPxIkTreP4+Hhz5MiR1rHb7TajoqLMBQsWtHoMANePwsJCMywsrNn5AwcOmJJMm81mBgUFmYZhmJLMvn37mg0NDR5133nnHVOSefToUY/zjY2NZlZWlinJdDgc5rBhw8x58+aZ//nPf67iFQG4Fi40d9TV1Zn33nuvKcmMj483hw8fbr700ktmfX19s7pN80xFRUWzsvj4eNPPz88MCgoyfX19TUmm3W439+3bdxWuBsDFHDt2zPT39zcXLVrUrOzFF180w8PDzbq6OuvcunXrzA4dOpiffvqpaZrn7i/i4+PNs2fPWnW6d+9u3nXXXdZxY2OjGRQUZL766qumaf53figoKLDqnDlzxuzcubP53HPPmabZ/PNHYWGhKcmsrq622syfP9+Mjo62jhMSEsxly5Z5XENeXp7Zv39/0zRNc+HChWanTp3MU6dOWeULFiy44FyFGwsrkdBu3X777TIMwzru37+/9u3bp7Nnz2rbtm0aMmSI4uLiFBISIpfLJUmqra2VJI0bN07Lly/Xd7/7XU2ZMkXvv/++1c+OHTtUXV2tkJAQBQcHKzg4WHa7XfX19aqpqWl1fL169bLeNy1lb3qkzltjALg+rFixQhUVFSopKVFiYqKKior0ne98p1VtfXx8VFhYqMOHD2v27NmKiYlRfn6+UlJSmi0tB9A+BAUFad26daqurtZvf/tbBQcHa9KkSerXr59Onjx5SX1NnjxZ27dv19tvv63bbrtNf/jDH5SYmHiVIgdwIVVVVTp9+rQGDhx43rLevXsrKCjIOnfnnXfK7XZ77JmWkpKiDh3+ewsfHR2t1NRU69jHx0edOnXy2KZDOncf1MTX11dpaWmqqqq6YKyBgYFKSEiwjm+66SarzxMnTqimpkaPPPKIdZ8SHBys3/3ud9Z9SlVVlXr16qWAgIDzxoAbGxtr41unvr5e6enpSk9P19KlSxUZGana2lqlp6dbj4oNGjRIH3/8sd544w29+eabGjhwoMaPH685c+aorq5Offv21dKlS5v13bQvQWt88wbSMAy53W5J8toYAK4PsbGxSkpKUlJSkhobG/WTn/xEu3btkr+/f6v7iImJ0ahRozRq1Cjl5eWpW7dueuGFFzRjxoyrGDmAtpSQkKCEhAQ9+uijevrpp9WtWzetWLFCo0ePbnUfERERSkxMVGJiolauXKnU1FSlpaUpOTn5KkYO4Ju8sY/h+e4fLnZP4c1xzP/fW7Zpn6ZFixbptttu86jn4+NzRePixsBKJLRbW7Zs8TjevHmzkpKStGfPHn355ZcqKCjQXXfdpR49ejTL1kvnkjWZmZl65ZVX9Mc//tHa+PrWW2/Vvn37FBUVZX0oa3p566cxr8UYANrGAw88IF9f32YbUF6K8PBw3XTTTTpx4oQXIwNwPevSpYsCAwOv6P8+NjZWw4cP17Rp07wYGYDWSEpKks1m01tvvdWszOl0aseOHR7/3+Xl5erQoYO6d+9+xWNv3rzZet/Y2Kht27bJ6XReVl/R0dG6+eabtX///mb3KV27dpV07nr+/e9/q76+/rwx4MZGEgntVm1trX71q19p7969evXVVzVv3jxNnDhRcXFx8vPz07x587R//36tWbNGeXl5Hm1zc3P197//XdXV1dq9e7def/11a6IdMWKEIiIiNHToUG3atEkHDhxQaWmpnnjiiWab2F2uazEGgLZhGIaeeOIJFRQUtOqxlIULF2rcuHHasGGDampqtHv3bk2dOlW7d+/WkCFDrkHEAK616dOna8qUKSotLdWBAwdUUVGh7OxsnTlzRvfcc88V9T1x4kStXbtW//rXv7wULYDWCAgI0NSpUzVlyhS9/PLLqqmp0ebNm/XSSy9pxIgRCggIUGZmpnbt2qV33nlHjz/+uEaNGqXo6OgrHnv+/Pl67bXXtGfPHo0fP15Hjx5Vdnb2Zfc3Y8YMzZo1S88//7w++ugj7dy5U4WFhZo7d64k6aGHHpJhGBozZowqKyv1xhtvaM6cOVd8Hbg+kERCu/Xwww/r1KlT6tevn8aPH6+JEyfqscceU2RkpIqKirRy5UolJyeroKCg2aTm5+enadOmqVevXvrBD34gHx8fLV++XNK5Z4TLysoUFxenn/70p3I6nXrkkUdUX1+v0NBQr8R+LcYA0HYyMzN15swZ/fnPf26xbr9+/VRXV6exY8cqJSVFLpdLmzdv1urVq6393AC0Ly6XS/v379fDDz+sHj16aNCgQfr000+1YcOGK16VkJycrB/96EfKzc31UrQAWisnJ0eTJk1Sbm6unE6nhg8frs8++0yBgYFav369vvrqK33ve9/TAw88oIEDB7bqc0JrFBQUqKCgQL1799Z7772nNWvWWL88fTkeffRRLV68WIWFhUpNTZXL5VJRUZG1Eik4OFhr167Vzp071adPHz399NN67rnnvHItaHuG2fRwIwAAAAAAAHABrEQCAAAAAABAi0giAQAAAAAAoEUkkQAAAAAAANAikkgAAAAAAABoEUkkAAAAAAAAtIgkEgAAAAAAAFpEEgkAAAAAAAAtIokEAAAAAACAFpFEAgAAuMEZhqHVq1e3dRgAAKCdI4kEAADgBVlZWTIMQ2PHjm1WNn78eBmGoaysrFb1VVpaKsMw9PXXX7eq/pEjRzRo0KBLiBYAAODSkUQCAADwktjYWC1fvlynTp2yztXX12vZsmWKi4vz+ngNDQ2SJIfDIX9/f6/3DwAA8L9IIgEAAHjJrbfeqtjYWK1atco6t2rVKsXFxalPnz7WObfbrVmzZqlr166y2Wzq3bu3/va3v0mSDh48qB/+8IeSpPDwcI8VTAMGDNCECRP05JNPKiIiQunp6ZKaP852+PBhPfjgg7Lb7QoKClJaWpq2bNlyla8eAAC0d75tHQAAAEB7kp2drcLCQo0YMUKS9Ne//lWjR49WaWmpVWfWrFl65ZVX9MILLygpKUllZWUaOXKkIiMj9f3vf18lJSW6//77tXfvXoWGhspms1lti4uLNW7cOJWXl593/Lq6OrlcLsXExGjNmjVyOBz68MMP5Xa7r+p1AwCA9o8kEgAAgBeNHDlS06ZN08cffyxJKi8v1/Lly60k0unTp5Wfn6+NGzeqf//+kqRbbrlF7733nhYuXCiXyyW73S5JioqKUseOHT36T0pK0uzZsy84/rJly/T5559r69atVj+JiYlevkoAAPBtRBIJAADAiyIjI3XfffepqKhIpmnqvvvuU0REhFVeXV2tkydP6p577vFo19DQ4PHI24X07dv3ouXbt29Xnz59rAQSAACAt5BEAgAA8LLs7GxNmDBBkjR//nyPsrq6OknSunXrFBMT41HWms2xg4KCLlr+v4++AQAAeBNJJAAAAC/LyMhQQ0ODDMOwNr9ukpycLH9/f9XW1srlcp23vZ+fnyTp7Nmzlzx2r169tHjxYn311VesRgIAAF7Fr7MBAAB4mY+Pj6qqqlRZWSkfHx+PspCQED311FP65S9/qeLiYtXU1OjDDz/UvHnzVFxcLEmKj4+XYRh6/fXX9fnnn1url1rjwQcflMPh0LBhw1ReXq79+/erpKRE//znP716jQAA4NuHJBIAAMBVEBoaqtDQ0POW5eXlKScnR7NmzZLT6VRGRobWrVunrl27SpJiYmI0Y8YM/frXv1Z0dLT1aFxr+Pn5acOGDYqKitK9996r1NRUFRQUNEtmAQAAXCrDNE2zrYMAAAAAAADA9Y2VSAAAAAAAAGgRSSQAAAAAAAC0iCQSAAAAAAAAWkQSCQAAAAAAAC0iiQQAAAAAAIAWkUQCAAAAAABAi0giAQAAAAAAoEUkkQAAAAAAANAikkgAAAAAAABoEUkkAAAAAAAAtIgkEgAAAAAAAFr0fzFIStHIB+Q5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mse_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mse_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MSE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs {\" | jitter factor = \" + str(jitter_factor) if syn_data_type == \"jitter\" else \"\"}')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mae_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mae_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MAE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs {\" | jitter factor = \" + str(jitter_factor) if syn_data_type == \"jitter\" else \"\"}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2*1e06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_data_augmentation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
