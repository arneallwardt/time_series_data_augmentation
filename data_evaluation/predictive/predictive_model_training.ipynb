{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Füge das übergeordnete Verzeichnis zu sys.path hinzu\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "from utilities import split_data_into_sequences, load_sequential_time_series, reconstruct_sequential_data, Scaler, extract_features_and_targets_reg, get_discriminative_test_performance\n",
    "from data_evaluation.visual.visual_evaluation import visual_evaluation\n",
    "from predictive_evaluation import predictive_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../../data\")\n",
    "REAL_DATA_FOLDER = DATA_FOLDER / \"real\"\n",
    "SYNTHETIC_DATA_FOLDER = DATA_FOLDER / \"synthetic\" / \"usable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of loading data\n",
    "- Laden der Originaldaten: als pd dataframe \n",
    "- Laden der synthetischen, sequentiellen Daten: als np array (GAN, (V)AE)\n",
    "- Laden der synthetischen, sequentiellen Daten: als pd dataframe (brownian, algorithmit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible types: 'timegan_lstm', 'timegan_gru', 'jitter', 'timewarp', 'autoencoder', 'vae'\n",
    "syn_data_type = 'timegan_lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " syn data:\n",
      "\n",
      "       traffic_volume           temp        rain_1h        snow_1h  \\\n",
      "count   341988.000000  341988.000000  341988.000000  341988.000000   \n",
      "mean      3223.797936     282.704303       0.086439       0.000249   \n",
      "std       1943.974204      12.922822       0.321004       0.000466   \n",
      "min         41.627638     250.083873       0.000008       0.000000   \n",
      "25%       1152.987320     270.511312       0.000130       0.000002   \n",
      "50%       3608.409516     285.328962       0.000575       0.000006   \n",
      "75%       5010.047921     293.711888       0.037928       0.000324   \n",
      "max       7076.619110     305.881726      12.279954       0.004205   \n",
      "\n",
      "          clouds_all  \n",
      "count  341988.000000  \n",
      "mean       39.871618  \n",
      "std        39.339560  \n",
      "min         0.016394  \n",
      "25%         4.172619  \n",
      "50%        15.465574  \n",
      "75%        87.893841  \n",
      "max        97.951007  \n",
      "\n",
      "\n",
      "real data:\n",
      "\n",
      "       traffic_volume          temp       rain_1h       snow_1h    clouds_all\n",
      "count     28511.00000  28511.000000  28511.000000  28511.000000  28511.000000\n",
      "mean       3313.74238    282.688768      0.061611      0.000250     42.122795\n",
      "std        1971.53206     12.367361      0.678185      0.008298     39.316195\n",
      "min           0.00000    243.390000      0.000000      0.000000      0.000000\n",
      "25%        1289.00000    273.480000      0.000000      0.000000      1.000000\n",
      "50%        3507.00000    284.550000      0.000000      0.000000     40.000000\n",
      "75%        4948.00000    292.790000      0.000000      0.000000     90.000000\n",
      "max        7280.00000    310.070000     42.000000      0.510000    100.000000\n"
     ]
    }
   ],
   "source": [
    "# Load real time series\n",
    "data_real_df = pd.read_csv(REAL_DATA_FOLDER/'metro_interstate_traffic_volume_label_encoded_no_categorical.csv')\n",
    "data_real_numpy = dc(data_real_df).to_numpy()\n",
    "\n",
    "if syn_data_type == 'timegan_lstm':\n",
    "    # load sequential data (which should already be scaled)\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_lstm_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'timegan_gru':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_gru_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'autoencoder':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28478_12_5_lstm_autoencoder_unscaled_15.csv', shape=(28478, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'vae':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28511_12_5_lstm_vae_unscaled.csv', shape=(28511, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'jitter':\n",
    "    jitter_factor = 0.1\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'mitv_jittered_{str(jitter_factor).replace(\".\", \"\")}.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "elif syn_data_type == 'timewarp':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'mitv_time_warped.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "# Loot at real and syn data\n",
    "df = pd.DataFrame(data_syn_numpy.reshape(-1, data_syn_numpy.shape[-1]), columns=data_real_df.columns)\n",
    "\n",
    "print('\\n\\n syn data:\\n')\n",
    "print(df.describe())\n",
    "\n",
    "print('\\n\\nreal data:\\n')\n",
    "print(data_real_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>traffic_volume</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>341988.000000</td>\n",
       "      <td>341988.000000</td>\n",
       "      <td>341988.000000</td>\n",
       "      <td>341988.000000</td>\n",
       "      <td>341988.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3223.797936</td>\n",
       "      <td>282.704303</td>\n",
       "      <td>0.086439</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>39.871618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1943.974204</td>\n",
       "      <td>12.922822</td>\n",
       "      <td>0.321004</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>39.339560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>41.627638</td>\n",
       "      <td>250.083873</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1152.987320</td>\n",
       "      <td>270.511312</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.172619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3608.409516</td>\n",
       "      <td>285.328962</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>15.465574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5010.047921</td>\n",
       "      <td>293.711888</td>\n",
       "      <td>0.037928</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>87.893841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7076.619110</td>\n",
       "      <td>305.881726</td>\n",
       "      <td>12.279954</td>\n",
       "      <td>0.004205</td>\n",
       "      <td>97.951007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       traffic_volume           temp        rain_1h        snow_1h  \\\n",
       "count   341988.000000  341988.000000  341988.000000  341988.000000   \n",
       "mean      3223.797936     282.704303       0.086439       0.000249   \n",
       "std       1943.974204      12.922822       0.321004       0.000466   \n",
       "min         41.627638     250.083873       0.000008       0.000000   \n",
       "25%       1152.987320     270.511312       0.000130       0.000002   \n",
       "50%       3608.409516     285.328962       0.000575       0.000006   \n",
       "75%       5010.047921     293.711888       0.037928       0.000324   \n",
       "max       7076.619110     305.881726      12.279954       0.004205   \n",
       "\n",
       "          clouds_all  \n",
       "count  341988.000000  \n",
       "mean       39.871618  \n",
       "std        39.339560  \n",
       "min         0.016394  \n",
       "25%         4.172619  \n",
       "50%        15.465574  \n",
       "75%        87.893841  \n",
       "max        97.951007  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>traffic_volume</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28511.00000</td>\n",
       "      <td>28511.000000</td>\n",
       "      <td>28511.000000</td>\n",
       "      <td>28511.000000</td>\n",
       "      <td>28511.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3313.74238</td>\n",
       "      <td>282.688768</td>\n",
       "      <td>0.061611</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>42.122795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1971.53206</td>\n",
       "      <td>12.367361</td>\n",
       "      <td>0.678185</td>\n",
       "      <td>0.008298</td>\n",
       "      <td>39.316195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>243.390000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1289.00000</td>\n",
       "      <td>273.480000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3507.00000</td>\n",
       "      <td>284.550000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4948.00000</td>\n",
       "      <td>292.790000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7280.00000</td>\n",
       "      <td>310.070000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       traffic_volume          temp       rain_1h       snow_1h    clouds_all\n",
       "count     28511.00000  28511.000000  28511.000000  28511.000000  28511.000000\n",
       "mean       3313.74238    282.688768      0.061611      0.000250     42.122795\n",
       "std        1971.53206     12.367361      0.678185      0.008298     39.316195\n",
       "min           0.00000    243.390000      0.000000      0.000000      0.000000\n",
       "25%        1289.00000    273.480000      0.000000      0.000000      1.000000\n",
       "50%        3507.00000    284.550000      0.000000      0.000000     40.000000\n",
       "75%        4948.00000    292.790000      0.000000      0.000000     90.000000\n",
       "max        7280.00000    310.070000     42.000000      0.510000    100.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_real_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Predictive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"seq_len\": 12,\n",
    "    \"lr\": 0.0004,\n",
    "    \"batch_size\": 32,\n",
    "    \"hidden_size\": 4,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"num_evaluation_runs\": 5,\n",
    "    \"num_epochs\": 1000,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETERS:\n",
      "seq_len :  12\n",
      "lr :  0.0004\n",
      "batch_size :  32\n",
      "hidden_size :  4\n",
      "num_layers :  1\n",
      "bidirectional :  True\n",
      "num_evaluation_runs :  5\n",
      "num_epochs :  1000\n",
      "device :  cpu\n",
      "Synthetic Data is sequential: True\n",
      "Shape of the data after splitting into sequences: (22797, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2841, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2840, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.05632632641385598 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.02823659336047896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.007192034516313739 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004247024139975396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0067822383017883976 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0038524161961473774 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.006391771129877169 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0034967221194103864 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.006101701661701436 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0032054049622141914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00593440185237655 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.003064933518115222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.005732695346268424 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0029294212921679524 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.005510968932043369 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0027542748878280937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.005204599453717585 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002463069757488504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.004959692240027818 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002369240829537968 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00484081789956338 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002295216222377389 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.004763983895735708 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022474961627400323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0046977176747988875 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022070937459043156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.004633421434917674 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021669948113862466 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00456990142268629 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021245338551155973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.004511040177256349 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020791604778771238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004462412028994051 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020366389228533327 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004423746792795489 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002001380138465444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0043921459809587 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019728582623163553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0043652866619082655 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019494390410758303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004341721537096496 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019299864168592802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004320666282375697 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00191353548715791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004301542272532638 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018994935514834407 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004284026358308607 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001887551326372692 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004267906336858999 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018773831177262192 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004253029794648399 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018687171173501719 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004239267792891016 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018612897995430348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004226494552832107 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018548719333798698 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004214585137972804 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018492389327762753 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004203418573690513 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018442066607400357 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004192885212671419 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018396143699483422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004182887744931778 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018353373678191827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004173343583760551 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018312768027018964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004164183656747099 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001827353431733346 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004155353844971729 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018235093736079303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004146807454216201 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018197028764282887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004138506889597318 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018159002922423016 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004130423024997538 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018120769586602455 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00412252955332981 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018082108522576982 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004114806446913619 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018042866591876897 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.00410723602253915 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018002962218754495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004099804041779837 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017962295881189991 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0040924998720635 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017920750280936363 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004085313626823014 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001787832669333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004078238505885125 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017834949284907054 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004071269478818054 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017790736917019142 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004064401396243934 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017745734468426848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004057630307062661 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017700035051767076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004050954210498144 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017653870732481645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0040443725437402656 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017607255683154957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.004037883085427939 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017560371642492795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.004031485980876224 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017513302161379226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.004025182572126827 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017466266368766933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.004018974168892939 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017419342930020576 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.004012860179954531 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017372604362729392 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.0040068426974402506 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017326211666667394 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.004000923733310552 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017280209635572737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.003995103631341676 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017234722506830448 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.003989384311978244 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017189788331246276 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.003983768246719172 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017145540757533791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.0039782563519473405 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017101968183401931 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.00397285163503135 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017059164669814655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.003967555617779065 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017017133473784904 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.003962371268781192 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016975940239165775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.003957300672900626 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001693559136225978 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.003952347272144941 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016896121774203658 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.003947514068331376 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016857491959189849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.003942803329925953 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016819759902512927 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.0039382182933725834 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016782933877508963 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.003933761123130894 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016746982968715803 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.00392943336068343 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016711927627297014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.003925236980682226 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016677735786717594 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.0039211724863419 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016644429062294323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.003917240477209522 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001661199078309151 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.0039134399075065 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016580376273094353 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.003909768862666126 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016549672954977396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.0039062266837091015 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016519777624595785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.003902809074452881 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016490719215735124 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.0038995136584248 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001646248215621107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.0038963361605048328 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016435026923675886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.003893271901221557 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016408375231549144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.0038903161377527164 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016382508060968158 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.003887464593835358 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016357361278862934 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.0038847107629469356 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016332988554004872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.0038820505272015816 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016309270541068543 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.0038794791734931525 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016286284656242959 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 861\n",
      "Train Loss: 0.0038769901844412924 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016263919068877114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 871\n",
      "Train Loss: 0.0038745798879018284 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016242223745996781 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 881\n",
      "Train Loss: 0.0038722418011087785 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016221122612151203 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 891\n",
      "Train Loss: 0.0038699722763276324 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016200597328657096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 901\n",
      "Train Loss: 0.0038677672818819057 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016180666659309875 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 911\n",
      "Train Loss: 0.003865621839016467 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.00161612293947216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 921\n",
      "Train Loss: 0.003863532228890394 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016142282264798964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 931\n",
      "Train Loss: 0.003861494280063504 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001612386068465251 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 941\n",
      "Train Loss: 0.0038595051406869643 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016105848271661344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 951\n",
      "Train Loss: 0.003857561064081405 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016088239048094896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 961\n",
      "Train Loss: 0.0038556587462069214 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016071027925687977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 971\n",
      "Train Loss: 0.0038537956309063312 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016054203531056057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 981\n",
      "Train Loss: 0.0038519701341638633 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016037702071218846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 991\n",
      "Train Loss: 0.0038501773809042315 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001602151465806273 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive\\predictive_evaluation.py:272: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([{'Model': evaluation_method, 'Metric': 'MAE', 'Error': mae}])], ignore_index=True)\n",
      " 20%|██        | 1/5 [17:02<1:08:10, 1022.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.17581262361117198 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.04789163126202112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.007735243245605301 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004940653680248207 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.007208504557000867 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004263039607701175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.006978512313354076 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0038427053427512055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.006306349788367915 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0029469631622765172 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.005597170901962858 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0025457265653406804 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.005268162524270163 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0023275253538230663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0050312484673983296 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021795869361494113 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.004899486826065285 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021021638819006053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0048113851407986935 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020603870062597014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.004742914587905863 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020326767188727102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00468582270107249 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020101806076944628 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.004634526287346587 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019890449187728795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.004584988590093463 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001966883514071239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.004535023215113189 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019414410756356762 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.004484030936333922 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019111383092861747 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004432977908839528 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018765525473804956 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004384527467178157 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018408537241944186 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004341550184437303 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018076417729614323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004305185520235151 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017787680929798758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004274912176393808 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017543715121371023 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004249614641461574 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017339141386707596 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004228202837958885 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017167693487985917 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004209779852773958 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017023444400702634 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004193642715006534 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016900883069303765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004179255135679359 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016794975060875413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0041662191367691165 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016701434290277237 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004154245219576116 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001661698968971169 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004143126817847253 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016539298400827004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004132715529559094 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016466810902988726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0041229063834404625 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016398430282637225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0041136197573707805 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016333582082480778 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004104795826283554 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001627177097494557 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004096386048336727 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016212781009609612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004088350588917539 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016156289984179096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004080657293314054 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016102123766185276 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004073277757902137 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016050128903229502 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0040661880910902075 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016000140373322988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004059365975097746 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015951992493918102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0040527931763953825 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001590555697271412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0040464519777303875 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015860720638321776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0040403258296769495 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015817328998452742 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004034400055250923 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015775311772261611 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004028661930812771 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015734552655609722 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004023097124052604 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00156949581826901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004017694232963172 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015656420008771205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004012440886253133 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015618864393248902 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004007324547967959 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015582235574492075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004002333306844593 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015546444802168296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.003997454406833341 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015511396831539826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.003992671970876265 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001547705214679911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.003987963840386309 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015443319819731574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.00398330111766814 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015410269987298532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.0039786830852082495 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001537819228820461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.003974140846796809 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015346933662462351 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.003969668964466777 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015315882883850862 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.003965246812378411 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001528492640819024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.0039608575422303725 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015254129120671933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.003956488800283422 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015223559215821827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.0039521320725446315 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015193250277498213 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.003947779312560029 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015163157158792856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.003943424508531367 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015133330009856753 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.0039390623265796385 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015103724076192867 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.003934689212200555 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001507435464221733 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.003930302150596487 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001504525298964274 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.003925898579650704 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015016361904916552 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.003921472780236015 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014987689282044015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.003917008539273823 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00149593064483004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.0039125146421206776 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014931692648940626 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.003908014705575752 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001490433664411683 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.0039035095070746074 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014876824956690745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.003898995821031142 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014849298769670925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.003894473521315152 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014821813602701583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.003889943811021024 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014794371679446169 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.003885408236856114 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001476695926551278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.0038808701924168535 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014739525491329895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.003876333919596737 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001471204908821062 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.003871804316411236 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014684531037182954 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.0038672877366388108 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014657008859297533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.0038627908873041313 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001462943489948455 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.003858321939521896 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014601905416210673 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.003853890085612459 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014574469735563387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.003849504341232247 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001454720450525978 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.003845174689117563 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014520248668294484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.0038409115728493754 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001449371423190355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.0038367249727964945 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001446775116256616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 861\n",
      "Train Loss: 0.0038326242545960528 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014442514080104283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 871\n",
      "Train Loss: 0.003828618497990051 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014418102329952663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 881\n",
      "Train Loss: 0.003824714291301983 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014394566696660405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 891\n",
      "Train Loss: 0.0038209161866485006 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014371933177469319 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 901\n",
      "Train Loss: 0.003817226147497279 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014350085572883739 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 911\n",
      "Train Loss: 0.0038136429461809907 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001432890187245218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 921\n",
      "Train Loss: 0.003810162500734343 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014308181106050112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 931\n",
      "Train Loss: 0.003806779450092982 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014287733235474928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 941\n",
      "Train Loss: 0.0038034862825296932 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001426736474968493 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 951\n",
      "Train Loss: 0.003800275046943307 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014246960838974108 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 961\n",
      "Train Loss: 0.0037971380394315256 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014226443913838503 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 971\n",
      "Train Loss: 0.00379406651700775 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014205719108871195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 981\n",
      "Train Loss: 0.003791053950454162 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001418479661368294 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 991\n",
      "Train Loss: 0.0037880936483388862 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014163670287038503 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [33:52<50:45, 1015.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.04526097265556768 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.017427181500648516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.006827810888771662 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0039053163765877317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.006386980970996219 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0035522574201759913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.006110997959876075 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0033343263837676287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.005851019772101418 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0031118927696154692 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0056502793271938365 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0029192096122090568 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.005472364580078471 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0027134092773174806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0053329095548099795 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0025438110218540338 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.005232876810659282 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0024333771421532198 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005157314665121814 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00236156051245968 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005095447688252465 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00230945116876928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005042298547374959 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002267848123440498 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0049956849056817226 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022326862548080304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.004954556603739652 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022020062897354364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00491822841165778 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021746292876425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.004886028521594972 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002149755342502482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0048570465515295844 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021266893495470703 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004829441773905033 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021041751115875884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004799009650504167 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020791478330803134 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004762347159335105 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002048494544774922 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004723766727698249 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002014895993954596 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004686743314580425 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001982794408791102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004651000315870771 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001953281217589556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004615641809865404 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019257478232840809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004579970894282133 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018993735599875702 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004543373272244833 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001873242341144157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004505255612574758 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018463970359488067 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004465213036007897 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018180668225846767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004423384366427688 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001788103935999398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004380776129563719 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017573302193136697 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004339150649294268 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017273104004038686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004300285786942829 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016995820745281623 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004265223815090181 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016749520319137262 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004234173268028816 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016534985115050516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004206865115827903 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016349668678660154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004182838458145954 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016190658337855188 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004161562835262124 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016055011505711027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0041425030114884175 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001593940240576905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004125164281057656 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015840533312431045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004109119564891749 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015755223569439284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0040940162985354436 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001568050848403841 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004079579283188778 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015613796723106604 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.00406560644471161 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001555275731181119 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004051965565096096 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015495284277359756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004038583419349718 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015439556014274112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.00402543884568141 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015384022459084316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004012551885650089 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015327559942327272 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.003999966758546298 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001526946355614895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.003987746848104336 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015209729533401852 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.003975954704701138 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001514875769531459 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.003964643614470494 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015087244669864939 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.00395384624162506 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015026016155613607 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.003943572023071657 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014965778760863155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.003933811689097419 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014906968591797552 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.003924541326068592 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001484994964250311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.003915729321918854 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014794866823894756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.003907341361231971 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001474175671196616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.003899339349992014 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001469066439149378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.0038916858529174365 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014641523590766606 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.00388434627694228 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014594382661384311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.003877287210869181 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014549188219073615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.0038704784293293296 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014505954416798364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.0038638947570603733 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014464612952119597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.0038575126505737698 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014425206620748458 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.003851311944058063 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014387713810638263 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.0038452762816503622 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014352101088355013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.0038393893918194934 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014318369716154725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.0038336394653494927 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014286469400002298 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.003828015661002287 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014256314770841783 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.00382250699238802 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014227837241736106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.003817105733888775 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001420105879425249 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.003811803605403558 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014175794030450745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.0038065948271483892 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0014151977744670272 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.0038014726029502602 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001412955462001264 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.0037964317734715063 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014108387436822393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.0037914677684719126 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014088388330962384 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.003786576894091685 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014069478588016556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.003781755759183521 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014051548162804853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.003777000172469284 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001403451714078697 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.003772307650041133 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0014018338233143552 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.003767674305782553 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001400290758825127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.003763095244780303 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013988151328162071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.0037585646961489105 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013974050711971194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.0037540745986579835 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013960496355057432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.003749611397514833 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001394751462418314 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.0037451748905029426 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001393499782863544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 861\n",
      "Train Loss: 0.0037407858790646908 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013922947737534813 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 871\n",
      "Train Loss: 0.003736454959198582 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013911329942959455 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 881\n",
      "Train Loss: 0.0037321735663779685 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013900173401585622 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 891\n",
      "Train Loss: 0.003727927783502169 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001388943059241734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 901\n",
      "Train Loss: 0.0037237052645875198 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013879014468840794 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 911\n",
      "Train Loss: 0.003719499364409068 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013868903582313966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 921\n",
      "Train Loss: 0.0037153004245769864 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013858979418115125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 931\n",
      "Train Loss: 0.0037111013350214126 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001384923825160799 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 941\n",
      "Train Loss: 0.0037068955969753797 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013839701976067272 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 951\n",
      "Train Loss: 0.0037026741011440493 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013830194951511207 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 961\n",
      "Train Loss: 0.0036984290983162163 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013820777691016498 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 971\n",
      "Train Loss: 0.003694154394641623 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013811303956010326 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 981\n",
      "Train Loss: 0.0036898499815483142 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013801648891060038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 991\n",
      "Train Loss: 0.003685521654331427 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0013791714854449494 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [50:43<33:46, 1013.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.04511335907637195 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.02118245823036754 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.007302175021108611 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004414939321577549 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.006932517909738764 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.003918332482028878 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.006723155054299038 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0037528994346686293 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.006530845197842444 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.003600082024042442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006266939309771796 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0033418920382941035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.005989125792784838 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0031197244573510094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.005752273887653181 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0028927174064571435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.005597528273652492 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0027510131842697436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005503769143357831 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0026621817587613224 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00542918339190119 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0025930425912152263 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00535917612671758 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0025324373364741548 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005289896286672652 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0024794248695317865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005224210700133834 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002438218583316239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0051643898427162485 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0024066845293868374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005107492134680379 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0023774120026644778 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005051479006426469 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00234565603889504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004997805533878411 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0023118310962447876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004948892882080725 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022785195824428558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0049049701782656625 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022464418578422053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004863359149497102 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00221392812516038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004820954743779277 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021796340869708257 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0047818733673793525 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002148075897136724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004751362783480357 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002122652012556582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004726445311347354 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002101184734211335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004704075232592279 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020816679246033008 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004682728899401177 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020630224280875553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004662138713278179 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020445785230140756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004641652040459153 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020260030941336594 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004620813641522137 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020068981652043424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004599206747655687 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019868358863737393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0045764212103738214 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019654352826150028 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004552130614992539 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001942536055916146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004526296202883615 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001918573856610135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0044993792226548516 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018949274220922438 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00447220205254905 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018733821775282868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004445431020723455 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018548428380516556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004419319942622311 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018390181371052696 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00439393302398965 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001825187603807014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004369502505392024 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018127534932226696 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004346394907326541 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018013116780647568 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004324944447242717 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017907123828275401 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0043054313760062714 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017809085796451133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004287949925697399 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017718060440488495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004272355393377273 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001763248488469196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004258375480832456 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017550697633891962 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004245715657135453 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017471480872639025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004234111995199828 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017394173165699571 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004223348500682524 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017318533742643391 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004213255882847056 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017244560739517296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.004203700805984197 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001717245150645253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.004194578743258264 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001710225834532149 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.004185803777304429 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017034004970840858 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.004177306276902186 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016967660310584006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.004169033713270433 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016902938349109688 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.00416094974564391 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016839599552082964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.004153031486574414 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016777579956478701 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.004145258890992608 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016716914990487811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.004137612090574716 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016657748100023424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.004130061074985559 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016600216249673721 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.004122570079658572 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016544297268206065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.004115097217691532 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016489812124551933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.0041076028753832235 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001643658521470059 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.004100049688476876 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016384317625962783 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.004092401308043459 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016332631950329445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.004084632552207969 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016281248614443153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.004076724768231582 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016229857469574034 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.004068672480675566 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016178277947709718 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.004060482107730501 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016126528472555906 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.004052173437796883 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016074782247874845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.004043777898276619 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0016023530971436772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.004035331381254321 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015973380297223587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.0040268673053563825 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015925224556514385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.004018406022364877 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015879916138371473 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.004009951957613151 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015838195561965027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.004001488541919075 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015800664616026654 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.0039929701899489205 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001576776452770645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.003984322737387106 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015739862986296164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.003975488765029365 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015716444840916338 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.003966491150527378 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015695889897675913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.00395742496650282 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001567634744072605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.0039484471637497 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001565580806312978 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.00393975064759946 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00156323189240242 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.0039314888569566865 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015604837478421043 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.003923730666832124 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015573746176813258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.003916480445494382 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015540154620722522 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 861\n",
      "Train Loss: 0.003909707798829582 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015505119880891583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 871\n",
      "Train Loss: 0.003903365418051081 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015469663691434884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 881\n",
      "Train Loss: 0.0038973979790222165 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015434414361540773 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 891\n",
      "Train Loss: 0.003891748529342937 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015399814616062083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 901\n",
      "Train Loss: 0.003886365881130925 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00153660558208593 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 911\n",
      "Train Loss: 0.0038812052788994005 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015333291179803984 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 921\n",
      "Train Loss: 0.0038762280613322433 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015301421622149229 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 931\n",
      "Train Loss: 0.003871403104229863 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001527041415919372 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 941\n",
      "Train Loss: 0.0038667029926003457 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001524017452722771 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 951\n",
      "Train Loss: 0.0038621063676609188 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015210601732689427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 961\n",
      "Train Loss: 0.003857593180260361 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015181582421064377 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 971\n",
      "Train Loss: 0.0038531483172244414 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015152990824563869 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 981\n",
      "Train Loss: 0.0038487574844497034 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015124746027999044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 991\n",
      "Train Loss: 0.0038444076073362505 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0015096820386001067 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:07:34<16:52, 1012.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.049323783278318975 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.021095342986453117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.007633002454168091 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004749932708323337 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0071644350986922525 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004110552993174014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007019856532736515 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004001070245619068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0068216332894727736 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0038354557133122775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006637150112495119 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0037543607412659553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006543278224736212 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0037109923085951236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006395525151738299 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0035811640384994198 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006159687029925367 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.003280811913028945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005965429568972809 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.003029569737571344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005817705133001138 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0028639373662515304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0056976003100152476 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002742978472137049 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0056039427913566016 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0026502096345334242 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0055330620801368885 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002580517583809207 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005476532978603948 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002526278786094378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005426701633579674 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0024794486686168763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005379830921874715 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0024366922509134487 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005334396264515817 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0023977765525701675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005288550039644903 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002361321137051276 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005240444557971176 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002324822660289663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005189331252321204 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002286007038573996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005135813041402061 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022435645969176477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005081461966860658 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021980554094339273 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00502831320461868 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002152209319932951 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004978386039824746 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002109272181830798 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004933523375842705 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002071121459685559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004894821833738167 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002038621102339389 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004861950001621888 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020118391238090195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0048337769017033645 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001989630970507442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004808885909892556 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001970431098342988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004785762666783402 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019528789490231135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004763104530641754 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019358776063721083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004740192866583746 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019184588160355356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004717013280009942 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019001010053246962 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004693625606028272 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018810882361270905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00466972148584053 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018617721069133265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0046448893438525716 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001842236758492301 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004618861884770844 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018225369289559259 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004591838927413134 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0018030615638564812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004564794816999587 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0017844465805814088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004538981564722428 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0017670142716790936 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004515073189415889 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0017508332306268007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0044931331607787435 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0017359798180274248 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004472988008715468 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.001722499426046198 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004454427001165777 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0017103550404969478 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004437246902693627 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0016994436076591104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004421265404455907 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0016896432858180214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004406320357216499 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0016808394542790614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004392270344606017 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0016729335203829608 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004378993326548597 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0016658426683626316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.004366382136780946 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0016594979554948428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.004354343141708654 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0016538429353909379 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.004342791392108057 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0016488240511167082 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.0043316498385882075 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016443968222089364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.004320847070553872 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016405143064781606 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.004310315616672459 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016371275715561311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.004299995843970921 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001634192387468778 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.00428983430197883 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016316680618289733 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.004279781994132291 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001629503621730242 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.004269801420737632 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001627664395681258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.004259859580173697 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016261036455463827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.0042499321867838545 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016247836920024639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.0042400010981463775 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016236582599590751 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.004230055171127077 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001622685917988001 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.004220085471707199 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001621829029277302 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.004210089999355564 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016210573402317136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.004200072012558256 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016203419229517994 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.004190036519623242 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016196649776656557 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.004179995268864784 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001619013893585443 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.004169959160463354 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016183964018657636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.004159937432946227 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016177924247877148 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.004149938878445882 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016171607604836312 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.00413996827995031 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016164339488905885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.0041300289431321704 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001615532037535201 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.004120178709262499 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016144908345932288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.00411038902213635 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016133077017533896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.004100644880831294 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016119354201763283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.004090951332351556 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016103393720002489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.004081312677288117 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016084864010950655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.004071739953707481 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001606355984355166 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.004062244991643216 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016039375737675707 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.004052839105750263 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001601225582704869 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.0040435318163691445 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015982248352087113 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.004034329643166071 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015949555692325733 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.004025234813869866 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001591445152841383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.004016242250045801 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015877199299068432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 861\n",
      "Train Loss: 0.004007342707615236 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015838160415002135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 871\n",
      "Train Loss: 0.003998518976050483 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015797716018670562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 881\n",
      "Train Loss: 0.003989746824963616 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015756243583484647 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 891\n",
      "Train Loss: 0.0039809979261637134 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001571398238217186 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 901\n",
      "Train Loss: 0.003972240436223282 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015671277815067952 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 911\n",
      "Train Loss: 0.003963439089381939 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015628320136201683 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 921\n",
      "Train Loss: 0.003954558912631996 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015585390508195825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 931\n",
      "Train Loss: 0.003945565794651406 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015542870786386343 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 941\n",
      "Train Loss: 0.003936417191762864 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015501779630999971 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 951\n",
      "Train Loss: 0.003926963797315882 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015465126083797534 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 961\n",
      "Train Loss: 0.003916930525007215 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001543279914473173 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 971\n",
      "Train Loss: 0.003906741344662537 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0015396434327147985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 981\n",
      "Train Loss: 0.003896492007884052 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0015356278333425774 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 991\n",
      "Train Loss: 0.003886181470036094 // Train Acc: 0.013148667601683029\n",
      "Val Loss: 0.0015314505173918906 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:24:26<00:00, 1013.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (22797, 12, 5)\n",
      "Shape of the data after splitting into sequences: (5692, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.04241453412382081 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.021201082913393385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.007475150127008265 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.005016453874171868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.007042400564882137 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004543363066976157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00671875132913969 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004278945164974737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00638683084127471 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0039967232975020505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.005897881216461003 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.003471018755651508 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.005299827333103453 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0028994921215527345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.004999378123967209 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0026813359336608374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.004829335861269849 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002557538683902933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.004700455963425469 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0024550021511601976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.004608918186133469 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002370748290886286 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0045381053421890616 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002306579629199977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.004482989084187159 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002270057927908669 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0044386551479010895 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022486726085660623 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00440534523954069 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002215602771991215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.004376373655256798 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021612625683558413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004346380144051377 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021410013347592078 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004318253927995407 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002122457103877004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004292835120299797 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002098204417931167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004268059465213879 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0020756535375951214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004244968289790902 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.002050749512174784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004223634292868581 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.002027995000839275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004204126645639137 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.002013380196811422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0041860513605287124 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.002002875433104594 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004168943284362878 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019931467511596973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004152516558257599 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019833198055269176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004136777875195865 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.00197350309397816 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004121937221967646 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001964116691718966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004108206656918865 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019554573417316827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004095682500421395 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019476159477051808 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004084346344649431 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019405609516039743 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004074108618789001 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019341992813403184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004064847776753799 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019284339410415067 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004056431728462214 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019231682497448173 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004048731751713202 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019183254442988696 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004041629437674594 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001913818458796301 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004035016236569456 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019095655765483846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004028795085939465 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019054936351034831 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004022880741685686 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019015393653502583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.00401719996526375 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018976618986435527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004011692999856257 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018938231138706082 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0040063112405968665 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018900014575194107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004001017598511043 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001886173821475587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.003995783368405085 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018823315815220513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.003990587691935158 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018784570961594164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.003985415151989973 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018745469487941918 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.003980257383728632 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001870595011925534 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.003975107253243073 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018666004758783397 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.003969963806025621 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001862563956738188 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.003964826051648027 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018584947402620416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.003959694724604435 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018544135984096132 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.003954571831712126 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018503387678372727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.003949458732728181 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018462757566901908 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.003944354967869228 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018422251257893703 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.003939257290241731 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018381802146087579 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.003934158809299529 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018341325781323812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.003929050245288326 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018300343484073626 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.003923926396990328 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018257900890684949 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.00391878988685251 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018217446236070663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.003913730205275735 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018179017306158968 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.003908716883692567 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001813806976346048 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.003903745448766707 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001809681018518828 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.0038988177579184955 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018055651240767598 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.0038939364631477413 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018014344759547057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.0038891007536290757 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017973179010062196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.003884309538111723 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017933364306168442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.003879575487252372 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017895340004522521 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.0038749101978457607 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017858475415308153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.0038703184963111226 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017822516985300384 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.0038658055387206145 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017787440486377879 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.0038613727854905707 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017753202086209915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.0038570238324339393 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017719832441921272 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.003852759718226836 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00176873487591079 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.003848582486831681 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017655740754961382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.0038444930643462384 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001762503470304642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.0038404916435986923 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017595141944593195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.0038365783751289117 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017566068007694453 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.003832749853666276 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001753775090936655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.003829004136397442 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017510167825993234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.0038253372874885515 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001748329829823571 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.0038217457330055526 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001745712168035987 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.003818226554319994 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017431643394180416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.003814774281462866 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017406786947309699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.003811385859307529 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017382592256141702 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.003808057129802378 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017359016178335387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.003804785182319456 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017336060108585555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 861\n",
      "Train Loss: 0.0038015654955195137 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001731370854010973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 871\n",
      "Train Loss: 0.003798394915249413 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017291907689683244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 881\n",
      "Train Loss: 0.0037952697097694797 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017270649488350857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 891\n",
      "Train Loss: 0.003792186664759611 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017249947605102605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 901\n",
      "Train Loss: 0.003789142915227308 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00172297717539693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 911\n",
      "Train Loss: 0.003786135266037321 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017210088015908083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 921\n",
      "Train Loss: 0.003783160028881217 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00171908624718761 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 931\n",
      "Train Loss: 0.0037802132271063027 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017172046533191912 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 941\n",
      "Train Loss: 0.0037772923052901346 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017153639501690592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 951\n",
      "Train Loss: 0.003774391815260962 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017135571564150073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 961\n",
      "Train Loss: 0.0037715090228914745 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017117835958017392 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 971\n",
      "Train Loss: 0.0037686374373335605 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017100361488456054 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 981\n",
      "Train Loss: 0.003765771706911198 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001708310021916823 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 991\n",
      "Train Loss: 0.003762905349429869 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001706601498881355 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [17:17<1:09:08, 1037.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.12608067933757375 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.04646819528568996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.00882157857162776 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0061676083158785375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.007297609621354576 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004600310791022239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007053639678360286 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004339213523584721 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.006703864808682859 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004070532684779486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006357637915380174 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0038070444317189244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.005879421920880028 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0033313720670183388 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.005639358609620884 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0031988264298265317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00548513304833375 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.003106840309985191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005356797197086509 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00300147291599888 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005243825541220104 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0028844720074446516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005156073703425368 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002784022154270807 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005094820848400973 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002714579524681642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005048758592776599 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0026645483279056597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005010692646980369 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0026232942826313415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0049770972539630275 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002586058056141574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004945103301788154 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0025498175534440642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004911489061734327 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002511778407863582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004871678232479198 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002468739238599043 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004818023592517757 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0024171515836856547 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004743487552444478 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0023565055479379264 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004663107092907002 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022987785276281907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004600117810127335 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022546248232521997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0045531106671758605 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022194538771677136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004515473159398429 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002189215208571195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004483564324130998 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021619391146906015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004455435557714121 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021364304571925255 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004429982766785971 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021121136852660415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004406531547672321 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020888381784133028 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004384577745290246 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002066638657504044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004363674989906054 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020455747275100497 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004343476337083546 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002025639812927693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004323862560225982 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020067451941634244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004304947862764076 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001988725135040082 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004286890379727922 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001971483874472193 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004269758530345133 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019550817930976743 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004253562848079756 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019397475096799978 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004238312733836399 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019257443592742378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004223993384092524 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019132476868997381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004210537599042439 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019022630255543784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004197825446821551 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018926466775278487 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004185707318801616 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018841802169375246 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004174034066872507 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018766256143454086 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004162678793924998 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001869775047473442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004151542676086335 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018634725838842059 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004140556373827415 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001857602152923708 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004129669298486442 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018520918030558635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004118839366659313 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018468987182949492 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004108026387221552 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001841992267022914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.00409719083504882 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00183734951934547 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.004086302381582287 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018329519691505584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.004075349432802897 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018287894781678915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.004064352291673396 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00182484306154375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.004053372547450393 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018210962089040222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.004042510866405649 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018175160142807603 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.004031892705723111 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018140597931459923 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.004021639496543239 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018106722354684862 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.004011837271911446 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018072928651235998 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.004002518379041301 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018038588001790426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.0039936605431753385 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018003106812470374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.00398520384405754 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001796611857775787 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.003977072870601778 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017927485002136759 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.003969196339485955 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017887346068491343 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.003961523528407403 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017846165516929116 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.003954034322578209 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001780461254425012 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.003946742420881558 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001776363017178779 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.00393968537632052 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017724213804500282 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.00393291073794141 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017687309589930235 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.003926461643335602 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017653583745822687 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.003920363939725134 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017623292501973973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.003914625891781482 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001759649080249188 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.003909241235445986 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017572801998028992 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.0039041899919391022 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017551813424065228 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.0038994474864374132 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001753302681157784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.0038949869365593336 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017515995058694101 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.0038907794171792742 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017500318537120906 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.00388679768523426 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017485678211649816 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.003883016441535952 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017471769417497444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.00387941061842045 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001745840293561956 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.00387595834001196 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017445354233234284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.003872637548709228 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017432516695489438 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.0038694278341959666 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017419746748938256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.003866310993067105 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017406895023893585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.0038632678256114114 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017393821599602888 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.003860284274637955 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017380478513607587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.0038573614243766732 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017366302333735558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 854\n",
      "INFO: Validation loss did not improve in epoch 856\n",
      "INFO: Validation loss did not improve in epoch 858\n",
      "INFO: Validation loss did not improve in epoch 860\n",
      "Epoch: 861\n",
      "Train Loss: 0.0038544891802476506 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017351397891003074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 862\n",
      "INFO: Validation loss did not improve in epoch 864\n",
      "INFO: Validation loss did not improve in epoch 866\n",
      "INFO: Validation loss did not improve in epoch 868\n",
      "INFO: Validation loss did not improve in epoch 870\n",
      "Epoch: 871\n",
      "Train Loss: 0.0038515950603244713 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017336820534448758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 872\n",
      "INFO: Validation loss did not improve in epoch 874\n",
      "INFO: Validation loss did not improve in epoch 876\n",
      "INFO: Validation loss did not improve in epoch 878\n",
      "INFO: Validation loss did not improve in epoch 880\n",
      "Epoch: 881\n",
      "Train Loss: 0.003848712791089193 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001732188092022768 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 882\n",
      "INFO: Validation loss did not improve in epoch 884\n",
      "INFO: Validation loss did not improve in epoch 886\n",
      "INFO: Validation loss did not improve in epoch 888\n",
      "INFO: Validation loss did not improve in epoch 890\n",
      "Epoch: 891\n",
      "Train Loss: 0.003845836838487233 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017306532979265295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 892\n",
      "INFO: Validation loss did not improve in epoch 894\n",
      "INFO: Validation loss did not improve in epoch 896\n",
      "INFO: Validation loss did not improve in epoch 898\n",
      "INFO: Validation loss did not improve in epoch 900\n",
      "Epoch: 901\n",
      "Train Loss: 0.0038429688672740195 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017290895041523061 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 902\n",
      "INFO: Validation loss did not improve in epoch 904\n",
      "INFO: Validation loss did not improve in epoch 906\n",
      "INFO: Validation loss did not improve in epoch 908\n",
      "INFO: Validation loss did not improve in epoch 910\n",
      "Epoch: 911\n",
      "Train Loss: 0.003840113578228408 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017274999462939234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 912\n",
      "INFO: Validation loss did not improve in epoch 914\n",
      "INFO: Validation loss did not improve in epoch 916\n",
      "INFO: Validation loss did not improve in epoch 918\n",
      "INFO: Validation loss did not improve in epoch 920\n",
      "Epoch: 921\n",
      "Train Loss: 0.003837276886326364 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017258927528223211 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 922\n",
      "INFO: Validation loss did not improve in epoch 924\n",
      "INFO: Validation loss did not improve in epoch 926\n",
      "INFO: Validation loss did not improve in epoch 928\n",
      "INFO: Validation loss did not improve in epoch 930\n",
      "Epoch: 931\n",
      "Train Loss: 0.0038344692016774097 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001724273022957502 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 932\n",
      "INFO: Validation loss did not improve in epoch 934\n",
      "INFO: Validation loss did not improve in epoch 936\n",
      "INFO: Validation loss did not improve in epoch 938\n",
      "INFO: Validation loss did not improve in epoch 940\n",
      "Epoch: 941\n",
      "Train Loss: 0.0038316977680739155 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017226519652712943 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 942\n",
      "INFO: Validation loss did not improve in epoch 944\n",
      "INFO: Validation loss did not improve in epoch 946\n",
      "INFO: Validation loss did not improve in epoch 948\n",
      "INFO: Validation loss did not improve in epoch 950\n",
      "Epoch: 951\n",
      "Train Loss: 0.003828969831104282 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001721039574149881 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 952\n",
      "INFO: Validation loss did not improve in epoch 954\n",
      "INFO: Validation loss did not improve in epoch 956\n",
      "INFO: Validation loss did not improve in epoch 958\n",
      "INFO: Validation loss did not improve in epoch 960\n",
      "Epoch: 961\n",
      "Train Loss: 0.003826292168899592 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017194356677760736 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 962\n",
      "INFO: Validation loss did not improve in epoch 964\n",
      "INFO: Validation loss did not improve in epoch 966\n",
      "INFO: Validation loss did not improve in epoch 968\n",
      "INFO: Validation loss did not improve in epoch 970\n",
      "Epoch: 971\n",
      "Train Loss: 0.0038236676569836053 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001717855524352825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 972\n",
      "INFO: Validation loss did not improve in epoch 974\n",
      "INFO: Validation loss did not improve in epoch 976\n",
      "INFO: Validation loss did not improve in epoch 978\n",
      "INFO: Validation loss did not improve in epoch 980\n",
      "Epoch: 981\n",
      "Train Loss: 0.0038210999061906027 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00171629758208357 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 982\n",
      "INFO: Validation loss did not improve in epoch 984\n",
      "INFO: Validation loss did not improve in epoch 986\n",
      "INFO: Validation loss did not improve in epoch 988\n",
      "INFO: Validation loss did not improve in epoch 990\n",
      "Epoch: 991\n",
      "Train Loss: 0.0038185902165188907 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001714770390300258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 992\n",
      "INFO: Validation loss did not improve in epoch 994\n",
      "INFO: Validation loss did not improve in epoch 996\n",
      "INFO: Validation loss did not improve in epoch 998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [34:31<51:46, 1035.60s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 1000\n",
      "Epoch: 1\n",
      "Train Loss: 0.11857661942404082 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.05674474966827403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0075248660048506475 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0054133477335628335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.007054356687021707 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004651730754569675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.006842077052714447 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004407790329914247 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.006648396009410801 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004242924904696685 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006425191070962838 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004053299856801214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006054672628587574 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0036700397463034044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0055594427522851824 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0033139550395890694 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.005375289345789716 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0031608441371073046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00521647842564368 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0029950867262485783 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005080880355749535 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0028500369098560696 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.004981719430106291 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002740268093884368 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00490804967946578 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002654664054921085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0048413441547807854 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0025774407507250024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.004770149101782721 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0024990647687220926 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00469075021795082 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0024171640155993906 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004621477873632123 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002352307970816102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004571182278101169 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002308206309648126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004532570209661773 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002272977797495034 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004500962186116219 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022436660522166013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00447512345635404 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002220853861013239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004453758003864813 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022031041702616624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004435716299268287 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021883742425988398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004420129868017737 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021751640489445266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0044062463600948705 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002162733719569076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004393491995237444 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021507901337725014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00438150854896869 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021392158889918054 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004370116260112362 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002127964547612924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004359249137345242 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021170588734094054 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004348893716492459 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00210657131366348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004339045826032194 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020965874117270657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004329684366707835 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020871682827010374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004320774643732885 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020783208111128403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0043122744195713165 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020700387576244395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004304138283675659 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020622783654442663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00429632312866048 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002054997635782916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004288784430400164 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00204814786550568 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004281480680219829 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002041688718011582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004274371341912554 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002035574914934637 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004267415731425592 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002029769426392224 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004260576858157801 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020242387770931496 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004253816706955245 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020189417533974133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004247101839925038 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00201385165499314 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004240397818133151 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020089408718284973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004233671750006008 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020041812297605563 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004226893090481672 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001999546042211835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004220027430097312 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019950128395536787 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004213044405434813 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019905624835649923 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004205915444451522 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001986175596163336 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004198626471424755 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001981841251966962 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.00419119392271215 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019775620057362686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.004183660508450225 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019733251682011767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.00417606457107835 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019691042378714853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.004168423385128149 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019648801944129627 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.004160741066771101 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019606401971502103 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.004153023296846325 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001956366804550046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.004145282528772663 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019520610816818609 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.0041375373365085824 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019477162470261493 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.004129802918866235 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019433258928350707 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.004122092638708139 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019388852625944013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.004114414212998619 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019343864714551529 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.004106767181709388 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001929823364774707 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.004099149211064284 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019251861303110785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.004091554033984058 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019204665081355762 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.0040839753423257096 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019156572352074547 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.004076406522784617 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019107597315647336 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.004068846213644394 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019057800586523717 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.004061294044923105 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019007218067058143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.004053757664029778 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018956076551302013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.00404624767382048 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001890450933970396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.004038779699990753 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018852715425617115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.004031370152529266 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018800909191453725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.004024034282986106 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018749353166844884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.004016784936396032 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018698296250037724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.004009635209809299 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018647965128805613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.00400259513445499 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001859853113968752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.003995676389745166 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001855014185708937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.003988887245249562 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001850296635464556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.003982235427565747 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018457062460686174 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.0039757254306843496 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018412497849792262 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.0039693590214895285 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001836924300610161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.003963139937272319 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018327355679462068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.00395706637408506 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018286814055081164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.0039511366553348585 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018247558136556946 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.003945347688831572 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018209513938028281 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.003939697352485525 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018172662836313772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 861\n",
      "Train Loss: 0.003934180451446666 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018136917271394952 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 871\n",
      "Train Loss: 0.003928792622426282 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018102164294248907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 881\n",
      "Train Loss: 0.003923527139051573 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018068357793289875 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 891\n",
      "Train Loss: 0.003918379841719821 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018035402460013297 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 901\n",
      "Train Loss: 0.003913344463433412 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018003239852946587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 911\n",
      "Train Loss: 0.00390841305611177 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017971813992997423 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 921\n",
      "Train Loss: 0.00390357912833532 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017941019486693477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 931\n",
      "Train Loss: 0.0038988351280478365 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017910836273647343 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 941\n",
      "Train Loss: 0.0038941732857919716 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017881270395356409 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 951\n",
      "Train Loss: 0.0038895827057956866 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017852161149625153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 961\n",
      "Train Loss: 0.0038850538831479786 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001782354636816308 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 971\n",
      "Train Loss: 0.0038805777065947047 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017795459628288288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 981\n",
      "Train Loss: 0.003876140898258163 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017767803978274335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 991\n",
      "Train Loss: 0.0038717307375194074 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017740592661999125 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [52:55<35:33, 1066.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.08572177818308874 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.06421125887401319 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.007308907195570425 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.005013395849029335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.006125543189632117 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0037980600818730056 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.005861113983014439 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0036432973149734854 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.005676139925566056 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0035214268653444274 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.005497189827275111 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.003375190809850445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.005321955210516206 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.003260149668870766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.005226633536138246 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.003168236558440696 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0051616690749221525 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0031066397440441874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005105045821313939 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0030634374288433974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005051706301382786 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0030247277604400317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.004999423529208229 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0029789357337566993 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.004947761831469416 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0029206802160086716 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.004897267802756901 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002851201767779447 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.004849015400865146 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002776132309887809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.004803547408068644 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002700759856268408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004760381030271464 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0026270542898754264 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004718111346470555 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002553777569405253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0046751095469568195 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0024791243901408247 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004630264117667219 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0024025263232074342 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004584255919897561 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0023255426364066603 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00454031102843243 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.002255098037801034 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004501298220657063 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0021980216640330266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0044676586067980625 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0021526986629089987 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004438484201226044 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0021145455565387277 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004412749169758805 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.002082113063112006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004389601758796184 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0020540786535594236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004368454747739265 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0020294430089481384 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004348888624750788 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0020074571202917213 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0043306008004485455 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019875715612276885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004313361139701572 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019693706286652538 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004296957543753296 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019526045960426498 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004281252140942099 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019368118461969655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004266086846281779 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019218000394339266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004251324043473357 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0019074218587039479 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004236847373525002 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018935895157360545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004222574008159799 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018802721444465052 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004208462761498225 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.00186750191176216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004194519328747629 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018553487097143374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004180776135248836 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018438814156244018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004167279816522599 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001833148637794356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004154072249103155 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018231624379633727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004141182272804726 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0018138910425968176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.00412862046664518 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001805286393095373 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0041163860662509835 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017972819505814002 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004104467727740911 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017898146624237383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004092852647225588 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017828236869013125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.00408152685511131 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001776254933727696 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004070479348027585 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001770071194800182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004059703990495091 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017642427737474065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.004049200238562012 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017587420029638943 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.004038968321875124 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017535546729131864 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.004029014497390604 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017486704340769668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.004019346164094041 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017440833898229797 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.004009971147609501 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017397930846272266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.004000895474266736 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017357931081257928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.003992122887969435 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017320843906602247 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.003983656017532308 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017286696058588123 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.003975490293613155 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017255435052217936 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.003967619961327488 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017227018579024445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.003960033205989618 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001720145264943915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.003952713356043773 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017178581018403717 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.003945640853715886 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001715834409827785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.003938790430676657 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017140586958580724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.003932133012321918 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017125144884712325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.003925633935622223 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017111815094916422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.003919256062212244 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017100321763362527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.003912952982217758 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001709027610865704 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.003906675464203281 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017081217270009637 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.0039003702429342667 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001707253948552534 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.0038940008413105376 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017063545201789964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.0038875755120028298 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017053694202658835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.003881181886685466 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017042977953188414 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.0038749530203157165 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001703207106054813 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.003868989767748366 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017021727255085127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.003863320753029267 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017012465350183376 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.0038579178938601653 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0017004211592158366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.0038527365249639107 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001699669404332055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.0038477303237369515 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016989661223684135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.003842852529997808 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001698281651616536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.0038380656482979686 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016975921400158216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.0038333363100549345 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016968822586768714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.003828633728722531 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016961464056028366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.003823935603827426 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001695381850592813 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.003819228751701793 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016945896986113296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.0038145105032835465 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016937864266775465 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 861\n",
      "Train Loss: 0.003809785746301813 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016929763797644442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 871\n",
      "Train Loss: 0.003805059993585381 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016921676866699722 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 881\n",
      "Train Loss: 0.003800336579951558 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.001691352162350576 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 891\n",
      "Train Loss: 0.00379561304231011 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016905509352061396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 901\n",
      "Train Loss: 0.0037908809429993344 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016897742877180657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 911\n",
      "Train Loss: 0.0037861251417412927 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016890358349198496 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 921\n",
      "Train Loss: 0.0037813232795284413 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016882890473078058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 931\n",
      "Train Loss: 0.0037764885219866815 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016873878772825715 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 941\n",
      "Train Loss: 0.0037716818773573025 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016862350300849112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 951\n",
      "Train Loss: 0.0037669432430324336 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016849387248747804 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 961\n",
      "Train Loss: 0.0037622751865985307 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016836191189417804 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 971\n",
      "Train Loss: 0.003757677876229201 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016823206685504385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 981\n",
      "Train Loss: 0.003753154664538764 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016810490015313406 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 991\n",
      "Train Loss: 0.0037487084929325827 // Train Acc: 0.00876577840112202\n",
      "Val Loss: 0.0016798051368455622 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:14:03<19:06, 1146.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.0996921548977766 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.04836848772685515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.00766603272704755 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0053346993485361 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.007285701359137579 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0047422663408090895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007031459232163975 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.004370137341477479 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.006227048528988076 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.003371315336592621 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.005376227789945107 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0028225303074250812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.005095797042061312 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002611800885021561 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.004934567796963765 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0024704775100003592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00484168767312876 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0023762268973043545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.004778272397623517 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002311000363048417 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.004727069689496453 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0022635512594185937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.004682227903206377 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002226043035069564 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.004641656523673608 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021941601462254112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.004604414108680851 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002165496669803861 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0045699766220859295 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002138733263768444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.004538070688874502 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0021133799808298686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004508585620689144 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020895293038050654 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004481470730902854 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020675071223450595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004456665999253462 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.00204755653387846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00443408069975392 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.002029741974285898 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0044135787139116395 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020139854469367964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004394973856318601 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0020001140554576817 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004378039364132842 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019879053376897584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004362534747545857 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001977129830858471 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004348228495668615 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019675707402037395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0043349134321696795 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001959039626598588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004322411408951587 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019513775981711537 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004310568061583449 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001944463175533109 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004299248613042921 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019381900002075924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004288330714827009 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019324702822885822 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004277696482543958 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019272236330722448 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0042672317444384725 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019223578002832285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004256813257585213 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019177734260604287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004246313407932909 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019133752051693795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004235603558346969 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019090731871534098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004224571509683194 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0019047964790056386 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004213129229099734 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001900496541096023 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0042012219566578855 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018961444425820426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004188828655717135 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018917255690896863 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004175952893190692 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018872144704637525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004162614695942695 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018825662005797364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004148853710900893 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018777234837533173 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004134742207080648 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001872631708935709 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004120391791916307 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018672516400758386 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004105956341890386 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001861569066002201 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004091610780902087 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018556019715739811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004077525347256367 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018494062748962616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004063828796135891 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018430653502680674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0040505978920977685 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018366783943349035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004037854734215191 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018303480293285206 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.004025578678116662 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001824164236117458 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.0040137232547406455 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001818197914902801 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.004002227884851824 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018125027181054832 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.003991031086465493 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018071036086368029 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.00398007249203282 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0018020249831914024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.00396930395157258 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017972683826372488 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.00395868438933447 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017928402727663392 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.003948186481597495 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017887356130568458 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.003937796631117377 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.001784957062748648 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.003927512710089802 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017814910826744156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.003917344307675853 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017783390794647858 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.003907307261502472 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017754888463519473 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.0038974258873505888 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017729301610242135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.003887722291069322 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017706488983135335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.003878218582508038 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017686306179783653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.00386892959792598 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017668557885688928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.003859867946914858 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017653112198367422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.0038510386982761576 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017639760319091212 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.0038424430201633416 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017628370716532148 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.003834074323844729 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017618797804501032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.003825923620441048 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017610898707716024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.0038179767442649026 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017604602942585925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.003810215979696318 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017599811860205857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.0038026177160589477 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017596472365705798 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.003795155550423111 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017594540886727444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.00378779998113797 // Train Acc: 0.00438288920056101\n",
      "Val Loss: 0.0017593897100187461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 752\n",
      "INFO: Validation loss did not improve in epoch 753\n",
      "INFO: Validation loss did not improve in epoch 754\n",
      "INFO: Validation loss did not improve in epoch 755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:29:52<00:00, 1078.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 756\n",
      "Early stopping after 756 epochs\n",
      "Shape of the data after splitting into sequences: (28500, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.08483540846445033 // Train Acc: 0.0\n",
      "Val Loss: 0.031785587046565957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0021444086135962003 // Train Acc: 0.0\n",
      "Val Loss: 0.00215544954040504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0015854844457538783 // Train Acc: 0.0\n",
      "Val Loss: 0.0017030672962041105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0014992277548572658 // Train Acc: 0.0\n",
      "Val Loss: 0.001619810734871598 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0014159406756075702 // Train Acc: 0.0\n",
      "Val Loss: 0.0015317545868794554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0013599501360283882 // Train Acc: 0.0\n",
      "Val Loss: 0.0014692146676415628 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0013080833888848458 // Train Acc: 0.0\n",
      "Val Loss: 0.001411407285040848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.001257890099481592 // Train Acc: 0.0\n",
      "Val Loss: 0.0013565216033801426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0012159589601717863 // Train Acc: 0.0\n",
      "Val Loss: 0.0013137820387986233 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0011826103243279319 // Train Acc: 0.0\n",
      "Val Loss: 0.0012836136920871888 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0011528371296088938 // Train Acc: 0.0\n",
      "Val Loss: 0.0012580956449939 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0011237624805588158 // Train Acc: 0.0\n",
      "Val Loss: 0.00123210933677951 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0010965732805908307 // Train Acc: 0.0\n",
      "Val Loss: 0.001206079613617049 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0010723036528050018 // Train Acc: 0.0\n",
      "Val Loss: 0.0011813964744491305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0010514021904186868 // Train Acc: 0.0\n",
      "Val Loss: 0.0011588939066908672 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0010339441892252073 // Train Acc: 0.0\n",
      "Val Loss: 0.001139063617118957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0010193205643219934 // Train Acc: 0.0\n",
      "Val Loss: 0.0011219824690309316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0010066450873359143 // Train Acc: 0.0\n",
      "Val Loss: 0.0011071462515714271 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0009952322110707765 // Train Acc: 0.0\n",
      "Val Loss: 0.001093934661225551 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.000984704856278158 // Train Acc: 0.0\n",
      "Val Loss: 0.0010823693761196217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0009749463926026007 // Train Acc: 0.0\n",
      "Val Loss: 0.0010721262425051575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0009658946021978233 // Train Acc: 0.0\n",
      "Val Loss: 0.001062419910182885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0009573318838668585 // Train Acc: 0.0\n",
      "Val Loss: 0.001052817865746361 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0009492411308230892 // Train Acc: 0.0\n",
      "Val Loss: 0.0010436542571831868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0009414802138981975 // Train Acc: 0.0\n",
      "Val Loss: 0.001034319407697481 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0009340099218238777 // Train Acc: 0.0\n",
      "Val Loss: 0.0010243549979715894 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0009269384086384473 // Train Acc: 0.0\n",
      "Val Loss: 0.001015791893619966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0009201340873116368 // Train Acc: 0.0\n",
      "Val Loss: 0.0010078827288442724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0009135527635748507 // Train Acc: 0.0\n",
      "Val Loss: 0.0010002470103270692 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0009071491188250501 // Train Acc: 0.0\n",
      "Val Loss: 0.0009927707679560616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0009008568016964704 // Train Acc: 0.0\n",
      "Val Loss: 0.0009853521320648886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.000894593413632499 // Train Acc: 0.0\n",
      "Val Loss: 0.000977896795593145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0008882636646307459 // Train Acc: 0.0\n",
      "Val Loss: 0.000970321003163582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0008817533798745035 // Train Acc: 0.0\n",
      "Val Loss: 0.0009625521294191198 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0008749177113547986 // Train Acc: 0.0\n",
      "Val Loss: 0.0009544911034575627 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0008676388503784094 // Train Acc: 0.0\n",
      "Val Loss: 0.0009459612577114328 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0008599688217566685 // Train Acc: 0.0\n",
      "Val Loss: 0.0009371470931498594 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0008517431608559779 // Train Acc: 0.0\n",
      "Val Loss: 0.0009280019894199363 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0008429323194205936 // Train Acc: 0.0\n",
      "Val Loss: 0.0009184160620018361 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0008336575015566082 // Train Acc: 0.0\n",
      "Val Loss: 0.0009083607554853264 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0008241564882120776 // Train Acc: 0.0\n",
      "Val Loss: 0.0008979694904465562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.000814679806047383 // Train Acc: 0.0\n",
      "Val Loss: 0.0008875055414365667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0008054345548586419 // Train Acc: 0.0\n",
      "Val Loss: 0.0008772403934571174 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0007965721899488756 // Train Acc: 0.0\n",
      "Val Loss: 0.0008673877997535889 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0007881838036439843 // Train Acc: 0.0\n",
      "Val Loss: 0.0008580959884920703 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0007802988517654571 // Train Acc: 0.0\n",
      "Val Loss: 0.000849430526090674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.000772899006523024 // Train Acc: 0.0\n",
      "Val Loss: 0.0008413768987618153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0007659386781337057 // Train Acc: 0.0\n",
      "Val Loss: 0.0008338657072836314 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0007593647921297877 // Train Acc: 0.0\n",
      "Val Loss: 0.0008268115790651892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.000753126134198756 // Train Acc: 0.0\n",
      "Val Loss: 0.0008201290770481319 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.0007471788695839176 // Train Acc: 0.0\n",
      "Val Loss: 0.0008137480789267422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.0007414862722718303 // Train Acc: 0.0\n",
      "Val Loss: 0.0008076091281362086 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.0007360192505960505 // Train Acc: 0.0\n",
      "Val Loss: 0.00080166609116242 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.0007307552891006389 // Train Acc: 0.0\n",
      "Val Loss: 0.0007958851074464957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.0007256776790642721 // Train Acc: 0.0\n",
      "Val Loss: 0.0007902410379045138 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.0007207723379373187 // Train Acc: 0.0\n",
      "Val Loss: 0.0007847154963541745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.0007160281282798178 // Train Acc: 0.0\n",
      "Val Loss: 0.0007792956991818364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.000711433897763138 // Train Acc: 0.0\n",
      "Val Loss: 0.0007739713647683781 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.0007069784734757495 // Train Acc: 0.0\n",
      "Val Loss: 0.0007687357117180639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.0007026510640704021 // Train Acc: 0.0\n",
      "Val Loss: 0.0007635822987760152 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.0006984400411891923 // Train Acc: 0.0\n",
      "Val Loss: 0.0007585073558986968 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.0006943349416070876 // Train Acc: 0.0\n",
      "Val Loss: 0.000753507665039805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.0006903247272984032 // Train Acc: 0.0\n",
      "Val Loss: 0.0007485795129001447 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.000686399876259249 // Train Acc: 0.0\n",
      "Val Loss: 0.0007437214382723177 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.0006825520304756775 // Train Acc: 0.0\n",
      "Val Loss: 0.0007389320721381536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.0006787749685753309 // Train Acc: 0.0\n",
      "Val Loss: 0.0007342088431986205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.0006750632660307688 // Train Acc: 0.0\n",
      "Val Loss: 0.0007295504056860942 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.0006714141166592304 // Train Acc: 0.0\n",
      "Val Loss: 0.0007249555871552038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.0006678261012203834 // Train Acc: 0.0\n",
      "Val Loss: 0.0007204238072365481 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.0006642996088708105 // Train Acc: 0.0\n",
      "Val Loss: 0.0007159554571246301 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.000660836743770512 // Train Acc: 0.0\n",
      "Val Loss: 0.0007115518325902703 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.0006574403713303807 // Train Acc: 0.0\n",
      "Val Loss: 0.0007072153164892635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.000654115494919306 // Train Acc: 0.0\n",
      "Val Loss: 0.0007029508156014162 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.0006508660206550322 // Train Acc: 0.0\n",
      "Val Loss: 0.0006987623004858999 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.0006476970503716873 // Train Acc: 0.0\n",
      "Val Loss: 0.0006946563620628222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.0006446130074024936 // Train Acc: 0.0\n",
      "Val Loss: 0.0006906426977813335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.0006416181785722791 // Train Acc: 0.0\n",
      "Val Loss: 0.0006867300496028876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.000638716343980835 // Train Acc: 0.0\n",
      "Val Loss: 0.000682927441056304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.0006359093725492069 // Train Acc: 0.0\n",
      "Val Loss: 0.0006792453370619937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.0006331982657879586 // Train Acc: 0.0\n",
      "Val Loss: 0.000675691190273592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.0006305839462436149 // Train Acc: 0.0\n",
      "Val Loss: 0.000672273020849104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.0006280651681865128 // Train Acc: 0.0\n",
      "Val Loss: 0.0006689946869565122 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.0006256399596790681 // Train Acc: 0.0\n",
      "Val Loss: 0.000665859170336089 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.0006233056875491589 // Train Acc: 0.0\n",
      "Val Loss: 0.000662866354694456 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.0006210592193388593 // Train Acc: 0.0\n",
      "Val Loss: 0.000660015203045062 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.000618897104387003 // Train Acc: 0.0\n",
      "Val Loss: 0.0006573020848476487 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 861\n",
      "Train Loss: 0.0006168151382105601 // Train Acc: 0.0\n",
      "Val Loss: 0.0006547238206306821 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 871\n",
      "Train Loss: 0.0006148097223025383 // Train Acc: 0.0\n",
      "Val Loss: 0.000652274966270659 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 881\n",
      "Train Loss: 0.0006128766846717015 // Train Acc: 0.0\n",
      "Val Loss: 0.0006499489992780562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 891\n",
      "Train Loss: 0.0006110118031196445 // Train Acc: 0.0\n",
      "Val Loss: 0.000647742014353758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 901\n",
      "Train Loss: 0.000609212327936467 // Train Acc: 0.0\n",
      "Val Loss: 0.0006456471859788335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 911\n",
      "Train Loss: 0.0006074739022318079 // Train Acc: 0.0\n",
      "Val Loss: 0.0006436597447427427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 921\n",
      "Train Loss: 0.0006057939255294333 // Train Acc: 0.0\n",
      "Val Loss: 0.0006417737218604017 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 931\n",
      "Train Loss: 0.0006041694004205382 // Train Acc: 0.0\n",
      "Val Loss: 0.0006399849036174525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 941\n",
      "Train Loss: 0.0006025973640800765 // Train Acc: 0.0\n",
      "Val Loss: 0.0006382877322594446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 951\n",
      "Train Loss: 0.000601075652068831 // Train Acc: 0.0\n",
      "Val Loss: 0.000636675683792886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 961\n",
      "Train Loss: 0.000599601708841902 // Train Acc: 0.0\n",
      "Val Loss: 0.0006351439688491632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 971\n",
      "Train Loss: 0.0005981734088604236 // Train Acc: 0.0\n",
      "Val Loss: 0.0006336874986885573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 981\n",
      "Train Loss: 0.0005967884975571876 // Train Acc: 0.0\n",
      "Val Loss: 0.0006323007397014158 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 991\n",
      "Train Loss: 0.0005954447892467722 // Train Acc: 0.0\n",
      "Val Loss: 0.0006309779899555138 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [21:10<1:24:42, 1270.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.08474457226066653 // Train Acc: 0.0\n",
      "Val Loss: 0.017674960071148153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0022048399462263383 // Train Acc: 0.0\n",
      "Val Loss: 0.002215151008421441 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0014864816458748527 // Train Acc: 0.0\n",
      "Val Loss: 0.0015854573476599145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0013874575932603917 // Train Acc: 0.0\n",
      "Val Loss: 0.001497081969019192 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0013121121332309305 // Train Acc: 0.0\n",
      "Val Loss: 0.0014180488965419439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0012651443386990525 // Train Acc: 0.0\n",
      "Val Loss: 0.0013673498344675288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0012229954820422804 // Train Acc: 0.0\n",
      "Val Loss: 0.0013250329890405479 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.001183051428931741 // Train Acc: 0.0\n",
      "Val Loss: 0.0012861275102412483 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0011506901263515857 // Train Acc: 0.0\n",
      "Val Loss: 0.0012527251193057954 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0011229749025375652 // Train Acc: 0.0\n",
      "Val Loss: 0.001222845173994334 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0010971339310457027 // Train Acc: 0.0\n",
      "Val Loss: 0.0011941900789151072 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0010721276023875476 // Train Acc: 0.0\n",
      "Val Loss: 0.0011666957736919327 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0010481627261372895 // Train Acc: 0.0\n",
      "Val Loss: 0.001141178717267018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0010261990393106702 // Train Acc: 0.0\n",
      "Val Loss: 0.0011187218914625323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0010067209017733739 // Train Acc: 0.0\n",
      "Val Loss: 0.001099972181346049 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0009898265729144642 // Train Acc: 0.0\n",
      "Val Loss: 0.001084482071624663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0009752216777139896 // Train Acc: 0.0\n",
      "Val Loss: 0.0010712401101309755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0009623818801337825 // Train Acc: 0.0\n",
      "Val Loss: 0.0010594208862999318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0009508309370521678 // Train Acc: 0.0\n",
      "Val Loss: 0.0010484858071652641 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0009402632220124279 // Train Acc: 0.0\n",
      "Val Loss: 0.001038068698656709 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0009304218885063802 // Train Acc: 0.0\n",
      "Val Loss: 0.001027940551349737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0009210980904774661 // Train Acc: 0.0\n",
      "Val Loss: 0.0010180294240711716 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0009121664026824772 // Train Acc: 0.0\n",
      "Val Loss: 0.0010083061809600825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0009035498740120176 // Train Acc: 0.0\n",
      "Val Loss: 0.0009987452983636377 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0008951932294140759 // Train Acc: 0.0\n",
      "Val Loss: 0.0009893167547051739 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0008870518272149939 // Train Acc: 0.0\n",
      "Val Loss: 0.0009799877362288067 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0008790921212295149 // Train Acc: 0.0\n",
      "Val Loss: 0.0009707283738684482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0008712933671004028 // Train Acc: 0.0\n",
      "Val Loss: 0.000961522615886054 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0008636435323734774 // Train Acc: 0.0\n",
      "Val Loss: 0.0009523714444952543 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0008561318073023978 // Train Acc: 0.0\n",
      "Val Loss: 0.0009432850412402677 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0008487401000909974 // Train Acc: 0.0\n",
      "Val Loss: 0.0009342693861452967 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0008414585492349175 // Train Acc: 0.0\n",
      "Val Loss: 0.0009253319484889299 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0008342647619184132 // Train Acc: 0.0\n",
      "Val Loss: 0.0009164620081434942 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0008271111989529185 // Train Acc: 0.0\n",
      "Val Loss: 0.0009076356249321616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0008199485381932924 // Train Acc: 0.0\n",
      "Val Loss: 0.0008988510753065816 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0008127063527884684 // Train Acc: 0.0\n",
      "Val Loss: 0.0008901303381055552 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0008049245814281433 // Train Acc: 0.0\n",
      "Val Loss: 0.0008814787061614447 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0007975671083932849 // Train Acc: 0.0\n",
      "Val Loss: 0.0008726951941931048 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0007904841399830466 // Train Acc: 0.0\n",
      "Val Loss: 0.0008640955809559725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0007836288120809859 // Train Acc: 0.0\n",
      "Val Loss: 0.0008558094319068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.000776990457520958 // Train Acc: 0.0\n",
      "Val Loss: 0.0008478406163917167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0007705662944686383 // Train Acc: 0.0\n",
      "Val Loss: 0.0008401742035766294 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0007643659215958966 // Train Acc: 0.0\n",
      "Val Loss: 0.0008328044726077582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0007584054570979482 // Train Acc: 0.0\n",
      "Val Loss: 0.0008257452317080383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0007526497148927778 // Train Acc: 0.0\n",
      "Val Loss: 0.0008189958775827047 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0007470919848126647 // Train Acc: 0.0\n",
      "Val Loss: 0.0008125648312381203 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0007417747093756294 // Train Acc: 0.0\n",
      "Val Loss: 0.0008065225936956287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0007367358621412158 // Train Acc: 0.0\n",
      "Val Loss: 0.000800881122971245 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0007319760109606685 // Train Acc: 0.0\n",
      "Val Loss: 0.0007956132131619587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0007274691115692193 // Train Acc: 0.0\n",
      "Val Loss: 0.0007906843333045936 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.0007231901471632394 // Train Acc: 0.0\n",
      "Val Loss: 0.0007860590184306435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.0007191205277118433 // Train Acc: 0.0\n",
      "Val Loss: 0.0007817005750322558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.0007152431064871823 // Train Acc: 0.0\n",
      "Val Loss: 0.0007775800008984938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.0007115403046532041 // Train Acc: 0.0\n",
      "Val Loss: 0.0007736790455700882 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.0007079943550765634 // Train Acc: 0.0\n",
      "Val Loss: 0.0007699864025340889 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.0007045874499071253 // Train Acc: 0.0\n",
      "Val Loss: 0.000766488660940531 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.0007013034127726283 // Train Acc: 0.0\n",
      "Val Loss: 0.0007631719054188579 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.000698126642346375 // Train Acc: 0.0\n",
      "Val Loss: 0.00076002025844753 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.0006950433646997523 // Train Acc: 0.0\n",
      "Val Loss: 0.0007570180371267118 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.0006920425539608845 // Train Acc: 0.0\n",
      "Val Loss: 0.00075414622673705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.0006891147760190409 // Train Acc: 0.0\n",
      "Val Loss: 0.0007513885046635083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.0006862523763423078 // Train Acc: 0.0\n",
      "Val Loss: 0.0007487315940312968 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.0006834499644869736 // Train Acc: 0.0\n",
      "Val Loss: 0.0007461689991581218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.0006807040970372154 // Train Acc: 0.0\n",
      "Val Loss: 0.0007436982823136476 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.0006780125105987514 // Train Acc: 0.0\n",
      "Val Loss: 0.0007413222206374058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.0006753733908049116 // Train Acc: 0.0\n",
      "Val Loss: 0.0007390441185522918 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.0006727870504300307 // Train Acc: 0.0\n",
      "Val Loss: 0.0007368552664525068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.0006702565078944103 // Train Acc: 0.0\n",
      "Val Loss: 0.0007347409093463741 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.0006677866529068892 // Train Acc: 0.0\n",
      "Val Loss: 0.0007326856090338619 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.0006653823131622896 // Train Acc: 0.0\n",
      "Val Loss: 0.0007306818133148537 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.0006630484559048303 // Train Acc: 0.0\n",
      "Val Loss: 0.0007287285698925057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.000660788891746383 // Train Acc: 0.0\n",
      "Val Loss: 0.0007268247185157893 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.0006586074280333124 // Train Acc: 0.0\n",
      "Val Loss: 0.0007249744799717814 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.0006565053397317731 // Train Acc: 0.0\n",
      "Val Loss: 0.000723180449460425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.0006544833373140249 // Train Acc: 0.0\n",
      "Val Loss: 0.0007214463790182763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.0006525401787343043 // Train Acc: 0.0\n",
      "Val Loss: 0.0007197756439986556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.0006506730211035121 // Train Acc: 0.0\n",
      "Val Loss: 0.0007181706435571284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.0006488780818625222 // Train Acc: 0.0\n",
      "Val Loss: 0.0007166320511465763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.0006471519787829476 // Train Acc: 0.0\n",
      "Val Loss: 0.0007151615409689107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.0006454919814009697 // Train Acc: 0.0\n",
      "Val Loss: 0.0007137526036841748 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.0006438980650620112 // Train Acc: 0.0\n",
      "Val Loss: 0.0007124016526923905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.0006423719271969803 // Train Acc: 0.0\n",
      "Val Loss: 0.0007111027998740489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.00064091567836423 // Train Acc: 0.0\n",
      "Val Loss: 0.0007098443156944629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.0006395259079866916 // Train Acc: 0.0\n",
      "Val Loss: 0.0007086206301032207 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.0006381966447481999 // Train Acc: 0.0\n",
      "Val Loss: 0.000707415598400741 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.0006369209797545284 // Train Acc: 0.0\n",
      "Val Loss: 0.0007062131807994352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 861\n",
      "Train Loss: 0.0006356878802048037 // Train Acc: 0.0\n",
      "Val Loss: 0.0007049949375838406 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 871\n",
      "Train Loss: 0.0006344800932013833 // Train Acc: 0.0\n",
      "Val Loss: 0.0007037352074676469 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 881\n",
      "Train Loss: 0.0006332788710200116 // Train Acc: 0.0\n",
      "Val Loss: 0.0007024074592608074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 891\n",
      "Train Loss: 0.000632074693095662 // Train Acc: 0.0\n",
      "Val Loss: 0.0007010017047807257 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 901\n",
      "Train Loss: 0.0006308752711780525 // Train Acc: 0.0\n",
      "Val Loss: 0.0006995358215023857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 911\n",
      "Train Loss: 0.0006296983689309273 // Train Acc: 0.0\n",
      "Val Loss: 0.000698061444417459 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 921\n",
      "Train Loss: 0.000628557960917686 // Train Acc: 0.0\n",
      "Val Loss: 0.000696627230334028 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 931\n",
      "Train Loss: 0.0006274593170338113 // Train Acc: 0.0\n",
      "Val Loss: 0.0006952616482450856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 941\n",
      "Train Loss: 0.0006264021898865396 // Train Acc: 0.0\n",
      "Val Loss: 0.0006939657037773992 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 951\n",
      "Train Loss: 0.0006253835271759355 // Train Acc: 0.0\n",
      "Val Loss: 0.0006927408715927916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 961\n",
      "Train Loss: 0.0006243997773366685 // Train Acc: 0.0\n",
      "Val Loss: 0.000691574708055887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 971\n",
      "Train Loss: 0.0006234474153662666 // Train Acc: 0.0\n",
      "Val Loss: 0.0006904607349629488 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 981\n",
      "Train Loss: 0.0006225230872523638 // Train Acc: 0.0\n",
      "Val Loss: 0.0006893906714206083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 991\n",
      "Train Loss: 0.0006216239697907445 // Train Acc: 0.0\n",
      "Val Loss: 0.0006883582408382563 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [42:01<1:02:57, 1259.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.17080513056142968 // Train Acc: 0.0\n",
      "Val Loss: 0.07954647287976142 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0034223984245101164 // Train Acc: 0.0\n",
      "Val Loss: 0.0035468214503671536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0017380949366257459 // Train Acc: 0.0\n",
      "Val Loss: 0.0017587123214340035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0013277260010257405 // Train Acc: 0.0\n",
      "Val Loss: 0.0014165820156851023 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0012222167938329843 // Train Acc: 0.0\n",
      "Val Loss: 0.0013096888098272615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0011689508337108549 // Train Acc: 0.0\n",
      "Val Loss: 0.0012498716727014911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0011328966881131862 // Train Acc: 0.0\n",
      "Val Loss: 0.0012107742473114979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0011039725404605335 // Train Acc: 0.0\n",
      "Val Loss: 0.0011796583237800248 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0010793624179820474 // Train Acc: 0.0\n",
      "Val Loss: 0.0011531507762866978 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0010578727742438603 // Train Acc: 0.0\n",
      "Val Loss: 0.0011302887419155439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0010386111974722064 // Train Acc: 0.0\n",
      "Val Loss: 0.00111033784924075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0010208794736613776 // Train Acc: 0.0\n",
      "Val Loss: 0.0010926073988413115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.001004226250138619 // Train Acc: 0.0\n",
      "Val Loss: 0.0010765456140409458 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0009884042966299231 // Train Acc: 0.0\n",
      "Val Loss: 0.001061721064128221 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0009733051148411095 // Train Acc: 0.0\n",
      "Val Loss: 0.0010477977821261326 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0009589073781138299 // Train Acc: 0.0\n",
      "Val Loss: 0.0010345618665467443 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0009452244252989233 // Train Acc: 0.0\n",
      "Val Loss: 0.001021926721503344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0009322490839961639 // Train Acc: 0.0\n",
      "Val Loss: 0.0010098694112264375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0009199188612403119 // Train Acc: 0.0\n",
      "Val Loss: 0.000998327790136129 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0009081104066504393 // Train Acc: 0.0\n",
      "Val Loss: 0.0009871590270383761 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0008966666398735503 // Train Acc: 0.0\n",
      "Val Loss: 0.0009761734937715043 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0008854526316330391 // Train Acc: 0.0\n",
      "Val Loss: 0.0009652113663660253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0008743989750691317 // Train Acc: 0.0\n",
      "Val Loss: 0.0009541938005534896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0008635000209977164 // Train Acc: 0.0\n",
      "Val Loss: 0.0009431198616375543 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0008527826984624573 // Train Acc: 0.0\n",
      "Val Loss: 0.000932028618705909 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0008422800465153862 // Train Acc: 0.0\n",
      "Val Loss: 0.0009209677721237161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0008320281060257742 // Train Acc: 0.0\n",
      "Val Loss: 0.0009099877838275588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0008220732087405924 // Train Acc: 0.0\n",
      "Val Loss: 0.0008991502380511052 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0008124641104538299 // Train Acc: 0.0\n",
      "Val Loss: 0.0008885326572367401 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0008032298947089818 // Train Acc: 0.0\n",
      "Val Loss: 0.0008782097600630027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0007943715723437161 // Train Acc: 0.0\n",
      "Val Loss: 0.0008682462462554452 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0007858789186884857 // Train Acc: 0.0\n",
      "Val Loss: 0.0008587005683780237 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0007777549754303285 // Train Acc: 0.0\n",
      "Val Loss: 0.000849645533198985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0007700247205031362 // Train Acc: 0.0\n",
      "Val Loss: 0.0008411579565798692 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.000762723307425697 // Train Acc: 0.0\n",
      "Val Loss: 0.0008333011850400557 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0007558778651213132 // Train Acc: 0.0\n",
      "Val Loss: 0.0008261027175938976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.000749494049468386 // Train Acc: 0.0\n",
      "Val Loss: 0.0008195388994528262 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0007435554241296883 // Train Acc: 0.0\n",
      "Val Loss: 0.0008135533377992551 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0007380296060220968 // Train Acc: 0.0\n",
      "Val Loss: 0.0008080741330427143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0007328737964934466 // Train Acc: 0.0\n",
      "Val Loss: 0.0008030229950663192 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0007280408238675667 // Train Acc: 0.0\n",
      "Val Loss: 0.0007983303927054223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0007234814402079608 // Train Acc: 0.0\n",
      "Val Loss: 0.0007939350031324085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0007191476752281255 // Train Acc: 0.0\n",
      "Val Loss: 0.0007897862400690499 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0007149938252299134 // Train Acc: 0.0\n",
      "Val Loss: 0.0007858386040663908 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0007109771207262184 // Train Acc: 0.0\n",
      "Val Loss: 0.0007820554484428536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0007070589632006999 // Train Acc: 0.0\n",
      "Val Loss: 0.0007784033042811866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.000703204254862486 // Train Acc: 0.0\n",
      "Val Loss: 0.0007748560148795973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.000699382471048853 // Train Acc: 0.0\n",
      "Val Loss: 0.0007713883641265676 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0006955669183224612 // Train Acc: 0.0\n",
      "Val Loss: 0.0007679818812517096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.000691734439391985 // Train Acc: 0.0\n",
      "Val Loss: 0.0007646205581727245 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.0006878670473962821 // Train Acc: 0.0\n",
      "Val Loss: 0.0007612927390494014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.0006839500179911538 // Train Acc: 0.0\n",
      "Val Loss: 0.000757989770451212 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.0006799735857699874 // Train Acc: 0.0\n",
      "Val Loss: 0.000754709191763105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.0006759316574193199 // Train Acc: 0.0\n",
      "Val Loss: 0.000751448577936861 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.0006718221558522444 // Train Acc: 0.0\n",
      "Val Loss: 0.0007482115543504667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.0006676474391627432 // Train Acc: 0.0\n",
      "Val Loss: 0.000744999906702771 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.0006634126772095357 // Train Acc: 0.0\n",
      "Val Loss: 0.0007418178499682141 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.0006591274272741914 // Train Acc: 0.0\n",
      "Val Loss: 0.0007386682778642403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.0006548059236707765 // Train Acc: 0.0\n",
      "Val Loss: 0.0007355522368280165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.0006504671333309036 // Train Acc: 0.0\n",
      "Val Loss: 0.0007324722714529649 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.0006461341300713771 // Train Acc: 0.0\n",
      "Val Loss: 0.000729432234378966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.0006418347714419365 // Train Acc: 0.0\n",
      "Val Loss: 0.0007264381701382938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.000637598499742831 // Train Acc: 0.0\n",
      "Val Loss: 0.0007235022085652473 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.0006334532514674299 // Train Acc: 0.0\n",
      "Val Loss: 0.00072064007654059 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.0006294227147968466 // Train Acc: 0.0\n",
      "Val Loss: 0.0007178683120314443 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.0006255251604157988 // Train Acc: 0.0\n",
      "Val Loss: 0.0007152078619913598 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.0006217713669972883 // Train Acc: 0.0\n",
      "Val Loss: 0.0007126707146783209 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.0006181672011571643 // Train Acc: 0.0\n",
      "Val Loss: 0.0007102675403044923 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.0006147127997916 // Train Acc: 0.0\n",
      "Val Loss: 0.0007080051752620938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.0006114050524899436 // Train Acc: 0.0\n",
      "Val Loss: 0.0007058805432437279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.0006082380463630921 // Train Acc: 0.0\n",
      "Val Loss: 0.0007038905278226106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.000605203991845318 // Train Acc: 0.0\n",
      "Val Loss: 0.0007020237181835168 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.0006022939530963262 // Train Acc: 0.0\n",
      "Val Loss: 0.0007002700484252415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.0005994975770458166 // Train Acc: 0.0\n",
      "Val Loss: 0.0006986180231851192 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.0005968046489840378 // Train Acc: 0.0\n",
      "Val Loss: 0.0006970524148594109 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.0005942054564515106 // Train Acc: 0.0\n",
      "Val Loss: 0.0006955638017740189 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.0005916905661589835 // Train Acc: 0.0\n",
      "Val Loss: 0.0006941399352404538 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.0005892516053977278 // Train Acc: 0.0\n",
      "Val Loss: 0.0006927661421723721 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.0005868818044912613 // Train Acc: 0.0\n",
      "Val Loss: 0.0006914361973986415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.0005845753211772731 // Train Acc: 0.0\n",
      "Val Loss: 0.0006901408175696546 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.0005823277432150596 // Train Acc: 0.0\n",
      "Val Loss: 0.0006888715571684206 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.0005801358207925399 // Train Acc: 0.0\n",
      "Val Loss: 0.0006876206092956585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.0005779979646895414 // Train Acc: 0.0\n",
      "Val Loss: 0.0006863867073250263 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.0005759122958976637 // Train Acc: 0.0\n",
      "Val Loss: 0.0006851606279030812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.0005738787088798049 // Train Acc: 0.0\n",
      "Val Loss: 0.0006839408996198249 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.0005718967342805731 // Train Acc: 0.0\n",
      "Val Loss: 0.0006827248556945244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 861\n",
      "Train Loss: 0.0005699664638403034 // Train Acc: 0.0\n",
      "Val Loss: 0.0006815156528359414 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 871\n",
      "Train Loss: 0.0005680873541582342 // Train Acc: 0.0\n",
      "Val Loss: 0.000680309943778391 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 881\n",
      "Train Loss: 0.0005662595908886389 // Train Acc: 0.0\n",
      "Val Loss: 0.0006791096854699365 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 891\n",
      "Train Loss: 0.0005644829736620535 // Train Acc: 0.0\n",
      "Val Loss: 0.0006779177309395668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 901\n",
      "Train Loss: 0.0005627566108318126 // Train Acc: 0.0\n",
      "Val Loss: 0.0006767340298246478 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 911\n",
      "Train Loss: 0.0005610796414980394 // Train Acc: 0.0\n",
      "Val Loss: 0.0006755602062896398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 921\n",
      "Train Loss: 0.0005594505346074033 // Train Acc: 0.0\n",
      "Val Loss: 0.0006743984403577763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 931\n",
      "Train Loss: 0.00055786828830238 // Train Acc: 0.0\n",
      "Val Loss: 0.0006732462836489961 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 941\n",
      "Train Loss: 0.0005563312749420671 // Train Acc: 0.0\n",
      "Val Loss: 0.000672108272824491 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 951\n",
      "Train Loss: 0.00055483723491813 // Train Acc: 0.0\n",
      "Val Loss: 0.0006709811441091738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 961\n",
      "Train Loss: 0.0005533842540618429 // Train Acc: 0.0\n",
      "Val Loss: 0.000669863893705428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 971\n",
      "Train Loss: 0.0005519700940314851 // Train Acc: 0.0\n",
      "Val Loss: 0.000668760004480026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 981\n",
      "Train Loss: 0.0005505926887306214 // Train Acc: 0.0\n",
      "Val Loss: 0.0006676621145458876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 991\n",
      "Train Loss: 0.0005492494218248997 // Train Acc: 0.0\n",
      "Val Loss: 0.0006665727165891846 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [59:32<38:47, 1163.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1706169189004878 // Train Acc: 0.0\n",
      "Val Loss: 0.04343628073787556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.002004024712189243 // Train Acc: 0.0\n",
      "Val Loss: 0.002054568443040223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.001518867199864633 // Train Acc: 0.0\n",
      "Val Loss: 0.0016320836878259402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0014389794531146386 // Train Acc: 0.0\n",
      "Val Loss: 0.001559274121465333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0013831971999759712 // Train Acc: 0.0\n",
      "Val Loss: 0.0015061988068325704 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0013322680938159812 // Train Acc: 0.0\n",
      "Val Loss: 0.00145506873003639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0012773402751157933 // Train Acc: 0.0\n",
      "Val Loss: 0.0013941686503542328 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0012313202674974796 // Train Acc: 0.0\n",
      "Val Loss: 0.0013475227194081202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0011962971862607673 // Train Acc: 0.0\n",
      "Val Loss: 0.001311573318212928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.001167169451762708 // Train Acc: 0.0\n",
      "Val Loss: 0.001281194449269067 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.001141264409861667 // Train Acc: 0.0\n",
      "Val Loss: 0.0012532499288442913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0011175458180619506 // Train Acc: 0.0\n",
      "Val Loss: 0.0012264505171642364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0010949681295406239 // Train Acc: 0.0\n",
      "Val Loss: 0.0011999234282372613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0010724357569895208 // Train Acc: 0.0\n",
      "Val Loss: 0.001172684506687476 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0010492425307810768 // Train Acc: 0.0\n",
      "Val Loss: 0.0011441183344600202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.001025844249779817 // Train Acc: 0.0\n",
      "Val Loss: 0.0011151160467799764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0010037437873487726 // Train Acc: 0.0\n",
      "Val Loss: 0.0010879453980606492 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0009839237362560607 // Train Acc: 0.0\n",
      "Val Loss: 0.0010641030904231462 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0009664386258649817 // Train Acc: 0.0\n",
      "Val Loss: 0.001043763332770989 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.000950959161202179 // Train Acc: 0.0\n",
      "Val Loss: 0.0010264169713940768 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.000937069586334818 // Train Acc: 0.0\n",
      "Val Loss: 0.0010112922059992515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0009243702389319299 // Train Acc: 0.0\n",
      "Val Loss: 0.000997644526987693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0009125386647265183 // Train Acc: 0.0\n",
      "Val Loss: 0.0009849541368175626 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0009013434864296043 // Train Acc: 0.0\n",
      "Val Loss: 0.0009729313624222169 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0008906325509197465 // Train Acc: 0.0\n",
      "Val Loss: 0.0009614377584857619 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0008803357679992333 // Train Acc: 0.0\n",
      "Val Loss: 0.0009504163683851915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0008704694015661164 // Train Acc: 0.0\n",
      "Val Loss: 0.0009398728563112412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0008611030862390731 // Train Acc: 0.0\n",
      "Val Loss: 0.0009298593754135138 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0008522929128957912 // Train Acc: 0.0\n",
      "Val Loss: 0.000920427645027882 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0008440410968765772 // Train Acc: 0.0\n",
      "Val Loss: 0.0009115918333949157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0008363000458365353 // Train Acc: 0.0\n",
      "Val Loss: 0.0009033138743912614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0008289959002645401 // Train Acc: 0.0\n",
      "Val Loss: 0.0008955201627834577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0008220501458122124 // Train Acc: 0.0\n",
      "Val Loss: 0.0008881305883474018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0008153926726581848 // Train Acc: 0.0\n",
      "Val Loss: 0.000881065735001606 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0008089668488506813 // Train Acc: 0.0\n",
      "Val Loss: 0.0008742626917069809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0008027286123945463 // Train Acc: 0.0\n",
      "Val Loss: 0.0008676681955881201 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.000796648359051001 // Train Acc: 0.0\n",
      "Val Loss: 0.0008612382280363573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0007907091574829926 // Train Acc: 0.0\n",
      "Val Loss: 0.0008549425005205509 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0007849082063447924 // Train Acc: 0.0\n",
      "Val Loss: 0.0008487623049074384 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0007792539323382452 // Train Acc: 0.0\n",
      "Val Loss: 0.0008426915298803975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0007737621624263717 // Train Acc: 0.0\n",
      "Val Loss: 0.000836742326085405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0007684493262112908 // Train Acc: 0.0\n",
      "Val Loss: 0.000830936239848262 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0007633264679435594 // Train Acc: 0.0\n",
      "Val Loss: 0.0008253015159080052 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0007583935256601837 // Train Acc: 0.0\n",
      "Val Loss: 0.0008198633901309872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0007536405079610624 // Train Acc: 0.0\n",
      "Val Loss: 0.0008146387262968346 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0007490480332666365 // Train Acc: 0.0\n",
      "Val Loss: 0.0008096319842423825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0007445928848737643 // Train Acc: 0.0\n",
      "Val Loss: 0.0008048336995930109 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0007402502914530952 // Train Acc: 0.0\n",
      "Val Loss: 0.0008002346025958043 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0007359972613969613 // Train Acc: 0.0\n",
      "Val Loss: 0.0007958097184856042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0007318150765033306 // Train Acc: 0.0\n",
      "Val Loss: 0.0007915428688657969 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.0007276883569183083 // Train Acc: 0.0\n",
      "Val Loss: 0.000787413646727989 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.0007236072099723541 // Train Acc: 0.0\n",
      "Val Loss: 0.0007834080217699468 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.0007195649670411181 // Train Acc: 0.0\n",
      "Val Loss: 0.0007795160588341396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.0007155597938791565 // Train Acc: 0.0\n",
      "Val Loss: 0.0007757299338069396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.000711592949359798 // Train Acc: 0.0\n",
      "Val Loss: 0.0007720484017679342 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.0007076691304657446 // Train Acc: 0.0\n",
      "Val Loss: 0.0007684753451518475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.0007037944071722899 // Train Acc: 0.0\n",
      "Val Loss: 0.0007650140272459931 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.0006999772628276187 // Train Acc: 0.0\n",
      "Val Loss: 0.0007616717086431138 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.0006962276582279097 // Train Acc: 0.0\n",
      "Val Loss: 0.0007584577720767074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.0006925547508138192 // Train Acc: 0.0\n",
      "Val Loss: 0.0007553789166306983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.0006889674868701623 // Train Acc: 0.0\n",
      "Val Loss: 0.0007524416716031174 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.000685474312559164 // Train Acc: 0.0\n",
      "Val Loss: 0.0007496497853722179 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.0006820819803062456 // Train Acc: 0.0\n",
      "Val Loss: 0.0007470051814329175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.0006787946930025027 // Train Acc: 0.0\n",
      "Val Loss: 0.0007445016741237535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.0006756161083320357 // Train Acc: 0.0\n",
      "Val Loss: 0.0007421346039865062 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.0006725465812554471 // Train Acc: 0.0\n",
      "Val Loss: 0.0007398955294063742 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.000669586290171462 // Train Acc: 0.0\n",
      "Val Loss: 0.0007377715322254719 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.0006667322718672557 // Train Acc: 0.0\n",
      "Val Loss: 0.0007357505689047969 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.0006639815485270643 // Train Acc: 0.0\n",
      "Val Loss: 0.0007338172030328206 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.0006613296496515001 // Train Acc: 0.0\n",
      "Val Loss: 0.0007319620432587102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.0006587715851990238 // Train Acc: 0.0\n",
      "Val Loss: 0.0007301728497551984 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.0006563016395222527 // Train Acc: 0.0\n",
      "Val Loss: 0.0007284373775327463 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.000653914121475311 // Train Acc: 0.0\n",
      "Val Loss: 0.0007267454463478657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.0006516032275176564 // Train Acc: 0.0\n",
      "Val Loss: 0.0007250936713356489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.0006493639035119713 // Train Acc: 0.0\n",
      "Val Loss: 0.0007234703947860857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.0006471912101443255 // Train Acc: 0.0\n",
      "Val Loss: 0.0007218753933801196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.0006450810088337004 // Train Acc: 0.0\n",
      "Val Loss: 0.0007203017078472077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.0006430296899907899 // Train Acc: 0.0\n",
      "Val Loss: 0.0007187496168280733 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.000641033499680677 // Train Acc: 0.0\n",
      "Val Loss: 0.0007172170975252039 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.0006390900759622037 // Train Acc: 0.0\n",
      "Val Loss: 0.0007157003504254039 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.0006371969324477843 // Train Acc: 0.0\n",
      "Val Loss: 0.0007142014453556992 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.0006353519179847328 // Train Acc: 0.0\n",
      "Val Loss: 0.000712721484115571 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.0006335530440936317 // Train Acc: 0.0\n",
      "Val Loss: 0.0007112568040123448 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.0006317988411126806 // Train Acc: 0.0\n",
      "Val Loss: 0.0007098089851889052 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.0006300870893135482 // Train Acc: 0.0\n",
      "Val Loss: 0.0007083796237220769 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.0006284167188243238 // Train Acc: 0.0\n",
      "Val Loss: 0.0007069659815755199 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 861\n",
      "Train Loss: 0.000626785490068517 // Train Acc: 0.0\n",
      "Val Loss: 0.0007055704344916626 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 871\n",
      "Train Loss: 0.0006251918024477042 // Train Acc: 0.0\n",
      "Val Loss: 0.0007041918777850295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 881\n",
      "Train Loss: 0.0006236342344561077 // Train Acc: 0.0\n",
      "Val Loss: 0.0007028270270720946 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 891\n",
      "Train Loss: 0.0006221104759006496 // Train Acc: 0.0\n",
      "Val Loss: 0.0007014818198188057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 901\n",
      "Train Loss: 0.0006206189393469812 // Train Acc: 0.0\n",
      "Val Loss: 0.0007001479793591658 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 911\n",
      "Train Loss: 0.0006191578407466234 // Train Acc: 0.0\n",
      "Val Loss: 0.0006988298649266856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 921\n",
      "Train Loss: 0.0006177251214831924 // Train Acc: 0.0\n",
      "Val Loss: 0.0006975273717313162 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 931\n",
      "Train Loss: 0.0006163187290631302 // Train Acc: 0.0\n",
      "Val Loss: 0.0006962380376540374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 941\n",
      "Train Loss: 0.0006149372966779 // Train Acc: 0.0\n",
      "Val Loss: 0.0006949616038089607 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 951\n",
      "Train Loss: 0.0006135786555614877 // Train Acc: 0.0\n",
      "Val Loss: 0.0006936996404615532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 961\n",
      "Train Loss: 0.0006122410763014267 // Train Acc: 0.0\n",
      "Val Loss: 0.0006924462032763876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 971\n",
      "Train Loss: 0.0006109223111472543 // Train Acc: 0.0\n",
      "Val Loss: 0.0006912068355169704 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 981\n",
      "Train Loss: 0.000609620607276901 // Train Acc: 0.0\n",
      "Val Loss: 0.0006899760079229648 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 991\n",
      "Train Loss: 0.0006083341260671029 // Train Acc: 0.0\n",
      "Val Loss: 0.0006887577438347208 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:16:46<18:32, 1112.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.09103852978595343 // Train Acc: 0.0\n",
      "Val Loss: 0.03046260082147308 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0020195573939305704 // Train Acc: 0.0\n",
      "Val Loss: 0.002034957007200629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.001525799551451981 // Train Acc: 0.0\n",
      "Val Loss: 0.0016337666272279395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0014324428227383626 // Train Acc: 0.0\n",
      "Val Loss: 0.0015345643354237064 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0013736049606646692 // Train Acc: 0.0\n",
      "Val Loss: 0.0014660470199858375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0013285363948377252 // Train Acc: 0.0\n",
      "Val Loss: 0.0014135191875071572 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0012812148205105537 // Train Acc: 0.0\n",
      "Val Loss: 0.0013608022971148992 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0011951671682231568 // Train Acc: 0.0\n",
      "Val Loss: 0.0012756277240914236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0011143707639453175 // Train Acc: 0.0\n",
      "Val Loss: 0.0011886054659379366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.001062575036862788 // Train Acc: 0.0\n",
      "Val Loss: 0.0011295132671418474 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.001027833199389586 // Train Acc: 0.0\n",
      "Val Loss: 0.001091007127680776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0010036390307948822 // Train Acc: 0.0\n",
      "Val Loss: 0.0010647658328306225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.000985095290511064 // Train Acc: 0.0\n",
      "Val Loss: 0.0010448413515872714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0009695637909342668 // Train Acc: 0.0\n",
      "Val Loss: 0.0010280111801068172 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.000955780169388265 // Train Acc: 0.0\n",
      "Val Loss: 0.0010128642483038812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0009430440190998605 // Train Acc: 0.0\n",
      "Val Loss: 0.0009987651934738626 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0009308537973712596 // Train Acc: 0.0\n",
      "Val Loss: 0.0009853388353539682 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0009188226762729459 // Train Acc: 0.0\n",
      "Val Loss: 0.0009722842286510313 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0009067163990837781 // Train Acc: 0.0\n",
      "Val Loss: 0.0009593937871847614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0008945233958269937 // Train Acc: 0.0\n",
      "Val Loss: 0.0009466515758153514 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0008823864562421427 // Train Acc: 0.0\n",
      "Val Loss: 0.0009341857773033959 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.000870481006718511 // Train Acc: 0.0\n",
      "Val Loss: 0.0009221565690104246 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0008589907575017109 // Train Acc: 0.0\n",
      "Val Loss: 0.000910713431014362 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.000848086364034594 // Train Acc: 0.0\n",
      "Val Loss: 0.0008999525326142847 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0008378644016945081 // Train Acc: 0.0\n",
      "Val Loss: 0.0008898829125006444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0008283312783244609 // Train Acc: 0.0\n",
      "Val Loss: 0.0008804415304759419 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0008194386926584134 // Train Acc: 0.0\n",
      "Val Loss: 0.0008715448206630094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0008111229504225612 // Train Acc: 0.0\n",
      "Val Loss: 0.0008631116485333086 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0008033208636701536 // Train Acc: 0.0\n",
      "Val Loss: 0.0008550726189704055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0007959751876872451 // Train Acc: 0.0\n",
      "Val Loss: 0.0008473710448665846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0007890338593642299 // Train Acc: 0.0\n",
      "Val Loss: 0.000839962052058119 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0007824508625653808 // Train Acc: 0.0\n",
      "Val Loss: 0.0008328142280410065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00077618589183266 // Train Acc: 0.0\n",
      "Val Loss: 0.0008259059520023381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0007702038027906985 // Train Acc: 0.0\n",
      "Val Loss: 0.0008192218971983711 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0007644738827856208 // Train Acc: 0.0\n",
      "Val Loss: 0.000812750910277096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0007589687395820147 // Train Acc: 0.0\n",
      "Val Loss: 0.0008064836180978975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0007536642927501572 // Train Acc: 0.0\n",
      "Val Loss: 0.000800409217489631 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0007485384635752502 // Train Acc: 0.0\n",
      "Val Loss: 0.0007945170161277237 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0007435711737818327 // Train Acc: 0.0\n",
      "Val Loss: 0.0007887954427194053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0007387441106417855 // Train Acc: 0.0\n",
      "Val Loss: 0.0007832300726990293 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0007340401975744304 // Train Acc: 0.0\n",
      "Val Loss: 0.0007778081185101173 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.000729444364642889 // Train Acc: 0.0\n",
      "Val Loss: 0.0007725151226216506 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0007249427716083482 // Train Acc: 0.0\n",
      "Val Loss: 0.0007673386001000805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0007205239468980561 // Train Acc: 0.0\n",
      "Val Loss: 0.0007622656975801275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0007161784610444246 // Train Acc: 0.0\n",
      "Val Loss: 0.0007572872491843489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.000711899801228603 // Train Acc: 0.0\n",
      "Val Loss: 0.0007523953942770617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0007076835609245249 // Train Acc: 0.0\n",
      "Val Loss: 0.0007475865657285881 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0007035283577410624 // Train Acc: 0.0\n",
      "Val Loss: 0.0007428579037217553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0006994348629643921 // Train Acc: 0.0\n",
      "Val Loss: 0.0007382121480001115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0006954053281870495 // Train Acc: 0.0\n",
      "Val Loss: 0.0007336511198293376 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 501\n",
      "Train Loss: 0.00069144467544129 // Train Acc: 0.0\n",
      "Val Loss: 0.0007291835695443357 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 511\n",
      "Train Loss: 0.000687558675893216 // Train Acc: 0.0\n",
      "Val Loss: 0.0007248178153531626 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 521\n",
      "Train Loss: 0.0006837539295507135 // Train Acc: 0.0\n",
      "Val Loss: 0.0007205654018023311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 531\n",
      "Train Loss: 0.0006800386142023616 // Train Acc: 0.0\n",
      "Val Loss: 0.0007164395536561419 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 541\n",
      "Train Loss: 0.0006764211799647103 // Train Acc: 0.0\n",
      "Val Loss: 0.0007124531319216622 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 551\n",
      "Train Loss: 0.0006729095897457787 // Train Acc: 0.0\n",
      "Val Loss: 0.0007086199435617452 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 561\n",
      "Train Loss: 0.0006695116299733459 // Train Acc: 0.0\n",
      "Val Loss: 0.0007049523140485892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 571\n",
      "Train Loss: 0.0006662339860585085 // Train Acc: 0.0\n",
      "Val Loss: 0.0007014559054998915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 581\n",
      "Train Loss: 0.0006630807159395711 // Train Acc: 0.0\n",
      "Val Loss: 0.0006981355117148766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 591\n",
      "Train Loss: 0.0006600530429570648 // Train Acc: 0.0\n",
      "Val Loss: 0.0006949898131844046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 601\n",
      "Train Loss: 0.0006571501943797998 // Train Acc: 0.0\n",
      "Val Loss: 0.0006920130388546532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 611\n",
      "Train Loss: 0.0006543680435407555 // Train Acc: 0.0\n",
      "Val Loss: 0.0006891947833339257 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 621\n",
      "Train Loss: 0.0006517004339746265 // Train Acc: 0.0\n",
      "Val Loss: 0.0006865230556814827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 631\n",
      "Train Loss: 0.0006491398139058176 // Train Acc: 0.0\n",
      "Val Loss: 0.0006839842577016127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 641\n",
      "Train Loss: 0.0006466778993861604 // Train Acc: 0.0\n",
      "Val Loss: 0.0006815646338915874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 651\n",
      "Train Loss: 0.0006443060744164556 // Train Acc: 0.0\n",
      "Val Loss: 0.0006792500837300852 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 661\n",
      "Train Loss: 0.000642015710935538 // Train Acc: 0.0\n",
      "Val Loss: 0.000677028153053948 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 671\n",
      "Train Loss: 0.0006397989999121975 // Train Acc: 0.0\n",
      "Val Loss: 0.0006748890184667924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 681\n",
      "Train Loss: 0.000637648398188716 // Train Acc: 0.0\n",
      "Val Loss: 0.0006728219062264869 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 691\n",
      "Train Loss: 0.0006355577332018594 // Train Acc: 0.0\n",
      "Val Loss: 0.000670819674309944 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 701\n",
      "Train Loss: 0.0006335206869916976 // Train Acc: 0.0\n",
      "Val Loss: 0.0006688755166075614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 711\n",
      "Train Loss: 0.0006315323455825932 // Train Acc: 0.0\n",
      "Val Loss: 0.0006669851957073087 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 721\n",
      "Train Loss: 0.0006295877629765316 // Train Acc: 0.0\n",
      "Val Loss: 0.0006651454317375664 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 731\n",
      "Train Loss: 0.0006276826523895033 // Train Acc: 0.0\n",
      "Val Loss: 0.0006633514758962332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 741\n",
      "Train Loss: 0.0006258135420365943 // Train Acc: 0.0\n",
      "Val Loss: 0.000661603276967071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 751\n",
      "Train Loss: 0.0006239768959248607 // Train Acc: 0.0\n",
      "Val Loss: 0.0006598977889659472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 761\n",
      "Train Loss: 0.0006221698312443807 // Train Acc: 0.0\n",
      "Val Loss: 0.0006582331799774551 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 771\n",
      "Train Loss: 0.0006203904269586597 // Train Acc: 0.0\n",
      "Val Loss: 0.0006566087692727559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 781\n",
      "Train Loss: 0.0006186375723055173 // Train Acc: 0.0\n",
      "Val Loss: 0.0006550228428557338 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 791\n",
      "Train Loss: 0.0006169094660007206 // Train Acc: 0.0\n",
      "Val Loss: 0.0006534758532177865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 801\n",
      "Train Loss: 0.0006152055577655338 // Train Acc: 0.0\n",
      "Val Loss: 0.0006519661540820461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 811\n",
      "Train Loss: 0.0006135252571146677 // Train Acc: 0.0\n",
      "Val Loss: 0.0006504923013944891 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 821\n",
      "Train Loss: 0.0006118689135287961 // Train Acc: 0.0\n",
      "Val Loss: 0.0006490533578573132 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 831\n",
      "Train Loss: 0.0006102354411452292 // Train Acc: 0.0\n",
      "Val Loss: 0.0006476480861918938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 841\n",
      "Train Loss: 0.0006086244288672593 // Train Acc: 0.0\n",
      "Val Loss: 0.0006462756518377629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 851\n",
      "Train Loss: 0.0006070356151686006 // Train Acc: 0.0\n",
      "Val Loss: 0.00064493355336947 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 861\n",
      "Train Loss: 0.0006054683631019233 // Train Acc: 0.0\n",
      "Val Loss: 0.0006436204730663786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 871\n",
      "Train Loss: 0.0006039220822329612 // Train Acc: 0.0\n",
      "Val Loss: 0.0006423355316757562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 881\n",
      "Train Loss: 0.0006023964264894134 // Train Acc: 0.0\n",
      "Val Loss: 0.0006410766291489315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 891\n",
      "Train Loss: 0.0006008908859929879 // Train Acc: 0.0\n",
      "Val Loss: 0.0006398412876768303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 901\n",
      "Train Loss: 0.0005994051073137886 // Train Acc: 0.0\n",
      "Val Loss: 0.0006386300752109486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 911\n",
      "Train Loss: 0.000597939536071054 // Train Acc: 0.0\n",
      "Val Loss: 0.0006374426357374609 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 921\n",
      "Train Loss: 0.0005964937645356304 // Train Acc: 0.0\n",
      "Val Loss: 0.0006362770212304976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 931\n",
      "Train Loss: 0.0005950682635630023 // Train Acc: 0.0\n",
      "Val Loss: 0.0006351337529575471 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 941\n",
      "Train Loss: 0.0005936632020206963 // Train Acc: 0.0\n",
      "Val Loss: 0.0006340123115615678 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 951\n",
      "Train Loss: 0.0005922795510806762 // Train Acc: 0.0\n",
      "Val Loss: 0.000632912649586244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 961\n",
      "Train Loss: 0.0005909170974237024 // Train Acc: 0.0\n",
      "Val Loss: 0.0006318346802281819 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 971\n",
      "Train Loss: 0.0005895762525342613 // Train Acc: 0.0\n",
      "Val Loss: 0.000630778613108127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 981\n",
      "Train Loss: 0.0005882570868452553 // Train Acc: 0.0\n",
      "Val Loss: 0.0006297448225923899 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 991\n",
      "Train Loss: 0.0005869591901477865 // Train Acc: 0.0\n",
      "Val Loss: 0.0006287315738528535 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:34:03<00:00, 1128.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (22797, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2841, 12, 5)\n",
      "Shape of the data after splitting into sequences: (2840, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.04345953096069711 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.013449843337215233 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 10\n",
      "Epoch: 11\n",
      "Train Loss: 0.004526254498631856 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.003924506528726831 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 16\n",
      "INFO: Validation loss did not improve in epoch 17\n",
      "INFO: Validation loss did not improve in epoch 18\n",
      "INFO: Validation loss did not improve in epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:41<02:47, 41.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 20\n",
      "Early stopping after 20 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.09104065447270654 // Train Acc: 0.004128137384412154\n",
      "Val Loss: 0.018622389510587863 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.004607215242100821 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.003920567889038599 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 13\n",
      "INFO: Validation loss did not improve in epoch 14\n",
      "INFO: Validation loss did not improve in epoch 15\n",
      "INFO: Validation loss did not improve in epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [01:17<01:54, 38.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 17\n",
      "Early stopping after 17 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.17123325500647135 // Train Acc: 0.004128137384412154\n",
      "Val Loss: 0.04152775349702393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 10\n",
      "INFO: Validation loss did not improve in epoch 11\n",
      "Epoch: 11\n",
      "Train Loss: 0.004866381179045546 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.004540638675065606 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 12\n",
      "Epoch: 21\n",
      "Train Loss: 0.004712802955138441 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.0043236214126031215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.004612585893098439 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.004082919144545148 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.004553925897299418 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.00404018532106093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.004485097874012558 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.00395107673209035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.004408192719714107 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.0038142679648322125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00434164818975065 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.003705724251712934 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.004281058255844483 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.0036362632634929183 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.004219931549603095 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.00357713971598979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.004163248624805333 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.0035164689096122497 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.004110146584817395 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.003470207881077789 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00406014426690025 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.003439081703270838 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.004013403952167101 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.003409409344528561 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.003968940748944768 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.003374005560115702 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.003925186815470779 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.0033363519779934935 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00388092947211867 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.003303092076728288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.003836067285138192 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.003280596844551122 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 181\n",
      "Epoch: 181\n",
      "Train Loss: 0.0037917493397103623 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.003273844597939653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 182\n",
      "INFO: Validation loss did not improve in epoch 183\n",
      "INFO: Validation loss did not improve in epoch 184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [07:46<06:36, 198.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 185\n",
      "Early stopping after 185 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.08867358520369893 // Train Acc: 0.004128137384412154\n",
      "Val Loss: 0.014409252479044574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.004807476118229123 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.0038045862460053745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.004537847659152611 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.0036483977076314975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 31\n",
      "Epoch: 31\n",
      "Train Loss: 0.004350385236084957 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.0035824072597943394 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 32\n",
      "INFO: Validation loss did not improve in epoch 33\n",
      "INFO: Validation loss did not improve in epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [08:59<02:29, 149.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 35\n",
      "Early stopping after 35 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.03675316981239247 // Train Acc: 0.002064068692206077\n",
      "Val Loss: 0.012968136413608876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 7\n",
      "INFO: Validation loss did not improve in epoch 8\n",
      "INFO: Validation loss did not improve in epoch 9\n",
      "INFO: Validation loss did not improve in epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [09:23<00:00, 112.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 11\n",
      "Early stopping after 11 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictive performance\n",
    "predictive_results = predictive_evaluation(data_real_numpy, data_syn_numpy, hyperparameters, include_baseline=True, verbose=True)\n",
    "\n",
    "# save results\n",
    "bidirectionality = \"bi\" if hyperparameters[\"bidirectional\"] else 'no_bi'\n",
    "predictive_results.to_csv(DATA_FOLDER / f\"results_{syn_data_type}_{hyperparameters['num_epochs']}_{hyperparameters['num_evaluation_runs']}_{bidirectionality}.csv\", index=False)\n",
    "\n",
    "# split in mse and mae results\n",
    "mse_results = predictive_results.loc[predictive_results['Metric'] == 'MSE']\n",
    "mae_results = predictive_results.loc[predictive_results['Metric'] == 'MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1cf78d82420>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIgAAAK9CAYAAABPS1fnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB3DUlEQVR4nOzdaXgUVf728buzhyTdIQgJkRBW2UVBhQiMDiJhdcMFZIfBkU0RBWT+iIgIyLjiAmNEUNkUAUdRQUAWkbDKDqIoJigEEEiaAFm7nhc86bFN0AQ6qST1/VxXX9qnTlX9qtM09J1zTtkMwzAEAAAAAAAAy/IxuwAAAAAAAACYi4AIAAAAAADA4giIAAAAAAAALI6ACAAAAAAAwOIIiAAAAAAAACyOgAgAAAAAAMDiCIgAAAAAAAAsjoAIAAAAAADA4giIAAAAAAAALI6ACACAMmTChAmqUaPGFR2jX79+V3yM8spms2nChAlml/Gnfv75Z9lsNq1du9bsUq7Y2rVrL/ta8l6HOXPmeL0uAACsiIAIAABJc+bMkc1mk81m04YNG/JtNwxDMTExstls6tKlS4HHSE1NVVBQkGw2mw4cOFBgn379+rnP88dHUFCQ167n6NGjmjBhgnbu3Om1Y6Jg8+fP1yuvvGJ2GQW61Putfv36l7XfHx/9+vUrmQspZfKCrbyHr6+vqlSponvvvfeSf/YBACjt/MwuAACA0iQoKEjz589X69atPdrXrVunX375RYGBgZfcd9GiRbLZbIqKitK8efM0adKkAvsFBgbq7bffztfu6+t7ZcX/ztGjR/XMM8+oRo0auu666zy2JSQkyOVyee1cVjd//nzt3btXI0aMMLuUAhX0fnM4HH+6zz//+U+1a9fO/fzw4cMaP368HnroIbVp08bdXrt27Suq7W9/+5suXLiggICAIu8bGxurCxcuyN/f/4pquBKPPPKIbrzxRmVnZ2v37t2aOXOm1q5dq7179yoqKsq0ugAAuBwERAAA/E6nTp20aNEiTZ8+XX5+//trcv78+WrevLl+++23S+47d+5cderUSbGxsZo/f/4lAyI/Pz/16tXL67UXlplfqFHyLuf9FhcXp7i4OPfzbdu2afz48YqLi/vTY507d04hISGFPo+Pj89lj5zz9qi7y9GmTRvde++97uf16tXT4MGD9d5772n06NEmVgYAQNExxQwAgN/p0aOHTp06pZUrV7rbsrKy9NFHH+nBBx+85H7Jycn6+uuv1b17d3Xv3l2HDx/Wxo0bS6LkfNauXasbb7xRktS/f3/3NJi8tVr+uAZR3louL7zwgt544w3VqlVLFSpUUPv27XXkyBEZhqFnn31W1apVU3BwsO68806dPn0633m/+OILtWnTRiEhIQoLC1Pnzp21b9++fP0WLVqkhg0bKigoSI0bN9bSpUsLXBfphRde0M0336xKlSopODhYzZs310cffZTveDabTcOGDdPHH3+sxo0bKzAwUI0aNdLy5csv/0X8/86ePasRI0aoRo0aCgwMVJUqVXT77bfr22+/lSTdeuut+uyzz5SUlOR+nfOuI28a0ocffqhnnnlGV199tcLCwnTvvfcqLS1NmZmZGjFihKpUqaLQ0FD1799fmZmZV1xzQXJzc+V0Or16zLxpmevWrdOQIUNUpUoVVatWTZKUlJSkIUOGqF69egoODlalSpV033336eeff/Y4RkFrEN16661q3Lix9u/fr7///e+qUKGCrr76ak2bNs1j34LWIOrXr59CQ0P166+/6q677lJoaKgqV66sJ554Qrm5uR77nzp1Sr1795bdbld4eLj69u2rXbt2XdG6Rnmjq3788UePmgpa82vChAmy2WwebYV9L//V+xIAgMvBCCIAAH6nRo0aiouL04IFC9SxY0dJF4OPtLQ0de/eXdOnTy9wvwULFigkJERdunRRcHCwateurXnz5unmm28usH9BI5ECAgJkt9uv+BoaNGigiRMn5psSdKla8sybN09ZWVkaPny4Tp8+rWnTpun+++9X27ZttXbtWo0ZM0aHDh3Sa6+9pieeeELvvPOOe9/3339fffv2VXx8vJ5//nmdP39eM2bMUOvWrbVjxw73F+TPPvtMDzzwgJo0aaIpU6bozJkzGjhwoK6++up89bz66qu644471LNnT2VlZWnhwoW67777tGzZMnXu3Nmj74YNG7RkyRINGTJEYWFhmj59urp166bk5GRVqlTpsl/Lhx9+WB999JGGDRumhg0b6tSpU9qwYYMOHDigZs2a6f/+7/+UlpamX375RS+//LIkKTQ01OMYU6ZMUXBwsJ588kn36+fv7y8fHx+dOXNGEyZM0KZNmzRnzhzVrFlT48ePv+x6C3L+/HnZ7XadP39eFStWVI8ePfT888/nq/NyDRkyRJUrV9b48eN17tw5SdLWrVu1ceNGde/eXdWqVdPPP/+sGTNm6NZbb9X+/ftVoUKFPz3mmTNn1KFDB91zzz26//779dFHH2nMmDFq0qSJ+8/lpeTm5io+Pl4tWrTQCy+8oFWrVunFF19U7dq1NXjwYEmSy+VS165dtWXLFg0ePFj169fXf//7X/Xt2/eKXou8AKxixYqXfYzCvJf/6n0JAMBlMQAAgDF79mxDkrF161bj9ddfN8LCwozz588bhmEY9913n/H3v//dMAzDiI2NNTp37pxv/yZNmhg9e/Z0P//Xv/5lXHXVVUZ2drZHv759+xqSCnzEx8f/ZZ1PP/20ERsb+5f9tm7dakgyZs+enW9b3759PY5x+PBhQ5JRuXJlIzU11d0+duxYQ5LRtGlTj+vo0aOHERAQYGRkZBiGYRhnz541wsPDjUGDBnmcJyUlxXA4HB7tTZo0MapVq2acPXvW3bZ27VpDUr7rynv982RlZRmNGzc22rZt69EuyQgICDAOHTrkbtu1a5chyXjttdcu8QoVTJLx9NNPu587HA5j6NChf7pP586dC/yZrFmzxpBkNG7c2MjKynK39+jRw7DZbEbHjh09+sfFxRXqZ5v381qzZs1f9n3yySeNMWPGGB988IGxYMEC9/uvVatW+d6bf6ag91Pen5nWrVsbOTk5Hv3/+LMzDMNITEw0JBnvvfeeuy3vNfr9tdxyyy35+mVmZhpRUVFGt27d3G15r8Pva8q7vokTJ3qc+/rrrzeaN2/ufr548WJDkvHKK6+423Jzc422bdte8s/N7+XV/c477xgnT540jh49aixfvtyoU6eOYbPZjC1btnjUVNDP9emnnzb++E/xwr6XC/O+BACgqJhiBgDAH9x///26cOGCli1bprNnz2rZsmV/Or1s9+7d2rNnj3r06OFu69Gjh3777TetWLEiX/+goCCtXLky32Pq1KnFcj2Fdd9993ksXtyiRQtJUq9evTzWY2rRooWysrL066+/SpJWrlyp1NRU9zXnPXx9fdWiRQutWbNG0sWFs/fs2aM+ffp4jF655ZZb1KRJk3z1BAcHu///zJkzSktLU5s2bQqcRtOuXTuPBZOvvfZa2e12/fTTT5f7ckiSwsPDtXnzZh09evSyj9GnTx+PdZ9atGghwzA0YMAAj34tWrTQkSNHlJOTc9nn+qMpU6Zo6tSpuv/++9W9e3fNmTNHzz33nL755psCp+tdjkGDBuVbYP33P7vs7GydOnVKderUUXh4eKGmQYWGhnqsdRQQEKCbbrqp0D/Phx9+2ON5mzZtPPZdvny5/P39NWjQIHebj4+Phg4dWqjj5xkwYIAqV66s6OhodejQQWlpaXr//ffdUzwvR2Hey954XwIA8EdMMQMA4A8qV66sdu3aaf78+Tp//rxyc3M9FqL9o7lz5yokJES1atXSoUOHJF0MgWrUqKF58+blmw7l6+vrcYeo0qJ69eoez/PCopiYmALbz5w5I0n64YcfJElt27Yt8Lh50+aSkpIkSXXq1MnXp06dOvmCg2XLlmnSpEnauXOnx9o8f1y3paDapYvTfPJqvFzTpk1T3759FRMTo+bNm6tTp07q06ePatWqVehjFOV1dblcSktLu6JpcX/lscce01NPPaVVq1ape/fuV3y8mjVr5mu7cOGCpkyZotmzZ+vXX3+VYRjubWlpaX95zGrVquX7OVesWFG7d+/+y32DgoJUuXLlfPv+/r2QlJSkqlWr5pvqVtB788+MHz9ebdq0UXp6upYuXaqFCxfKx+fKfv9amPeyN96XAAD8EQERAAAFePDBBzVo0CClpKSoY8eOCg8PL7CfYRhasGCBzp07p4YNG+bbfuLECaWnp3ttvZfi9MdRIH/Vnvel3+VySbq4DlFBt/b+/eijwvr66691xx136G9/+5vefPNNVa1aVf7+/po9e7bmz59f5Bov1/333682bdpo6dKl+vLLL/Xvf/9bzz//vJYsWfKXa+H8VW3FVfNfyVs0uqCFxi/3eH80fPhwzZ49WyNGjFBcXJwcDodsNpu6d+/ufr/8mSt5bS61b3Fo0qSJO+y96667dP78eQ0aNEitW7d2B4AFBZqS8i2anacw1+6N9yUAAH9EQAQAQAHuvvtu/fOf/9SmTZv0wQcfXLLfunXr9Msvv2jixIlq0KCBx7YzZ87ooYce0scff1zit7W/1JfS4pA3HaZKlSp/OjIqNjZWktyjrH7vj22LFy9WUFCQVqxYocDAQHf77NmzvVFykVStWlVDhgzRkCFDdOLECTVr1kzPPfec+4t4Sb7W3nD27Fn99ttv+UbZeNNHH32kvn376sUXX3S3ZWRkKDU1tdjOWRSxsbFas2aNzp8/7zGKqKD3ZlFMnTpVS5cu1XPPPaeZM2dKujj6p6DrzhtRd7n+6n0JAEBRsQYRAAAFCA0N1YwZMzRhwgR17dr1kv3yppeNGjVK9957r8dj0KBBqlu3rubNm1eClV8UEhIiSSXyhTw+Pl52u12TJ09WdnZ2vu0nT56UJEVHR6tx48Z67733lJ6e7t6+bt067dmzx2MfX19f2Ww2j1EWP//8sz7++OPiuYgC5Obm5psOVaVKFUVHR3tMeQsJCSnUtKmSlpGRobNnz+Zrf/bZZ2UYhjp06FBs5/b19c032ue111675KiZkhYfH6/s7GwlJCS421wul954440rOm7t2rXVrVs3zZkzRykpKe62tLQ0j+lxx44d09KlSy/rHIV9XwIAUFSMIAIA4BL+6pbXmZmZWrx4sW6//XYFBQUV2OeOO+7Qq6++qhMnTqhKlSqSpJycHM2dO7fA/nfffbc73LkStWvXVnh4uGbOnKmwsDCFhISoRYsWBa4Xc6XsdrtmzJih3r17q1mzZurevbsqV66s5ORkffbZZ2rVqpVef/11SdLkyZN15513qlWrVurfv7/OnDmj119/XY0bN/YIjTp37qyXXnpJHTp00IMPPqgTJ07ojTfeUJ06dQq1Do03nD17VtWqVdO9996rpk2bKjQ0VKtWrdLWrVs9RsY0b95cH3zwgUaOHKkbb7xRoaGhfxoqlpSUlBRdf/316tGjh+rXry9JWrFihT7//HN16NBBd955Z7Gdu0uXLnr//fflcDjUsGFDJSYmatWqVcW6tlJR3HXXXbrpppv0+OOP69ChQ6pfv74++eQT97S7KxkVNmrUKH344Yd65ZVXNHXqVHXv3l1jxozR3XffrUceeUTnz5/XjBkzdM011xRqwe4/Kuz7EgCAoiIgAgDgMn322WdKTU390zCga9euevHFF7Vw4UI98sgjki4GS7179y6w/+HDh70SEPn7++vdd9/V2LFj9fDDDysnJ0ezZ88uloBIurhmU3R0tKZOnap///vfyszM1NVXX602bdqof//+7n5du3bVggULNGHCBD355JOqW7eu5syZo3fffVf79u1z92vbtq1mzZqlqVOnasSIEapZs6aef/55/fzzzyUWEFWoUEFDhgzRl19+qSVLlsjlcqlOnTp68803NXjwYHe/IUOGaOfOnZo9e7ZefvllxcbGloqAKDw8XF26dNHKlSv17rvvKjc3V3Xq1NHkyZP1xBNPXPFiyn/m1Vdfla+vr+bNm6eMjAy1atVKq1atUnx8fLGdsyh8fX312Wef6dFHH9W7774rHx8f3X333Xr66afVqlWrSwa+hXHDDTfo1ltv1YwZMzR27FhVqlRJS5cu1ciRIzV69GjVrFlTU6ZM0Q8//HBZAVFh35cAABSVzSjulRABAIDXTJgwQXPmzNHPP/9sdiledd1116ly5cpauXKl2aWUej///LNq1qypNWvW6NZbbzW7nHLl448/1t13360NGzaoVatWZpcDAECJYg0iAABQYrKzs5WTk+PRtnbtWu3atYuwAyXqwoULHs9zc3P12muvyW63q1mzZiZVBQCAeZhiBgAASsyvv/6qdu3aqVevXoqOjtZ3332nmTNnKioqSg8//HCxnDM3N9e9UPalhIaGKjQ0tFjOj9Jp+PDhunDhguLi4pSZmaklS5Zo48aNmjx5soKDg80uDwCAEkdABAAASkzFihXVvHlzvf322zp58qRCQkLUuXNnTZ06tdgWMD5y5Mhfrr309NNPa8KECcVyfpRObdu21Ysvvqhly5YpIyNDderU0WuvvaZhw4aZXRoAAKZgDSIAAFCuZWRkaMOGDX/ap1atWqpVq1YJVQQAAFD6EBABAAAAAABYHItUAwAAAAAAWBxrEElyuVw6evSowsLCZLPZzC4HAAAAAADAKwzD0NmzZxUdHS0fn0uPEyIgknT06FHFxMSYXQYAAAAAAECxOHLkiKpVq3bJ7QREksLCwiRdfLHsdrvJ1QAAAAAAAHiH0+lUTEyMO/u4FAIiyT2tzG63ExABAAAAAIBy56+W1GGRagAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLYw0iAAAAAABgOsMwlJOTo9zcXLNLKVN8fX3l5+f3l2sM/RUCIgAAAAAAYKqsrCwdO3ZM58+fN7uUMqlChQqqWrWqAgICLvsYBEQAAAAAAMA0LpdLhw8flq+vr6KjoxUQEHDFo2GswjAMZWVl6eTJkzp8+LDq1q0rH5/LW02IgAgAAAAAAJgmKytLLpdLMTExqlChgtnllDnBwcHy9/dXUlKSsrKyFBQUdFnHYZFqAAAAAABgussd+QLvvHa8+gAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWZ2pAdPbsWY0YMUKxsbEKDg7WzTffrK1bt7q3G4ah8ePHq2rVqgoODla7du30ww8/eBzj9OnT6tmzp+x2u8LDwzVw4EClp6eX9KUAAAAAAACL6devn2w2mx5++OF824YOHSqbzaZ+/fp5tCcmJsrX11edO3fOt8/PP/8sm81W4GPTpk3FdRmSTA6I/vGPf2jlypV6//33tWfPHrVv317t2rXTr7/+KkmaNm2apk+frpkzZ2rz5s0KCQlRfHy8MjIy3Mfo2bOn9u3bp5UrV2rZsmVav369HnroIbMuCQAAAAAAmGTnzp2aMmWKHnnkEU2ZMkU7d+4s9nPGxMRo4cKFunDhgrstIyND8+fPV/Xq1fP1nzVrloYPH67169fr6NGjBR5z1apVOnbsmMejefPmxXYNkokB0YULF7R48WJNmzZNf/vb31SnTh1NmDBBderU0YwZM2QYhl555RWNGzdOd955p6699lq99957Onr0qD7++GNJ0oEDB7R8+XK9/fbbatGihVq3bq3XXntNCxcuvOSLDAAAAAAAyp+dO3dq5syZ7tu9JyUlaebMmcUeEjVr1kwxMTFasmSJu23JkiWqXr26rr/+eo++6enp+uCDDzR48GB17txZc+bMKfCYlSpVUlRUlMfD39+/OC/DvIAoJydHubm5CgoK8mgPDg7Whg0bdPjwYaWkpKhdu3bubQ6HQy1atFBiYqKki8OywsPDdcMNN7j7tGvXTj4+Ptq8efMlz52ZmSmn0+nxAAAAAAAAZdcXX3xRYPvy5cuL/dwDBgzQ7Nmz3c/feecd9e/fP1+/Dz/8UPXr11e9evXUq1cvvfPOOzIMo9jrKwzTAqKwsDDFxcXp2Wef1dGjR5Wbm6u5c+cqMTFRx44dU0pKiiQpMjLSY7/IyEj3tpSUFFWpUsVju5+fnyIiItx9CjJlyhQ5HA73IyYmxstXBwAAAAAAStKxY8cKbC+JGUa9evXShg0blJSUpKSkJH3zzTfq1atXvn6zZs1yt3fo0EFpaWlat25dvn4333yzQkNDPR7FzdQ1iN5//30ZhqGrr75agYGBmj59unr06CEfn+Ita+zYsUpLS3M/jhw5UqznAwAAAAAAxatq1aoFtkdHRxf7uStXruyeMjZ79mx17txZV111lUefgwcPasuWLerRo4ekiwNcHnjgAc2aNSvf8T744APt3LnT41Hc/Ir9DH+idu3aWrdunc6dOyen06mqVavqgQceUK1atRQVFSVJOn78uMcP+fjx47ruuuskSVFRUTpx4oTHMXNycnT69Gn3/gUJDAxUYGCg9y8IAAAAAACYomPHjpo5c2aB7SVhwIABGjZsmCTpjTfeyLd91qxZysnJ8QisDMNQYGCgXn/9dTkcDnd7TEyM6tSpU/xF/46pI4jyhISEqGrVqjpz5oxWrFihO++8UzVr1lRUVJRWr17t7ud0OrV582bFxcVJkuLi4pSamqrt27e7+3z11VdyuVxq0aJFiV8HAAAAAAAwx3XXXaeHH35YNWrUUEBAgGrUqKHBgweradOmJXL+Dh06KCsrS9nZ2YqPj/fYlpOTo/fee08vvviix6igXbt2KTo6WgsWLCiRGv+MqSOIVqxYIcMwVK9ePR06dEijRo1S/fr11b9/f9lsNo0YMUKTJk1S3bp1VbNmTT311FOKjo7WXXfdJUlq0KCBOnTooEGDBmnmzJnKzs7WsGHD1L179xIZQgYAAAAAAEqP6667zj3rqKT5+vrqwIED7v//vWXLlunMmTMaOHCgx0ghSerWrZtmzZqlhx9+2N126tSpfGsrh4eH57vRlzeZOoIoLS1NQ4cOVf369dWnTx+1bt1aK1ascN+6bfTo0Ro+fLgeeugh3XjjjUpPT9fy5cs9XpB58+apfv36uu2229SpUye1bt1ab731llmXBAAAAAAALMput8tut+drnzVrltq1a5cvHJIuBkTbtm3T7t273W3t2rVT1apVPR4ff/xxcZYum1Fa7qdmIqfTKYfDobS0tAJ/kAAAAEBBfvrpJ61fv16pqamqXbu2br31VoWFhZldFgCUKRkZGTp8+LBq1qxZrCNkyrM/ew0Lm3mYOsUMAAAAKKu2bdumWbNmKe/3rd999502bdqkJ598kpAIAFDmlIpFqgEAAICyxDAMLVmyRH8cjH/q1CmPm6wAAFBWMIIIAAAAZU5GRoaSkpJMO/+ZM2eUnJxc4LZNmzapQYMGJVxR2RAbG8v0EQAopQiIAAAAUOYkJSVp0KBBpp3fMAylpaXlG0EkSQcOHNDGjRtNqKr0S0hIUL169cwuAwBQAAIiAAAAlDmxsbFKSEgwtYb//ve/HnecydOzZ0/VqlXrso6ZlJSkSZMmady4cYqNjb3SEkud8nhNALyHe2hdPm+8dgREAAAAKHOCgoJMH4ny6KOPat68edq2bZtcLpdCQ0N15513qk2bNld87NjYWNOvDwBKir+/vyTp/PnzCg4ONrmasun8+fOS/vdaXg4CIgAAAOAyBAYGasCAAbrvvvvkdDoVGRkpPz/+eQ0AReXr66vw8HCdOHFCklShQgXZbDaTqyobDMPQ+fPndeLECYWHh8vX1/eyj8XfYAAAAMAVCAsL47b2AHCFoqKiJMkdEqFowsPD3a/h5SIgAgAAAAAAprLZbKpataqqVKmi7Oxss8spU/z9/a9o5FAeAiIAAAAAAFAq+Pr6eiXsQNH5mF0AAAAAAAAAzEVABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFudndgEAAACAVWzevFnffPONzp8/rwYNGqh9+/YKCwszuywAAAiIAAAAgJLwySef6PPPP3c//+WXX7Rr1y49+eSTqlChgomVAQDAFDMAAACg2KWnp2vlypX52k+cOKGNGzeaUBEAAJ4YQQQAAFCOHT9+XKmpqWaXYXmHDx/WmTNnCty2detWxcTESJKSkpI8/ovSLzw8XJGRkWaXAQBXzGYYhmF2EWZzOp1yOBxKS0uT3W43uxwAAACvOH78uHr26qmszCyzS7G83NxcOZ3OArcFBQUpODi4hCuCtwQEBmje3HmERABKrcJmHowgAgAAKKdSU1OVlZkl100uGXbL/07QdH57/JR9KtujzeZjk9+NfsoNzjWpKlwJm9OmrC1ZSk1NJSACUOYREAEAAJRzht2QKppdBYLjgmXbZ1P20WwZLkO+dl8FNQqSTyWWBS2rDBG8Aig/CIgAAACAEmDztyn4umAFNQmSkWvIJ4BgCABQehAQAQAAACXI5muTzddmdhkAAHjg1xYAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcX5mFwAAAACUJ9nHspX9a7bkkvyi/ORfzV82H5vZZQEA8KcIiAAAAAAvydiXoczDme7n2SeylZOSo+Abg2WzERIBAEovppgBAAAAXuA651LW4ax87dknspX7W64JFQEAUHiMIAIAACjvnGYXYA05KTkyso2CtyXnyM+Pf3qXO/zZAlCO8LcUAABAOee7xdfsEizBL9tPtvSCp5H5pvvK9yd+DgCA0ouACAAAoJzLvSlXsptdRflnM2yybbbJleHybPe1ybeFr3IDmGZW7jgJYAGUHwREAAAA5Z1dUkWziyj/bLKpQpsKurDzgnJTL4ZBviG+CmoaJJ8Ilv4EAJRuBEQAAACAl/iG+iq0dahc51wyXIZ8Qn24exkAoEwgIAIAAAC8zCeEEUMAgLKFv7kAAAAAAAAsjoAIAAAAAADA4giIAAAAAAAALI6ACAAAAAAAwOIIiAAAAAAAACyOgAgAAAAAAMDiCIgAAAAAAAAsjoAIAAAAAADA4vzMLgAAAADAn8tNz1X2kWwZWYZ8K/nKP9pfNh+b2WUBAMoRAiIAAACgFMs+lq0LOy7IcBkXG45I2UeyVeGmCrL5EhIBALyDKWYAAABAKWW4DGXszfhfOPT/5ZzKUfYv2SZVBQAojxhBBAAAUM7ZnDYZMv66I0odl9MlV7qrwG05yTkKsAeUcEX4PZuTEVwAyg9TA6Lc3FxNmDBBc+fOVUpKiqKjo9WvXz+NGzdONtvFD1vDMPT0008rISFBqampatWqlWbMmKG6deu6j3P69GkNHz5cn376qXx8fNStWze9+uqrCg0NNevSAAAATBceHq6AwABlbckyuxRcrtxLhxA+aT7yTfEt4YLwRwGBAQoPDze7DAC4YqYGRM8//7xmzJihd999V40aNdK2bdvUv39/ORwOPfLII5KkadOmafr06Xr33XdVs2ZNPfXUU4qPj9f+/fsVFBQkSerZs6eOHTumlStXKjs7W/3799dDDz2k+fPnm3l5AAAApoqMjNS8ufOUmppqdikopKSkJE2aNEnjxo1TbGysJGn27Nn65Zdf8vXt06ePuw/MEx4ersjISLPLAIArZjMMw7Txxl26dFFkZKRmzZrlbuvWrZuCg4M1d+5cGYah6OhoPf7443riiSckSWlpaYqMjNScOXPUvXt3HThwQA0bNtTWrVt1ww03SJKWL1+uTp066ZdfflF0dHS+82ZmZiozM9P93Ol0KiYmRmlpabLb7cV81QAAAEDBDh48qEGDBikhIUH16tWTdHG0/MyZM5WcnCxJCggI0B133KF27dqZWSoAoIxwOp1yOBx/mXmYukj1zTffrNWrV+v777+XJO3atUsbNmxQx44dJUmHDx9WSkqKx19+DodDLVq0UGJioiQpMTFR4eHh7nBIktq1aycfHx9t3ry5wPNOmTJFDofD/YiJiSmuSwQAAACuSEREhP71r39p7NixGjZsmKZOnUo4BADwOlOnmD355JNyOp2qX7++fH19lZubq+eee049e/aUJKWkpEhSviGbkZGR7m0pKSmqUqWKx3Y/Pz9FRES4+/zR2LFjNXLkSPfzvBFEAAAAQGnFdDIAQHEyNSD68MMPNW/ePM2fP1+NGjXSzp07NWLECEVHR6tv377Fdt7AwEAFBgYW2/EBAAAAAADKElMDolGjRunJJ59U9+7dJUlNmjRRUlKSpkyZor59+yoqKkqSdPz4cVWtWtW93/Hjx3XddddJkqKionTixAmP4+bk5Oj06dPu/QEAAAAAAHBppq5BdP78efn4eJbg6+srl8slSapZs6aioqK0evVq93an06nNmzcrLi5OkhQXF6fU1FRt377d3eerr76Sy+VSixYtSuAqAAAAAAAAyjZTRxB17dpVzz33nKpXr65GjRppx44deumllzRgwABJks1m04gRIzRp0iTVrVvXfZv76Oho3XXXXZKkBg0aqEOHDho0aJBmzpyp7OxsDRs2TN27dy/wDmYAAAAAAADwZGpA9Nprr+mpp57SkCFDdOLECUVHR+uf//ynxo8f7+4zevRonTt3Tg899JBSU1PVunVrLV++XEFBQe4+8+bN07Bhw3TbbbfJx8dH3bp10/Tp0824JAAAAAAAgDLHZhiGYXYRZnM6nXI4HEpLS5Pdbje7HAAAAFjUwYMHNWjQICUkJKhevXpmlwMAKAcKm3mYugYRAAAAAAAAzEdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcX5mFwAAAACUBZmZmVq7dq3279+vwMBAtWrVSk2bNjW7LAAAvIKACAAAAPgLWVlZeumll5SUlORu2717t7p27arOnTubWBkAAN7BFDMAAADgL2zdutUjHMrzxRdfKD093YSKAADwLkYQAQAAoMzJyMgoMLApLhs3brxkELRu3TrVqVPHK+fJu6aSvLaSFBsbq6CgILPLAAAUwGYYhmF2EWZzOp1yOBxKS0uT3W43uxwAAAD8hYMHD2rQoEEldr4LFy4oIyOjwG12u12+vr4lVktZlpCQoHr16pldBgBYSmEzD0YQAQAAoMyJjY1VQkJCiZ3vzJkzmjlzpnJycjzaq1Wrpv79+5dYHWVdbGys2SUAAC6BgAgAAABlTlBQUImPRHE4HFqwYIFOnDghSWrcuLH69OnDCHQAQLnAFDMxxQwAAACFYxiGTp48qaCgIP7dCAAoE5hiBgAAAHiZzWZTlSpVzC4DAACv4zb3AAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZnakBUo0YN2Wy2fI+hQ4dKkjIyMjR06FBVqlRJoaGh6tatm44fP+5xjOTkZHXu3FkVKlRQlSpVNGrUKOXk5JhxOQAAAAAAAGWSqQHR1q1bdezYMfdj5cqVkqT77rtPkvTYY4/p008/1aJFi7Ru3TodPXpU99xzj3v/3Nxcde7cWVlZWdq4caPeffddzZkzR+PHjzflegAAAAAAAMoim2EYhtlF5BkxYoSWLVumH374QU6nU5UrV9b8+fN17733SpK+++47NWjQQImJiWrZsqW++OILdenSRUePHlVkZKQkaebMmRozZoxOnjypgICAQp3X6XTK4XAoLS1Ndru92K4PAAAAAACgJBU28yg1axBlZWVp7ty5GjBggGw2m7Zv367s7Gy1a9fO3ad+/fqqXr26EhMTJUmJiYlq0qSJOxySpPj4eDmdTu3bt++S58rMzJTT6fR4AAAAAAAAWFWpCYg+/vhjpaamql+/fpKklJQUBQQEKDw83KNfZGSkUlJS3H1+Hw7lbc/bdilTpkyRw+FwP2JiYrx3IQAAAAAAAGVMqQmIZs2apY4dOyo6OrrYzzV27FilpaW5H0eOHCn2cwIAAAAAAJRWfmYXIElJSUlatWqVlixZ4m6LiopSVlaWUlNTPUYRHT9+XFFRUe4+W7Zs8ThW3l3O8voUJDAwUIGBgV68AgAAAAAAgLKrVIwgmj17tqpUqaLOnTu725o3by5/f3+tXr3a3Xbw4EElJycrLi5OkhQXF6c9e/boxIkT7j4rV66U3W5Xw4YNS+4CAAAAAAAAyjDTRxC5XC7Nnj1bffv2lZ/f/8pxOBwaOHCgRo4cqYiICNntdg0fPlxxcXFq2bKlJKl9+/Zq2LChevfurWnTpiklJUXjxo3T0KFDGSEEAAAAAABQSKYHRKtWrVJycrIGDBiQb9vLL78sHx8fdevWTZmZmYqPj9ebb77p3u7r66tly5Zp8ODBiouLU0hIiPr27auJEyeW5CUAAAAAAACUaTbDMAyzizCb0+mUw+FQWlqa7Ha72eUAAAAAAAB4RWEzj1KxBhEAAAAAAADMQ0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcQREAAAAAAAAFkdABAAAAAAAYHEERAAAAAAAABZHQAQAAAAAAGBxBEQAAAAAAAAWR0AEAAAAAABgcX5mFwAAQHmWkpKiHTt2yM/PT82bN1dERITZJQEAAAD5EBABAFBMPv/8c33yySfu50uXLlXv3r0VFxdnYlUAAABAfkWaYpabm6v169crNTW1mMoBAKB8OHbsmEc4JEkul0vz5s1Tenq6SVUBAAAABSvSCCJfX1+1b99eBw4cUHh4eDGVBACwioyMDCUlJZldRrH4+uuvLxkEff7552ratGkJV+RdsbGxCgoKMrsMAAAAeEmRp5g1btxYP/30k2rWrFkc9QAALCQpKUmDBg0yu4xikZGRoQsXLhS47dChQwoICCjhirwrISFB9erVM7sMAAAAeInNMAyjKDssX75cY8eO1bPPPqvmzZsrJCTEY7vdbvdqgSXB6XTK4XAoLS2tTNYPAGVVeR5BtHv3bk2cOFHXXHONgoOD3e3+/v4aMWJEmR99wwgiAACAsqGwmUeRRxB16tRJknTHHXfIZrO52w3DkM1mU25u7mWUCwCwoqCgoHI9CqVChQoKDQ11B0QBAQEaOHBgmZ9eBgAAgPKnyAHRmjVriqMOAADKncDAQD3yyCPKyMiQj4+PmjZtqgoVKphdFgAAAJBPkQOiW265pTjqAACgXAoNDVXz5s3NLgMAAAD4U0UOiCQpNTVVs2bN0oEDByRJjRo10oABA+RwOLxaHAAAAAAAAIqfT1F32LZtm2rXrq2XX35Zp0+f1unTp/XSSy+pdu3a+vbbb4ujRgAAAAAAABSjIo8geuyxx3THHXcoISFBfn4Xd8/JydE//vEPjRgxQuvXr/d6kQAAAAAAACg+RQ6Itm3b5hEOSZKfn59Gjx6tG264wavFAQAAAAAAoPgVeYqZ3W5XcnJyvvYjR44oLCzMK0UBAAAAAACg5BQ5IHrggQc0cOBAffDBBzpy5IiOHDmihQsX6h//+Id69OhRHDUCAAAAAACgGBV5itkLL7wgm82mPn36KCcnR5Lk7++vwYMHa+rUqV4vEAAAAAAAAMWrSAFRbm6uNm3apAkTJmjKlCn68ccfJUm1a9dWhQoViqVAAAAAAAAAFK8iBUS+vr5q3769Dhw4oJo1a6pJkybFVRcAAAAAAABKSJHXIGrcuLF++umn4qgFAAAAAAAAJihyQDRp0iQ98cQTWrZsmY4dOyan0+nxAAAAAAAAQNlS5EWqO3XqJEm64447ZLPZ3O2GYchmsyk3N9d71QEAAAAAAKDYFTkgWrNmTXHUAQAAAAAAAJMUKSDKzs7WxIkTNXPmTNWtW7e4agIAAAAAAEAJKtIaRP7+/tq9e3dx1QIAAAAAAAATFHmR6l69emnWrFnFUQsAAAAAAABMUOQ1iHJycvTOO+9o1apVat68uUJCQjy2v/TSS14rDgAAAAAAAMWvyAHR3r171axZM0nS999/77Ht93c1AwAAAAAAQNnAXcwAAAAAAAAsrsgB0Z85ceKEqlSp4s1DAgBgil9//VWJiYm6cOGCGjRooGbNmsnHp8hL9wEAAABlQqH/pVuhQgWdPHnS/bxz5846duyY+/nx48dVtWrVIhfw66+/qlevXqpUqZKCg4PVpEkTbdu2zb3dMAyNHz9eVatWVXBwsNq1a6cffvjB4xinT59Wz549ZbfbFR4eroEDByo9Pb3ItQAAIEmbNm3SpEmTtGrVKn3zzTd6++239eabbyo3N9fs0gAAAIBiUeiAKCMjQ4ZhuJ+vX79eFy5c8Ojz++2FcebMGbVq1Ur+/v764osvtH//fr344ouqWLGiu8+0adM0ffp0zZw5U5s3b1ZISIji4+OVkZHh7tOzZ0/t27dPK1eu1LJly7R+/Xo99NBDRaoFAABJysrK0ocffpjv77S9e/fq22+/NakqAAAAoHh5dYpZURepfv755xUTE6PZs2e722rWrOn+f8Mw9Morr2jcuHG68847JUnvvfeeIiMj9fHHH6t79+46cOCAli9frq1bt+qGG26QJL322mvq1KmTXnjhBUVHR3vhygDAfMePH1dqaqrZZZR7hw8f1okTJwrc9tVXX8lutxfqOElJSR7/RekXHh6uyMhIs8sAAAAwhVcDoqL65JNPFB8fr/vuu0/r1q3T1VdfrSFDhmjQoEGSLv4jPSUlRe3atXPv43A41KJFCyUmJqp79+5KTExUeHi4OxySpHbt2snHx0ebN2/W3Xffne+8mZmZyszMdD93Op3FeJUAcOWOHz+uXj17KjMry+xSyr2cnBydPXu2wG3fffedPvvssyIdb9KkSd4oCyUgMCBAc+fNIyQCAACWVOiAyGazeYwQ+uPzy/HTTz9pxowZGjlypP71r39p69ateuSRRxQQEKC+ffsqJSVFkvL9Qy0yMtK9LSUlJd/C2H5+foqIiHD3+aMpU6bomWeeuaLaAaAkpaamKjMrS4MbnVN0COvgFLcFe3J16kL+adP3NbQpMjTHhIpQ3I6e89WMfRf/rBEQAQAAKyp0QGQYhq655hp3KJSenq7rr7/efUeXoq4/JEkul0s33HCDJk+eLEm6/vrrtXfvXs2cOVN9+/Yt8vEKa+zYsRo5cqT7udPpVExMTLGdDwC8JTokVzXtBETF7aHrfTR/b65Onr/4d5u/r9Sxtq9ujDYk8foDAACg/Cl0QPT7dYK8pWrVqmrYsKFHW4MGDbR48WJJUlRUlKT8d0g7fvy4rrvuOnefP64VkZOTo9OnT7v3/6PAwEAFBgZ66zIAAOXMVRVseuQmPx1xGrqQbSjWYVOg35WNmgUAAABKs0IHRMUxoqdVq1Y6ePCgR9v333+v2NhYSRcXrI6KitLq1avdgZDT6dTmzZs1ePBgSVJcXJxSU1O1fft2NW/eXNLFRURdLpdatGjh9ZoBANYRY7dJIhgCAABA+WfqItWPPfaYbr75Zk2ePFn333+/tmzZorfeektvvfWWpIvrHI0YMUKTJk1S3bp1VbNmTT311FOKjo7WXXfdJeniiKMOHTpo0KBBmjlzprKzszVs2DB1796dO5gBAAAAAAAUgqkB0Y033qilS5dq7NixmjhxomrWrKlXXnlFPXv2dPcZPXq0zp07p4ceekipqalq3bq1li9frqCgIHefefPmadiwYbrtttvk4+Ojbt26afr06WZcEgAAAAAAQJljakAkSV26dFGXLl0uud1ms2nixImaOHHiJftERERo/vz5xVEeAAAAAABAuedjdgEAAAAAAAAw12UHRFlZWTp48KBycnK8WQ8AAAAAAABKWJEDovPnz2vgwIGqUKGCGjVqpOTkZEnS8OHDNXXqVK8XCAAAAAAAgOJV5IBo7Nix2rVrl9auXeuxUHS7du30wQcfeLU4AAAAAAAAFL8iL1L98ccf64MPPlDLli1ls9nc7Y0aNdKPP/7o1eIAAAAAAABQ/Io8gujkyZOqUqVKvvZz5855BEYAAAAAAAAoG4ocEN1www367LPP3M/zQqG3335bcXFx3qsMAAAAAAAAJaLIU8wmT56sjh07av/+/crJydGrr76q/fv3a+PGjVq3bl1x1AgAAAAAAIBiVOQRRK1bt9bOnTuVk5OjJk2a6Msvv1SVKlWUmJio5s2bF0eNAAAAAAAAKEZFHkEkSbVr11ZCQoK3awEAAAAAAIAJijyCyNfXVydOnMjXfurUKfn6+nqlKAAAAAAAAJScIgdEhmEU2J6ZmamAgIArLggAAAAAAAAlq9BTzKZPny7p4l3L3n77bYWGhrq35ebmav369apfv773KwQAAAAAAECxKnRA9PLLL0u6OIJo5syZHtPJAgICVKNGDc2cOdP7FQIAAAAAAKBYFTogOnz4sCTp73//u5YsWaKKFSsWW1EAAAAAAAAoOUW+i9maNWuKow4AAAAAAACYpMgB0YABA/50+zvvvHPZxQAAAAAAAKDkFTkgOnPmjMfz7Oxs7d27V6mpqWrbtq3XCgMAAAAAAEDJKHJAtHTp0nxtLpdLgwcPVu3atb1SFAAAAAAAAEqOj1cO4uOjkSNHuu90BgAAAAAAgLLDKwGRJP3444/Kycnx1uEAAAAAAABQQoo8xWzkyJEezw3D0LFjx/TZZ5+pb9++XisMAAAAAAAAJaPIAdGOHTs8nvv4+Khy5cp68cUX//IOZwAAAAAAACh9ihwQrVmzpjjqAAAAAAAAgEm8tgYRAAAAAAAAyqZCjSC6/vrrZbPZCnXAb7/99ooKAgAAAAAAQMkqVEB01113FXMZAAAAAAAAMEuhAqKnn366uOsAAAAAAACASYq8SHWe7du368CBA5KkRo0a6frrr/daUQAAAAAAACg5RQ6ITpw4oe7du2vt2rUKDw+XJKWmpurvf/+7Fi5cqMqVK3u7RgAAAAAAABSjIt/FbPjw4Tp79qz27dun06dP6/Tp09q7d6+cTqceeeSR4qgRAIBSLSXd0KL9uXp1S47e35Orn864zC4JAAAAKJIijyBavny5Vq1apQYNGrjbGjZsqDfeeEPt27f3anEAAJR2x9INJezIUXbuxee/nTf0/SmpRyOpYeUi/x4GAAAAMEWR/+Xqcrnk7++fr93f318uF78xBQBYy7oklzsc+r1Vh/k7EQAAAGVHkUcQtW3bVo8++qgWLFig6OhoSdKvv/6qxx57TLfddpvXCwQA/M/Rc4xIKW2+O5Wr8zm2fO1JTumHVB/5+eTfhtKHP1sAAMDqihwQvf7667rjjjtUo0YNxcTESJKOHDmixo0ba+7cuV4vEADwPzP2hZpdAv4gPT1d2dnZ+dp9fHw0YatdNhsBEQAAAEq/IgdEMTEx+vbbb7Vq1Sp99913kqQGDRqoXbt2Xi8OAOBpcKN0RYcwdak0OXzGpc9+yMnXHlfNT82jz5pQES7H0XM+BLAAAMDSihwQSZLNZtPtt9+u22+/XdLF29wDAIpfdIhLNe0FLHgD09S0SxGBPlr9s0upGYZC/G2Kq2bT36obstn4WQEAAKBsKHJA9Pzzz6tGjRp64IEHJEn333+/Fi9erKioKH3++edq2rSp14sEAKA0uy7KR00jbbqQIwX6Sr4+NmXlGtqZ4lKS01BYgHRDVR9dVYHpZgAAACidirwi48yZM91rD61cuVIrV67UF198oY4dO2rUqFFeLxAAgLLAZrOpgr9Nvj42ZeQYStiRq09/yNXu4y59c8Sl17fm6OAppgcCAACgdCryCKKUlBR3QLRs2TLdf//9at++vWrUqKEWLVp4vUAAAMqaLUcNpaQbHm25hvT5IZeuibCxcDUAAABKnSIHRBUrVtSRI0cUExOj5cuXa9KkSZIkwzCUm8taCwBQnI6e8zW7BBTCtmMunc/JHwKdPyvtPOmr8CACotKGP1sAAMDqihwQ3XPPPXrwwQdVt25dnTp1Sh07dpQk7dixQ3Xq1PF6gQAAKTw8XIEBAZqxz+xKUBjnzp1TVlZWgdv+vTNMPj5FnuGNEhAYEKDw8HCzywAAADBFkQOil19+WTVq1NCRI0c0bdo0hYZevCXssWPHNGTIEK8XCACQIiMjNXfePO4aWUb88MMPmj17tr7//ntdc801Cg4OliRdc8017ps8oPQJDw9XZGSk2WUAAACYwmYYhvHX3co3p9Mph8OhtLQ02e12s8sBAJQD77zzjl555RVde+21Cg0NVb169TRo0CD3L1YAAACAklDYzKPII4gk6eDBg3rttdd04MABSVKDBg00fPhw1atX7/KqBQCgnGnVqpVmz56tXr16qWnTpqpatarZJQEAAACXVORFEBYvXqzGjRtr+/btatq0qZo2bapvv/1WjRs31uLFi4ujRgAAyiSbzaaaNWsSDgEAAKDUK/IIotGjR2vs2LGaOHGiR/vTTz+t0aNHq1u3bl4rDgCA0iYrK0uff/65tm7dKpfLpWbNmqlTp04KCQkxuzQAAADgshV5BNGxY8fUp0+ffO29evXSsWPHvFIUAACl1Ztvvqnly5fr1KlTOnPmjFavXq1XXnlFLpfL7NIAAACAy1bkgOjWW2/V119/na99w4YNatOmjVeKAgCgNDp06JC+++67fO1HjhzR7t27TagIAAAA8I5CTTH75JNP3P9/xx13aMyYMdq+fbtatmwpSdq0aZMWLVqkZ555pniqBACUSxkZGUpKSjK7jELbtm2b0tPTC9y2detW9+3sJbmvqyxdX1HExsYqKCjI7DIAAADgJYW6zb2PT+EGGtlsNuXm5l5xUSWN29wDgDkOHjyoQYMGmV1GoWVnZ18yIAoJCVFAQEAJV2SehIQE7l4KAABQBnj1NvesqwAAKA6xsbFKSEgwu4xCMwxD77zzjo4ePerRHhERoYcffli+vr4mVVbyYmNjzS4BAAAAXlSoEUSFkZqaqrlz52rYsGHeOFyJYgQRAKCw0tPTtXjxYm3fvl0ul0vXXXed7rnnHkVERJhdGgAAAJBPYTOPKw6IVq9erVmzZmnp0qWqUKGCTp06dSWHMwUBEQAAAAAAKI8Km3kU+S5m0sW7tUycOFE1a9ZU+/btZbPZtHTpUqWkpFx2wQAAAAAAADBHoQOi7OxsLVq0SPHx8apXr5527typf//73/Lx8dH//d//qUOHDvL39y/OWgEAAAAAAFAMCrVItSRdffXVql+/vnr16qWFCxeqYsWKkqQePXoUW3EAAAAAAAAofoUeQZSTkyObzSabzWapu7QAAAAAAACUd4UOiI4ePaqHHnpICxYsUFRUlLp166alS5fKZrMVZ30AAAAAAAAoZoUOiIKCgtSzZ0999dVX2rNnjxo0aKBHHnlEOTk5eu6557Ry5Url5uYWZ60AAAAAAAAoBpd1F7PatWtr0qRJSkpK0meffabMzEx16dJFkZGR3q4PAAAAAAAAxazQi1QXxMfHRx07dlTHjh118uRJvf/++96qCwAAAAAAACXEZhiGYXYRZnM6nXI4HEpLS5Pdbje7HAAAAAAAAK8obOZxWVPMAAAAAAAAUH4QEAEAAAAAAFgcAREAAAAAAIDFERABAAAAAABYXJHvYpabm6s5c+Zo9erVOnHihFwul8f2r776ymvFAQAAAAAAoPgVOSB69NFHNWfOHHXu3FmNGzeWzWYrjroAAAAAAABQQoocEC1cuFAffvihOnXqVBz1AAAAAAAAoIQVeQ2igIAA1alTpzhqAQAAAAAAgAmKHBA9/vjjevXVV2UYRnHUAwAAAAAAgBJW5ClmGzZs0Jo1a/TFF1+oUaNG8vf399i+ZMkSrxUHAAAAAACA4lfkEUTh4eG6++67dcstt+iqq66Sw+HweBTFhAkTZLPZPB7169d3b8/IyNDQoUNVqVIlhYaGqlu3bjp+/LjHMZKTk9W5c2dVqFBBVapU0ahRo5STk1PUywIAAAAAALCsIo8gmj17tlcLaNSokVatWvW/gvz+V9Jjjz2mzz77TIsWLZLD4dCwYcN0zz336JtvvpEk5ebmqnPnzoqKitLGjRt17Ngx9enTR/7+/po8ebJX6wQAAAAAACivihwQeb0APz9FRUXla09LS9OsWbM0f/58tW3bVtLFcKpBgwbatGmTWrZsqS+//FL79+/XqlWrFBkZqeuuu07PPvusxowZowkTJiggIKCkLwcAAAAAAKDMKfIUM0n66KOPdP/996tly5Zq1qyZx6OofvjhB0VHR6tWrVrq2bOnkpOTJUnbt29Xdna22rVr5+5bv359Va9eXYmJiZKkxMRENWnSRJGRke4+8fHxcjqd2rdv3yXPmZmZKafT6fEAAAAAAACwqiIHRNOnT1f//v0VGRmpHTt26KabblKlSpX0008/qWPHjkU6VosWLTRnzhwtX75cM2bM0OHDh9WmTRudPXtWKSkpCggIUHh4uMc+kZGRSklJkSSlpKR4hEN52/O2XcqUKVM81k2KiYkpUt0AAAAAAADlSZGnmL355pt666231KNHD82ZM0ejR49WrVq1NH78eJ0+fbpIx/p9oHTttdeqRYsWio2N1Ycffqjg4OCillZoY8eO1ciRI93PnU4nIREAAAAAALCsIo8gSk5O1s033yxJCg4O1tmzZyVJvXv31oIFC66omPDwcF1zzTU6dOiQoqKilJWVpdTUVI8+x48fd69ZFBUVle+uZnnPC1rXKE9gYKDsdrvHAwAAAAAAwKqKHBBFRUW5RwpVr15dmzZtkiQdPnxYhmFcUTHp6en68ccfVbVqVTVv3lz+/v5avXq1e/vBgweVnJysuLg4SVJcXJz27NmjEydOuPusXLlSdrtdDRs2vKJaAAAAAAAArKLIAVHbtm31ySefSJL69++vxx57TLfffrseeOAB3X333UU61hNPPKF169bp559/1saNG3X33XfL19dXPXr0kMPh0MCBAzVy5EitWbNG27dvV//+/RUXF6eWLVtKktq3b6+GDRuqd+/e2rVrl1asWKFx48Zp6NChCgwMLOqlAQAAAAAAWFKR1yB666235HK5JElDhw5VpUqVtHHjRt1xxx365z//WaRj/fLLL+rRo4dOnTqlypUrq3Xr1tq0aZMqV64sSXr55Zfl4+Ojbt26KTMzU/Hx8XrzzTfd+/v6+mrZsmUaPHiw4uLiFBISor59+2rixIlFvSwAAAAAAADLshlXOi+sHHA6nXI4HEpLS2M9IgAAAAAAUG4UNvMo8hQzSfr666/Vq1cvxcXF6ddff5Ukvf/++9qwYcPlVQsAAAAAAADTFDkgWrx4seLj4xUcHKwdO3YoMzNTkpSWlqbJkyd7vUAAAAAAAAAUryIHRJMmTdLMmTOVkJAgf39/d3urVq307bfferU4AAAAAAAAFL8iB0QHDx7U3/72t3ztDodDqamp3qgJAAAAAAAAJajIAVFUVJQOHTqUr33Dhg2qVauWV4oCAAAAAABAySlyQDRo0CA9+uij2rx5s2w2m44ePap58+bpiSee0ODBg4ujRgAAAAAAABQjv6Lu8OSTT8rlcum2227T+fPn9be//U2BgYF64oknNHz48OKoEQAAAAAAAMXIZhiGcTk7ZmVl6dChQ0pPT1fDhg0VGhrq7dpKjNPplMPhUFpamux2u9nlAAAAAAAAeEVhM48ijyDKExAQoIYNG17u7gAAAAAAACglCh0QDRgwoFD93nnnncsuBgAAAAAAACWv0AHRnDlzFBsbq+uvv16XOSsNAAAAAAAApVChA6LBgwdrwYIFOnz4sPr3769evXopIiKiOGsDAAAAAABACSj0be7feOMNHTt2TKNHj9ann36qmJgY3X///VqxYgUjigAAAAAAAMqwy76LWVJSkubMmaP33ntPOTk52rdvX5m9kxl3MQMAAAAAAOVRYTOPQo8gyrejj49sNpsMw1Bubu7lHgYAAAAAAAAmK1JAlJmZqQULFuj222/XNddcoz179uj1119XcnJymR09BAAAAAAAYHWFXqR6yJAhWrhwoWJiYjRgwAAtWLBAV111VXHWBgAAAAAAgBJQ6DWIfHx8VL16dV1//fWy2WyX7LdkyRKvFVdSWIMIAAAAAACUR4XNPAo9gqhPnz5/GgwBAAAAAACgbCp0QDRnzpxiLAMAAAAAAABmuey7mAEAAAAAAKB8ICACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIvzM7sAAAAAAACsKjMzUwcPHlRAQIDq1q0rX19fs0uCRREQAQAAAABggm3btmnevHm6cOGCJCk8PFwPPfSQatWqZXJlsCKmmAEAAAAAUMJOnTqld955xx0OSVJqaqpmzJih7OxsEyuDVTGCCAAAAABgCRkZGUpKSjK7DEnShg0b5HQ687Wnp6friy++UL169UyoqnSKjY1VUFCQ2WWUewREAAAAAABLSEpK0qBBg8wuQ5J04cIFZWRkFLjt0KFDCggIKOGKSq+EhAQCsxJAQAQAAAAAsITY2FglJCSYXYYk6ciRI5ozZ06+dl9fXz366KMKCQkp9LGSkpI0adIkjRs3TrGxsV6ssnQoj9dUGhEQAQAAAAAsISgoqNSMRKlXr56OHz+ur7/+2qP9vvvuU7NmzS7rmLGxsaXm+lD2EBABAAAAAGCCnj17qlmzZtq1a5f8/PzUokULxcTEmF0WLIqACAAAAAAAkzRo0EANGjQw7fw5OTnavXu3fvvtN1WvXl3169c3rRaYi4AIAAAAAAALOn36tF555RWlpKRIknx8fFSvXj0NHTqURbItiIAIAAAAAAALmjNnjjZv3qyzZ89Kkux2u7KysrRixQp17drV5OpQ0giIAAAAAAD5HD9+XKmpqWaXgUJISkry+G9hZGZmasmSJcrKynK3nT59Wunp6Vq9erWuueYar9cJT+Hh4YqMjDS7DDebYRiG2UVI0tSpUzV27Fg9+uijeuWVVyRJGRkZevzxx7Vw4UJlZmYqPj5eb775pscLmJycrMGDB2vNmjUKDQ1V3759NWXKFPn5FT77cjqdcjgcSktLk91u9/alAQAAAECZcvz4cfV88EFlZWebXQqKSWZmptLS0grcFhAQoPDw8JItyIIC/P01b/78Yg+JCpt5+BRrFYW0detW/ec//9G1117r0f7YY4/p008/1aJFi7Ru3TodPXpU99xzj3t7bm6uOnfurKysLG3cuFHvvvuu5syZo/Hjx5f0JQAAAABAuZGamko4VM4ZhiEfn4IjAV9f3xKuxpqysrNL1Sg906eYpaenq2fPnkpISNCkSZPc7WlpaZo1a5bmz5+vtm3bSpJmz56tBg0aaNOmTWrZsqW+/PJL7d+/X6tWrVJkZKSuu+46PfvssxozZowmTJjAoloAAAAAcAXulVTZ7CJQLI75+mqVj49+Mwzl/G5iUaDNpvuDghQlKcswtOvCBf2claVcSVf7++v6oCCFEiBdsZOSPjK7iD8wPSAaOnSoOnfurHbt2nkERNu3b1d2drbatWvnbqtfv76qV6+uxMREtWzZUomJiWrSpInHcKz4+HgNHjxY+/bt0/XXX1/gOTMzM5WZmel+7nQ6i+HKAAAAAKBsqywpWjazy0AxqOrnryT/AFVQts4bhnJkKEA21QsMUDP/i4Mtlp5L16/ZOfKV5CvpZFa2NubkqqfdLn8b74srUypW+/FgakC0cOFCffvtt9q6dWu+bSkpKQXOe4yMjHTfgi8lJSXfXL2853l9CjJlyhQ988wzV1g9AAAAAJRvJyWVxi+ylytbUqrZRZQWNqlGaIhcmZk69v8Xqo4OCFC1wEDtkqHUnBwdKGCaodOVqy+yMhUbGFhipYZL8i+xs5WMk2YXUADTAqIjR47o0Ucf1cqVKxUUFFSi5x47dqxGjhzpfu50OhUTE1OiNQAAAABAaRUeHq7AgAB99Ls7XKEcstmkoKCLD0knJO38/5uyXC6du8Rup10uBZdAeeVdYClbDNy0gGj79u06ceKEmjVr5m7Lzc3V+vXr9frrr2vFihXKyspSamqqxwt2/PhxRUVFSZKioqK0ZcsWj+MeP37cve1SAgMDFViCaScAAAAAlCWRkZGaO29eqVpA1xsyMzP/dLYJ/ufUqVP66KOCV8m59dZbVa9evRKrJSoqqlx+hy9tt7k3LSC67bbbtGfPHo+2/v37q379+hozZoxiYmLk7++v1atXq1u3bpKkgwcPKjk5WXFxcZKkuLg4Pffcczpx4oSqVKkiSVq5cqXsdrsaNmxYshcEAAAAAOVIZGRkqfry6i1/vHs2Li01NVW7du3yaIuMjNQ///lPbgpVDpkWEIWFhalx48YebSEhIapUqZK7feDAgRo5cqQiIiJkt9s1fPhwxcXFqWXLlpKk9u3bq2HDhurdu7emTZumlJQUjRs3TkOHDi2X6SIAAAAAACXlH//4h5YvX64tW7YoKytLTZs2VdeuXQmHyinT72L2Z15++WX5+PioW7duyszMVHx8vN588033dl9fXy1btkyDBw9WXFycQkJC1LdvX02cONHEqgEAAAAAKPv8/f3VtWtXde3a1exSUAJshmGUnyXpL5PT6ZTD4VBaWprsdrvZ5QAAAAAAAHhFYTMPnxKsCQAAAAAAAKUQAREAAAAAAIDFERABAAAAAABYHAERAAAAAACAxREQAQAAAAAAWBwBEQAAAAAAgMUREAEAAAAAAFgcAREAAAAAAIDFERABAAAAAABYHAERAAAAAACAxREQAQAAAAAAWBwBEQAAAAAAgMX5mV0AAAAAAAC4PLm5udq9e7dOnTqlSpUq6dprr5Wvr6/ZZaEMIiACAAAAAKAMWrdund544w2lpKS426KiojR06FDdcsstOn/+vBITE5WSkqKrr75aLVq0UHBwsIkVozSzGYZhmF2E2ZxOpxwOh9LS0mS3280uBwAAAACAP7Vu3TqNHz9ecXFx6t27t2rWrKnDhw/r/fffV2JiokaOHKmNGzcqLS3NvU9ERISeeOIJRUREmFg5SlphMw8CIhEQAQAAAADKjtzcXPXo0UO1atXS5MmT5ePzv+WFXS6X/vWvf2nTpk2qV6+ebDabx74tW7ZUv379SrhimKmwmQeLVAMAAAAAUIbs3r1bKSkp6t27t0c4JEk+Pj7q1auXTp8+LafTmW/fvXv3llSZKGMIiAAAAAAAKENOnTolSapZs2aB22vVqiWbzaasrKx82wIDA4u1NpRdBEQAAAAAAJQhlSpVkiQdPny4wO0//fSTAgICFBAQkG9by5Yti7U2lF0ERAAAAAAAlCHXXnutoqKi9P7778vlcnlsc7lcmjt3rmrUqKHWrVt7bGvevLk6duxYkqWiDOE29wAAAAAAlCG+vr4aOnSoxo8fr3/961/q1auXatWqpZ9++klz585VYmKiJk6cqFtuuUXHjh1TSkqKoqOjFRkZaXbpKMW4i5m4ixkAAAAAoOxZt26d3njjDaWkpLjbqlatqiFDhuiWW24xsTKUJtzmvggIiAAAAAAAZVFubq52796tU6dOqVKlSrr22mvl6+trdlkoRQqbeTDFDAAAAACAMsrX11fXX3+92WWgHGCRagAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIszNSCaMWOGrr32WtntdtntdsXFxemLL75wb8/IyNDQoUNVqVIlhYaGqlu3bjp+/LjHMZKTk9W5c2dVqFBBVapU0ahRo5STk1PSlwIAAAAAAFBmmRoQVatWTVOnTtX27du1bds2tW3bVnfeeaf27dsnSXrsscf06aefatGiRVq3bp2OHj2qe+65x71/bm6uOnfurKysLG3cuFHvvvuu5syZo/Hjx5t1SQAAAAAAAGWOzTAMw+wifi8iIkL//ve/de+996py5cqaP3++7r33XknSd999pwYNGigxMVEtW7bUF198oS5duujo0aOKjIyUJM2cOVNjxozRyZMnFRAQUKhzOp1OORwOpaWlyW63F9u1AQAAAAAAlKTCZh6lZg2i3NxcLVy4UOfOnVNcXJy2b9+u7OxstWvXzt2nfv36ql69uhITEyVJiYmJatKkiTsckqT4+Hg5nU73KKSCZGZmyul0ejwAAAAAAACsyvSAaM+ePQoNDVVgYKAefvhhLV26VA0bNlRKSooCAgIUHh7u0T8yMlIpKSmSpJSUFI9wKG973rZLmTJlihwOh/sRExPj3YsCAAAAAAAoQ0wPiOrVq6edO3dq8+bNGjx4sPr27av9+/cX6znHjh2rtLQ09+PIkSPFej4AAAAAAIDSzM/sAgICAlSnTh1JUvPmzbV161a9+uqreuCBB5SVlaXU1FSPUUTHjx9XVFSUJCkqKkpbtmzxOF7eXc7y+hQkMDBQgYGBXr4SAAAAAACAssn0EUR/5HK5lJmZqebNm8vf31+rV692bzt48KCSk5MVFxcnSYqLi9OePXt04sQJd5+VK1fKbrerYcOGJV47AAAAAABAWWTqCKKxY8eqY8eOql69us6ePav58+dr7dq1WrFihRwOhwYOHKiRI0cqIiJCdrtdw4cPV1xcnFq2bClJat++vRo2bKjevXtr2rRpSklJ0bhx4zR06FBGCAEAAAAAABSSqQHRiRMn1KdPHx07dkwOh0PXXnutVqxYodtvv12S9PLLL8vHx0fdunVTZmam4uPj9eabb7r39/X11bJlyzR48GDFxcUpJCREffv21cSJE826JAAAAAAAgDLHZhiGYXYRZnM6nXI4HEpLS5Pdbje7HAAAAAAAAK8obOZR6tYgAgAAAAAAQMkiIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIiAAAAAAAAiyMgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiIAAAAAAACLIyACAAAAAACwOAIi4C8YhqHMzEwZhmF2KQAAAAAAFAs/swsASrP169dr+fLlOn36tCIiItSpUye1bt3a7LIAAAAAAPAqAiLgEhITEzV//nz389OnT2vu3Lny9/dXixYtTKwMAAAAAADvIiCCV2RkZCgpKcnsMrzqww8/VHp6er72RYsWKTw8vOQLKiaxsbEKCgoyuwwAAAAAgIkIiOAVSUlJGjRokNlleFVqamqB6w7t3r1bW7ZsydduGIays7OVk5MjHx8fBQQEyMen9C/zlZCQoHr16pldBgAAAADARARE8IrY2FglJCSYXYZXzZ49W7t27VJSUpKqVq2qq666Sv7+/oqNjVWfPn08+mZmZmru3Lk6evSouy0oKEg9evRQtWrVSrr0IomNjTW7BAAAAACAyQiI4BVBQUHlahRKTk6OAgMDdfr0ablcLp09e1YZGRmqWbOmevfune9aP//8czmdToWGhnq0b968WbfddltJlg4AAAAAQJEREJng+PHjSk1NNbsM/Ildu3bpxx9/1FVXXaWzZ8/Kx8dHgYGB8vHxkcvl0sGDBz36r1u3rsD1ivbv369t27YpLCyspEqHpPDwcEVGRppdBgAAAACUGTajoEVWLMbpdMrhcCgtLU12u71Yz3X8+HE9+GBPZWdnFet5cGXOnTunrKyCf0ZhYWHy8/PMVtPT05WdnV1gf4fDUSbWIipP/P0DNH/+PEIiAAAAAJZX2MyDb60lLDU1lXCojLPZbPna/P39C+zr7+9POGSC7OwsRukBAAAAQBEwxcwkGbVvlREcbnYZuATj7Cm5fsp/pzKfYIeyrrk5f3/DkM/RA8r5LVnSxUF5PhXC5VOjmS74BxZ3ufgd24VUBf241uwyAAAAAKBMISAyiREcLlfIVWaXUa4YmedlGLnyCbry9X5sIVfJNzdXOb/ukwzXxbagMPnVayPXJY7vd01l+caekyv9tGyBFeQTWkmG8uIilBTGawEAAABA0REQocxzZZ5T9o9b5Dp7QtLFUT7+NW+QT9iVBXD+VzeUX5Vacp39TfILkE9Y5QKnl/2eLTBEvoEhV3ReAAAAAABKGr9sR5lmGIayDq53h0OS5LqQpqyD62VkZ1zx8W3+QfKNqCZfe5W/DIcAAAAAACirGEFkEtuFVNI5L8g9e0pK/035opvcLLl+3Sv/yjVMqApmsl1INbsEAAAAAChzCIhKWHh4uAICAiUW0fWKrKwsZZ87V+A2/6xUBR/fWbIFoVQICAhUeHi42WUAAAAAQJlBQFTCIiMjNW/eXG7B7SVnzpzRG2+8IcP431LQLpdLaWlpqlmzpqpWraqmTZuqbt26l3X8pKQkTZo0SePGjVNsbKy3ykYxCw8PV2RkpNllAAAAAECZQUBkgsjISL68elFycrK++uorSRfDoZ9++kk+Pj7Kzs7WkSNHdOTIEd1xxx3q1KnTZZ8jNjZW9erV81bJAAAAAACUKiyDgzLv/vvv1z/+8Q81btxYDodD4eHhqlGjhsei0p9//rnS09NNrBIAAAAAgNKLEUTwioyMDCUlJZl2/rCwMN1+++1atmyZjh07pvPnz+frs3bt2iJPNcu7JjOvrbjFxsYqKCjI7DIAAAAAACayGb9fvMWinE6nHA6H0tLSZLfbzS6nTDp48KAGDRpkdhm6cOGCMjIKvr19WFiY/PzIRP8oISGB6XMAAAAAUE4VNvPg2zK8IjY2VgkJCWaXoTNnzmjmzJnKycnxaI+OjtbAgQNNqqp0Y/FtAAAAAAABEbwiKCio1IxCCQ0N1cKFC/Xbb79JkurVq6d+/fqpYsWKJlcGAAAAAEDpxBQzMcWsPDIMQykpKQoMDFRERITZ5QAAAAAAYAqmmMHSbDabqlatanYZAAAAAACUCdzmHgAAAAAAwOIIiAAAAAAAACyOgAgAAAAAAMDiCIgAAAAAAAAsjoAIAAAAAADA4giIAAAAAAAALI6ACAAAAAAAwOIIiAAAAAAAACzO1IBoypQpuvHGGxUWFqYqVarorrvu0sGDBz36ZGRkaOjQoapUqZJCQ0PVrVs3HT9+3KNPcnKyOnfurAoVKqhKlSoaNWqUcnJySvJSAAAAAAAAyixTA6J169Zp6NCh2rRpk1auXKns7Gy1b99e586dc/d57LHH9Omnn2rRokVat26djh49qnvuuce9PTc3V507d1ZWVpY2btyod999V3PmzNH48ePNuCQAAAAAAIAyx2YYhmF2EXlOnjypKlWqaN26dfrb3/6mtLQ0Va5cWfPnz9e9994rSfruu+/UoEEDJSYmqmXLlvriiy/UpUsXHT16VJGRkZKkmTNnasyYMTp58qQCAgL+8rxOp1MOh0NpaWmy2+3Feo0AAAAAAAAlpbCZR6lagygtLU2SFBERIUnavn27srOz1a5dO3ef+vXrq3r16kpMTJQkJSYmqkmTJu5wSJLi4+PldDq1b9++As+TmZkpp9Pp8QAAAAAAALCqUhMQuVwujRgxQq1atVLjxo0lSSkpKQoICFB4eLhH38jISKWkpLj7/D4cytuet60gU6ZMkcPhcD9iYmK8fDUAAAAAAABlR6kJiIYOHaq9e/dq4cKFxX6usWPHKi0tzf04cuRIsZ8TAAAAAACgtPIzuwBJGjZsmJYtW6b169erWrVq7vaoqChlZWUpNTXVYxTR8ePHFRUV5e6zZcsWj+Pl3eUsr88fBQYGKjAw0MtXAQAAAAAAUDaZOoLIMAwNGzZMS5cu1VdffaWaNWt6bG/evLn8/f21evVqd9vBgweVnJysuLg4SVJcXJz27NmjEydOuPusXLlSdrtdDRs2LJkLAQAAAAAAKMNMHUE0dOhQzZ8/X//9738VFhbmXjPI4XAoODhYDodDAwcO1MiRIxURESG73a7hw4crLi5OLVu2lCS1b99eDRs2VO/evTVt2jSlpKRo3LhxGjp0KKOEAAAAAAAACsHU29zbbLYC22fPnq1+/fpJkjIyMvT4449rwYIFyszMVHx8vN58802P6WNJSUkaPHiw1q5dq5CQEPXt21dTp06Vn1/h8i9ucw8AAAAAAMqjwmYepgZEpQUBEQAAAAAAKI8Km3mUmruYAQAAAAAAwBwERAAAAAAAABZHQAQAAAAAAGBxpt7FrLTIW4bJ6XSaXAkAAAAAAID35GUdf7UENQGRpLNnz0qSYmJiTK4EAAAAAADA+86ePSuHw3HJ7dzFTJLL5dLRo0cVFhYmm81mdjkoRZxOp2JiYnTkyBHucAegSPj8AHA5+OwAcDn47MCfMQxDZ8+eVXR0tHx8Lr3SECOIJPn4+KhatWpml4FSzG6380EL4LLw+QHgcvDZAeBy8NmBS/mzkUN5WKQaAAAAAADA4giIAAAAAAAALI6ACPgTgYGBevrppxUYGGh2KQDKGD4/AFwOPjsAXA4+O+ANLFINAAAAAABgcYwgAgAAAAAAsDgCIgAAAAAAAIsjIAIAAAAAALA4AiKUObfeeqtGjBhh2vn79eunu+66q9TUAwAAAMB6fv75Z9lsNu3cufOSfdauXSubzabU1FTTa0HpR0AEXKElS5bo2WefNbsMAF5ks9n+9DFhwgT3P4TyHhEREbrlllv09ddfS5Jq1Kjxp8fo16+fJGndunVq27atIiIiVKFCBdWtW1d9+/ZVVlaWia8AgMtRmM8OSVq6dKlatmwph8OhsLAwNWrUyP3LpltvvfVPj3HrrbdK8vyMqVChgpo0aaK3337bnAsHUGrdfPPNOnbsmBwOh9mloAzwM7sAoKyLiIgwuwQAXnbs2DH3/3/wwQcaP368Dh486G4LDQ3Vb7/9JklatWqVGjVqpN9++03PPfecunTpou+//15bt25Vbm6uJGnjxo3q1q2bDh48KLvdLkkKDg7W/v371aFDBw0fPlzTp09XcHCwfvjhBy1evNi9L4CyozCfHatXr9YDDzyg5557TnfccYdsNpv279+vlStXSrr4i6e8gPjIkSO66aab3J8zkhQQEOA+3sSJEzVo0CCdP39eixYt0qBBg3T11VerY8eOJXG5AMqAgIAARUVFmV0GyghGEKFMysnJ0bBhw+RwOHTVVVfpqaeekmEYkqT3339fN9xwg8LCwhQVFaUHH3xQJ06ccO975swZ9ezZU5UrV1ZwcLDq1q2r2bNnu7cfOXJE999/v8LDwxUREaE777xTP//88yVr+eMUsxo1amjy5MkaMGCAwsLCVL16db311lse+xT1HABKVlRUlPvhcDhks9k82kJDQ919K1WqpKioKDVu3Fj/+te/5HQ6tXnzZlWuXNndPy9IrlKlisdxv/zyS0VFRWnatGlq3LixateurQ4dOighIUHBwcFmXT6Ay1SYz45PP/1UrVq10qhRo1SvXj1dc801uuuuu/TGG29IuviLp7z+lStXlvS/z5nff55Icv9bp1atWhozZowiIiLcQROAkudyuTRt2jTVqVNHgYGBql69up577jlJ0p49e9S2bVsFBwerUqVKeuihh5Senu7eN28Zi8mTJysyMlLh4eGaOHGicnJyNGrUKEVERKhatWoe31vyfPfdd7r55psVFBSkxo0ba926de5tf5xiNmfOHIWHh2vFihVq0KCBQkND1aFDB4+AW5LefvttNWjQQEFBQapfv77efPNNj+1btmzR9ddfr6CgIN1www3asWOHt15GmIiACGXSu+++Kz8/P23ZskWvvvqqXnrpJfew6uzsbD377LPatWuXPv74Y/3888/uqRyS9NRTT2n//v364osvdODAAc2YMUNXXXWVe9/4+HiFhYXp66+/1jfffOP+0CzKdI8XX3zR/UE5ZMgQDR482P0bRG+dA0DpcuHCBb333nuSPH/D/2eioqJ07NgxrV+/vjhLA1CKREVFad++fdq7d6/XjulyubR48WKdOXOm0J8/ALxv7Nixmjp1qvv7xvz58xUZGalz584pPj5eFStW1NatW7Vo0SKtWrVKw4YN89j/q6++0tGjR7V+/Xq99NJLevrpp9WlSxdVrFhRmzdv1sMPP6x//vOf+uWXXzz2GzVqlB5//HHt2LFDcXFx6tq1q06dOnXJOs+fP68XXnhB77//vtavX6/k5GQ98cQT7u3z5s3T+PHj9dxzz+nAgQOaPHmynnrqKb377ruSpPT0dHXp0kUNGzbU9u3bNWHCBI/9UYYZQBlzyy23GA0aNDBcLpe7bcyYMUaDBg0K7L9161ZDknH27FnDMAyja9euRv/+/Qvs+/777xv16tXzOHZmZqYRHBxsrFixwjAMw+jbt69x5513etTz6KOPup/HxsYavXr1cj93uVxGlSpVjBkzZhT6HABKj9mzZxsOhyNf++HDhw1JRnBwsBESEmLYbDZDktG8eXMjKyvLo++aNWsMScaZM2c82nNycox+/foZkoyoqCjjrrvuMl577TUjLS2tGK8IQEm41GdHenq60alTJ0OSERsbazzwwAPGrFmzjIyMjHx98z5nduzYkW9bbGysERAQYISEhBh+fn6GJCMiIsL44YcfiuFqAPwVp9NpBAYGGgkJCfm2vfXWW0bFihWN9PR0d9tnn31m+Pj4GCkpKYZhXPyOERsba+Tm5rr71KtXz2jTpo37eU5OjhESEmIsWLDAMIz/fUZMnTrV3Sc7O9uoVq2a8fzzzxuGkf/fILNnzzYkGYcOHXLv88YbbxiRkZHu57Vr1zbmz5/vcQ3PPvusERcXZxiGYfznP/8xKlWqZFy4cMG9fcaMGZf8vELZwQgilEktW7aUzWZzP4+Li9MPP/yg3Nxcbd++XV27dlX16tUVFhamW265RZKUnJwsSRo8eLAWLlyo6667TqNHj9bGjRvdx9m1a5cOHTqksLAwhYaGKjQ0VBEREcrIyNCPP/5Y6PquvfZa9//nDS/Pm+bmrXMAKB0++OAD7dixQ4sXL1adOnU0Z84c+fv7F2pfX19fzZ49W7/88oumTZumq6++WpMnT1ajRo3yDfUGUD6EhITos88+06FDhzRu3DiFhobq8ccf10033aTz588X6VijRo3Szp079dVXX6lFixZ6+eWXVadOnWKqHMCfOXDggDIzM3XbbbcVuK1p06YKCQlxt7Vq1Uoul8tjnbJGjRrJx+d/X9EjIyPVpEkT93NfX19VqlTJY/kM6eJ3oTx+fn664YYbdODAgUvWWqFCBdWuXdv9vGrVqu5jnjt3Tj/++KMGDhzo/q4SGhqqSZMmub+rHDhwQNdee62CgoIKrAFlF4tUo1zJyMhQfHy84uPjNW/ePFWuXFnJycmKj493T9/q2LGjkpKS9Pnnn2vlypW67bbbNHToUL3wwgtKT09X8+bNNW/evHzHzlsHoDD++OXQZrPJ5XJJktfOAaB0iImJUd26dVW3bl3l5OTo7rvv1t69exUYGFjoY1x99dXq3bu3evfurWeffVbXXHONZs6cqWeeeaYYKwdgptq1a6t27dr6xz/+of/7v//TNddcow8++ED9+/cv9DGuuuoq1alTR3Xq1NGiRYvUpEkT3XDDDWrYsGExVg6gIN5YO7Cg7xB/9r3Cm+cx/v96rnnrIiUkJKhFixYe/Xx9fa/ovCj9GEGEMmnz5s0ezzdt2qS6devqu+++06lTpzR16lS1adNG9evXz5ewSxeDmL59+2ru3Ll65ZVX3ItIN2vWTD/88IOqVKni/gdX3sNbt4YsiXMAMMe9994rPz+/fAs5FkXFihVVtWpVnTt3zouVASjNatSooQoVKlzRn/uYmBg98MADGjt2rBcrA1BYdevWVXBwsFavXp1vW4MGDbRr1y6PP+PffPONfHx8VK9evSs+96ZNm9z/n5OTo+3bt6tBgwaXdazIyEhFR0frp59+yvddpWbNmpIuXs/u3buVkZFRYA0ouwiIUCYlJydr5MiROnjwoBYsWKDXXntNjz76qKpXr66AgAC99tpr+umnn/TJJ5/o2Wef9dh3/Pjx+u9//6tDhw5p3759WrZsmfsDtGfPnrrqqqt055136uuvv9bhw4e1du1aPfLII/kWg7tcJXEOAOaw2Wx65JFHNHXq1EJNFfnPf/6jwYMH68svv9SPP/6offv2acyYMdq3b5+6du1aAhUDKGkTJkzQ6NGjtXbtWh0+fFg7duzQgAEDlJ2drdtvv/2Kjv3oo4/q008/1bZt27xULYDCCgoK0pgxYzR69Gi99957+vHHH7Vp0ybNmjVLPXv2VFBQkPr27au9e/dqzZo1Gj58uHr37q3IyMgrPvcbb7yhpUuX6rvvvtPQoUN15swZDRgw4LKP98wzz2jKlCmaPn26vv/+e+3Zs0ezZ8/WSy+9JEl68MEHZbPZNGjQIO3fv1+ff/65XnjhhSu+DpiPgAhlUp8+fXThwgXddNNNGjp0qB599FE99NBDqly5subMmaNFixapYcOGmjr1/7V3NyFRrXEcx3+HuYyoNZSoDQxOKAmNoTJZQRCdIirNTVgQkpiZCyUXRS2SUIigmdpWUCDpgMgsUkRzkbgQcagQRAiKIMUiKBKkxWA2kXMXlw4N9jJz73S91/P9LM//nOdlMww//s85wRU/Vk6nU21tbSorK9PevXvlcDgUDocl/XUed3x8XF6vVzU1NfL5fDpz5oyWlpbkcrnSsvZ/Yw4Aq+fUqVP6/Pmzbt269ct7d+3apWg0qubmZm3btk2maerx48caGBiw3p8GYG0xTVOzs7Oqr6/X1q1bVVVVpXfv3mlkZOQfdxKUlJTo0KFD6ujoSNNqAaSivb1dFy5cUEdHh3w+n06cOKH3798rKytLDx8+1MLCgnbu3Knjx4/rwIEDSf1XSEYwGFQwGFR5ebkmJiY0ODhofaX572hqalJnZ6e6urpUWloq0zTV3d1tdRCtW7dOQ0NDevr0qfx+vy5fvqzr16+nZS9YXUb862FDAAAAAAAA2BIdRAAAAAAAADZHQAQAAAAAAGBzBEQAAAAAAAA2R0AEAAAAAABgcwREAAAAAAAANkdABAAAAAAAYHMERAAAAAAAADZHQAQAAAAAAGBzBEQAAAD/YYZhaGBgYLWXAQAA1jgCIgAAgF9oaGiQYRhqbm5eUTt79qwMw1BDQ0NSY42NjckwDH348CGp+9++fauqqqoUVgsAAJA6AiIAAIAkFBQUKBwO6+PHj9a1paUl9fb2yuv1pn2+WCwmSXK73crIyEj7+AAAAN8iIAIAAEjC9u3bVVBQoP7+futaf3+/vF6v/H6/dW15eVmBQECFhYXKzMxUeXm57t+/L0mam5vT/v37JUkbN25M6Dzat2+fWltbde7cOeXm5urw4cOSVh4xe/PmjWpra5WTk6Ps7Gzt2LFDT548+c27BwAAa90fq70AAACA/4vGxkZ1dXXp5MmTkqR79+7p9OnTGhsbs+4JBALq6enRnTt3VFxcrPHxcdXV1SkvL0979uxRX1+fjh07phcvXsjlcikzM9N6NhQKqaWlRZFI5LvzR6NRmaYpj8ejwcFBud1uTU1NaXl5+bfuGwAArH0ERAAAAEmqq6tTW1ubXr16JUmKRCIKh8NWQPTp0yddu3ZNo6Oj2r17tySpqKhIExMTunv3rkzTVE5OjiQpPz9fGzZsSBi/uLhYN27c+OH8vb29mp+f1+TkpDXOli1b0rxLAABgRwREAAAAScrLy1N1dbW6u7sVj8dVXV2t3Nxcq/7y5UstLi7q4MGDCc/FYrGEY2g/UlFR8dP69PS0/H6/FQ4BAACkCwERAABAChobG9Xa2ipJun37dkItGo1KkoaHh+XxeBJqybxoOjs7+6f1b4+jAQAApBMBEQAAQAoqKysVi8VkGIb1IumvSkpKlJGRodevX8s0ze8+73Q6JUlfvnxJee6ysjJ1dnZqYWGBLiIAAJBWfMUMAAAgBQ6HQ8+fP9ezZ8/kcDgSauvXr9fFixd1/vx5hUIhzczMaGpqSjdv3lQoFJIkbd68WYZh6MGDB5qfn7e6jpJRW1srt9uto0ePKhKJaHZ2Vn19fXr06FFa9wgAAOyHgAgAACBFLpdLLpfru7WrV6+qvb1dgUBAPp9PlZWVGh4eVmFhoSTJ4/HoypUrunTpkjZt2mQdV0uG0+nUyMiI8vPzdeTIEZWWlioYDK4IqgAAAFJlxOPx+GovAgAAAAAAAKuHDiIAAAAAAACbIyACAAAAAACwOQIiAAAAAAAAmyMgAgAAAAAAsDkCIgAAAAAAAJsjIAIAAAAAALA5AiIAAAAAAACbIyACAAAAAACwOQIiAAAAAAAAmyMgAgAAAAAAsDkCIgAAAAAAAJv7E1dumwkUVYVNAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mse_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mse_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MSE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs {\" | jitter factor = \" + str(jitter_factor) if syn_data_type == \"jitter\" else \"\"}')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mae_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mae_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MAE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs {\" | jitter factor = \" + str(jitter_factor) if syn_data_type == \"jitter\" else \"\"}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2*1e06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_data_augmentation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
