{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Füge das übergeordnete Verzeichnis zu sys.path hinzu\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "from utilities import split_data_into_sequences, load_sequential_time_series, reconstruct_sequential_data, Scaler, extract_features_and_targets_reg, get_discriminative_test_performance\n",
    "from data_evaluation.visual.visual_evaluation import visual_evaluation\n",
    "from predictive_evaluation import predictive_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../../data\")\n",
    "REAL_DATA_FOLDER = DATA_FOLDER / \"real\"\n",
    "SYNTHETIC_DATA_FOLDER = DATA_FOLDER / \"synthetic\" / \"usable\" / \"1y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of loading data\n",
    "- Laden der Originaldaten: als pd dataframe \n",
    "- Laden der synthetischen, sequentiellen Daten: als np array (GAN, (V)AE)\n",
    "- Laden der synthetischen, sequentiellen Daten: als pd dataframe (brownian, algorithmit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible types: 'timegan_lstm', 'timegan_gru', 'jitter', 'timewarp', 'autoencoder', 'vae'\n",
    "syn_data_type = 'vae'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " syn data:\n",
      "\n",
      "       traffic_volume           temp        rain_1h       snow_1h  \\\n",
      "count   105108.000000  105108.000000  105108.000000  1.051080e+05   \n",
      "mean      3148.838149     284.774122       0.000006  7.282879e-08   \n",
      "std       1568.433179      11.482328       0.001798  1.720454e-05   \n",
      "min          0.000000     248.318405       0.000000  0.000000e+00   \n",
      "25%       1950.211853     277.012169       0.000000  0.000000e+00   \n",
      "50%       3213.790527     285.608795       0.000000  0.000000e+00   \n",
      "75%       4360.239624     292.805458       0.000000  0.000000e+00   \n",
      "max      10416.874023     330.338318       0.583064  5.320169e-03   \n",
      "\n",
      "          clouds_all  \n",
      "count  105108.000000  \n",
      "mean       42.894571  \n",
      "std        29.525214  \n",
      "min         0.000000  \n",
      "25%        18.017132  \n",
      "50%        42.057602  \n",
      "75%        65.470171  \n",
      "max       140.194412  \n",
      "\n",
      "\n",
      "real train data:\n",
      "\n",
      "       traffic_volume         temp      rain_1h      snow_1h   clouds_all\n",
      "count     8759.000000  8759.000000  8759.000000  8759.000000  8759.000000\n",
      "mean      3244.668912   282.208136     0.086792     0.000233    44.397306\n",
      "std       1946.247953    12.114907     0.901360     0.006145    39.195308\n",
      "min          0.000000   243.390000     0.000000     0.000000     0.000000\n",
      "25%       1252.500000   273.605500     0.000000     0.000000     1.000000\n",
      "50%       3402.000000   283.650000     0.000000     0.000000    40.000000\n",
      "75%       4849.500000   292.060000     0.000000     0.000000    90.000000\n",
      "max       7260.000000   307.330000    42.000000     0.250000   100.000000\n",
      "\n",
      "\n",
      "real test data:\n",
      "\n",
      "       traffic_volume         temp  rain_1h  snow_1h   clouds_all\n",
      "count     2135.000000  2135.000000   2135.0   2135.0  2135.000000\n",
      "mean      3325.263700   270.553730      0.0      0.0    45.065105\n",
      "std       1996.851023     7.864566      0.0      0.0    40.781402\n",
      "min        216.000000   248.660000      0.0      0.0     0.000000\n",
      "25%       1222.500000   265.735000      0.0      0.0     1.000000\n",
      "50%       3563.000000   271.550000      0.0      0.0    40.000000\n",
      "75%       4946.000000   275.680000      0.0      0.0    90.000000\n",
      "max       7280.000000   290.150000      0.0      0.0    92.000000\n"
     ]
    }
   ],
   "source": [
    "# Load real time series\n",
    "data_train_real_df = pd.read_csv(REAL_DATA_FOLDER/'mitv_prep_1y.csv')\n",
    "data_train_real_numpy = dc(data_train_real_df).to_numpy()\n",
    "\n",
    "data_test_real_df = pd.read_csv(REAL_DATA_FOLDER/'mitv_prep_3mo.csv')\n",
    "data_test_real_numpy = dc(data_test_real_df).to_numpy()\n",
    "\n",
    "if syn_data_type == 'timegan_lstm':\n",
    "    # load sequential data (which should already be scaled)\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_lstm_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'timegan_gru':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_gru_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'autoencoder':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'8726_12_5_lstm_autoencoder.csv', shape=(8726, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'vae':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'8759_12_5_fc_vae.csv', shape=(8759, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'jitter':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'jittered_01.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "elif syn_data_type == 'timewarp':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'time_warped.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "# Loot at real and syn data\n",
    "df = pd.DataFrame(data_syn_numpy.reshape(-1, data_syn_numpy.shape[-1]), columns=data_train_real_df.columns)\n",
    "\n",
    "print('\\n\\n syn data:\\n')\n",
    "print(df.describe())\n",
    "\n",
    "print('\\n\\nreal train data:\\n')\n",
    "print(data_train_real_df.describe())\n",
    "\n",
    "print('\\n\\nreal test data:\\n')\n",
    "print(data_test_real_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Predictive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"seq_len\": 12,\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 32,\n",
    "    \"hidden_size\": 12,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"num_evaluation_runs\": 10,\n",
    "    \"num_epochs\": 500,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETERS:\n",
      "seq_len :  12\n",
      "lr :  0.0001\n",
      "batch_size :  32\n",
      "hidden_size :  12\n",
      "num_layers :  1\n",
      "bidirectional :  True\n",
      "num_evaluation_runs :  10\n",
      "num_epochs :  500\n",
      "device :  cpu\n",
      "Synthetic Data is sequential: True\n",
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1057, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1056, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.17799467115193932 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.13792057511026917 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02619252136031533 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.02818889425629202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01294557518074221 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014286062330938876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009564649145013523 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010656911595587563 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008451577846171593 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009094199788866235 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007526742886279431 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007898219176294172 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007000043038635712 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007104547605301966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006726853751720194 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0067187864607309595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006509026694774329 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006452250979183351 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006319226841544257 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006239568355821949 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006149765875200694 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006062210530645269 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00599332135909978 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005926087950630223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 119\n",
      "INFO: Validation loss did not improve in epoch 120\n",
      "INFO: Validation loss did not improve in epoch 121\n",
      "Epoch: 121\n",
      "Train Loss: 0.005861391649448932 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005894603591193171 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive\\predictive_evaluation.py:277: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([{'Model': evaluation_method, 'Metric': 'MAE', 'Error': mae}])], ignore_index=True)\n",
      " 10%|█         | 1/10 [00:52<07:56, 52.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 123\n",
      "Early stopping after 123 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.07347066969658336 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.07661422429715886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.015566260334661734 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.017666862236664575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011930029498103897 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013311360065606148 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010483552008377809 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01147655863657265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009472924283787227 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01016294994213454 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008479219892769237 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009039441394099198 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007723233047116507 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008258547897532801 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007165172165361689 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007690490353578592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006866066483355868 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007254364677732263 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0067017809279691296 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0070942565901478865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0065810692009692804 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007042568909716518 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006479177598919665 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007023130844840232 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006389534347741627 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007013146683354588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006308326172645809 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007001963539925569 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006230824827947348 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006983158459393855 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006152467674280034 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006952513420187375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0060715707431948426 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0069057399489204675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005991254322210422 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006843995576833978 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005915790655221926 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0067785568466848314 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005845393745031507 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006719557034290012 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005777583242968459 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006671108962858424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005711401920650073 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006636695032391478 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005650167974191344 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006618136704406317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005596539236343445 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006611281875794863 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005549879408149171 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006608709980569342 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005509001850813328 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006604602923342849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005472976811668652 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006595933789332562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005441111256283483 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006582420435734093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005412819462634596 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00656507261480917 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005387569261446296 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006543801850913202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005364862529401171 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006518217043348533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005344204573185068 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006488270242698491 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005325077981501138 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0064558671129977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005306988405182713 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0064251389190116345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005289653950476217 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006399172549957738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005273095628222872 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006376418812364778 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005257417183168858 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0063537603709846735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0052426412487470535 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006330065203227979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0052287203491747 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006305540939245154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005215583817802206 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006280852337915669 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005203151826646491 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006256622255451101 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005191347151136461 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0062331749838502966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005180106993278584 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006210718913387288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005169371767547389 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006189353089533089 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005159109214577402 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006169211187892977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005149264137360886 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006150626851355328 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005139797218959697 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00613350748865153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005130668495990758 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0061176907141035535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005121840578104526 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0061030129025525905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005113280020113763 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00608936432913384 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [04:18<19:03, 142.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.26058847030937454 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.19805623440291076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.027751523229789778 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03106750950545949 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012452352608479288 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014041340471628834 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010872691650501025 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012075259476927492 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009793603291648379 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010395201125100036 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009086083702362366 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009257781699381988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008416741825113358 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008432100711883429 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007596887992677979 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007791913141879965 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007259978939112901 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00757793929455254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007021058180427219 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007378391678417649 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006839475011534609 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007198766161523321 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006707496264691118 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007076741723508081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006601105595388225 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00698494041502914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006503594165708679 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00689857778385939 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006404272993146204 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00680256200105171 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006295266355571412 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006683416938518777 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006172757340428576 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006533796221072621 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006044162377412601 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006370743147700149 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00592533366909305 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006218950729817152 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0058210796666379174 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0060680398292949096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005727973309461109 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005914820358157158 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005643936168116918 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005774143686079804 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005567736784336123 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0056547886240022145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005498056437643449 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00555616443264572 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005433623372223636 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005473509067943429 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0053733670680416595 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005401701125425889 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0053164556789905325 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005336755879825968 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005262279519760288 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005276034353300929 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0052104074912431245 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00521791646914447 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005160545995696889 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005161452470073367 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005112504686847547 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0051060759750030495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00506616682753164 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005051478572354159 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005021464652985158 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004997593590387088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004978370515418912 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004944577162592288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004936886422359894 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0048927393509075046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0048970275065253225 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004842536243171815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004858805628469636 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0047944036161746174 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004822206489289737 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004748693550936878 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004787185590291042 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004705644534517299 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004753665170712179 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004665386383695637 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004721546913777567 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004627927507766906 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0046907317149962715 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004593234813963885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0046611360899244106 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004561213906580473 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004632695030376832 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00453179283067584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004605374091439098 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004504903692149502 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004579160005375947 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004480468403712353 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004554061129023015 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0044583869440590635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004530102638468269 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004438529085532269 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.00450731115564398 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004420805777258733 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004485712023825813 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00440505993388155 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [07:44<20:01, 171.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1906199593769971 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.11572467983590767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02858628821389301 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03182912311133217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012503075453343998 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013900957987917698 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009483994808588449 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010476158901035567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008308734243861434 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009201973567114156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007909621815851475 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008553901666720562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007667532744927903 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008124858565518962 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007419265144084832 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007764531128272852 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007105385454670683 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007424491546664606 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006843606933329363 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007172890219782644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0066862784017618394 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007031422128955669 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006584991986462235 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006947133265545263 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006506777016690722 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006887346533510615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006437848671339452 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006837437888059546 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006372455257266406 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006790555465747328 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006307685591389228 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00674307018326705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006241773626508776 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006692809210268452 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006173567430041459 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006638339864473571 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0061023398442703715 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006578584756318699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006027835385574803 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0065126355107435405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0059505741916644475 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006439813967410694 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005872358841631208 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006359933539951111 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005796409716377592 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006273488911307033 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005726200096346574 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006182101541472708 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005663312596003831 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006089292713167036 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005606804553327579 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0059999667231322214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005554876060192779 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005917787825798287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005506238611848399 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005844095591729616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005460134141884717 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005778582663932706 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005416038587823659 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005720296381589244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005373527536153399 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0056682989576502755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00533228808734396 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005621960078475668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005292130669453147 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0055809626081848845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005252972697706581 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00554518258500406 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005214802732386959 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00551438784254167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005177638230694078 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005488259471295511 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005141472755582922 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005466155940666795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005106270140551887 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005447064094957621 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005071960442298412 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005429925184751696 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005038434376364003 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0054139848182197005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005005566983009573 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005398318034541958 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004973234825242433 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005381790181512342 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0049413232443799825 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053632577867521085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004909750627120957 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005341528401271824 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004878473055785195 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005315468995831907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004847432055466394 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005283583017230472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0048168321137136375 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00524607692119282 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0047866678854066496 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005202199284480337 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0047571211093084985 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005152405718998874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0047283669316335614 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005097757210023701 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [11:13<18:37, 186.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.15702153214110728 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.12353809017633252 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.025953389530879084 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.027969788047758973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012874764478238584 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014410507413284743 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01080313926931457 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011885406517916742 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00962609643614545 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010404208811986096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008750727498329442 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009363432777771616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008075795608426506 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008447424953748636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007277006062468255 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007350196712650359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006882585128378395 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0069653383825960405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006656110581999685 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006844255463768016 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006461444731893521 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0067355168421807535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006275656512839869 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006635858139762764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006105397760793295 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0065654342863982655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005961855344922982 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006522371960968217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005843076432620033 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006494868485092679 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0057413190574364835 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006477926114765818 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005651899751523904 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0064713473000344544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 167\n",
      "INFO: Validation loss did not improve in epoch 168\n",
      "INFO: Validation loss did not improve in epoch 169\n",
      "INFO: Validation loss did not improve in epoch 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [12:23<12:01, 144.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 171\n",
      "Early stopping after 171 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.1484942818400416 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.11603222908589113 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0204930516201187 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.022822709986940026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011536514103144109 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012426458018393639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009546910511817525 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010064454506123987 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008050418370930193 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008721632040653597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00739969616185663 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008088004556210601 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007060734031468725 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007629312424208312 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006905110468357176 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0074384686578174725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006807808882393704 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007359804448616856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006726510010839841 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007302293437533081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006652111555942518 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007246427731040646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006581948158317619 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00718986923696802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006514440749493176 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007132924723384135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006448089813449876 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007075546747621368 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0063812698472977825 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007016832988216158 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0063122870217002655 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006955100613755777 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006239502885749387 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0068879982196342416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006161659700629458 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006812805097604937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006078338537732288 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00672694672967362 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00599024225711265 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006628637407522868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005899317997225635 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006517655322300818 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005808554839573964 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0063961126964867995 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0057211972101350195 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006268511642701924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005639784242430988 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006140486135229687 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005565644026671053 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006017305751276367 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00549900613068331 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0059028229683035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005439429655654125 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005799117213909459 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005386217942556543 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0057067211121594645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005338667571989915 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00562518066384227 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005296080470311177 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005553467456004857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005257785646584782 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005490248509244446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005223223517001369 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005434323526809321 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005191755988788078 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005384154586970587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005162976565770805 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005338678228915395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005136474103235171 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005296891082680839 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005111926194258293 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0052580527981798 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005089057254244703 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00522155809106634 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005067629435007889 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0051869044888435915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005047433149455333 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005153685972532805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.00502825344519275 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005121532793375938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.00500991686976372 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005090165383401601 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004992271637557632 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005059303616529659 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.00497516429554807 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005028701514033053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004958458300671425 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004998168651261093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004942030976246022 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0049674895503010385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004925773369639527 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004936523858786505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004909590256517672 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004905102812164628 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0048933987523578654 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004873120893642087 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.00487713022650403 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004840463588205988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0048607439639719785 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004807044656070716 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [15:48<11:00, 165.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3533200718000205 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.23965308823458412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02760863538668321 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.029260489678777316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014264194449464227 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.016294362598701435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011782237524455617 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013200320882092723 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010666954964586282 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011753623308750856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009976197023923597 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010843890065135545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009321901064797774 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0100078922456733 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008660732317685292 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009300588979385793 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008080348480377265 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008770531081074081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007518793290980623 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008375033013769151 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007117343904623884 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007992324094041525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006838751941525044 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007624143001842587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006646280942559514 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007334411603069919 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006520820125583967 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007152390070533489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006430784139670024 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007043475031797939 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006353741136114419 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006957926451885963 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006279981383258464 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006877151612356743 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006204748113663445 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0067963099093450345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006125151629671862 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006712212521747193 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006039751363479716 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006621732246404623 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005949085031085423 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006522707346662441 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005855917157758245 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006414716015569866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005764129402092148 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006299018298330552 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005677010512649061 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006177818430039813 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005596520732680376 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0060551546596209795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005523347531200597 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005938709610799218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005457762828909105 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005835531156181413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005399041670632895 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005749848863000379 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005345838715026603 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005681203979560558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005296700603046529 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0056264333509127885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005250538397243885 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005581812145571937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005206705840902715 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005543775815407143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0051648476106678914 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005509070695980507 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005124723331418133 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005474655813647106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005086137972862195 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005438007380101173 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005048988737084352 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005397414521533339 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005013224962388376 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053520975189338274 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00497882212465459 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005302076501881375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004945793227771855 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005248232956920915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004914127985341814 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005191896008053685 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004883817090724262 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00513461882741574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004854850096918683 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005077777663245797 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0048272077743657426 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005022385267212111 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004800918638446184 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004969577887095511 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004775967587883428 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004920029334778733 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004752337173125401 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004874049730407184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004729981436249698 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004831633085439748 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004708851150570125 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004792582096449812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004688890201564756 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004756730005336816 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004670025255228563 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004723872636746177 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [19:14<08:55, 178.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2590277914406501 // Train Acc: 0.0\n",
      "Val Loss: 0.18107442784479216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03196434636879032 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03512113985112485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012479214340594536 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01378180568366695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010866845774389532 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011929882012362428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010147670043435246 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011029640647500534 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009461461274500555 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0101273882387699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008720787600223247 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009099843490677065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00791163937006839 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008003380522816716 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007340538132779409 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007428689596161027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006966492846050728 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007143416647415827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006702652606810613 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0069639243267695695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006508836382065295 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006850934669594555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006373241216192416 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006805390557822059 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 130\n",
      "INFO: Validation loss did not improve in epoch 131\n",
      "Epoch: 131\n",
      "Train Loss: 0.006272798886753782 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006798868581159588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 132\n",
      "INFO: Validation loss did not improve in epoch 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [20:10<04:38, 139.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 134\n",
      "Early stopping after 134 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.15105167465678748 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.12019260096884168 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02377727632512794 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0249082978869624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01210217933165399 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013126149821533439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009138326389616504 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009901111294953701 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008092164606651304 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008646484306903885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007605751924111379 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00789168624974349 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007337820994919234 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007519177934976623 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007108168647179285 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007307409089716042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006911434689077827 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007125404478489038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0067267305753873595 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00692463106037501 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006492801260434254 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006665512850946363 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006344083722096181 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006515170420136522 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006222478966364624 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006396594553199761 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006103033869082013 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006284121184281129 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005980440352560721 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006179502829635406 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005853014324288679 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0060867564547259145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005722089115713798 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006009570720112499 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005591206563582277 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005945641054388355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00546404564826253 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005881141097394421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005343306630367182 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005796842263354098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005231527675556726 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0056797921013854 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005131262938328849 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005528353836031302 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005044202564792354 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005354659837287138 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0049704651978805005 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005179923709125861 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004908497852101636 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005022062046919018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0048558828132023125 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004888372574521995 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0048102736294636655 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0047775250243242175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0047698301930417285 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004685280673370203 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0047332167582515025 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004607289296794026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0046995327672853145 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004539812490453615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004668134599554448 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004479816443670322 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004638611515992436 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0044252440223799035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004610723099840404 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004374747719232212 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004584324505901556 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004327496986233574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0045593359938229565 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004282951594659072 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00453570849040394 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004240730664391509 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004513411925439417 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004200787975124139 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004492407682288886 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004163047791842152 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004472644290084658 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004127443764660069 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004454058083029205 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004093913132023504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004436571530536341 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00406242415363736 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004420096587464485 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004032913701581385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004404539023364561 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004005265791955239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004389808139760374 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0039794016047380865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0043758174687467625 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0039552354171653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004362482357053018 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.003932639031999689 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004349729794427671 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0039114876999519765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004337494731685283 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.003891665303586599 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004325721061739214 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.003873048713156844 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004314358665456615 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.003855524964712779 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [23:36<02:40, 160.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.21909921851865674 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.16890074868056484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02629713809634321 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.02830514933585244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01194141980134848 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013067380460116136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009210547070736133 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01007516762269113 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00797460641011759 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008415330701288493 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007289508384835981 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007383339839768322 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006746110064487387 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006578200113247423 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00637801344095165 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006067185601055184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006143051604519388 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005810710553572897 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005965761809752588 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005657704060842448 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005821903128292493 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00554797173176399 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005698135762091346 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005451154244570609 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00559240181031331 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053769001490710415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005503601376324158 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053363452761379234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 141\n",
      "Epoch: 141\n",
      "Train Loss: 0.00542902514843243 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005326540914692861 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 142\n",
      "INFO: Validation loss did not improve in epoch 143\n",
      "INFO: Validation loss did not improve in epoch 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [24:35<00:00, 147.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 145\n",
      "Early stopping after 145 epochs\n",
      "Shape of the data after splitting into sequences: (6996, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1741, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.15826724270639353 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.1013856262137944 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.032623045877914994 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02767664214426821 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014583733759939535 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011928062746301293 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011668208621327497 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009345631932162425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009861394256931734 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00787950037047267 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008789720133874038 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007050223839045925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008303685535328063 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006627300462092866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007999119762922361 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006348626971752806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007681754488948749 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006093069648539478 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007335820746157302 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005851271278648214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007102287300016102 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005690372701395642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006912433978735754 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005592770603570071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006726063776242475 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005495544158938256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006557936874243801 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005372213961725885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006432256668265994 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052403036674315276 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0063421744911731585 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005129837591878393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006272401960490093 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005043201902034608 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006213829877624739 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004971192510459911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006161929889088794 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0049066466406326404 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0061143755314225275 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00484594918618148 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00606985646778437 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004787682691081004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006027597934007645 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00473143131556836 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005987104582897978 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004677156811918725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0059480136510231384 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004624944734810428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005910046422560635 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004574916253543713 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005873026409756273 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004527122866023671 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0058367751739689545 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044816006271337925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005801130111699235 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00443834755146368 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005765950116945524 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043973070378838615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00573111015965218 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004358417614871128 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005696485327242036 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004321604655970904 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005661974253864389 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042868135623972525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005627485290361084 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004253999433818866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005592961868985671 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004223166304555806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005558420866770355 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00419431847139177 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005523989446988525 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004167465577748689 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005489895374888393 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004142573681152002 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005456383756285355 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041195126694881104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005423593725031029 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004098057757470418 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005391527597737163 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040779511401937765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0053601076387370765 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004058944861489264 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005329234496856185 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004040824577466331 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005298831286153344 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040234113344922665 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005268865526753762 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040065760427916595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.00523936732105494 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003990246565081179 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005210438300695985 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003974421114914797 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005182233289549697 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003959161324680529 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005154979709199881 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003944604439576241 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0051289144233634615 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003930928183465519 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005104243025485613 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003918306635354053 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:49<25:26, 169.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2974454102076624 // Train Acc: 0.0\n",
      "Val Loss: 0.1966146393255754 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.049212150080086975 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.04312622855332765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.020348024201583645 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.016725225890563293 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012959750070445001 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010259092946282841 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010945714277121744 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008623469815674153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0099847762474416 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007827445860444145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009113739845321671 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007069833233783191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00843296312359183 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006460283137857914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00808424852335545 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0061462873550639915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007830835204809632 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005919609240002253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00764548254877618 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005771156764504584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007528091646883994 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005698160213333639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007440919734826779 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005647589715028351 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007360990948605109 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005596273697235367 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.007282732293269168 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005545199239118533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.007205068240086782 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005498732275075533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.007126604118618315 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005458532557399435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.007044827819112943 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0054231617303395815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006956676180957132 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005389992020685564 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006858653959299366 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005354858282953501 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006747088766099724 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005310186680237001 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006618952619434934 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00524602241136811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006474324262841311 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005157145527614789 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006320558499532101 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005051333605396477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006170856423535678 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004944264579733664 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006039392779321839 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048531321194869555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005933491750216307 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004786507806486704 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005850247686333342 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004737817436795343 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005783898385930551 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004699372265233912 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005729722277980008 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004667144088836556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005684324278130481 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004639400706880472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005645254306860915 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004615278205495666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005610735393788444 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004594198279929432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005579498869188334 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004575665500438349 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005550647902378991 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004559228503653271 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005523569636859136 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004544480111111294 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005497844923249356 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004531095202334903 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005473190440145666 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004518764741210775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005449413855083831 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004507262708449905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005426383096210182 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004496380036950789 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0054040014755499625 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004485953330400992 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005382204509380627 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004475858409635046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005360941645013143 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004465997276235033 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005340176038061307 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004456289391964674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005319880478968631 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004446684920483015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005300035972103528 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004437150909904052 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005280630665163607 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004427650314755738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005261657774697173 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044181596233763476 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005243109076147789 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004408670887774364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005224980698041533 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043991574975238605 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [05:37<22:29, 168.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.10666182377805176 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.06571700088679791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.027551272650117472 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.023594747263599527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013193285165005895 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01066662986678156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01031897680676589 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008288982256569646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009086635048803129 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007307922111993486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008563287828338746 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006812317927621983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008266758928011644 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006492307765240019 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00807287837920889 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006293404555286874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007920958791204545 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006170112669298595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007783123584927727 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006084302524951371 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007643042235915894 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006004343207248233 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007490927991925071 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005910922654650428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007329362145301856 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005804489438676021 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007184452586169419 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005710910671305927 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.007077746715218033 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005647286328233101 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006993506369010873 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005596262411299077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00691819406905505 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005546864012086933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0068474099104883805 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005496575924652544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00677862340907596 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005443830623037436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006710456513595602 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005387951937419447 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006642368085763192 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0053292078050700105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0065742088412678885 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005268735002556985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006505833282752056 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0052084134468300776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0064367875610967275 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005150464503094554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0063659182040565155 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005096526325426318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006290395275294168 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005045935003594918 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.006202981598913398 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004992752780460499 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.006088112568117031 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004921992449089885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005941295515231312 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004822282052852891 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005802487784924334 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004713144331154498 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005696256924101649 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046154279388826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0056116825461889445 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045339828949760306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005540022067831974 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004468917012722655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005479727390373219 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004419723343612118 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005430138537013844 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004384683931923725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005389061079734775 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004359947856176983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005353881356616815 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004341024185784839 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005322553857695311 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004324594840661369 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005293735296871581 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004308797397904776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005266574339262852 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004292746618474749 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005240517977537953 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004276128610680726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0052151876540544135 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042588601502674545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005190308017605175 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004240968675267967 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005165680745386914 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004222532914189453 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005141173985618757 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00420365637506951 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005116706671507936 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004184467074545947 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005092264729026773 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004165133021094582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005067881573891038 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041458069219846615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005043625536313085 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004126667557284236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005019592356732005 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004107900120487267 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [08:24<19:35, 167.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.13154154258980055 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.08581822968342087 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.039915502258495654 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03430630896579136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015298682479740686 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012425338091667402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011188720701130524 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009345068837600676 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010004441391519318 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008382123391228643 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009061278816265773 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007535877963528037 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008559636170997127 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007054366641254588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008236823176654676 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006727651769126004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007965169610991462 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00648757388222624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007701070450850205 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0063027729788287115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007441934201330559 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006102652454071425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007229824938500921 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005872785714878277 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00707700036330038 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005668807643550364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006956928302789081 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0055211339865557174 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006851243001906431 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005414318289099769 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00675547683928332 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005330443022434007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006670164816681142 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005261694398623976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0065937163329525894 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005204658849503507 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006522426374796693 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005155861403115771 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006452794488174969 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005112013233486902 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006382640218893789 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005070828621021726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006311206526088966 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005030780247497288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006238812205411459 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004990979339080778 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0061662241026597545 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0049512782345779915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006094046261323165 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004911857944997875 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006022459321302366 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004872535914182663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005951439980927804 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004832432774657553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005881247989892892 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004790304795923558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005812699543992611 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004745246948335658 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005746911409030324 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004697362363169139 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005684721695055088 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046478964515369045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005626225470327113 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004598610882054675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005570819641284848 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045510778902098535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005517686126098785 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00450647678649561 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0054662799912921604 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004465803379108283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005416624280188198 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004429990901950408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005369285470008697 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00439973977944729 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0053250787837630705 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004375322728248482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005284614567049528 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004356433573940938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005248020701567519 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004342297845604745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005215037071632995 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004331936401484365 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005185245158848221 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00432438355320218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005158202422859325 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004318762687034905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.00513348125184984 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004314309798858383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0051106882364527415 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004310417837801982 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005089475334989817 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004306665653447536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005069552190372185 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004302775834433057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005050683974822107 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004298589704558253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.00503267531153495 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004294014267030764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005015374042229837 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004289036110805517 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [11:12<16:47, 167.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2625840151529402 // Train Acc: 0.0\n",
      "Val Loss: 0.1633458602496169 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.041419171630383625 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03600704603913155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014041723336888367 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011659961464730176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011786120661457821 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009815068500624462 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010586076766650137 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009045221986757083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0096167531624632 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008439801472493194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009026682296168213 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007828736660832709 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008607737676157579 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0071770212020386345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00816718766042648 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006685692211613059 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007725401349327437 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006355297734791582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007387492535678355 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006017920738932761 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007176127103763339 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005750830831344832 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007050960269686447 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005587605365806005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00697141274364376 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005496611031280322 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00691272505903489 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005440248536284675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006863410328940135 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005397516239265149 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00681881964383179 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005360108546235345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0067769766399279644 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005325202025811781 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006736850648795225 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005291845645247535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006697755423906051 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005259623086418618 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006659143364597027 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005228296823968941 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006620526750737878 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005197650448165157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0065814432641278665 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005167487856339324 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006541446690685775 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005137604978782209 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006500135670250205 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005107811237262054 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0064571999123991895 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050779417199506 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.006412514288735376 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005047903743318536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.006366240474093518 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050176680045710366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.006318930537571714 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0049872487190772185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.006271502416108876 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004956623632460832 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.006225000019185245 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004925793553279205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.006180248759606147 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004894966776059433 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.006137620500867259 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004864517971873284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.006097083371217665 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004834799536249854 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0060584124195767934 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004805969142101027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.006021357333865952 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004777976205911149 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005985731247293691 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004750626644288952 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005951412144376467 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004723706782202829 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0059183413820919585 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004697070567106659 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005886505711419704 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004670668973333457 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005855909767177926 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046445322667502545 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005826569267561379 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046187557004900145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0057984458018927815 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004593794610859318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005771617080323126 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00456984105350619 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.00574593841721257 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045465168056332255 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005721421940486079 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045239427888935264 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00569793296080576 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004502363170666451 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005675365844290687 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00448183090087365 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0056536400820379525 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004462362720038403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005632696742910602 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004444079477847977 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [13:59<13:57, 167.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.18661242361992733 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.11742410412566229 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.038301053214563084 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.031836084513501686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01649926089018277 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.013755534047430212 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011604790222835323 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009526459174230695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009239614594591671 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0075161115986041045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008401532906222425 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0066399159798906605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008003991613904499 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006219357019290328 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007708702121593364 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006037741357629949 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007427118982817791 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005921821622177958 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007180719979247716 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005772411175580187 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006991169679608009 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005637678868052634 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006832194846276551 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005542826110666448 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0066836429743912735 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005460786552761089 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006525137005097416 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0053721068638630885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006329944789979488 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005248852472075008 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006138032675958898 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005087338045070117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006006137660883163 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004938871396536177 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005914716006439502 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004830989004536108 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005843359586297956 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004752503001046452 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005782661945176349 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046903680015186015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0057283515104232996 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046370081222531475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005678300705385398 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004588608665984463 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005631337832174685 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004543552767824043 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005586806836763406 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004501382873224264 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005544295729580229 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004461761070838706 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005503552970869815 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004424549711190841 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005464413484768968 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043896462239155715 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005426751567448434 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004356915167194199 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0053904413968031235 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00432614871004427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005355335324184042 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004297042322683741 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0053212585139777255 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042692165525460785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005288011127057991 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00424224125136706 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005255390858845402 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004215712974440645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005223199503483531 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041892781642011625 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.00519127701090215 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004162682567469099 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005159499302833898 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004135811944830824 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.00512779758207619 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004108703087761321 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005096139665905676 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004081508688713339 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005064548814794703 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004054500964808871 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005033066522831854 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004027971462346613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005001757515607121 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004002203184857287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004970695229128139 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003977426916191524 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0049399676167652794 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003953832104294138 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004909680725700439 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00393154562281614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004879950611643731 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003910665385509757 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004850904417668627 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038912893378768455 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004822662141658876 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003873501254499636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004795320197029813 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038573662221262402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.00476895992978493 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038429111157628625 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004743651294236637 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038301217581399464 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [16:46<11:09, 167.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.18643351737835093 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.11389622397043488 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.030091346590248143 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.024944304725663228 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01479290472024618 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012276745337823575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011240354529688279 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009227142766626045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00917274430319752 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007416577480564063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007834753471311845 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006342957549813119 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007208449959002231 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005797783696007999 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006855555053068895 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0054204102817245504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0065253735121263015 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005091142387722026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0062776876089362995 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004810279726304791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00613436695043695 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004633310767398639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006039139021030674 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004522502223368395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0059644681795980765 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004440582937306979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005900916679587947 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004373599737035957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005844505856740692 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004315870077433911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005793204207874869 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004264446305619045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005745805965813055 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004217665367336435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005701547003506373 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004174531666053967 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005659912446020468 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004134388469074937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005620522723059047 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004096818728033792 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005583115551965183 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004061524720269848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005547508160683611 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004028271369382061 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005513571558911438 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003996877698227763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005481221269524856 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003967201237736101 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005450397415445973 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003939148066142066 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005421057811004777 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003912648840130053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005393168967127902 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038876667703417215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005366693965274177 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038641842738301917 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005341594763680629 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038422038418833506 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005317817012339269 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038217205363749104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0052952985356340704 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003802733138118955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005273969761855033 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037852325540205295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005253760765261999 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037691949401050806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00523459581290393 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037545837038619953 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005216420895513572 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037413475781001826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005199144108775901 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037294336933303964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005182706212442045 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037187919613312593 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0051670560029071656 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037093365713107315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0051521338773870065 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003700977796688676 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005137874558249054 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003693621929480948 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.00512421354449452 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003687164574776861 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005111093651482062 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003681502546268431 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005098461853361531 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0036765423434024507 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005086273929852669 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0036721945973113177 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0050744924547584495 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003668378716842695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005063093013287475 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003665010353804312 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005052056863901873 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003662029416723685 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0050413672802714655 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0036593722031367094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005031015444334307 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0036569884131577883 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005020996652964554 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003654841071164066 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [19:33<08:22, 167.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.11294176076543114 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.07678496607325294 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03803110259189589 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03209651317447424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014970069955268952 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.013558887664906004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011370604253931132 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009703407893803986 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009506984691977603 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007917071222750979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008534068338462411 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006983397575095296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008024342359441277 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006421058726581661 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007638052231355753 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005999844902279702 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007316224155022622 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005667529183185914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0070434572320265615 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005411598044024272 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00681024160635866 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005213265277614647 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0066243399696883885 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005062334662811322 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00649211324377147 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004959316107190468 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00638718841508133 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004888582377779213 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0062861023502654 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004828513324768705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006175069675300287 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004759236001832918 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0060452212043718935 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004662734574892304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0059058934058909005 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004537131671201099 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005781584128537577 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004405430814420635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005679902360140937 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004285537330857055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005597667803150096 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004184392492540858 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005530790582120622 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004103328082287176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005475399468187668 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004040039275688204 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005428092532416967 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003990636607208713 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005386146867741325 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003951244790699672 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00534763348184645 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003918809234164655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005311315231233597 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038912215164269913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005276432972120708 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003867096892050044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.00524253774669285 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038455388212407177 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005209379544541172 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038259812143885278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005176857535644396 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038080727745016868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005145025192377196 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037916968555443665 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005114060047307992 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003776883810165931 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00508421508475211 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037637702168219467 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0050557465287197605 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003752468528480015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0050288446494229306 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037429829974743455 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005003607647703986 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037351799663156273 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0049800375743862665 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003728800876574083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004958064159002053 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037235408725047655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004937572082075116 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003719089216214012 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004918427239267845 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00371518332130191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004900490650885896 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003711605480533432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0048836313988630696 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00370820649179884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0048677332013033195 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003704896232705902 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004852691171722716 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037016212855550375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004838418607816916 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003698344888504256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004824837957047454 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0036950512069531464 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0048118878292030395 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003691748897968368 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004799510937048012 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003688428726639937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.00478765718106257 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003685095182365992 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [22:21<05:34, 167.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.15270298088318138 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.09539959379895167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03319144436927963 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.028882363726469604 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014964590013903229 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012661910700527105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011245040102334498 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00906460277990184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010105051747714616 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00800822496583516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00951536700233698 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007490597013384104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009096974623389542 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007123729925264012 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008713367689064954 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006774031167680567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008336883087719038 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0064415269404311075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008088567386143181 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006264366552403027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007911315979329725 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0061669102277268064 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007740965342153391 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006122425812381235 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0075700716955425665 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0060626635454933755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0073834012855762896 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006017185671424324 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.007197566084844405 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005951379899951544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.007041991737747743 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005843773479996757 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00691847536727769 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005739774508401752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006814668210197801 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005654492728750814 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006717854131068439 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005577434345402501 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006615844495662543 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005494134238159114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00649563646046062 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005389998899772763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006349091081067784 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005254790474745361 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006183459886485821 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005092376208102161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006016851846154218 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004926284906369718 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00586669975211291 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004785410776226358 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00574117412388784 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004682140236466446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00563948244105208 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004610065804709765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0055570065965620825 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004555600388398902 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005488442223557361 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004508367678235201 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00542943423441675 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004464318973689594 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005377116202978528 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044232332062992185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005329791750688718 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00438572147607126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0052864482712965954 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00435209156606685 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005246417788835455 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004322291724383831 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005209206247311001 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004296022219668058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005174434269389316 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042728671602989465 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005141781556386007 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042523870689117095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005110979862149686 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042341759872876784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005081797467298023 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004217869650826536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005054045235738158 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042031476041302085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005027558133335674 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004189715794795616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005002196376386298 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00417729943821376 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0049778390314310925 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004165656688961793 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004954379407282851 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00415456108960577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004931718591849435 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004143799604340033 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004909771105984745 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00413318629461256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004888454933234036 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004122569278644567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004867696907099115 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004111819927030328 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004847427565640596 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004100834132722495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0048275855627680645 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004089534128169444 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [25:08<02:47, 167.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.11858649208257187 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.08077001273632049 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.026198538828249934 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02480962930077856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01578345556252266 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.015143649068407037 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01214633400118208 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010860855526036836 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009983462610571182 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00842243702235547 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008846783850524897 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007233220584351908 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008310718681678158 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006721514924852685 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007923232320776916 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006352852936834097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0075749186559720605 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006019105152650313 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007290824462625636 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005736740572716702 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0070814621421087486 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00553875640458004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006919756989403028 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005411178690635345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006780905457698319 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005317902594635432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006653323148156104 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005237051158804785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006534663800864596 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005160654657943682 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00642669791638528 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005088783546604894 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006331163391015879 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005023973286998543 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006247542321694734 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004967430437153036 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006173617945405651 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004918568962338296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0061069681329363485 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004876064881682396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006045704824450217 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004838530672714114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005988459713591186 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048046369105577465 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005934205875821309 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004773101210594178 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005882145053405128 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004742765236137943 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005831727804828725 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004712767494757744 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005782663724363939 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004682622843591327 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005734909373148425 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004652217824266038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005688620367636011 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046217627061361614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005644054020772004 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004591692397794263 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00560145485629519 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004562540290961889 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005560960170927725 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004534818824719299 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005522567443932368 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004508904888379303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005486162073461586 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00448499600911005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00545156176242399 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004463114572519606 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005418564755358992 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004443134750578214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005386969019057119 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004424818952313878 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005356598159055902 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004407851389524611 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005327309047235559 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004391911788843572 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0052990070251816 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004376689471643079 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005271659192276191 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004361862450076098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005245203936108936 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004347265729765323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005219661878936469 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004332730358212509 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005195015846309358 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004318202496506274 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005171248657930829 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004303676772608676 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005148326704856944 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.00428921105128459 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005126202139061489 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004274879268963229 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005104819370101984 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004260773755694655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005084111520033925 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004246968610889532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0050640134635215545 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004233522270806134 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.00504446658279245 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004220478334040804 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [27:55<00:00, 167.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.0832499596183975 // Train Acc: 0.0\n",
      "Val Loss: 0.0479745169932192 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.008810879013046079 // Train Acc: 0.0\n",
      "Val Loss: 0.008105050831694495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0007965566148422233 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.0007457108273890546 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00038912115741114995 // Train Acc: 0.48562012078362055\n",
      "Val Loss: 0.00037238692219199784 // Val Acc: 0.7007575757575757\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00022233534061703958 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 0.0002303348590926775 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00019625106702301413 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 0.0002046470947045071 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00018322704787542576 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 0.00019020402370105414 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00017314924171181676 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 0.0001784432179358026 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00016466676558622533 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 0.00016851730536224997 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0001571916125189315 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 0.00015993938577594236 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00015040851755893775 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 0.0001523572985950688 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00014415931049096463 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 0.00014555237271865321 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00013837982730269667 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00013940918473632667 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00013305843923640647 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.0001338800785809078 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00012819988541565538 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.0001289359178785658 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00012379418490438606 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00012453773260031912 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00011980335168874307 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.0001206242269704076 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0001161655138491772 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.0001171140239669264 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00011280838657408681 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00011391881853342056 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00010966453198585383 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00011095549780293368 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0001066836176675874 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00010816034733910452 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00010383977039707038 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00010549694469029253 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00010113182909426292 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00010295690286015584 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 9.85761086987257e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00010055066767646085 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 9.61936662969239e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 9.829308956713331e-05 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 9.399822177009266e-05 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 9.619219325197099e-05 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 9.198882186811388e-05 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 9.424198969182642e-05 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 9.015033673548212e-05 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 9.242526284651831e-05 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 8.845950717895321e-05 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 9.07186957854058e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 8.689066327121705e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.90989504114259e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 8.542028665041592e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.754554060446522e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 8.402902393376712e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.604379446213981e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 8.27015744136555e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.458210623674942e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 8.142663223204335e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 8.315335746208968e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 8.019575813360466e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 8.175237691664899e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 7.90028604141917e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 8.037723422272724e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 7.784355780110826e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 7.902695162391121e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 7.671475418335407e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 7.77017206101763e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 7.561425884152213e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 7.64025194009512e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 7.454053977124029e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 7.513106112045617e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 7.349260984267455e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 7.388897803056435e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 7.246964502040154e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 7.26780992739474e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 7.147104973905055e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 7.14994782439052e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 7.049590490231361e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 7.035384759629695e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 6.954332155594921e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.924111499260602e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 6.861217267405705e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.816071876197715e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 6.770138857458541e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.711160587490832e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 6.681039476420062e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.60930838586699e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 6.593964591550058e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.510526400234084e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 6.509072025512243e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 6.414960057141302e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:47<25:10, 167.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1283389860806672 // Train Acc: 0.2996575342465753\n",
      "Val Loss: 0.08160961344838143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.004704856218081223 // Train Acc: 0.0\n",
      "Val Loss: 0.003178531638431278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0006517680360803797 // Train Acc: 0.0\n",
      "Val Loss: 0.0006155784263021566 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00029926457945746474 // Train Acc: 0.4570813079982324\n",
      "Val Loss: 0.000277009888139384 // Val Acc: 0.7007575757575757\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0002185092306298259 // Train Acc: 0.599775371925173\n",
      "Val Loss: 0.00020838877773547376 // Val Acc: 0.8143939393939393\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00018434864795460051 // Train Acc: 0.6853918102813374\n",
      "Val Loss: 0.00018023590634005482 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0001670219939463606 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 0.00016596849626776848 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00015591408474770584 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 0.00015627580499064854 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0001481093361494812 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 0.00014908759846796535 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00014227257682221568 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 0.00014347907483831725 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00013756641883072953 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 0.000138816224088342 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0001335191361777023 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 0.00013473444183050148 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00012987780003113667 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00013103298154998233 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00012649956031914557 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00012758994443257425 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00012329597344125522 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00012432315316717986 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00012020755777949866 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00012117359266002578 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00011719198555012603 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00011809860674971291 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00011422023559948663 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00011506654005973939 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00011127272553486243 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00011205529087667607 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00010833536523176872 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00010904807164810005 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00010539401042794482 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00010602803390197963 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00010242965258445302 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.0001029730789675589 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 9.941576811145847e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 9.985512813066386e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 9.632278929467708e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 9.664678635255602e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 9.313296681757991e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 9.334068368347784e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 8.986975477937159e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.998201850441878e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 8.66280300892224e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.669426877697167e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 8.356544241581865e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.365452701003629e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 8.082060365218427e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.099393135952679e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 7.842202736256159e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 7.871390162935396e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 7.62956603786368e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 7.671763427920683e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 7.433871253947494e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 7.489285112983039e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 7.246907424810555e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 7.31559080370723e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 7.063719122472579e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 7.145649641725785e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 6.88211382431835e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.977116787642113e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 6.702094778780545e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.809855469989337e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 6.525346350112715e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.645339023882777e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 6.354378818444744e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 6.486035890702624e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 6.191101413327545e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 6.334096800525335e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 6.035552310282109e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 6.190087978294204e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 5.886023879432881e-05 // Train Acc: 0.6853918102813374\n",
      "Val Loss: 6.0529608559921724e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 5.740234109700113e-05 // Train Acc: 0.6853918102813374\n",
      "Val Loss: 5.921029838976789e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 5.596346223791189e-05 // Train Acc: 0.6853918102813374\n",
      "Val Loss: 5.793067301221361e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 5.4535347144794544e-05 // Train Acc: 0.6853918102813374\n",
      "Val Loss: 5.668689865904691e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 5.31237909019357e-05 // Train Acc: 0.6853918102813374\n",
      "Val Loss: 5.548580527168021e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 5.175176257792261e-05 // Train Acc: 0.6853918102813374\n",
      "Val Loss: 5.434656516378957e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 5.04567314108906e-05 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 5.3294173002624035e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 4.927759235290243e-05 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 5.234573086041068e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 4.823717451003193e-05 // Train Acc: 0.6996612166740315\n",
      "Val Loss: 5.150071751400405e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 4.7334960558370384e-05 // Train Acc: 0.6996612166740315\n",
      "Val Loss: 5.074333937293638e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [05:35<22:20, 167.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.05916653189076681 // Train Acc: 0.0\n",
      "Val Loss: 0.03710947009650144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.003026607125831691 // Train Acc: 0.0\n",
      "Val Loss: 0.002118645121597431 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0005526861963159996 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.0005305305816529488 // Val Acc: 0.22727272727272727\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00029028543412258005 // Train Acc: 0.599775371925173\n",
      "Val Loss: 0.0002865490699398585 // Val Acc: 0.7575757575757576\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00022632846859994463 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 0.0002294070666804063 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00019488357462732603 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 0.00020209825809367678 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00017539598394919413 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 0.00018373264240587807 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00016125670558659756 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00016897812588881193 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0001502900813315776 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00015658492378530685 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00014186128097523607 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00014664918033469637 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00013538597264397807 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00013903696711746637 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00013009098665238587 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00013300222212406384 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00012540683348237358 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00012783183800903233 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00012106546168917254 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.0001231685260956345 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00011693678563925837 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00011884749513807369 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00011293491149017137 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00011476421343765899 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00010901636340175373 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00011086182838136499 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00010517979876048434 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00010712087116024288 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00010144399636444746 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.0001035263565427158 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 9.782022733645849e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00010004822344688529 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 9.430240302893639e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 9.664727172507396e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 9.08779419157771e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 9.32954698245422e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 8.754468670877155e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.99881152277389e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 8.431952340217662e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.674563465650532e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 8.123620638432997e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.360611801353198e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 7.834131044700326e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.062381305287338e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 7.568869148923262e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 7.786503321602306e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 7.3324880281585e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 7.539135407238394e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 7.126706551808809e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 7.322958150656301e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 6.949253675888546e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 7.13577525113413e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 6.795230488169015e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.972320387087e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 6.65934838493595e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.82704833624567e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 6.537322211717192e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.695686973795422e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 6.426085932301219e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.575435628209644e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 6.323528391870959e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.46443252745402e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 6.228141404848363e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.361438655320935e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 6.138832760291954e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.26552550040122e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 6.0547652396734395e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.175935761182865e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 5.975269135946225e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.092023163959807e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 5.899834439366286e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 6.013240467423616e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 5.8280190956403e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 5.939079012684736e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 5.759457843390147e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 5.869094357794066e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 5.69384570703172e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 5.802888769143134e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 5.630918338399946e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 5.74009013441603e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 5.57043737014644e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 5.680374170796395e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 5.5121947372928644e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 5.623436895389618e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 5.456005382169778e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 5.569030436163303e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 5.401710760331825e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 5.516892577526795e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 5.3491494061529515e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 5.466823085126552e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 5.2981912087331226e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 5.418604573192583e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [08:22<19:31, 167.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.09220748068126913 // Train Acc: 0.0\n",
      "Val Loss: 0.058755959434942766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.001553028431502022 // Train Acc: 0.0\n",
      "Val Loss: 0.000987608407475901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.00033455744228771245 // Train Acc: 0.32865665046398584\n",
      "Val Loss: 0.0003195671494309367 // Val Acc: 0.625\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0002344633515320344 // Train Acc: 0.6568529974959493\n",
      "Val Loss: 0.00024391036872244015 // Val Acc: 0.8143939393939393\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00020103825725477877 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 0.00021133283841085027 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00018261331639871358 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 0.00019117347374875945 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00016985431224608488 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00017666388567621734 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00015981564711307317 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00016497526891974055 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.000151465771411233 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00015515840681142766 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00014438777961332317 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00014686918148072438 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00013835328038992918 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.0001399156208869747 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00013317501062586303 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00013410694505596027 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00012867768969480905 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00012923102635382252 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0001247079120871832 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.0001250819489624436 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00012114437072860506 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00012148714668001049 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00011789848198133444 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 0.0001183149669404057 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00011491022178649128 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.0001154719630325086 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00011213914646788874 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00011289262726098637 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00010955441569974306 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 0.00011052733627200889 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00010712718881534298 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 0.00010833129358202728 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0001048258846867819 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 0.00010625826868239198 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00010261488743509128 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 0.00010426057812186297 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00010045586331212174 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 0.00010229049042787996 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 9.831121491989095e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 0.00010030501844911751 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 9.615183465706916e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 9.827563738905486e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 9.397010565918794e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 9.620278618637133e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 9.179490955759661e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 9.41299709004604e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 8.969652309926592e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 9.214557959983887e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 8.77642357894953e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 9.03541950471911e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 8.606098466164742e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.88228111514102e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 8.459108433366985e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.754630835028365e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 8.331247926389344e-05 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 8.646632509639445e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 8.217106336333308e-05 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 8.551386788233438e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 8.112422745306997e-05 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 8.463726242718457e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 8.014483874793681e-05 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 8.380572756075046e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 7.921697988629886e-05 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 8.300265131269539e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 7.83315299984629e-05 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 8.222044135353909e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 7.748312216865263e-05 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 8.145553001668305e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 7.66678544071149e-05 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 8.070585264993663e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 7.588244500778908e-05 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 7.997006607845172e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 7.512363335326342e-05 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 7.92467414033705e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 7.438806037643933e-05 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 7.853409958691124e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 7.367235901164844e-05 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 7.783057815024883e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 7.297329762072109e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 7.713482115253679e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 7.228776431147043e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 7.644546855193436e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 7.161276197907744e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 7.576120640971401e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 7.094537484997535e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 7.508074182128026e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 7.028275627219469e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 7.440252236185849e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 6.962217848424238e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 7.372458464735908e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 6.896123344316009e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 7.304484977132895e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [11:09<16:43, 167.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.11993181788826097 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.07118974429639903 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.012892119035754166 // Train Acc: 0.0\n",
      "Val Loss: 0.011995297636498104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0011022303424314192 // Train Acc: 0.0\n",
      "Val Loss: 0.0009571129149249331 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00031382941316518735 // Train Acc: 0.5569671527470909\n",
      "Val Loss: 0.0003195583804468201 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00022940644082059387 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 0.00023170827639246868 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00019861386570142322 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.0001985347586345266 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00018146864859361762 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00018083643676205114 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00016995979602300865 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00016919339581032876 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0001613286061346112 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 0.00016047168359033424 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00015431425079225444 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 0.00015328744749157605 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0001482723737810632 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 0.0001469851923915981 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0001428692848339239 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 0.00014126646935685793 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0001379408985647462 // Train Acc: 0.8708940933863603\n",
      "Val Loss: 0.0001360211401664525 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00013341550667359298 // Train Acc: 0.8851634997790543\n",
      "Val Loss: 0.00013122479226281442 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.000129262322922621 // Train Acc: 0.8708940933863603\n",
      "Val Loss: 0.00012688503437940116 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00012546458915349088 // Train Acc: 0.8708940933863603\n",
      "Val Loss: 0.0001229930258969861 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00012200722682447126 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 0.00011952191588617015 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00011886997473586699 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 0.00011642671577812342 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00011602219451124754 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.0001136481349187141 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00011341917396246107 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 0.00011111449189642868 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00011100620018067392 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 0.00010875015299957754 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00010872558970615633 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.00010648524171715094 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00010652302716647118 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.00010426030838227068 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0001043507351852862 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.00010202802275836636 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00010216754072980497 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 9.975027615754781e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 9.993772016047207e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 9.739679656129076e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 9.763079977338627e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 9.49436239104464e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 9.522403201798497e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 9.237623638579283e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 9.270543387381905e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 8.969340964209882e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 9.00787038363933e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 8.691365312022919e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 8.736567952966245e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 8.407778811736286e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 8.460485865999881e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 8.12477721906775e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 8.18450756110017e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 7.849656156560576e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 7.913587384820824e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 7.589116827478971e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 7.651866332155526e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 7.347470256552862e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 7.402217775945005e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 7.125692500267177e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 7.166282482838342e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 6.921831849136982e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 6.944934194843757e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 6.732556701701304e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 6.738632848830766e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 6.554941038220105e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 6.547579865898144e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 6.387634348357097e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 6.371655618837294e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 6.230794718827714e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 6.210384529131012e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 6.085213231430812e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 6.0629006240984155e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 5.951496138798327e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 5.928047778992276e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 5.829511001965412e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 5.804516446030354e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 5.718520303410266e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 5.6909409146790483e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 5.617430618837137e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 5.586080172357785e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 5.525055206486617e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 5.4888325842649175e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 5.440312695348191e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 5.3983110026894965e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 5.3622157726997766e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 5.313782284367717e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 5.289965715333396e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [13:57<13:57, 167.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.11970399913790564 // Train Acc: 0.2711187214611872\n",
      "Val Loss: 0.08062426034699786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.00847943241550634 // Train Acc: 0.0\n",
      "Val Loss: 0.007497320146384564 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0006382733690563045 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0006126093000850894 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0002732151267692855 // Train Acc: 0.5426977463543968\n",
      "Val Loss: 0.00025032274398571725 // Val Acc: 0.7007575757575757\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00018745582945725654 // Train Acc: 0.8851634997790543\n",
      "Val Loss: 0.00018712803470076654 // Val Acc: 1.1553030303030303\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00016791658845042266 // Train Acc: 0.9565105317425247\n",
      "Val Loss: 0.00017271309572441335 // Val Acc: 1.1553030303030303\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00015703342952919682 // Train Acc: 0.9279717189571365\n",
      "Val Loss: 0.00016233876985180277 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00014874906473903127 // Train Acc: 0.9137023125644425\n",
      "Val Loss: 0.00015321608714822848 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00014151633880513117 // Train Acc: 0.9279717189571365\n",
      "Val Loss: 0.00014490352160382 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00013474773107411246 // Train Acc: 0.8994329061717484\n",
      "Val Loss: 0.00013710866643867287 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00012822852981424058 // Train Acc: 0.8851634997790543\n",
      "Val Loss: 0.00012975049314925194 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00012206273047042453 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 0.0001230629805253226 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00011653013633841598 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00011737875280563127 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00011176862649937994 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00011272390296322886 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00010766100140618581 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00010881657528278248 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00010400620958867237 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00010535430476672693 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00010065336725459281 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00010215384225540963 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 9.752283893157749e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 9.913535322993993e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 9.458845323265305e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 9.628240004531108e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 9.185468927387267e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 9.361029872459105e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 8.933354849814433e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 9.11393920515283e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 8.702522089434298e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 8.887517000072298e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 8.491053163135354e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 8.680134682931979e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 8.295619418318586e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 8.488590858839664e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 8.112544768764192e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 8.309213953907601e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 7.938562385917569e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 8.138809389492962e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 7.771097707892257e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 7.97497547120491e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 7.608221996268342e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 7.816041923599021e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 7.448538344428655e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 7.660931726356714e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 7.291100068059304e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 7.509082285898992e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 7.135391542636184e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 7.360269492809576e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 6.981310333307624e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 7.214701928949746e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 6.82913735516228e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 7.072833759593777e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 6.679504470300082e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 6.935471398967572e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 6.533315080223598e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 6.803568043820137e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 6.391604165816371e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 6.678100015216677e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 6.255410477173776e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 6.559863494740884e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 6.125604170652322e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 6.44936050693187e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 6.0028038538163676e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 6.346644063755362e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 5.8873335094434063e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 6.251354954491201e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 5.7791884795916224e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 6.162786779565398e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 5.678129767326342e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 6.079973877190654e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 5.583716549286799e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 6.001938246217006e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 5.495425704885253e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 5.92771937425079e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 5.412681851227455e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 5.85645274193005e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 5.334920844946378e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 5.787441365431402e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 5.261630745179107e-05 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 5.720203430841634e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 5.1923785709840306e-05 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 5.654386943867642e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 5.126815013270921e-05 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 5.589826078820889e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 5.064667019304409e-05 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 5.526483787848106e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [16:45<11:10, 167.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.08736156752188456 // Train Acc: 0.0\n",
      "Val Loss: 0.05282245444303209 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01020467016828931 // Train Acc: 0.0\n",
      "Val Loss: 0.009101379434154792 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.000508576483302537 // Train Acc: 0.17123287671232876\n",
      "Val Loss: 0.0004909391382666812 // Val Acc: 0.2840909090909091\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0002585860343289043 // Train Acc: 0.6283141847105612\n",
      "Val Loss: 0.00025187827824001114 // Val Acc: 0.7575757575757576\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0002104888507550408 // Train Acc: 0.8708940933863603\n",
      "Val Loss: 0.00021035035923970018 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.000191055578190289 // Train Acc: 0.8994329061717484\n",
      "Val Loss: 0.00019176112638722936 // Val Acc: 1.1553030303030303\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00017804901426435836 // Train Acc: 0.8994329061717484\n",
      "Val Loss: 0.00017842258702413264 // Val Acc: 1.1553030303030303\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0001684320607442158 // Train Acc: 0.9279717189571365\n",
      "Val Loss: 0.00016811980623804794 // Val Acc: 1.1553030303030303\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0001607282572370352 // Train Acc: 0.9137023125644425\n",
      "Val Loss: 0.00015961891212183137 // Val Acc: 1.1553030303030303\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00015417784314892855 // Train Acc: 0.8851634997790543\n",
      "Val Loss: 0.00015227046083468435 // Val Acc: 1.1553030303030303\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00014840333174874785 // Train Acc: 0.8708940933863603\n",
      "Val Loss: 0.0001457741781698794 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00014320297023378734 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 0.00013997528201963922 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00013845215563714977 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.00013476586391334422 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00013406735385229098 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00013005720267590898 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00012999432293082095 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00012577233018501747 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00012619329527919883 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00012184599836473353 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00012263027014815223 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00011822260547143576 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00011927455376097975 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00011485436242020859 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00011609971925821453 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 0.00011170322488231416 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0001130856160745854 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 0.00010873955682406879 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00011021885555104637 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 0.00010594321574899367 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0001074926364280764 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 0.00010330065150893377 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00010490566611021 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 0.00010080452471315352 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00010246077785201979 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 9.84523873326411e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00010016404945359748 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 9.624806347842837e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 9.802321510838098e-05 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 9.420036393831568e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 9.604395933268e-05 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 9.232071533915587e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 9.422546765114794e-05 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 9.061471633189781e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 9.255511587864153e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 8.907160160809078e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 9.100826104441615e-05 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 8.767167680409991e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 8.955471529169529e-05 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 8.638558981395115e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 8.816601129511337e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 8.518364381664221e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 8.681976943001465e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 8.404196801579514e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 8.550055222403732e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 8.294459439639468e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 8.419891661379888e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 8.188160017280923e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 8.290925154856621e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 8.084718392638024e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 8.16284508339914e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 7.983719855970279e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 8.035487644104596e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 7.884824838335837e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 7.908792599353123e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 7.787696161937096e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 7.782797684453664e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 7.691943643625233e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 7.657637348471392e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 7.597247128036212e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 7.533595114543696e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 7.503350303026805e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 7.411094215822398e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 7.410122045919045e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 7.290706783446715e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 7.317637199858754e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 7.17306101080599e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 7.225954243701628e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 7.058735394671763e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 7.135314808692783e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 6.948132521420245e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 7.045713980253574e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 6.841426415179674e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 6.957018257790795e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 6.738545697643028e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 6.86901458803649e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 6.639254005672622e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 6.781296843572901e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [19:32<08:22, 167.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1623289047242844 // Train Acc: 0.45662100456621\n",
      "Val Loss: 0.10024597509340806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.013681835958470492 // Train Acc: 0.0\n",
      "Val Loss: 0.01288024334406311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0024104441880763053 // Train Acc: 0.0\n",
      "Val Loss: 0.002164010455916551 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0006529810252603671 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.0006378429576712237 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0004419662573584249 // Train Acc: 0.21450139932243334\n",
      "Val Loss: 0.00042316427068065174 // Val Acc: 0.3409090909090909\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0002565320678742084 // Train Acc: 0.6140447783178671\n",
      "Val Loss: 0.00025287841192700645 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0002029907675571612 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00020810640734535726 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00017217315796092388 // Train Acc: 0.8851634997790543\n",
      "Val Loss: 0.00017899759074101564 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00014833230662468467 // Train Acc: 0.9137023125644425\n",
      "Val Loss: 0.00015309980378581465 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00013187780619009406 // Train Acc: 0.9137023125644425\n",
      "Val Loss: 0.00013421429764341817 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00012071970414668513 // Train Acc: 0.8994329061717484\n",
      "Val Loss: 0.00012142005619931628 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00011298282305642508 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 0.0001127232769001487 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00010741229784570222 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 0.00010655299351624721 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00010316127349532712 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00010185302612212995 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 9.973052419394098e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 9.802052577883429e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 9.682542852686344e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 9.472130510733801e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 9.425733277396634e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 9.17575533755801e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 9.189746524332214e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 8.900347393963868e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 8.965651825634916e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 8.637851325064812e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 8.747421512228743e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 8.383682169634003e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 8.531376770240378e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 8.136418913305864e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 8.315687681379255e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 7.897255510959606e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 8.099999170607917e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 7.669148028732955e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 7.884914039386453e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 7.455218586652666e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 7.671615542813637e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 7.256979644642508e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 7.461569554614462e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 7.073191237313123e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 7.25631884940566e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 6.900049980190075e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 7.057288207532892e-05 // Train Acc: 0.7424694358521137\n",
      "Val Loss: 6.732680621181234e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 6.86567340189826e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 6.567349059464918e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 6.682381839103375e-05 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 6.40260182329009e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 6.508161596398106e-05 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 6.239630526000506e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 6.343633611148307e-05 // Train Acc: 0.7139306230667256\n",
      "Val Loss: 6.0809087021466855e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 6.18939414891105e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 5.929360648389609e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 6.0459703619519594e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 5.7873707125757146e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 5.913791997730573e-05 // Train Acc: 0.7282000294594196\n",
      "Val Loss: 5.656609262752516e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 5.793015740173401e-05 // Train Acc: 0.7567388422448077\n",
      "Val Loss: 5.537962312122214e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 5.6834555980292273e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 5.431536384011534e-05 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 5.584545880544146e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 5.3369034139905126e-05 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 5.495395632588292e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 5.253101821422619e-05 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 5.4149312733262714e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 5.1789115307408134e-05 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 5.341993937067917e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 5.112985335555012e-05 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 5.275462676992121e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 5.053993409092073e-05 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 5.21434144938623e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 5.0007212782723154e-05 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 5.157770270114386e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 4.9520931001593866e-05 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 5.105010904403633e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 4.907264645391313e-05 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 5.0554781036263284e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 4.865496627065692e-05 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 5.0087060237656345e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 4.826229527349245e-05 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 4.964312473552559e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 4.7890377192578106e-05 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 4.922017964849953e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 4.7535806317517364e-05 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 4.881592834879725e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 4.719597570576959e-05 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [22:19<05:34, 167.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.029778884187953113 // Train Acc: 0.0\n",
      "Val Loss: 0.02215068516406146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.005416316301327998 // Train Acc: 0.0\n",
      "Val Loss: 0.004383888561278582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.00041019439111999556 // Train Acc: 0.24304021210782148\n",
      "Val Loss: 0.00039739704463334586 // Val Acc: 0.3977272727272727\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00025306427411439503 // Train Acc: 0.6425835911032552\n",
      "Val Loss: 0.0002477024655408141 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00020523025762498673 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.00020731299360208637 // Val Acc: 1.1553030303030303\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00018344297731674242 // Train Acc: 0.9279717189571365\n",
      "Val Loss: 0.00018723775329735045 // Val Acc: 1.1553030303030303\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00016798335419899727 // Train Acc: 0.9422411253498306\n",
      "Val Loss: 0.00017168502282319506 // Val Acc: 1.1553030303030303\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00015622703287100363 // Train Acc: 0.9137023125644425\n",
      "Val Loss: 0.00015933426951629702 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00014723760672429254 // Train Acc: 0.8994329061717484\n",
      "Val Loss: 0.00014970533408235166 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00014025545980370408 // Train Acc: 0.8851634997790543\n",
      "Val Loss: 0.0001421917299343146 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00013467014914941464 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 0.00013621244083465584 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00013004633071170545 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.00013130965162417852 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.000126096707722624 // Train Acc: 0.8566246869936662\n",
      "Val Loss: 0.00012716094199938444 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00012263453706538678 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.0001235488587884571 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00011953578469317968 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00012032889657844367 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00011671588691823252 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00011740539789157496 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0001141153294472162 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00011471275449201296 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00011169194244114112 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.0001122071778809186 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00010941625780654012 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.0001098585146378768 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00010726852256437386 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.00010764691026204012 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0001052366394384022 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.00010555950238168325 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00010331439868120167 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.00010359062071074732 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00010149999299805853 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.00010173805931117386 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 9.979369751319659e-05 // Train Acc: 0.8423552806009721\n",
      "Val Loss: 0.0001000018862462391 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 9.819646260289679e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 9.838291424040852e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 9.670788210925122e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 9.687967454242012e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 9.532470128711071e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 9.548869023671035e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 9.404077274965646e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 9.4201924697369e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 9.284708889458566e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 9.30094236867841e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 9.173308670923186e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 9.189975528649732e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 9.06879724433686e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 9.08605421996485e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 8.970149455511768e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 8.988129398362203e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 8.876487739239586e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 8.895285880664067e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 8.787083684120722e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 8.806726579099301e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 8.70131920087821e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 8.721912050863135e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 8.618732879178058e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 8.64037309590706e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 8.538942653170178e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 8.561784637673885e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 8.461668572910066e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 8.485914209317839e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 8.386677090125371e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 8.412524667536755e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 8.313804180146042e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 8.34152880336412e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 8.242915104862014e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 8.272794359734027e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 8.17391550689739e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.206242471880449e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 8.106740399325728e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.141814151630653e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 8.041327627528285e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.079448280940679e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 7.977632480081049e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 8.019077524130063e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 7.915620855500946e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 7.960673703399317e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 7.855259124079313e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 7.90416811055779e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 7.796525758616445e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 7.849522090427027e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 7.739376338394401e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 7.796677957892165e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 7.683781261229463e-05 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 7.745589285729115e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [25:06<02:47, 167.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.15109693534570198 // Train Acc: 0.8276255707762558\n",
      "Val Loss: 0.09135310141877695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0009813603919029202 // Train Acc: 0.0\n",
      "Val Loss: 0.0008581628806000067 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.00043376690131871534 // Train Acc: 0.24304021210782148\n",
      "Val Loss: 0.0004091805079951882 // Val Acc: 0.3977272727272727\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0002673828834362145 // Train Acc: 0.585505965532479\n",
      "Val Loss: 0.0002542767747813328 // Val Acc: 0.8143939393939393\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0001980804541354497 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 0.0001972597245846621 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0001760050325830399 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.0001771301654199223 // Val Acc: 1.0984848484848484\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0001651271314880615 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00016665243324082854 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0001579390892365067 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00015967550041916018 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0001524065266336478 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00015425942176741295 // Val Acc: 1.0416666666666665\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00014775124605817904 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.0001496291882093911 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00014363728446971526 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.0001454688440637917 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00013989793089139765 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00014163847808958962 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00013643609527705798 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00013805926639840683 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00013318536866583318 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00013467925031859936 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00013009460391630908 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.0001314547824387608 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00012712292679169292 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00012834894587285817 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00012423877121434657 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00012533694561253387 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00012142040100842757 // Train Acc: 0.8280858742082781\n",
      "Val Loss: 0.00012240677163142456 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00011865529232391594 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00011955796481660483 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00011593843149443872 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00011679670086860741 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00011326952994023407 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00011412813116542317 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00011065031266128048 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00011155346007647246 // Val Acc: 0.8712121212121212\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00010808352827515504 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00010906733124019493 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00010557298910124738 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.00010666118158207444 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00010312467609299879 // Train Acc: 0.813816467815584\n",
      "Val Loss: 0.00010432785585924814 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00010074713127448364 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 0.0001020650671720928 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 9.844984664737468e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 9.987327040024948e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 9.624107515763122e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 9.775366290176118e-05 // Val Acc: 0.928030303030303\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 9.41249553466877e-05 // Train Acc: 0.813816467815584\n",
      "Val Loss: 9.570508965142918e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 9.209900769111637e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 9.372080577040006e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 9.015377092102891e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 9.178726365710397e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 8.827411985713468e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 8.98867139105939e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 8.644291295793033e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 8.800016287502579e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 8.464560668882818e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 8.611245145650834e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 8.287395881846404e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 8.421819276753178e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 8.112786895357344e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 8.232368848191321e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 7.941417950104506e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 8.044822464977518e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 7.774333978255469e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 7.861732162630439e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 7.612671740210911e-05 // Train Acc: 0.7995470614228899\n",
      "Val Loss: 7.685755911304361e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 7.457438695295523e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 7.519067915960807e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 7.309422676890586e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 7.363189906754997e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 7.16914711788522e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 7.218897656209513e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 7.036861666684468e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 7.086319624663288e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 6.912611926764677e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 6.965061944687147e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 6.796237573728761e-05 // Train Acc: 0.7852776550301959\n",
      "Val Loss: 6.854364477145613e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 6.687451749625425e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 6.753256033830853e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 6.585867796279278e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 6.660658178919799e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 6.491043682653482e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 6.575492704789874e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 6.402489712713531e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 6.496723286876328e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 6.319702072907611e-05 // Train Acc: 0.7710082486375018\n",
      "Val Loss: 6.42343398133314e-05 // Val Acc: 0.9848484848484848\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [27:54<00:00, 167.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1057, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1056, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.03524293081433832 // Train Acc: 0.0\n",
      "Val Loss: 0.03766020864029141 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.003710189374125721 // Train Acc: 0.1083485401459854\n",
      "Val Loss: 0.007497680938446566 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0028556895168369845 // Train Acc: 0.18818430656934307\n",
      "Val Loss: 0.005731261037427056 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0025744356495044857 // Train Acc: 0.24520985401459855\n",
      "Val Loss: 0.0050981677931678645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.002292575272950293 // Train Acc: 0.4676094890510949\n",
      "Val Loss: 0.004451349654974525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.002053082295044516 // Train Acc: 0.6101733576642335\n",
      "Val Loss: 0.0039567669189315945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0019809445923909775 // Train Acc: 0.6101733576642335\n",
      "Val Loss: 0.0038087364690213956 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0019400205125755109 // Train Acc: 0.6101733576642335\n",
      "Val Loss: 0.003703722942565732 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0019087403834684637 // Train Acc: 0.5987682481751825\n",
      "Val Loss: 0.0036079676116488 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0018809673286467555 // Train Acc: 0.5816605839416058\n",
      "Val Loss: 0.0035185602336081073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0018543678830198992 // Train Acc: 0.5759580291970803\n",
      "Val Loss: 0.0034492800786050364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.001829076695286898 // Train Acc: 0.5531478102189781\n",
      "Val Loss: 0.0034041541238205835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0018050420947235786 // Train Acc: 0.541742700729927\n",
      "Val Loss: 0.0033691084998495437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0017811723527149403 // Train Acc: 0.5303375912408759\n",
      "Val Loss: 0.0033357706920736853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0017564056730194736 // Train Acc: 0.5246350364963503\n",
      "Val Loss: 0.0033010895165395647 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0017300613508799817 // Train Acc: 0.5246350364963503\n",
      "Val Loss: 0.0032635761753601186 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.001702141794837594 // Train Acc: 0.4904197080291971\n",
      "Val Loss: 0.0032223909408511486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00167358794310298 // Train Acc: 0.4619069343065693\n",
      "Val Loss: 0.003176148272092071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0016455198174232472 // Train Acc: 0.4333941605839416\n",
      "Val Loss: 0.0031240348300129615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0016179578086895248 // Train Acc: 0.4219890510948905\n",
      "Val Loss: 0.0030685230569147013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.001589895141139766 // Train Acc: 0.40488138686131386\n",
      "Val Loss: 0.0030142298611976646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.001561113959730664 // Train Acc: 0.41628649635036497\n",
      "Val Loss: 0.0029653510547873073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0015335171870954941 // Train Acc: 0.42769160583941607\n",
      "Val Loss: 0.0029243935954154413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0015091098350213364 // Train Acc: 0.41058394160583944\n",
      "Val Loss: 0.0028928319499006167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0014881114003022905 // Train Acc: 0.39917883211678834\n",
      "Val Loss: 0.002871562309070107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0014698564918811839 // Train Acc: 0.35355839416058393\n",
      "Val Loss: 0.0028608574511428527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 257\n",
      "INFO: Validation loss did not improve in epoch 258\n",
      "INFO: Validation loss did not improve in epoch 259\n",
      "INFO: Validation loss did not improve in epoch 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [03:27<31:07, 207.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 261\n",
      "Early stopping after 261 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.037162658252012336 // Train Acc: 0.0\n",
      "Val Loss: 0.035018744001932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0035922898261467537 // Train Acc: 0.10264598540145986\n",
      "Val Loss: 0.007176968447097084 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0028187740918858162 // Train Acc: 0.23380474452554745\n",
      "Val Loss: 0.005693240645889412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0026204515071471058 // Train Acc: 0.2908302919708029\n",
      "Val Loss: 0.005271532059208874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0024870412622064984 // Train Acc: 0.41058394160583944\n",
      "Val Loss: 0.004988036135567681 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00231799501579435 // Train Acc: 0.5531478102189781\n",
      "Val Loss: 0.004589021733195028 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.002116554738132854 // Train Acc: 0.6215784671532847\n",
      "Val Loss: 0.004114259753431029 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.002019734005005601 // Train Acc: 0.5816605839416058\n",
      "Val Loss: 0.003938059079433408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.001951125164665082 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.003774466325172826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0019111975621575249 // Train Acc: 0.4904197080291971\n",
      "Val Loss: 0.0036877106279408667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0018843280864317806 // Train Acc: 0.4961222627737226\n",
      "Val Loss: 0.0036477487678091753 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0018623677729442036 // Train Acc: 0.5018248175182481\n",
      "Val Loss: 0.003625792807296795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0018437830491086432 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.003611724549794898 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.001827909346641977 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.003600793774239719 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0018141249216157525 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.003590353956336484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0018018588164516315 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.003578941502115306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0017906450816387877 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.00356579726383857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0017801299835723612 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.0035505315206725806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.001770056194309158 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.0035328971776727807 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.001760230812811164 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.003512722351040472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0017505076208499636 // Train Acc: 0.5018248175182481\n",
      "Val Loss: 0.0034898610354182034 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.001740773809256861 // Train Acc: 0.5018248175182481\n",
      "Val Loss: 0.0034642007667571306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.001730947769530333 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.0034356670769150645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0017209764661133139 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.003404253672616666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0017108372584629331 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.0033700789550921936 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0017005334471421015 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.003333441224907908 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0016900902424543738 // Train Acc: 0.5018248175182481\n",
      "Val Loss: 0.0032948701341143426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0016795447755964107 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.003255131164812209 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.001668928346536273 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.0032151412657078575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0016582553138186663 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.0031758454655680586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0016475155062921558 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.0031380775548955973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0016366751362484869 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.0031024707200498704 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.001625682688433971 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.0030694268575376447 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0016144851191193682 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.0030391740455182597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.001603047552050761 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.0030118194134796366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.001591346227163687 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.002987436977598597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0015794035946776331 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.0029661019284771208 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0015672766723954519 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.0029478050864246838 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0015551390509370945 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.002932377960806822 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.001543093972649178 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.0029195582131197784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0015312922523795338 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.002908908039577963 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.001519875117653771 // Train Acc: 0.4961222627737226\n",
      "Val Loss: 0.0028998613261672504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.001508956801484415 // Train Acc: 0.4904197080291971\n",
      "Val Loss: 0.0028917803908424344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0014986254327927749 // Train Acc: 0.4847171532846715\n",
      "Val Loss: 0.0028839888229199194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0014889382496719413 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.0028758289117831737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0014799218044413299 // Train Acc: 0.4733120437956204\n",
      "Val Loss: 0.002866810026770348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0014715761910311634 // Train Acc: 0.4676094890510949\n",
      "Val Loss: 0.002856656991164474 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0014638704196774605 // Train Acc: 0.45050182481751827\n",
      "Val Loss: 0.0028453518787179798 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.001456763083921152 // Train Acc: 0.4447992700729927\n",
      "Val Loss: 0.0028330610845895376 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0014501979084389945 // Train Acc: 0.43909671532846717\n",
      "Val Loss: 0.002820056787801578 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [10:04<42:31, 318.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.02521656423868326 // Train Acc: 0.0\n",
      "Val Loss: 0.03380404286743963 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.004010818098441984 // Train Acc: 0.34215328467153283\n",
      "Val Loss: 0.008024969295708133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0023471935805203952 // Train Acc: 0.541742700729927\n",
      "Val Loss: 0.004790902240475749 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0020397574766732353 // Train Acc: 0.4904197080291971\n",
      "Val Loss: 0.004134193401756313 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0019085745999443938 // Train Acc: 0.4733120437956204\n",
      "Val Loss: 0.003808112650671426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0018178821988362372 // Train Acc: 0.4219890510948905\n",
      "Val Loss: 0.003563778652973911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.001760149580747506 // Train Acc: 0.38777372262773724\n",
      "Val Loss: 0.0033958098227979943 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0017202918248439725 // Train Acc: 0.3478558394160584\n",
      "Val Loss: 0.0032978326095925536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.001686517546094876 // Train Acc: 0.3193430656934307\n",
      "Val Loss: 0.0032381782452028025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0016568059066887715 // Train Acc: 0.302235401459854\n",
      "Val Loss: 0.0031992214909919046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0016314218193829021 // Train Acc: 0.302235401459854\n",
      "Val Loss: 0.00317024568828535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0016100676602813122 // Train Acc: 0.2908302919708029\n",
      "Val Loss: 0.0031444930149625767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0015918745469798191 // Train Acc: 0.2908302919708029\n",
      "Val Loss: 0.0031186520936898887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.001575974475234525 // Train Acc: 0.2908302919708029\n",
      "Val Loss: 0.0030919216258232206 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0015617561988832292 // Train Acc: 0.2851277372262774\n",
      "Val Loss: 0.0030646814180620235 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0015488641389430438 // Train Acc: 0.2851277372262774\n",
      "Val Loss: 0.0030376169375171337 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0015371029640202296 // Train Acc: 0.26802007299270075\n",
      "Val Loss: 0.003011336794931113 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0015263457794356785 // Train Acc: 0.26802007299270075\n",
      "Val Loss: 0.0029862689296030163 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0015164754648166487 // Train Acc: 0.2623175182481752\n",
      "Val Loss: 0.0029625890105414916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0015073733079489567 // Train Acc: 0.239507299270073\n",
      "Val Loss: 0.0029403337770525146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0014989286713912042 // Train Acc: 0.23380474452554745\n",
      "Val Loss: 0.002919518465847325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0014910506201321878 // Train Acc: 0.23380474452554745\n",
      "Val Loss: 0.002900300280410139 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0014836613827754223 // Train Acc: 0.2281021897810219\n",
      "Val Loss: 0.002883050385314752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0014766785067352754 // Train Acc: 0.2281021897810219\n",
      "Val Loss: 0.00286830631803776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0014700033653042717 // Train Acc: 0.23380474452554745\n",
      "Val Loss: 0.002856640576604096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.001463517464015864 // Train Acc: 0.239507299270073\n",
      "Val Loss: 0.002848577968132518 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0014570741551543393 // Train Acc: 0.25661496350364965\n",
      "Val Loss: 0.002844545981668703 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 267\n",
      "INFO: Validation loss did not improve in epoch 268\n",
      "INFO: Validation loss did not improve in epoch 269\n",
      "INFO: Validation loss did not improve in epoch 270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [13:39<31:40, 271.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 271\n",
      "Early stopping after 271 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.041952796059449876 // Train Acc: 0.0\n",
      "Val Loss: 0.03272356268237619 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0038163356778163457 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007869357048013411 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.002640034847256148 // Train Acc: 0.2166970802919708\n",
      "Val Loss: 0.005291328481261563 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.002180570447852497 // Train Acc: 0.38207116788321166\n",
      "Val Loss: 0.004183816672165823 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0019707584629197737 // Train Acc: 0.4562043795620438\n",
      "Val Loss: 0.0038459483595729314 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0018789881685229606 // Train Acc: 0.4676094890510949\n",
      "Val Loss: 0.0037439779125099234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0018198332311236086 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.0036007425001384143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0017695055134704057 // Train Acc: 0.4619069343065693\n",
      "Val Loss: 0.003467915004447979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0017266667306023381 // Train Acc: 0.4447992700729927\n",
      "Val Loss: 0.003370374735609135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0016895384994320283 // Train Acc: 0.42769160583941607\n",
      "Val Loss: 0.0032969730570638445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0016578236137074717 // Train Acc: 0.3592609489051095\n",
      "Val Loss: 0.003237280309857691 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0016317581794083728 // Train Acc: 0.33074817518248173\n",
      "Val Loss: 0.0031924403087674256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.001610197400981779 // Train Acc: 0.3079379562043796\n",
      "Val Loss: 0.0031579939121216096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.001591863431779523 // Train Acc: 0.2908302919708029\n",
      "Val Loss: 0.003127733029781238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0015758493204121426 // Train Acc: 0.2908302919708029\n",
      "Val Loss: 0.0030983786547200427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0015615252058371076 // Train Acc: 0.2737226277372263\n",
      "Val Loss: 0.0030688809295979272 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0015484908012197188 // Train Acc: 0.26802007299270075\n",
      "Val Loss: 0.003039419075802845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0015365052498977638 // Train Acc: 0.25661496350364965\n",
      "Val Loss: 0.003011077969430891 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0015254008906057388 // Train Acc: 0.2509124087591241\n",
      "Val Loss: 0.0029851683836622056 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00151505433045149 // Train Acc: 0.2509124087591241\n",
      "Val Loss: 0.002962239850175512 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0015053724696538194 // Train Acc: 0.25661496350364965\n",
      "Val Loss: 0.0029419218087294963 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0014962927274689347 // Train Acc: 0.23380474452554745\n",
      "Val Loss: 0.0029234183996724073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.001487773400032999 // Train Acc: 0.2281021897810219\n",
      "Val Loss: 0.002905877461136483 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0014797844824718522 // Train Acc: 0.2281021897810219\n",
      "Val Loss: 0.002888548413894194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0014722849041291293 // Train Acc: 0.22239963503649635\n",
      "Val Loss: 0.002870875510031029 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0014652127398540292 // Train Acc: 0.2281021897810219\n",
      "Val Loss: 0.002852535183431909 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0014584989813443746 // Train Acc: 0.2281021897810219\n",
      "Val Loss: 0.002833419797859867 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0014520833607545092 // Train Acc: 0.2166970802919708\n",
      "Val Loss: 0.0028135772274040126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.001445925751656173 // Train Acc: 0.2166970802919708\n",
      "Val Loss: 0.0027931270323118523 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0014400017655638972 // Train Acc: 0.20529197080291972\n",
      "Val Loss: 0.0027722157589296866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0014342921057617867 // Train Acc: 0.21099452554744524\n",
      "Val Loss: 0.002751043956617222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0014287776674439681 // Train Acc: 0.22239963503649635\n",
      "Val Loss: 0.002729848145356621 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0014234287863719212 // Train Acc: 0.22239963503649635\n",
      "Val Loss: 0.002708939979236354 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0014182082690585558 // Train Acc: 0.22239963503649635\n",
      "Val Loss: 0.0026887688813183237 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0014130734567569011 // Train Acc: 0.2281021897810219\n",
      "Val Loss: 0.0026698652603526544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0014079856513304204 // Train Acc: 0.23380474452554745\n",
      "Val Loss: 0.002652821841526448 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0014029026006360222 // Train Acc: 0.239507299270073\n",
      "Val Loss: 0.0026383079153567772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0013977723255578045 // Train Acc: 0.239507299270073\n",
      "Val Loss: 0.0026271842361893505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0013925222295780844 // Train Acc: 0.239507299270073\n",
      "Val Loss: 0.002620562044766677 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 388\n",
      "INFO: Validation loss did not improve in epoch 389\n",
      "INFO: Validation loss did not improve in epoch 390\n",
      "INFO: Validation loss did not improve in epoch 391\n",
      "Epoch: 391\n",
      "Train Loss: 0.001387060014100832 // Train Acc: 0.239507299270073\n",
      "Val Loss: 0.002619914152412949 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [18:50<28:43, 287.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 392\n",
      "Early stopping after 392 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.055594662000445556 // Train Acc: 0.0\n",
      "Val Loss: 0.03905401658266783 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.003553007524668742 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.007081613252761171 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.002804754933586215 // Train Acc: 0.16537408759124086\n",
      "Val Loss: 0.0055690604015527405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0024600198205113693 // Train Acc: 0.41628649635036497\n",
      "Val Loss: 0.004793846959193402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.002177186784078282 // Train Acc: 0.5759580291970803\n",
      "Val Loss: 0.004107745860785465 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0020340170579793864 // Train Acc: 0.5816605839416058\n",
      "Val Loss: 0.003792610005749499 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.001971411285397478 // Train Acc: 0.5474452554744526\n",
      "Val Loss: 0.0036887441152323257 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0019280962165050806 // Train Acc: 0.5360401459854015\n",
      "Val Loss: 0.003614897558958653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0018898015425996822 // Train Acc: 0.5303375912408759\n",
      "Val Loss: 0.00355136622195406 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.001853642210448095 // Train Acc: 0.5246350364963503\n",
      "Val Loss: 0.003502283536363393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0018184164332225494 // Train Acc: 0.5246350364963503\n",
      "Val Loss: 0.00347002426176058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0017830955365201735 // Train Acc: 0.5246350364963503\n",
      "Val Loss: 0.003447717134429909 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0017472549260189364 // Train Acc: 0.5246350364963503\n",
      "Val Loss: 0.0034291355781640638 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0017136460462037855 // Train Acc: 0.5303375912408759\n",
      "Val Loss: 0.0034210751456318095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 132\n",
      "INFO: Validation loss did not improve in epoch 133\n",
      "INFO: Validation loss did not improve in epoch 134\n",
      "INFO: Validation loss did not improve in epoch 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [20:38<18:33, 222.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 136\n",
      "Early stopping after 136 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.08046283500513783 // Train Acc: 0.0\n",
      "Val Loss: 0.035671315046356004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.003772425384612934 // Train Acc: 0.017107664233576642\n",
      "Val Loss: 0.0077097877784741715 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0029552551620267245 // Train Acc: 0.08553832116788321\n",
      "Val Loss: 0.005986584045732503 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.002690154437881123 // Train Acc: 0.20529197080291972\n",
      "Val Loss: 0.00539132217904005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0025074301242070393 // Train Acc: 0.24520985401459855\n",
      "Val Loss: 0.004948702815454453 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0023157858206078164 // Train Acc: 0.2737226277372263\n",
      "Val Loss: 0.004491597140098319 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0021407300565130694 // Train Acc: 0.35355839416058393\n",
      "Val Loss: 0.004146703529199038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0020321607821808794 // Train Acc: 0.38777372262773724\n",
      "Val Loss: 0.003981379973565173 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.001956167517675607 // Train Acc: 0.41058394160583944\n",
      "Val Loss: 0.003879938106846941 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0018870708694401959 // Train Acc: 0.40488138686131386\n",
      "Val Loss: 0.0037475782378083643 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0018304399795088632 // Train Acc: 0.4333941605839416\n",
      "Val Loss: 0.0036043156458831884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0017942914526920637 // Train Acc: 0.4619069343065693\n",
      "Val Loss: 0.003520699104924193 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0017672661495346995 // Train Acc: 0.4847171532846715\n",
      "Val Loss: 0.003461366892545758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0017422758771788948 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.003407463023904711 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0017166858487522457 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.0033531615969396252 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0016900527274243944 // Train Acc: 0.4676094890510949\n",
      "Val Loss: 0.0032985700527206063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0016627140910342169 // Train Acc: 0.4619069343065693\n",
      "Val Loss: 0.0032480127119924873 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0016355929052808585 // Train Acc: 0.4676094890510949\n",
      "Val Loss: 0.0032051158058867954 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0016098271746929382 // Train Acc: 0.4733120437956204\n",
      "Val Loss: 0.003169861259212827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.001586063359806167 // Train Acc: 0.4733120437956204\n",
      "Val Loss: 0.0031400657402734984 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0015644626556907358 // Train Acc: 0.4676094890510949\n",
      "Val Loss: 0.003113989975056885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.001544951852583902 // Train Acc: 0.4562043795620438\n",
      "Val Loss: 0.0030906884657109484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0015273282087880041 // Train Acc: 0.43909671532846717\n",
      "Val Loss: 0.0030694332090206444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0015113473759313415 // Train Acc: 0.4333941605839416\n",
      "Val Loss: 0.0030495988389970187 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0014967342165247137 // Train Acc: 0.42769160583941607\n",
      "Val Loss: 0.0030307533229728613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0014832353298761584 // Train Acc: 0.4219890510948905\n",
      "Val Loss: 0.0030126803390243474 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.001470628350426211 // Train Acc: 0.4219890510948905\n",
      "Val Loss: 0.0029952744645176128 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0014587291836678361 // Train Acc: 0.41058394160583944\n",
      "Val Loss: 0.0029783498509513106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0014473977592381472 // Train Acc: 0.41628649635036497\n",
      "Val Loss: 0.002961440266945454 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.001436550864884445 // Train Acc: 0.41058394160583944\n",
      "Val Loss: 0.0029436996059410054 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.001426148470618102 // Train Acc: 0.40488138686131386\n",
      "Val Loss: 0.0029239700216909543 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0014161869683939718 // Train Acc: 0.40488138686131386\n",
      "Val Loss: 0.0029008577454506476 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0014066910053989773 // Train Acc: 0.39347627737226276\n",
      "Val Loss: 0.0028728788448086776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0013977246633219919 // Train Acc: 0.39347627737226276\n",
      "Val Loss: 0.0028388927081216347 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0013894233379156556 // Train Acc: 0.38207116788321166\n",
      "Val Loss: 0.0027987815624596003 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0013819354254623129 // Train Acc: 0.38207116788321166\n",
      "Val Loss: 0.002754069120034247 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.001375354115265323 // Train Acc: 0.38207116788321166\n",
      "Val Loss: 0.0027078186291927364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0013696178832876026 // Train Acc: 0.40488138686131386\n",
      "Val Loss: 0.002663892781129107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.001364546550390169 // Train Acc: 0.41058394160583944\n",
      "Val Loss: 0.0026264671246548567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0013599472067056073 // Train Acc: 0.4333941605839416\n",
      "Val Loss: 0.002600473113809986 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 400\n",
      "INFO: Validation loss did not improve in epoch 401\n",
      "Epoch: 401\n",
      "Train Loss: 0.0013556286883042287 // Train Acc: 0.4447992700729927\n",
      "Val Loss: 0.0025928650232826304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 402\n",
      "INFO: Validation loss did not improve in epoch 403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [25:59<17:03, 255.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 404\n",
      "Early stopping after 404 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.15638694388071334 // Train Acc: 0.19388686131386862\n",
      "Val Loss: 0.0444718576310312 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.004349940747886609 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008589350344503628 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0026116905942715173 // Train Acc: 0.34215328467153283\n",
      "Val Loss: 0.005155600348884678 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0022511354743545097 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.004301270261160372 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0021548191208591435 // Train Acc: 0.5018248175182481\n",
      "Val Loss: 0.004104151280925554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0021129042887858395 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.004067727659061989 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.002080480314176039 // Train Acc: 0.5189324817518248\n",
      "Val Loss: 0.00404115511200336 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0020482158315179777 // Train Acc: 0.5246350364963503\n",
      "Val Loss: 0.004008860834052458 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.002014325894584797 // Train Acc: 0.5360401459854015\n",
      "Val Loss: 0.003970978706253364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0019783610579216545 // Train Acc: 0.5360401459854015\n",
      "Val Loss: 0.003930898366139873 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0019400849993679295 // Train Acc: 0.5360401459854015\n",
      "Val Loss: 0.003892246465993059 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.001899777042670885 // Train Acc: 0.5246350364963503\n",
      "Val Loss: 0.0038502936812994234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0018593199916709091 // Train Acc: 0.5189324817518248\n",
      "Val Loss: 0.0037938698010026094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0018225008963224229 // Train Acc: 0.5189324817518248\n",
      "Val Loss: 0.0037218929295811582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0017918815685374098 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.003647597349139259 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0017663802291613105 // Train Acc: 0.5189324817518248\n",
      "Val Loss: 0.003579392532735844 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0017439711413529564 // Train Acc: 0.5189324817518248\n",
      "Val Loss: 0.0035173702909720734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0017235005746900833 // Train Acc: 0.5246350364963503\n",
      "Val Loss: 0.0034603296731160406 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00170433067691803 // Train Acc: 0.5360401459854015\n",
      "Val Loss: 0.003407098852810176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0016860649153174038 // Train Acc: 0.5360401459854015\n",
      "Val Loss: 0.0033568199484280367 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0016684610260849994 // Train Acc: 0.5474452554744526\n",
      "Val Loss: 0.0033091597313828327 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0016514080384396523 // Train Acc: 0.5531478102189781\n",
      "Val Loss: 0.0032643409452729802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0016349052203908166 // Train Acc: 0.5645529197080292\n",
      "Val Loss: 0.003222935918636401 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0016190197347232308 // Train Acc: 0.5759580291970803\n",
      "Val Loss: 0.00318539487403434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0016038269606498785 // Train Acc: 0.5702554744525548\n",
      "Val Loss: 0.0031516139073680866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0015893582604690811 // Train Acc: 0.5759580291970803\n",
      "Val Loss: 0.0031208035036209315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0015755899022111327 // Train Acc: 0.5759580291970803\n",
      "Val Loss: 0.003091700335704338 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0015624608540880205 // Train Acc: 0.5702554744525548\n",
      "Val Loss: 0.003062995825209381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.001549912948635841 // Train Acc: 0.5759580291970803\n",
      "Val Loss: 0.0030336964717956588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0015379056706206692 // Train Acc: 0.5759580291970803\n",
      "Val Loss: 0.0030033338407520205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.001526425384433231 // Train Acc: 0.5645529197080292\n",
      "Val Loss: 0.002971957322990741 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0015154642666731265 // Train Acc: 0.5645529197080292\n",
      "Val Loss: 0.002939980578102062 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0015049533803438908 // Train Acc: 0.5645529197080292\n",
      "Val Loss: 0.0029080771577462334 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0014947081190722156 // Train Acc: 0.5588503649635036\n",
      "Val Loss: 0.0028772736704387866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0014844094986492431 // Train Acc: 0.5702554744525548\n",
      "Val Loss: 0.0028493092652163744 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0014736054147422453 // Train Acc: 0.5531478102189781\n",
      "Val Loss: 0.002827347516465713 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0014617139429529962 // Train Acc: 0.541742700729927\n",
      "Val Loss: 0.0028164982384837724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 364\n",
      "INFO: Validation loss did not improve in epoch 365\n",
      "INFO: Validation loss did not improve in epoch 366\n",
      "INFO: Validation loss did not improve in epoch 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [30:51<13:23, 267.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 368\n",
      "Early stopping after 368 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.06874902631380479 // Train Acc: 0.0\n",
      "Val Loss: 0.0407442320138216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0038769442234543087 // Train Acc: 0.06843065693430657\n",
      "Val Loss: 0.007776101801188334 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0028425122393052143 // Train Acc: 0.13686131386861314\n",
      "Val Loss: 0.00562485953902497 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.002493451930796872 // Train Acc: 0.2908302919708029\n",
      "Val Loss: 0.00486632767612772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.002136440482375617 // Train Acc: 0.5588503649635036\n",
      "Val Loss: 0.004210144188939868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.002062952462743639 // Train Acc: 0.5816605839416058\n",
      "Val Loss: 0.004034034128862378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0020235284067309897 // Train Acc: 0.5816605839416058\n",
      "Val Loss: 0.003926893516429974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.001995078936880106 // Train Acc: 0.5873631386861314\n",
      "Val Loss: 0.003855708027806352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.001970234627934873 // Train Acc: 0.593065693430657\n",
      "Val Loss: 0.0037986064511214327 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0019442925393138672 // Train Acc: 0.5987682481751825\n",
      "Val Loss: 0.0037362148321014553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0019137498909873288 // Train Acc: 0.5987682481751825\n",
      "Val Loss: 0.003655101703342927 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0018778542500088268 // Train Acc: 0.604470802919708\n",
      "Val Loss: 0.003563689471989432 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0018407717082608256 // Train Acc: 0.6215784671532847\n",
      "Val Loss: 0.0034901230212520153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0018081379173724805 // Train Acc: 0.5987682481751825\n",
      "Val Loss: 0.003444185244220802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.001781252818917673 // Train Acc: 0.5987682481751825\n",
      "Val Loss: 0.0034140495151993543 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0017583511195820671 // Train Acc: 0.5987682481751825\n",
      "Val Loss: 0.0033876309000120007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.001737687855913371 // Train Acc: 0.5816605839416058\n",
      "Val Loss: 0.003359230320818503 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0017183190234490657 // Train Acc: 0.5702554744525548\n",
      "Val Loss: 0.003327704897588667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0016999230437296405 // Train Acc: 0.5702554744525548\n",
      "Val Loss: 0.003294048139222843 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0016824951373803536 // Train Acc: 0.5702554744525548\n",
      "Val Loss: 0.003259915317518308 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0016660874571907632 // Train Acc: 0.5759580291970803\n",
      "Val Loss: 0.003226706330144011 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0016506401106377853 // Train Acc: 0.5759580291970803\n",
      "Val Loss: 0.003195018576019827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0016359445327283418 // Train Acc: 0.5645529197080292\n",
      "Val Loss: 0.0031645061201689873 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0016217146733579018 // Train Acc: 0.5645529197080292\n",
      "Val Loss: 0.0031340933089856714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0016076814867066428 // Train Acc: 0.5588503649635036\n",
      "Val Loss: 0.0031026260762968484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0015936383520288905 // Train Acc: 0.5474452554744526\n",
      "Val Loss: 0.0030695843743160367 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0015794326788311388 // Train Acc: 0.541742700729927\n",
      "Val Loss: 0.003035070960108629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.001564945403967022 // Train Acc: 0.5246350364963503\n",
      "Val Loss: 0.0029992405718932036 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0015501577271520646 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.0029618917478193693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0015353121312581986 // Train Acc: 0.4961222627737226\n",
      "Val Loss: 0.002922766306095154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0015210082582777504 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.00288240727626116 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.001507417169452881 // Train Acc: 0.4562043795620438\n",
      "Val Loss: 0.002842475105460514 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0014944916481537112 // Train Acc: 0.43909671532846717\n",
      "Val Loss: 0.0028045539545607477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.001482029827271272 // Train Acc: 0.39347627737226276\n",
      "Val Loss: 0.0027699869687614195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0014699914800938955 // Train Acc: 0.38777372262773724\n",
      "Val Loss: 0.0027414904308149263 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0014585437951290837 // Train Acc: 0.38777372262773724\n",
      "Val Loss: 0.002723361754118848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.001447931316701494 // Train Acc: 0.35355839416058393\n",
      "Val Loss: 0.002718042881871738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 362\n",
      "INFO: Validation loss did not improve in epoch 363\n",
      "INFO: Validation loss did not improve in epoch 364\n",
      "INFO: Validation loss did not improve in epoch 365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [35:41<09:09, 274.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 366\n",
      "Early stopping after 366 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.04848554669148594 // Train Acc: 0.0\n",
      "Val Loss: 0.03411115607356324 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.003380373600521125 // Train Acc: 0.005702554744525547\n",
      "Val Loss: 0.006748695874942795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0028292976125038525 // Train Acc: 0.07413321167883212\n",
      "Val Loss: 0.005575591602417476 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.002605149709288393 // Train Acc: 0.2166970802919708\n",
      "Val Loss: 0.005101420605034732 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.002429468279291687 // Train Acc: 0.302235401459854\n",
      "Val Loss: 0.004760807030834258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0022710526476803885 // Train Acc: 0.3079379562043796\n",
      "Val Loss: 0.0044623800443814084 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0021280083902326563 // Train Acc: 0.3364507299270073\n",
      "Val Loss: 0.0041975061107865155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0020303714071065026 // Train Acc: 0.3478558394160584\n",
      "Val Loss: 0.003990944748853936 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.001981459062103136 // Train Acc: 0.35355839416058393\n",
      "Val Loss: 0.003872990351416828 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0019543241095209976 // Train Acc: 0.35355839416058393\n",
      "Val Loss: 0.003812881316715742 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0019335040775517317 // Train Acc: 0.36496350364963503\n",
      "Val Loss: 0.0037759407628875446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0019154761667134125 // Train Acc: 0.36496350364963503\n",
      "Val Loss: 0.0037476650254307862 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0018992590978923279 // Train Acc: 0.37636861313868614\n",
      "Val Loss: 0.0037227750838022024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.001884366930578729 // Train Acc: 0.37636861313868614\n",
      "Val Loss: 0.0036989391459535587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.001870270680945688 // Train Acc: 0.38207116788321166\n",
      "Val Loss: 0.003674518912756706 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0018563167200791612 // Train Acc: 0.38207116788321166\n",
      "Val Loss: 0.0036484952125807896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.001841778964627756 // Train Acc: 0.41058394160583944\n",
      "Val Loss: 0.003621129860060618 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0018261583780816307 // Train Acc: 0.41058394160583944\n",
      "Val Loss: 0.003594590655719752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0018094230281481929 // Train Acc: 0.4219890510948905\n",
      "Val Loss: 0.0035704917322351214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0017916786292735993 // Train Acc: 0.41058394160583944\n",
      "Val Loss: 0.003546805583241889 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.001772992420294934 // Train Acc: 0.40488138686131386\n",
      "Val Loss: 0.0035206745310193475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0017539277489023156 // Train Acc: 0.4219890510948905\n",
      "Val Loss: 0.003490933429395013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0017353200475594163 // Train Acc: 0.42769160583941607\n",
      "Val Loss: 0.0034590887371450663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0017172489622109663 // Train Acc: 0.43909671532846717\n",
      "Val Loss: 0.003426346344092642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0016993481387584139 // Train Acc: 0.43909671532846717\n",
      "Val Loss: 0.0033922890177243113 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.001681587388609669 // Train Acc: 0.42769160583941607\n",
      "Val Loss: 0.0033568377839401364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0016643643185575356 // Train Acc: 0.4333941605839416\n",
      "Val Loss: 0.0033206124824252635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0016482599795202004 // Train Acc: 0.43909671532846717\n",
      "Val Loss: 0.0032843179600176345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.001633735882322496 // Train Acc: 0.4447992700729927\n",
      "Val Loss: 0.003248395290403791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0016209237224677426 // Train Acc: 0.4447992700729927\n",
      "Val Loss: 0.0032133244385477155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0016096537704862073 // Train Acc: 0.45050182481751827\n",
      "Val Loss: 0.003179792326751768 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0015996050049739263 // Train Acc: 0.4619069343065693\n",
      "Val Loss: 0.0031483951795106646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0015904247096058212 // Train Acc: 0.4847171532846715\n",
      "Val Loss: 0.003119454508551451 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0015818145258664868 // Train Acc: 0.4847171532846715\n",
      "Val Loss: 0.003093034200205961 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0015735533336598306 // Train Acc: 0.4847171532846715\n",
      "Val Loss: 0.0030690283045474 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0015654909507280988 // Train Acc: 0.4847171532846715\n",
      "Val Loss: 0.0030471345714485165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0015575239668067257 // Train Acc: 0.4847171532846715\n",
      "Val Loss: 0.003026895858484375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0015495822404693544 // Train Acc: 0.4847171532846715\n",
      "Val Loss: 0.003007826063206748 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0015416149394197924 // Train Acc: 0.4847171532846715\n",
      "Val Loss: 0.0029895176720695898 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.001533583518343168 // Train Acc: 0.4847171532846715\n",
      "Val Loss: 0.0029716894119594466 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.001525461030836675 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.0029541659579776667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.001517229199230082 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.0029368001577334806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0015088754882832077 // Train Acc: 0.4847171532846715\n",
      "Val Loss: 0.002919450368020026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0015003941359753525 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.002901988543967223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0014917876654771713 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.0028842906262177754 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0014830709987819339 // Train Acc: 0.4733120437956204\n",
      "Val Loss: 0.0028662708012715857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0014742728375574726 // Train Acc: 0.4733120437956204\n",
      "Val Loss: 0.002847920014413402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0014654392569381522 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.002829283422804164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0014566311289927665 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.0028104867293115925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.00144792428762854 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.0027917311629554365 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [42:17<05:12, 312.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.0865845432943481 // Train Acc: 0.0\n",
      "Val Loss: 0.03904098020318676 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0038862139955086524 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.007961414641399375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0029281081183963067 // Train Acc: 0.09124087591240876\n",
      "Val Loss: 0.005874894922325278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.002695151839109374 // Train Acc: 0.19958941605839417\n",
      "Val Loss: 0.005349962958408629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0025587189806187413 // Train Acc: 0.21099452554744524\n",
      "Val Loss: 0.005022937808569302 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0024143500031375905 // Train Acc: 0.25661496350364965\n",
      "Val Loss: 0.004649845342261388 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0022510350781421084 // Train Acc: 0.2851277372262774\n",
      "Val Loss: 0.004304189121980658 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0020589887006506775 // Train Acc: 0.39347627737226276\n",
      "Val Loss: 0.004044451107106665 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.001962680799839742 // Train Acc: 0.45050182481751827\n",
      "Val Loss: 0.003920564328676418 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0019153318230929889 // Train Acc: 0.4619069343065693\n",
      "Val Loss: 0.003858534321325886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0018784530307295864 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.0038090558715767283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0018496760931688133 // Train Acc: 0.4904197080291971\n",
      "Val Loss: 0.003765008573228603 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0018286498741036023 // Train Acc: 0.4904197080291971\n",
      "Val Loss: 0.0037247696651748438 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.001811681961083919 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.003684981804861523 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.001795851089325109 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.003643784497398883 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0017799673937289784 // Train Acc: 0.4904197080291971\n",
      "Val Loss: 0.0036013312672045737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0017637359092044337 // Train Acc: 0.4904197080291971\n",
      "Val Loss: 0.003558605214279583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0017471102270516304 // Train Acc: 0.4676094890510949\n",
      "Val Loss: 0.003516121849636821 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0017299221280599386 // Train Acc: 0.4733120437956204\n",
      "Val Loss: 0.003473130876527113 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0017118382210800564 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.003427938054836191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0016926060849524539 // Train Acc: 0.4904197080291971\n",
      "Val Loss: 0.0033789741368416477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0016724357328513199 // Train Acc: 0.4904197080291971\n",
      "Val Loss: 0.003325694295413354 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0016520877704823077 // Train Acc: 0.4904197080291971\n",
      "Val Loss: 0.003269403726067942 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0016324156186670654 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.0032132114146781318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0016139223795625496 // Train Acc: 0.5189324817518248\n",
      "Val Loss: 0.0031601581585538737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0015966468009690183 // Train Acc: 0.5303375912408759\n",
      "Val Loss: 0.0031113063827540506 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0015803014041383685 // Train Acc: 0.5303375912408759\n",
      "Val Loss: 0.0030660506848054114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0015644889689989266 // Train Acc: 0.5246350364963503\n",
      "Val Loss: 0.0030232237391721677 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0015488324514837692 // Train Acc: 0.5246350364963503\n",
      "Val Loss: 0.0029816128594307777 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0015330294132037542 // Train Acc: 0.5246350364963503\n",
      "Val Loss: 0.0029402825320550404 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0015169618740812944 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.0028989537540302776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.001500766806361919 // Train Acc: 0.5132299270072993\n",
      "Val Loss: 0.0028582043990301076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0014847528694913873 // Train Acc: 0.5075273722627737\n",
      "Val Loss: 0.002818952239228084 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0014691439202950969 // Train Acc: 0.5018248175182481\n",
      "Val Loss: 0.002782238887020332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0014540077910653825 // Train Acc: 0.5018248175182481\n",
      "Val Loss: 0.0027495172586949434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.001439352139628137 // Train Acc: 0.479014598540146\n",
      "Val Loss: 0.002722693798953996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0014253055176224043 // Train Acc: 0.4733120437956204\n",
      "Val Loss: 0.002703609720543575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0014121535776238667 // Train Acc: 0.4676094890510949\n",
      "Val Loss: 0.0026929126658668634 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.001400175427125846 // Train Acc: 0.4619069343065693\n",
      "Val Loss: 0.0026895083531545583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 383\n",
      "INFO: Validation loss did not improve in epoch 384\n",
      "INFO: Validation loss did not improve in epoch 385\n",
      "INFO: Validation loss did not improve in epoch 386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [47:25<00:00, 284.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 387\n",
      "Early stopping after 387 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictive performance\n",
    "predictive_results = predictive_evaluation(\n",
    "    data_train_real=data_train_real_numpy, \n",
    "    data_test_real=data_test_real_numpy,\n",
    "    data_syn=data_syn_numpy, \n",
    "    hyperparameters=hyperparameters, \n",
    "    include_baseline=True, \n",
    "    verbose=True)\n",
    "\n",
    "# save results\n",
    "bidirectionality = \"bi\" if hyperparameters[\"bidirectional\"] else 'no_bi'\n",
    "predictive_results.to_csv(DATA_FOLDER / f\"results_{syn_data_type}_{hyperparameters['num_epochs']}_{hyperparameters['num_evaluation_runs']}_{bidirectionality}.csv\", index=False)\n",
    "\n",
    "# split in mse and mae results\n",
    "mse_results = predictive_results.loc[predictive_results['Metric'] == 'MSE']\n",
    "mae_results = predictive_results.loc[predictive_results['Metric'] == 'MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x17532130a40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mse_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mse_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MSE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mae_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mae_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MAE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2*1e06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_data_augmentation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
