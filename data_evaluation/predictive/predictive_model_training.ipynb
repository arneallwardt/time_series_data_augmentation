{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Füge das übergeordnete Verzeichnis zu sys.path hinzu\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "from utilities import split_data_into_sequences, load_sequential_time_series, reconstruct_sequential_data, Scaler, extract_features_and_targets_reg, get_discriminative_test_performance\n",
    "from data_evaluation.visual.visual_evaluation import visual_evaluation\n",
    "from predictive_evaluation import predictive_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../../data\")\n",
    "REAL_DATA_FOLDER = DATA_FOLDER / \"real\"\n",
    "SYNTHETIC_DATA_FOLDER = DATA_FOLDER / \"synthetic\" / \"usable\" / \"1y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of loading data\n",
    "- Laden der Originaldaten: als pd dataframe \n",
    "- Laden der synthetischen, sequentiellen Daten: als np array (GAN, (V)AE)\n",
    "- Laden der synthetischen, sequentiellen Daten: als pd dataframe (brownian, algorithmit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible types: 'timegan_lstm', 'timegan_gru', 'jitter', 'timewarp', 'autoencoder', 'vae'\n",
    "syn_data_type = 'jitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " syn data:\n",
      "\n",
      "       traffic_volume         temp      rain_1h      snow_1h   clouds_all\n",
      "count     8759.000000  8759.000000  8759.000000  8759.000000  8759.000000\n",
      "mean      3242.999554   282.195389     0.121158     0.000483    44.847967\n",
      "std       1954.971277    12.166623     0.900317     0.006140    38.883359\n",
      "min          0.000000   242.975253     0.000000     0.000000     0.000000\n",
      "25%       1266.647637   273.603328     0.000000     0.000000     2.874516\n",
      "50%       3413.554372   283.584600     0.003998     0.000016    40.097688\n",
      "75%       4869.564219   292.030356     0.069818     0.000425    87.130849\n",
      "max       7355.044574   309.647461    42.019570     0.250347   104.283866\n",
      "\n",
      "\n",
      "real train data:\n",
      "\n",
      "       traffic_volume         temp      rain_1h      snow_1h   clouds_all\n",
      "count     8759.000000  8759.000000  8759.000000  8759.000000  8759.000000\n",
      "mean      3244.668912   282.208136     0.086792     0.000233    44.397306\n",
      "std       1946.247953    12.114907     0.901360     0.006145    39.195308\n",
      "min          0.000000   243.390000     0.000000     0.000000     0.000000\n",
      "25%       1252.500000   273.605500     0.000000     0.000000     1.000000\n",
      "50%       3402.000000   283.650000     0.000000     0.000000    40.000000\n",
      "75%       4849.500000   292.060000     0.000000     0.000000    90.000000\n",
      "max       7260.000000   307.330000    42.000000     0.250000   100.000000\n",
      "\n",
      "\n",
      "real test data:\n",
      "\n",
      "       traffic_volume         temp  rain_1h  snow_1h   clouds_all\n",
      "count     2135.000000  2135.000000   2135.0   2135.0  2135.000000\n",
      "mean      3325.263700   270.553730      0.0      0.0    45.065105\n",
      "std       1996.851023     7.864566      0.0      0.0    40.781402\n",
      "min        216.000000   248.660000      0.0      0.0     0.000000\n",
      "25%       1222.500000   265.735000      0.0      0.0     1.000000\n",
      "50%       3563.000000   271.550000      0.0      0.0    40.000000\n",
      "75%       4946.000000   275.680000      0.0      0.0    90.000000\n",
      "max       7280.000000   290.150000      0.0      0.0    92.000000\n"
     ]
    }
   ],
   "source": [
    "# Load real time series\n",
    "data_train_real_df = pd.read_csv(REAL_DATA_FOLDER/'mitv_prep_1y.csv')\n",
    "data_train_real_numpy = dc(data_train_real_df).to_numpy()\n",
    "\n",
    "data_test_real_df = pd.read_csv(REAL_DATA_FOLDER/'mitv_prep_3mo.csv')\n",
    "data_test_real_numpy = dc(data_test_real_df).to_numpy()\n",
    "\n",
    "if syn_data_type == 'timegan_lstm':\n",
    "    # load sequential data (which should already be scaled)\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_lstm_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'timegan_gru':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_gru_unscaled.csv', shape=(28499, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'autoencoder':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28478_12_5_lstm_autoencoder_unscaled_15.csv', shape=(28478, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'vae':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28511_12_5_lstm_vae_unscaled.csv', shape=(28511, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'jitter':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'jittered_01.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "elif syn_data_type == 'timewarp':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'time_warped.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "# Loot at real and syn data\n",
    "df = pd.DataFrame(data_syn_numpy.reshape(-1, data_syn_numpy.shape[-1]), columns=data_train_real_df.columns)\n",
    "\n",
    "print('\\n\\n syn data:\\n')\n",
    "print(df.describe())\n",
    "\n",
    "print('\\n\\nreal train data:\\n')\n",
    "print(data_train_real_df.describe())\n",
    "\n",
    "print('\\n\\nreal test data:\\n')\n",
    "print(data_test_real_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Predictive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"seq_len\": 12,\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 32,\n",
    "    \"hidden_size\": 12,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"num_evaluation_runs\": 10,\n",
    "    \"num_epochs\": 400,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETERS:\n",
      "seq_len :  12\n",
      "lr :  0.0001\n",
      "batch_size :  32\n",
      "hidden_size :  12\n",
      "num_layers :  1\n",
      "bidirectional :  True\n",
      "num_evaluation_runs :  10\n",
      "num_epochs :  400\n",
      "device :  cpu\n",
      "Synthetic Data is sequential: False\n",
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1057, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1056, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.38202117769605054 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.2727571666131125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03217184833203354 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03578124299426289 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012573079783877316 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014087841387235504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01079159299673064 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011790921480175765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009882438267210676 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010540508767863846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009212761836969396 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009553630521301837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008649695993071409 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008797207556199282 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00821399425554096 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00836436208063627 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007934837065463763 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00814770523335456 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007670645682560959 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007880782369551632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007209297060545017 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007368013927000849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006857780029276644 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007035318190259311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006666185276329613 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006841208255055416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006520402891583482 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006740319659002125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006392821006351117 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006685133266728371 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006261807972188006 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0066234735320048295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006111076779758734 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006512891843586284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0059430254997182505 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006362414891448091 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005783086600135604 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006240422104704468 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005651063247529423 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00618036901441348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00554336944380843 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006152150912812966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005448647341385049 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006125113666605423 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005360161612425543 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006093670497648418 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0052760695141271085 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006057401201413835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005197330959647703 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0060135192607584246 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005125299471270728 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005958468242384055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005060662518435559 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005892162498853663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0050035096920795575 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005818608648362844 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0049535434801015925 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005743714333648849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004910135944841266 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005671853387920076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004872399551392853 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005605231744565946 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004839326825097414 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0055444692877833456 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004809980623782646 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005489369563292712 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0047835861653494 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005439539190025672 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004759538900464039 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005394400213845074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004737382938228819 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005353378690094413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004716775041025975 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005315946832792286 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004697453974147946 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005281634022728266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00467921921836355 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0052500273826915555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004661913880446151 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.005220765816321706 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive\\predictive_evaluation.py:277: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([{'Model': evaluation_method, 'Metric': 'MAE', 'Error': mae}])], ignore_index=True)\n",
      " 10%|█         | 1/10 [02:45<24:46, 165.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.15671525440131226 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.1183478479301009 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02187787253158099 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.023762117934358472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011961495226340872 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013048997813123553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010232319467765599 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01095890492433682 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00927824251282362 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009658767139840433 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008543861196308403 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008758942288456155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007861370469331334 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008147688185357872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007465602187218865 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007765477112330058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007270090241175499 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007590182098176549 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007120241050312744 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007501948850832003 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006980666666407219 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007435506031684139 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006844286092669179 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007372867028393289 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006714683402088356 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007311591775813962 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006594668491743505 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00724740673596149 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006481167732017373 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007177042706376489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0063684925221987616 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007106104037067031 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006252043787899841 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007045228226000772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006131103902164656 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006993776454371126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0060119056852405255 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006937319392283612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005905653009306721 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006868156748275985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005818469034351517 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006793143357807661 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005746803130448055 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006719477495233364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005684721924323779 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006650055272449904 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005628687060645435 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006585101110805922 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005577213616096788 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006523581084740513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005529779361316213 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006464536205920226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005486180541023992 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006407517706975341 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005446249880848357 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0063523822601008065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.00540975555964494 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006299166796345483 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005376395974631156 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006247952101094758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005345828430115307 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006198658244958257 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005317703720494887 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006151178447694024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005291681462239447 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006105498760007322 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0052674609528732125 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006061384920030832 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005244778474728257 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00601868866258027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005223396310266651 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00597720398047172 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005203114304586184 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00593673693947494 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00518376423159859 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005897115991341279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005165195780004082 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0058581948704908 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005147280004113423 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005819824613247286 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [05:26<21:41, 162.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.106816287465176 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.09828081058666986 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.027717284790247026 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.02999363450662178 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012142107172369495 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01352470345548628 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010812445437488058 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01164776981150841 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009998685782299425 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010536485255750664 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009069603262384198 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009516776255194974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008312223926543455 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008915041956831427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00779155817224447 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008403890252606395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007235373132845156 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00781791067600031 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006892905414213229 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007487303402740508 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006703274132918403 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007355164599550121 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006560476698630564 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007281651186263736 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006439920574304287 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007223326192401788 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006330273899310914 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007164560463827322 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006231024418192759 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007104357793543707 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006141341132465342 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007041466840104584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006059230136154831 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00697430650539258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005982939185350317 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00690249409800505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005910655942041917 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0068264518083785385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005840703709914356 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006747436141321326 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005772027453869908 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0066665405585594915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005704628259460204 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006585137644673095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005639604464883706 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006504625328542555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005578241616072827 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006425837048000711 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00552119763634324 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006350340143613079 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005468539289882012 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006280878141029354 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0054200852632243866 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006219524499850676 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005375707557724014 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00616658031118705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005335346496082761 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00612055478097104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0052987265754932955 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006079154643777977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00526511841147703 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006040510742048568 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005233551047990493 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006003436084617586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005203236638394993 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005967310506521779 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005173711069854466 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005931418875287122 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005144775050809526 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005895108703578658 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005116362511953611 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005858005058732541 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005088471513951918 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0058200546362272955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005061129553763563 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0057814707294763885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005034381872974336 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005742667907136767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005008276447443033 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005704092916429919 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [08:08<18:58, 162.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.07392983271801559 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.07297869377276477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01647482799998328 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01815970244762652 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011864672168319447 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01288605177336756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010046007830554442 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0109021467026597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0089324113350348 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010002604398109457 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008320692431667725 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009398336077163763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007764568608883007 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008558923816856216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007176340205839625 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007664516328505296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006871220159062259 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007316387415973141 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006686706981795031 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007162289810366929 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006522850833670066 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0070644124712356746 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 108\n",
      "INFO: Validation loss did not improve in epoch 109\n",
      "INFO: Validation loss did not improve in epoch 110\n",
      "INFO: Validation loss did not improve in epoch 111\n",
      "Epoch: 111\n",
      "Train Loss: 0.006317412564412004 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007050381329677561 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [08:53<11:37, 116.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 112\n",
      "Early stopping after 112 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.07979519852185554 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.07768256315851912 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.018033946351909563 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.020166905206518575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01217967633542054 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013363144470050055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0097042308930379 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01031801917398458 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008382515829688714 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0084419387515963 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007824102005781946 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007659013177651693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007431687259482369 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00695598403484944 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00702644523019039 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006497758495457032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006749973901765027 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0063574259539189585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0065433598651608495 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006265308228595292 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006367502119273192 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0061745591982103444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00619805544439744 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0060585980673375376 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00603080486447931 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005911428193726084 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005884717077732317 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005757074922953239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005770628302516144 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005619330478914301 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005677340185757468 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005503340028500294 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005593094296445798 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005410904448260279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005512583592734945 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005345250520964756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005435056349026366 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005309183349120705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005362591074004661 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0052997131749768465 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 192\n",
      "INFO: Validation loss did not improve in epoch 193\n",
      "INFO: Validation loss did not improve in epoch 194\n",
      "INFO: Validation loss did not improve in epoch 195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [10:13<08:35, 103.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 196\n",
      "Early stopping after 196 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.1083860789174146 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.08873384539037943 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02170243307295507 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.023914656629238057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01154218054415631 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012805076761116438 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009215994415138542 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010123557630268967 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008360740683639735 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009095575585139586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007916826198224224 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008460537064820528 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007630798715866939 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008064776946626166 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0074257489215637 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007782892597948804 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007221274557149541 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007544062196222299 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007000480766618883 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007369695546324639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006761693619157657 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007262482403722756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006508995091838314 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007215795867309412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006267854240705959 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007122933741330224 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006090436691170164 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006997466751593439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005970816187438195 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006907539449029547 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005875327635002424 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006851868772440974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005789712289378836 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006824600832153331 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 168\n",
      "INFO: Validation loss did not improve in epoch 169\n",
      "INFO: Validation loss did not improve in epoch 170\n",
      "INFO: Validation loss did not improve in epoch 171\n",
      "Epoch: 171\n",
      "Train Loss: 0.005707917144194069 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006821293527643909 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [11:24<06:08, 92.16s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 172\n",
      "Early stopping after 172 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.3174014524250788 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.26176519686465755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03151935701306066 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03451545120162122 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013483832187501944 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014771208625889438 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009278283403492975 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010239275898236562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008091757427493152 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008843204116120058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007291585408800356 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007478060556904358 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006681796455633466 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006403982865295428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006331827184763213 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005994421703850522 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00604119230049873 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005674190346754211 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005799981531768657 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00542883513330975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 96\n",
      "INFO: Validation loss did not improve in epoch 97\n",
      "INFO: Validation loss did not improve in epoch 98\n",
      "INFO: Validation loss did not improve in epoch 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [12:06<03:47, 75.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 100\n",
      "Early stopping after 100 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.2790559481176799 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.18441490379079958 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.025965495354557125 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.028645414256435985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012018530256961249 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013353593085947283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0098552961504923 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010637050501399618 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008901351120239667 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009512769373352914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008397156661155423 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008928114452454098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00808693358783413 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00858275284853709 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007849046173607455 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008334183866870315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007632621419938947 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008137922193033291 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0073504702446822515 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007874685702571535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006901503800598048 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007356341592693592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006676108151483927 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007130201015730991 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006533379630393002 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006987038650549948 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006409823579244642 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00684975936104927 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006290085543198579 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006686814136200529 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006164906795024464 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0064673044321620285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006026808355584822 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006170854985933094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005891039832852047 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0058981589493615665 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005777699978432421 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005745766821372158 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005680256482191642 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0056596474092015445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005593347540346185 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005591906709870433 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005515737586525561 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005528260330560014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00544754747321734 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005469033857533599 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005388420387884996 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005417868440203807 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0053370638990971895 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005376722177435809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005291828152767797 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005345229429247624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005251281348356202 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053217130822732165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005214377182975996 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005304272320833714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005180398096801223 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005291029701338094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005148886265972439 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005280445836594 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005119550523235986 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005271422628806356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005092204002881028 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005263224867282107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0050667128612773395 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005255487528355683 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005042952933624033 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0052479309139444544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005020809209443302 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005240370163365322 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005000182546005605 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005232792806482929 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004980948726038183 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005225303945789004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004962974869098895 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005217928144916454 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004946117091585753 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005210717400426374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004930239916230164 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005203597685870002 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [14:52<03:28, 104.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.21142676188848422 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.15842335900250712 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.026385417765509473 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.02895818349412259 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012094631603745621 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013098942040575339 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009680853844756247 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010277772685238981 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008142471579872208 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008549375808797777 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007303825650962382 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0077473255257834404 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006937856000362059 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007167597198584939 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006616050278067752 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006874368368538425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006304639040230753 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006678256206214428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006101704351298076 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006580611656639068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 93\n",
      "INFO: Validation loss did not improve in epoch 94\n",
      "INFO: Validation loss did not improve in epoch 95\n",
      "INFO: Validation loss did not improve in epoch 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [15:32<01:24, 84.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 97\n",
      "Early stopping after 97 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.12088662779543304 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.09347203045206912 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0220061384182233 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.024476332042147133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011741731008667716 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012877502095173387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009490778157317562 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01023762927198892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00839450572070115 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00922973662176553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007806762077430957 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008489264319102992 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0073659920845738185 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007884239236933781 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006877945048180511 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007339186891091659 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006548349929541812 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007086488616871922 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0062409657232946015 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006965219755383099 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006052981090840686 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006889830658431439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0059267068413480515 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006772854002466535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005840065094122296 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006694159559522043 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005771838906510006 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006650857497280573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005713856366061925 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006623550496228477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005663174752615066 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0066000677932820774 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005618562733662743 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006575209533740931 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0055793290167543905 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006548112004940563 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005544894507042656 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006520167869680068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005514634651196509 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006493190851281671 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005487875943242089 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006468457060263437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005463935863090692 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006446329938412151 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005442182517297784 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006426344598259996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005422082066410187 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006407683139995617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005403209241008535 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006389529396341566 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005385254322802716 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006371072157943512 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00536798895761561 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006351723178180263 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005351249346540167 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006330957458190182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005334911869250809 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006308406680438887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005318890496099327 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006283623994985486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0053031178328113455 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006256211507955895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005287532165670346 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00622588794623666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005272087425224646 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00619235062314307 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005256741269437229 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006155655856299049 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005241448683392284 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006115817348472774 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005226176090616678 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0060730410726083555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.00521089551616868 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006027699365992756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005195584248863568 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005980293245931321 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005180211825700763 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00593318760408746 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005164680333322009 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0059041189070900574 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [18:16<00:00, 109.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (6996, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1741, 12, 5)\n",
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3912803360503422 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.27503037655895407 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.034248628934258465 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02831870698454705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.016370757183136596 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.014490563291209665 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012294442249359945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010759333631193095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010035303860932555 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008692310881716283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008801317672228194 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007433846901932901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008248255307973282 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00681704165075313 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007912398749732808 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006500823452899402 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007685980969533902 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006320732925087213 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0075301510674164514 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006184675917029381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007415970602417237 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006067273164675994 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007323029841249596 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005967222539369356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007240673903477927 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00588040610098026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007163637452206008 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005801289896904068 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.007088978522346614 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005725848666307601 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00701480248802635 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005651544834571806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0069397455320553214 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005576555545187809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006862740560230634 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0054993280971592125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0067828346674275244 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005418443023650484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0066989719858137915 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005332832533696836 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006609882047871918 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005242102182554928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0065142037235327984 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005147177958860993 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006410735844290012 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005050571961328387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006298676637622218 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004954972930929878 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006178534475950414 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004860808602957563 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006054518742223173 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004765989216552539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005936263328456409 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046702360184016556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005834289257133966 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045811696951700885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005751649673164877 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004508319837887856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005684900620088968 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004453617143868045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0056298355012175255 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004413633575578305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0055833463828168705 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004384017919867553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005543284209075142 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00436123478946022 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005508163725674475 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004342850064858794 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005476951557126525 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004327305086719042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005448913527014729 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004313637436875566 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005423510826158775 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004301258460195227 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005400332046000789 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004289824576963755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005379053681385044 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004279097899879244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005359403825722226 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004268923628313297 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:14<20:10, 134.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.10010847308966395 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.07152614132924513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03750930484129141 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03235140893269669 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014256213161218317 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011669960715384646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010757810275121401 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008585652336478233 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00942981859071597 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007504816962914033 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008779454749868545 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006945347036658363 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0083562694075326 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006551845795051618 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00797838349528951 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006333290510387583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007643695577307175 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006217799699780616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0073489839536431305 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006011936165900393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007110222418724504 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005839337675239552 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0069006506128228146 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005719666559757156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006705974197317102 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005595444739711556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006552098912174163 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005454083760692315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006443930736139043 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005333564591340044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006364782230788204 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052417029753666034 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006301697616818357 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005171780258586461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006248218706409238 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005115358933637088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006201102878349795 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005065833540125327 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006158583674720195 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005019789832559499 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006119550687375746 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004975811256603761 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006083138646413338 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004933510377833789 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006048605779449553 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048928459217263895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0060153479850532056 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004853877729990266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005982920794841327 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004816617219793526 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005951037493291987 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047810179562392556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005919552290680303 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004746991594914686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005888428944630812 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00471447213891555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005857705200818202 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004683397071097385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0058274633675328875 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004653668683022261 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005797802380256389 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004625215267085216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005768810990521659 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004597947213121436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005740560594941893 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004571777501736175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005713093612283536 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045466046695682135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005686430284460011 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004522302818738602 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005660570463161388 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004498767746951092 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005635502739154551 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00447585421021689 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005611216611139579 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004453455529768359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005587696977550819 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004431524000723254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0055649279272311355 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044100073928182775 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [04:27<17:47, 133.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.16125215803906662 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.09466067630458962 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03146814184101749 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.027316736768592486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015360919316405574 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.013443373219872063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012234470240355014 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01023026909255846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010597917230557394 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008617208834568207 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009339249941103637 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007494697105986151 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008443367077445942 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006773437119343064 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008068883104237078 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006430209042843093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007786254707902466 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0061607655137777325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00751579908088496 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005919741923836145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007268426779308888 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005717819789424539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0070339870574293 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005542699014768004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006773378708523158 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005362678590145978 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0065107068183011116 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005212485557422042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006323809064627034 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005124621292237531 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00618947299673609 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050417904064736585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006079535142488812 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004942335383119908 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005988248533007931 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004837882692332972 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0059139436503157835 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047444224632768464 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005853387091274811 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004670924636196684 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0058029409047199165 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004617417525415393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005759760483856989 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004578858751549639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005721942587813031 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045499813002110884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005688235292861825 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004527204746211117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005657773533491546 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004508407388559796 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005629926025293359 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004492322730154477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005604208595083269 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004478126186454161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005580230833182205 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004465254273434932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0055576644707088394 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004453250135041096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005536225017055988 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004441742666743019 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005515654021438177 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004430381731468845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005495717000539444 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004418859666806053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005476195242662051 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004406860862350599 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005456892552107283 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004394095236520198 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005437621640011703 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00438028060704131 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0054182055675781856 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004365138345482675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005398481685060106 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004348409074273976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005378306124914335 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004329876527613537 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005357557658125225 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004309366046535698 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005336159486727593 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004286833750930699 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [06:40<15:32, 133.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.09111543736297247 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.05975138179280541 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.022900465807822196 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.020704684050923045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014086006896583082 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012588847538625653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011897545254185422 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010355210515924475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010653979965213466 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009111061374741521 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009818234932226166 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00843639907173135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009065498069001013 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007878270156850869 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008368567469504292 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007342586907642809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007954106757047344 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006902752101252025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007629021523269447 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0064817760817029255 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007273542368359286 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0060862772759388796 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006954692716106398 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005770513516935435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0067518199724889445 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0055567428765987805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006607036983390173 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005416094219650735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00647939207323627 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005308837718753652 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006353881568903078 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005209725197743285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006235424870918569 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005115704221481627 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006133625456068205 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050315329551019455 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006052218580120945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004958775169639425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005987749950105503 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048972000897100024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005934579704556461 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004846476204693318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005888482209158951 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004805358234708282 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005846967764908967 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004771780698898841 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005808644997143106 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047435300721024925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005772736161675083 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004718728374097158 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005738807330758536 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046960130016404115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005706613175315808 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004674506109123203 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005675999532224075 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004653709496117451 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005646825591038453 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046333517625250596 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005618934069343388 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004613299657252025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005592145328463127 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004593473979779943 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005566305755722639 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004573906835337932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0055412453871441485 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004554625101048838 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005516805809914011 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004535640638575636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005492851889498753 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004516956944611262 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00546926561930136 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004498547855341299 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005445948035932533 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004480376989919353 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005422821612369491 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044624198164621535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005399828216435226 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044446508929302745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005376931564822861 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004427062720060349 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [08:52<13:18, 133.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.08876168641971943 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.059792680123990236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.021270418082487228 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01754450143568895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013261798579994266 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011390052066946571 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010791137361884135 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00910664749416438 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009323751146173674 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007868724997917359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008617266948806913 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007087194098329002 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008194280758534121 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006543625243516131 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007814486435567747 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00618128308349035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007540779784172944 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006049823735586621 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0073841216646509085 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00598704477941448 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007256344017640743 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005912868352606892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007135543533702929 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00582596729086204 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0070156001977061165 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005730115283619274 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006892716856014205 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005627188425172459 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006759673554756462 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005513533818620173 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006608947058480454 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0053862956521863285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00644108509914907 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005250099648467519 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0062700927408505655 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.00512137073515491 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006113875583962422 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.005016558558087457 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0059815294366568055 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004935395776886831 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0058730149813863925 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004870473284443671 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005783873303478677 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004816540180366825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005708608971685426 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004769707977009768 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005642787302731532 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0047271771060133526 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005583484242744659 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004687329904514957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005528877640768171 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004649458554658023 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005477816141029761 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004613333108665591 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005429555604034465 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004578844796527516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005383610593837571 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004545903917063367 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00533967136541492 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004514388543214988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005297554955343593 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004484176762740721 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005257165305567352 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004455174071798948 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0052184784983191216 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004427324008958584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005181508721384011 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004400605803609571 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005146286051112257 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004375030864453451 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0051128236310063226 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004350600478408689 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005081115202812401 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0043273293624886055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005051119699563882 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004305215563032438 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005022765542316722 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004284232262183319 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.00499596096629481 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004264346021227538 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [11:06<11:05, 133.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.15985225383447457 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.10236274496736851 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02305302353252627 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.018856895554133436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013238369322437198 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011262496319514785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009956467598485313 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008773804930123415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008195617950104653 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007245891105214303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007564119491237109 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006473905288360336 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007161959800571544 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005976854353635148 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0068584288960560885 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005638194283131849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0066293815084941445 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005396187648346478 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006445892953067235 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051976324059069155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006293657877414432 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005021072589707645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0061653991385052604 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004865255719050765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006055118586787248 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004734447323293848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005957991766666815 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00462873422075063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0058721895999441 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004544134766117416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005796652340402478 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004474925933490422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00572978374420678 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004416102266193113 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005670194021324079 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00436461422465403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0056167574945973344 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004318587560291317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005568415220490908 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004276717430911958 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005524129251690079 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00423800125929781 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0054829381411622986 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004201627943918786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005444050407181728 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004166997634721073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005406908814255233 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00413372748827731 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005371206567078387 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004101655657657168 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005336858645787454 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004070832383480261 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005303918597177861 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004041444463655353 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005272492985389956 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004013696475885809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005242673626497014 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003987738634036346 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005214488218840399 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0039636214517734265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005187901761276503 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003941302431154658 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005162818187775439 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003920653828589076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005139106467842511 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003901503404433077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005116608867753451 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038836447162215004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.00509515476038841 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003866864582100375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005074561797624632 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003850939120589332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005054644505933488 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038356579205190595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005035214896364878 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038208051097833296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0050161093679154676 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038061796899207614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004997229211555462 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0037916685369881716 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [13:19<08:52, 133.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.15244537436288513 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.09023504318161445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.023174301612713975 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02378286597403613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014796645744155124 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.013342188942161473 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011564984709794153 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009774853259494358 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010057204325904552 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008513294723392888 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009175684660264891 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0077122388280589475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008611748186918188 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007119477853517641 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008220047216870946 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006720319762825966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007891487415782154 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006432439340278507 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007586783336126777 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006198408416557041 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007333029209865793 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005964525759389455 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007158491013095327 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005745038017630577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0070402626241621085 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005580379394814372 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006949040283912529 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005463362764567136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006870037203624879 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005374459617517211 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006797624213969735 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005302031096917662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006729809475901056 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005241339073770426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006665453010719253 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005190036538988352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006603097695682929 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005145625126632777 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0065406920610540055 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005105022273280404 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006475781035805163 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005065115710551089 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006405677192963524 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005023288684473796 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006327422906125824 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004977720273150639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006237688542930537 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004927334265614098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006133047103686216 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048714070961895315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006011304331275732 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004808868806470524 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005874377152282898 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047372411089864645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005732945229785865 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004655733471736312 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005606788362070203 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004576744204810398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005508914301767265 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004516492110931061 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005437150182942413 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004476937677033923 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005383583540033089 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004451389409686354 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005341478635676069 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004433496912348677 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005306460279609889 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004419489941475066 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005275927519119561 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004407507733611221 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005248372447804535 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004396712278354574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005222903021832409 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043867377754808826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005198976557093702 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004377420171460306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005176249552639329 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004368671639399095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005154491912522544 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004360358396926048 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [15:32<06:39, 133.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.17157139321236306 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.10659464929591525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.039730398043922095 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03380082896487279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015347228584317788 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012546146030283787 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012038925032186659 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009810751981355927 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010199733598361889 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008275940967723727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008935242701538724 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007220151347362183 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008356577365927091 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0066788072232156995 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007970330234166217 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006367010707882317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007687023764829147 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00619444079189138 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007476938139438867 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006067450556226752 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007316845101325615 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005957370759411292 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007182954553276572 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005861543427983468 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007057251764321062 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005769497168843042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006926568827834235 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005670328412882306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006781428592538963 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005548629507591779 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006624945976290869 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0053996102715080435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006472151872667922 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052474429661577395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006333066676275541 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005106345763091337 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0062097745339415936 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004980098307979378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006101666893342246 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048716953701593655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006007837855095495 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004782967015423558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005927265227823327 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004713304251940413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005858636007497521 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004659724576314065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005800307012847773 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004618293116800487 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005750524101791623 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045855767792090775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005707620563608547 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004559049137275327 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0056701201063657355 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004536951170302927 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00563679587284154 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004518037342297083 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005606668618280169 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004501399211585521 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0055789825765386256 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004486351504667916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005553161048704597 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004472379234026779 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005528767498052726 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004459090211259371 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005505476116104961 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004446192056109959 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005483041709601522 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004433481153947386 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005461282737087458 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00442081791433421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005440059592936732 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004408093126998706 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0054192627694024005 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004395247009498152 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005398808289574434 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004382226006551223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005378627101359166 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043689980277452954 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005358668489139272 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043555422305044805 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [17:45<04:26, 133.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.29977779267313276 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.19275077798149803 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.026156966176870616 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.021872661178084937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014475271660200911 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012761468423361127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011879735706016829 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010186753549020398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010761275024740153 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009128296095877887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009921931365045579 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008433728876777671 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00923314380574308 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007843040535226465 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00868430834255058 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007310845021327788 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008231637744524315 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0068384641654450785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007847233218795803 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0064195199065249075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007548762808052066 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006094320596788418 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007338475074499981 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005876034642146392 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0071675699644352065 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005719917771321806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0070183708228289945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005600249284709042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006884278007145104 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005498181757601825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006762033385824259 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005404829276217655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0066476622910013414 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005317863182757388 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006535201371205146 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005233513309874318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006421139089020658 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005144534543664618 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006309980135857549 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005049206066707318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006211186852731389 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004957716446369886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00612976525316959 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004880898450077935 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006063904532456779 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004820201088759032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006009139819602273 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047712131022391 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0059615942903637 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004728997206654061 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005918741080012666 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046901662325994535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005879070291925035 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004652928467839957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005841641954239883 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004616468196565454 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005805797131783138 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004580470068718899 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005771032334666818 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004544773562387986 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00573696800375774 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004509268108416687 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00570336322472367 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044739087916571985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005670103684097494 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004438749489120462 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005637186067497792 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004403913057070564 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005604704211521298 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004369625728577376 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005572812731131129 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004336194725791839 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005541664466886998 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043039861389181835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0055113592813403845 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004273310453969646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0054819154713651565 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00424439109523188 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005453270998757894 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042173076929016545 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [19:58<02:13, 133.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.20012335581322238 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.14182063127783212 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03856939426516969 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.032937628911300136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01598772829925837 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.013288384942676533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012154752297165323 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010225293052975428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010628043173187196 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008912291610613465 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009533050380217668 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008105582625351168 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00886607207368515 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0074923214079304174 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008546530718268767 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007086844543333758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00831468535223593 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006767987802794034 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008116397585668792 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006509070877324451 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00792557160108516 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0062999614641409026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00771589672509751 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006140088721771132 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007488756175854639 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006058067252690142 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00731939695547994 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006030807821926745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.007199388251482556 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005976349822330204 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.007096702370103704 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005905672200870785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00699984773432147 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00582722157493911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006903855021103162 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005741961355405775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006806680892241191 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0056513796306469225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006708308843870277 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005560351332480258 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006609633430991679 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005476874570277604 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006510318769354663 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005404913548210805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006405898205591494 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005337694139135154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006287343590095061 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005262075097892772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00614744559096249 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005167225015942346 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00599542753207476 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005056945598599586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005861133875508184 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004956243953413584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005759086368443895 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0048796552614393555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005678282352695964 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004820699519901113 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0056089541559069325 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047707816628231245 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005547061543932958 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047238308216699144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005490731522887435 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004677033246579495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005438819758861951 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046300412875346164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0053905694943227585 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004583695844154466 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0053454901843404045 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045389855915511194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005303212160299868 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00449627869000489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.00526347371577349 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00445583187551661 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005226031141929737 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004417701031674038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005190675591010192 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004381848556328226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005157198140569899 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043481728781692005 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [22:11<00:00, 133.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (6996, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1741, 12, 5)\n",
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.20933349007851185 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.12075715112415226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0382294085293532 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03348616605455225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015201519582565177 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012906788772141391 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012963278667754643 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01115609190680764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.011749765112757955 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010191315006126057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.010759975742569967 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.009429993852972985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.010037687930839825 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.008773055714978413 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00961583175539086 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.008257901774380695 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.009292473023561718 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.007942659776149826 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008949131220068834 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.007802359184080904 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.008635167320594873 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.007648584123870188 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00842024948975267 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.007423366690901193 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.008282254198055766 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.0072536725931885565 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.008182833043509695 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.0071420817551287735 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00810038833772619 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.00706117904575711 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.008026505527554344 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006996246025135571 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.007957817702745534 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006940512426874854 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.007892664233817969 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.00689031388187273 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.007830072974692605 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006843448946760459 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.007769338531445151 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006798409128730947 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.007709798815587884 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006753969416868957 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0076507138764511234 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006708887202495879 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.007591199828311801 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006661757839504968 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.007530150600160434 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006610912969335914 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.007466086563888138 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006554160343313759 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.007396866820463443 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006488433857025071 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.007319233261184877 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006408993192863735 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.007228734946626847 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006309057513929226 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.007122371573273314 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006183295976370573 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.007005910077621335 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.006036218835718253 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00689712317328747 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005886189605702054 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.006810846251061348 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.005756624860011718 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.006745778379289935 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.005656752354380759 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.006693967005671704 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.00558107167228379 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.006649926079428632 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.00552195038815791 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0066110243197938755 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.0054743228437887 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.006576007776404625 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.005435175791552121 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0065441466596819564 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.005402534696358171 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0065149051715624115 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.00537492549893531 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.00648783234460492 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.0053511550776999105 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:12<19:53, 132.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.0946380041165439 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.06669894040308215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.028071539042747184 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.023490126922049305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014096698083714944 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01214058930054307 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011770644283653599 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.01026736545291814 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010619507885121286 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.009241471058604392 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009855386864853231 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.008530354415151205 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009360090869831849 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.008085873384367335 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008971114500455525 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.0077737970277667046 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008644718547100754 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.007518529519438743 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008425573270861485 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.007323530299419707 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.008253758619050389 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.007159312306479974 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.008093549860386203 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.007008643621917475 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007935076013215941 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.0068672470621426 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007769161363272634 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.0067283362446522174 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.007580212620937491 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006578251084482128 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0073465307830029155 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.0064025274647230455 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.007085014853337312 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.006228417026895014 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006894158027198625 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.006107630534097552 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0067726324129526476 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.006014646132561294 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006678320702429027 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.0059416985562579195 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006599583652946401 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.005885687326504426 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006535777007229626 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.005843304347416217 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006486337454394186 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.00581196215511723 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006448404596625193 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.005788485587320544 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006418386780971568 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.0057697646531530405 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006393389946985224 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.0057537190446799455 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0063715559801461015 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.005739324971694838 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.006351819178982548 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.00572615484283729 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.006333576456311937 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.005713994479314847 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.006316467383918969 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.0057026781311089346 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.006300257072894575 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.005692060224034569 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.006284782834155443 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.005682009378109466 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.006269926543891872 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.005672411629083482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0062556026352475905 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.005663173070008105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0062417438770453 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.005654219952835278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.006228294495182733 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.005645478923212398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.006215212075677636 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.005636913723058321 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0062024650673199126 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.005628489190712571 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.006190025731368356 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.005620185095308857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.006177875546623685 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.005611986806616187 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [04:24<17:39, 132.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.20142488927499616 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.12813204127279196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03397872982351067 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02910739075053822 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015781391382251427 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.013284392798827453 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012588877595544338 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010492229351604527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.011142082999303053 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.009219755342399532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009958639324719265 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.0083816873756322 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009112545944001849 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.007912397511642088 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00873869955731071 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.007497886890037493 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008505434059372095 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.007261111519553444 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008322192492074073 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.0071165046823973 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.008161691270479464 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.007003465540368448 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.008018162068549425 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006900911363349719 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007889570496108842 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006804517525332896 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007775840184706599 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.0067167555786330595 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.007674225344488474 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006634434850209139 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.007581868224892102 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006556702003052289 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0074965957760572705 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006483114675872705 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.007416818989972375 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006413246674293821 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0073414521667145224 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.006346718056804755 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0072697732346685235 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006283200290900739 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.007201320190327114 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006222456905313514 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.007135846683985158 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006164388159628619 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.007073306219682063 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.0061091499403119085 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.007013793599889635 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.006057122417471626 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006957417984437793 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.006008804496377707 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006904123341941092 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005964583387090401 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00685355443556338 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.005924509402195161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0068050598652318935 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.005888208425180478 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.00675783005168823 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005854964798147028 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.006711094992630796 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005823915180834857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.006664296694537922 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005794230878183788 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0066171845014007195 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005765218236906962 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0065698442926426254 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005736390056765892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.006522655217019526 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005707537590272047 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.006476187002611215 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005678780787539753 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.006431067888157178 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005650545423850417 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.006387858446389777 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005623453089290045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.006346989529351938 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005598152348433028 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.006308733366034313 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005575170008126985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.006273223317364341 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005554829508235509 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [06:37<15:26, 132.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3809484923471054 // Train Acc: 0.8133561643835616\n",
      "Val Loss: 0.25455792370167646 // Val Acc: 0.5113636363636364\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.043122297084835025 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0383614798838442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01852040442098017 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01608511521904306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012641505053326419 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010894036724824797 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.01123573696644781 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.009680648148059846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.010563123156848155 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.00912714261053638 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.010177479021370275 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.008792893021282824 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.009922401975313007 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.008551041011444547 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.009744432936613895 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.008373010971329429 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.009617372885873736 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.008246711463752119 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.009522973503449668 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.008154846947978843 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00944653198709839 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.008086040387438102 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.009378110276552163 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.008030954071066597 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.009310603620203799 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.007980703841894865 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.009236712102272195 // Train Acc: 0.17123287671232876\n",
      "Val Loss: 0.007929216206751086 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.009151369742581357 // Train Acc: 0.18550228310502284\n",
      "Val Loss: 0.007887711155820976 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00906167332014945 // Train Acc: 0.18550228310502284\n",
      "Val Loss: 0.007870270244099878 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.008974816016512647 // Train Acc: 0.18550228310502284\n",
      "Val Loss: 0.007856182500042698 // Val Acc: 0.22727272727272727\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.008889970137869522 // Train Acc: 0.18550228310502284\n",
      "Val Loss: 0.00783309138125994 // Val Acc: 0.22727272727272727\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.008805989107057521 // Train Acc: 0.18550228310502284\n",
      "Val Loss: 0.00780139242044904 // Val Acc: 0.22727272727272727\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.008721991333702248 // Train Acc: 0.18550228310502284\n",
      "Val Loss: 0.007760376356203448 // Val Acc: 0.22727272727272727\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.008636104392704286 // Train Acc: 0.17123287671232876\n",
      "Val Loss: 0.007706281288780949 // Val Acc: 0.22727272727272727\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.008544906818618378 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.007633121607994491 // Val Acc: 0.22727272727272727\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.008444061632519035 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.0075349803929301825 // Val Acc: 0.22727272727272727\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00833135453843805 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.007411017666824839 // Val Acc: 0.22727272727272727\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00821116229532006 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.007271529124541716 // Val Acc: 0.22727272727272727\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.008092966253899004 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.00713414510034702 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.007981980504408548 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.007008222685280172 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.007875043240728784 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.006890782358294183 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.007763942542005347 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.006774632294069637 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.007639212374358553 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.0066531606145541775 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00749655314678641 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006520500842651183 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.007343571946199252 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006377168135209517 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.007194028129656565 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006236552607945421 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.007056413196052795 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.0061106277926063 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.006932447523040246 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.006001138987696984 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0068198823413628765 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.005904421007091349 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.006715905659121874 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.005816838767548854 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.006619800918748322 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.005737535774030468 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.006533165683451037 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.00566834982230582 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [08:49<13:14, 132.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.22844336927362366 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.15004259781403975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03385723465160556 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.029239466549320654 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.016182195667398574 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.015020740946585481 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012646916216094744 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.01134071902117946 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010934557782201156 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.009685009167614307 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.01011116950151821 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.008857885798947378 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00975778328518481 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.008444100330499086 // Val Acc: 0.22727272727272727\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00955508864744137 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.008182115861299364 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.009387733986745823 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.007968583516776562 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.009227917826262443 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.007776736832139167 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.009065724858274199 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.007600340619683265 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.008896996021551425 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.0074403093704445795 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.008721743584886942 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.007301047723740339 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.008544934543538583 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.007186404217711904 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.008373833601639423 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.007095592333511873 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.008214952644241387 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.007021797490729527 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.008065069347330832 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006950937592509118 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.007916153443671048 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.0068719101942737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.007767375129100667 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.006783314370973544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.007622804790395035 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.006691503160717813 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.007480973539423181 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.0066030211789025505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.007338894320626373 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.006522559543902223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.007202173948934361 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.00644318204457787 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.007084191273408836 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.006346419601785866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006992876538198832 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.006249428083273498 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006918937251711749 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006168765765191479 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.006854855108338506 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006104425870051438 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00679649444739228 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.006051100693135099 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.006741390845814898 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.006004077797247605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0066879979487865756 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.005960323483767835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.006635329163565245 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.00591818506998772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.006582751430009734 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.005876858858391643 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.006529836135078647 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.005835979880595749 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.006476283337618969 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.005795329212295738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.006421920372445358 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.005754687336527489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0063667666123431495 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.0057138358200476925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.006311109063606794 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.00567269787107679 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00625555691273126 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005631599405949766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0062010981218004815 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005591461862522093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0061490158550441265 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.005553695737299594 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [11:02<11:02, 132.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.43122127444640684 // Train Acc: 0.8276255707762558\n",
      "Val Loss: 0.3160352897914973 // Val Acc: 0.5113636363636364\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04685769692811792 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.042048047990961505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01854865484101247 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.015989109670574014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.014453825769418656 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01265295296907425 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.012695950049838927 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011149206630546938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.011807099888398903 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.010428140134635297 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.011277563226213635 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.010012032133950428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0108953462647844 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.009706760172478178 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.01057117419810668 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.009427700928327712 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.010258227834992587 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.009115168824791908 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.009928057408257978 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.008741424325853587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.009581527186846829 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.008360965372147885 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.009230408519846663 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.008010621995411136 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.008916382609811275 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.00771975110877644 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.008688133329548554 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.0075173357459293174 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.008513630176979418 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.007361096833747896 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.008357951227260848 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.007218394030562856 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.008182921116750636 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.007046549043364146 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00792556194339764 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.006792793185873465 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.007811971846011097 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.0066764701919799505 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.007731450939568007 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.006601476182483814 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0076553449592846505 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006541629779067907 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00758058623172615 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006483469399708238 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0075062255494160466 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.0064237801569767975 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.007432751185324503 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.00636279151182283 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0073614368533086215 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006302191427146847 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.007293614346776413 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006244298223067414 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.007230240106370011 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006191623566502874 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.007171741686761379 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.0061459650454873385 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0071177977843201595 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.006107512840323828 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.007067511877742464 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.006075041631067341 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.007019825512011744 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.006046650833873586 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.006973793969880874 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006020425865426659 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.006928700726207124 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.0059948633424937725 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.006884102157814658 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.00596903543072668 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.006839797728414303 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.00594252206554467 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.006795773458067481 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.005915251395411112 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.006752128394949994 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.005887335898693312 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.006709005946259452 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.005858957492323084 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.006666545861944998 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.0058302867395634 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [13:14<08:50, 132.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.42184496851272235 // Train Acc: 0.7990867579908676\n",
      "Val Loss: 0.2813635089858012 // Val Acc: 0.3409090909090909\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.05376276824640357 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.04854811531576243 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.020775548847962053 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01762557981366461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01416748746502379 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012208151673390106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0115781562178519 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.010028017964214087 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.010249966635373017 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.008911891882731155 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00958080952040868 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.008316036055541851 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.009256913629679443 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.007993380429053848 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008992221806045265 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.00774615134366534 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008777842956348472 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.007568152422424067 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.008630352992071136 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.007467775622552092 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00852499811455063 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.007408570062199777 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.008440783037910382 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.0073629214386032385 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.008365291214896766 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.007319239823316986 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.008291539396946084 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.0072756916326894 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0082162505411224 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.007233411822976037 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00813965609862754 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.007190306649797342 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.008063479379823083 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.007140367994592949 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.007987931551471284 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.007080837466161359 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.007912155352405284 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.007013235236941413 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.007835448163531754 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.006939480944790624 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.007757520599750011 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.006861138983036984 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.007678435168114223 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.006780202394690026 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.007598381054685473 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006699217153205113 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.007517382710933957 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006620463432574814 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.007429683093007093 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.0065383752257647835 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.007280258121623841 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006414973600344224 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.007153930202058461 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.0062837247440422125 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0070494563929039186 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.006152937163344838 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.006966429663668961 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.006043745073574511 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.006899211647682164 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.005960377136414701 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.006842139436433849 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.005896902084350586 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00679129592916975 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.005845968387174335 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.00674431014659317 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.005802550497041507 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0066998455414046705 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.005763945830139247 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.006657205399266151 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.0057288531637327235 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.006616077217864527 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.00569667718373239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.006576372589597044 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.005667142349887978 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0065381220657778185 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.005640120186250318 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.006501392130187147 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.005615485984493386 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [15:27<06:37, 132.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1004930118430559 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.06739399240098216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.026688519533570498 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.022420080751180647 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014674948610271833 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012382059679789977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012486693871390397 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.010444625225764784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.011265744860220242 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.009495011471550572 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.01033697805801177 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.00883296469903805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009878773109340981 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.008517944169315424 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.009676713204079363 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.0083437566222115 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.009535754400360735 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.008202444999055428 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.009413670798440434 // Train Acc: 0.17123287671232876\n",
      "Val Loss: 0.00808131524615667 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.009295703818783436 // Train Acc: 0.17123287671232876\n",
      "Val Loss: 0.007968619719825008 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.009165592400195528 // Train Acc: 0.17123287671232876\n",
      "Val Loss: 0.007846475951373577 // Val Acc: 0.22727272727272727\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.008990942762067465 // Train Acc: 0.17123287671232876\n",
      "Val Loss: 0.0076860823753205215 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.008686756350516796 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.0074263507881286465 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00827576550357384 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.007170232630927455 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.008010802116483178 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.007038753852248192 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0077535402275341215 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.006853212902999737 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0074860919649674465 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.006614872398362918 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.007304972487918556 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.006462815348905596 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0072053574853827625 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006419354059140791 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.007131859950921137 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006396541714837605 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.007065063198394733 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006365766554054889 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.007001026408091936 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.006327311029996384 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006938785637227061 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.006284754050218246 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0068777968950748235 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.006240362140604041 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006817659863895953 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.006195261625742371 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.006758156370214234 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006150015016001734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0066992723245862616 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.0061049821744249624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.006641175785552727 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006060472219674424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.006584188858361821 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006016792166470127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0065287819534080895 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.005974223295396024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.006475574449108718 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.005933006378737363 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00642529606806395 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.005893355104225603 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.006378686445186842 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.005855479963462461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.006336288043982498 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.005819607013836503 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.006298256558514961 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.005785941827872937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0062643166819638385 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.005754605583338575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.006233895527088342 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.005725574319843541 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.006206320949030624 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.00569870632819154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.006180977610420419 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.005673775356262923 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [17:40<04:25, 132.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.37293862206138434 // Train Acc: 0.8276255707762558\n",
      "Val Loss: 0.24240891567685388 // Val Acc: 0.5113636363636364\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04489620527482196 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0395108794962818 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.017678327490596876 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.014919322331181982 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012897497343886049 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011120782784101639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.011944075355297762 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.0103990818627856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.011119754137109847 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.00975706231018359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.010152406717829202 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.00898681092499332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.009589621913087763 // Train Acc: 0.17123287671232876\n",
      "Val Loss: 0.00854029223661531 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.009418068837468006 // Train Acc: 0.18550228310502284\n",
      "Val Loss: 0.00838967092673887 // Val Acc: 0.17045454545454544\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.009281698812105476 // Train Acc: 0.17123287671232876\n",
      "Val Loss: 0.008277576958591288 // Val Acc: 0.22727272727272727\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00913632842052949 // Train Acc: 0.17123287671232876\n",
      "Val Loss: 0.008185460528528149 // Val Acc: 0.22727272727272727\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00897342571457125 // Train Acc: 0.18550228310502284\n",
      "Val Loss: 0.008098457448861816 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.008801454174243016 // Train Acc: 0.18550228310502284\n",
      "Val Loss: 0.007993637177754533 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.008646022912208416 // Train Acc: 0.18550228310502284\n",
      "Val Loss: 0.007855591254139488 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.008519701504458984 // Train Acc: 0.17123287671232876\n",
      "Val Loss: 0.007701291804286567 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00841351179193416 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.007559893661263314 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.008318444980801717 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.007442619236694141 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.008230916485955862 // Train Acc: 0.1569634703196347\n",
      "Val Loss: 0.00734662599014965 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00814929902744926 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.007265626444396648 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.008071627495512707 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.007193442705002698 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00799575471531889 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.007125326453454115 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.007920582377942935 // Train Acc: 0.1141552511415525\n",
      "Val Loss: 0.007059178298169916 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00784617030066034 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.006995516308498653 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.007772457492474007 // Train Acc: 0.08561643835616438\n",
      "Val Loss: 0.006935214009982618 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00769898601856559 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.00687762701629915 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.007625741599187187 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006821032245220108 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.007553386066074921 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006763730613006787 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0074828409932162524 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.006704767708751288 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.007414979512003765 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.006644084850664843 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.007350574024808359 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.006582182878628373 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.007290201675737931 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.006519760369238528 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0072340509061144505 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.006457480817863887 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00718186339724928 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.006395840119909156 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.007133093549865702 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.006335160339420492 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.007087112063133757 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.006275676847012205 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.007043339863321722 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.006217556984417818 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.007001276891338362 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.006160913928496567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00696050329824918 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.0061057605611329726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.006920653144450493 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.006052042603154074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.006881397506158339 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.0059996493711051615 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [19:53<02:12, 132.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3438754186000182 // Train Acc: 0.7848173515981736\n",
      "Val Loss: 0.22927247055552222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04244817157178165 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03793332129716873 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01917317033783622 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01754975450140509 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.013710708646571527 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012247005088085478 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.01168603079497695 // Train Acc: 0.04280821917808219\n",
      "Val Loss: 0.010307515497234734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.01052573104293328 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.009332682691853155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009842740177797774 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.008727734298868613 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.009513662736777846 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.00838378369808197 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.009280655486296573 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.00812274119393392 // Val Acc: 0.11363636363636363\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.009055327314704861 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.007879616370932622 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.008803735554141745 // Train Acc: 0.14269406392694065\n",
      "Val Loss: 0.007616894946179607 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.008558209507075483 // Train Acc: 0.1284246575342466\n",
      "Val Loss: 0.0073827097023075276 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.008363388336824942 // Train Acc: 0.09988584474885845\n",
      "Val Loss: 0.00722949281334877 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.008210734720448176 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.007125582808459347 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.008084823842028573 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.0070424420589750465 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.007978495623975923 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.0069721795449202715 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.007887190517216716 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006911486611616883 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.007806404376802243 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006856836332008243 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0077320941666472855 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006804972857406193 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.007661290625195321 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006753519617698409 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.007592212408249475 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006701189812950112 // Val Acc: 0.056818181818181816\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.007524092772074009 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.0066476574506271965 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.007456893041797969 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006593252659182657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00739100200525957 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006538523200222037 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.007326979572846465 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.00648397220806642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.007265383027362122 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006429942489855669 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.007206648586054235 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.0063766160810535604 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.007151013907878638 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006324029790068215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0070985180259890614 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006272134476933967 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.007049026028848504 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006220868428830396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.007002297765469034 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006170197106389837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.006958048027348981 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006120177688585087 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0069159741043517175 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.006070987867530096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.006875753270878911 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.0060228483624417675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.006837037712501439 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.005975986686958508 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.006799450506175524 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.005930558867244558 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.006762614945209156 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.005886616528203542 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0067261741129834565 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.005844182296740738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0066898265416568065 // Train Acc: 0.05707762557077625\n",
      "Val Loss: 0.005803258810192347 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.006653362269020005 // Train Acc: 0.07134703196347032\n",
      "Val Loss: 0.005763900360431184 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [22:05<00:00, 132.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1057, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1056, 12, 5)\n",
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1984558105707114 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.08665232675369172 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01316651408100872 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.013584316071287236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.009721497094413603 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.009571210080620778 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008419521328979522 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.008491707082344768 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007779812906363083 // Train Acc: 0.09140767824497258\n",
      "Val Loss: 0.007647845620179877 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00730616970126512 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.007193144924445625 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006816602064938484 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.006848375511574833 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006534840048621093 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.006553019485984217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006310833847683121 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.006262569955330999 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006129815854205387 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.006064647123874987 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005983626964575248 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.005928272093810579 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005872747750818335 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005776386235511917 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005790640011564292 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.005614563665243194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005726795593128494 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.005474868716726846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005673660212273027 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005360682179932208 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0056269667275212504 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005264718914130593 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005584430179798761 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.0051811566329835095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005544783962316729 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.005106833982555305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005507279620752746 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.0050402842667501635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005471415956499535 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.004980838124859421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00543676410785534 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.004928108989535009 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005402867192993284 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.004881600886994206 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005369176314841493 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.004840404264327577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005335106624850503 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.0048026945681639895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005300320841336258 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.004765179169172531 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005265237803003109 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.004724015492041979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005231079462106073 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.0046773920630050056 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005198991661457178 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.004626749151879374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005169257840015661 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.004574613409418175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00514147461858498 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.004522637371658622 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0051151073140422495 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.004471456265652224 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00508980793630282 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.0044213128002250895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005065444896467561 // Train Acc: 0.07998171846435101\n",
      "Val Loss: 0.004372396478977273 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005042036991567271 // Train Acc: 0.07998171846435101\n",
      "Val Loss: 0.00432494213558076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005019682421331112 // Train Acc: 0.07998171846435101\n",
      "Val Loss: 0.0042791899964761205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004998506265615796 // Train Acc: 0.07998171846435101\n",
      "Val Loss: 0.004235373967436745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004978611833229491 // Train Acc: 0.07998171846435101\n",
      "Val Loss: 0.004193730957066531 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004960061833083671 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.004154444774886703 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004942861930369289 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.00411764218348681 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0049269653541134705 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.0040833681397249594 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [05:14<47:09, 314.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.18108150478404772 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.06969951712252463 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.013155294686744852 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01402815873734653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01026207380425832 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.010331269932280788 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00894695293167657 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.008965748228707953 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008005883058165311 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.008063844822840217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007329406853406851 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.007117511834675337 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007138579998397774 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.00689478636281017 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007005216865808834 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.006730990963714088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006882958797102886 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.006565196624518756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006760980176672993 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.006393805769381716 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006632175233974453 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.0062189477783463456 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006488328896440783 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.0060537432813469105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006316412359661673 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005936495624208713 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 130\n",
      "INFO: Validation loss did not improve in epoch 131\n",
      "Epoch: 131\n",
      "Train Loss: 0.006109440946522453 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.005911041996167863 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 132\n",
      "INFO: Validation loss did not improve in epoch 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [06:59<25:32, 191.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 134\n",
      "Early stopping after 134 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.11436010039729871 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.06584624272278126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.012644929562991576 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0131949770866948 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008707099244610612 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.00919813135474482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008160869184016527 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.008410903967588264 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007449316032936582 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.007652626520104925 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006888626598781979 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.007038936094271348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0065135836841324025 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.006582696114064139 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006252947690133925 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.006229153642540469 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006077651052362876 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.005979516358553048 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005951062980247577 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.005817808847710052 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005849279981945902 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.005700630396056701 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005765205730387849 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.005598086541902055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005697917251914086 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.005501895307508462 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005644318191492095 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005416706450940932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00560004532690904 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005345675512216985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0055618802008050505 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005287406877066721 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005527931083035459 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005239021505557877 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005497108266490531 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005198036840952495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005468721401182189 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005162455552421948 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005442284816434229 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005130739115616854 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005417420251730836 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005101686788668089 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00539381482163393 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.005074298502329518 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005371202265786712 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.0050478396588005126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005349346240867488 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.005021754169480547 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00532803512403956 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.004995692096760168 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005307079674429184 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.004969470319338143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005286317883409267 // Train Acc: 0.09140767824497258\n",
      "Val Loss: 0.004943080725805724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0052656320777553145 // Train Acc: 0.09140767824497258\n",
      "Val Loss: 0.004916704468675615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005245002012481058 // Train Acc: 0.09712065813528337\n",
      "Val Loss: 0.004890751898261335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00522452353836519 // Train Acc: 0.09140767824497258\n",
      "Val Loss: 0.004865626451860675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005204348845668716 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.004841465276994687 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005184538939319061 // Train Acc: 0.09140767824497258\n",
      "Val Loss: 0.00481763416353394 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005165024683619228 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.004793414777583059 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005145688157616323 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.0047685688546420454 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005126440245168662 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.004743338401621098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0051072294581417585 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.004718043317552656 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.00508803040980169 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.004692834226743263 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005068843286370406 // Train Acc: 0.07998171846435101\n",
      "Val Loss: 0.004667795776827808 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00504968457679065 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.00464289025951396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0050305951658254945 // Train Acc: 0.07998171846435101\n",
      "Val Loss: 0.004618039371801869 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [12:15<28:56, 248.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1484680009607423 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07201402327593635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.015148824680778768 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.01631241206504295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01044281131129923 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.01065787730861784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008594558624698826 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.0082473073647741 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007262456160161297 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.007027800523621195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0066172670108186156 // Train Acc: 0.034277879341864714\n",
      "Val Loss: 0.006425044622600955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006324347707066665 // Train Acc: 0.034277879341864714\n",
      "Val Loss: 0.006270482143222848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006137920474808942 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.0061363770338871025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00599325018516917 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.005982415100066539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00587918780875176 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.005836626281961799 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005785704274973415 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.005696568008073989 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0057052190010555065 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.005559184903498082 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0056332950825058944 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.005425000796094537 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005567296256798182 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.005296339130248217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005505531193719159 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.005175511373261756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005446959427499146 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.00506395850714077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00539107669346627 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004961685626767576 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005337801634549407 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004867352004272535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005287388937357963 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004779097131069969 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005240316895199377 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.004695686169297379 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005197099653895011 // Train Acc: 0.034277879341864714\n",
      "Val Loss: 0.004617079123443759 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005158068328628477 // Train Acc: 0.034277879341864714\n",
      "Val Loss: 0.004544011601383853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005123280606916178 // Train Acc: 0.034277879341864714\n",
      "Val Loss: 0.004477336037191837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005092519917418939 // Train Acc: 0.02856489945155393\n",
      "Val Loss: 0.004417467543937485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005065385273732653 // Train Acc: 0.02856489945155393\n",
      "Val Loss: 0.004364281249068239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005041371855392739 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.004317178072490017 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005019951308747563 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.004275314520140562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005000636940549589 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.004237766120829345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004982999569646609 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.004203707335845512 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0049666908137173385 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.004172410788562368 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0049514301980835415 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.004143331908648286 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004937003955057306 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.004116039723157883 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004923247615168283 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.00409022591980722 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004910039862158573 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.004065662194454276 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004897292681841582 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.004042180843979996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00488494190782644 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0040196654530625575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004872939774449029 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.003998024265884477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004861254632065219 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0039771916413241445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004849860283822397 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.003957116517567021 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.00483874034277424 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0039377585498561315 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [17:30<27:26, 274.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.16205491194879496 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07902826566029997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.013046575874691213 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.013662967536434093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.009657792153877208 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.009583578181370874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008294265776859015 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.008169577169396421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007883136519494805 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.007699204225312261 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007647232440048301 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.007433651617782957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0072203420838545425 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.006952510461868609 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006815747467297005 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.006698704749236212 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006572788397371224 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.006447686204303275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006312708623649406 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.00605165698866853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00611532592832688 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005728487185586025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005990275772632367 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.005552872871596585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005897444522202397 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.005429325947154532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005822220896592047 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.005326534183148076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005758660775864705 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.005237664328888059 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005703275349890129 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.005158282658906982 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005653703546532902 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.0050858029862865806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0056083174580938 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.005018801572184791 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005566070812509158 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004956075291642372 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005526409839544704 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.00489657636567512 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005489181910945406 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.0048397163833108015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0054544383452241555 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004785312866956434 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005422210643047525 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.0047333817723590665 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0053924061990065515 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.0046839555520016484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005364842683722144 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004637225678272764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005339298058059441 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004593387636465623 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0053155151319475525 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.00455244562437977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005293225962008899 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004514212175683283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0052721622474261745 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004478376252604101 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005252077055127076 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.0044446630738949515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005232762966847296 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004412860743126229 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005214079194536205 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004382997919010985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005195986333416377 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004355257907745373 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005178575932113811 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.0043300857342889205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0051620635664783475 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.004307871547472828 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005146707301717626 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.004288856987841427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005132666564784038 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.004272844005540451 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005119902955540975 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.004259281802018557 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005108219064887906 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.004247519273228724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005097367433968155 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.0042369679431430995 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [22:44<24:04, 288.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1730512769916648 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07307219450526378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0120223411588744 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.012759241145378089 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.009790408872178597 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.009697096610633546 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008662847548781109 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.008153027217999539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007765774155619607 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.007185377680477412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007352231688868983 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.0069588155156987555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006935292679324433 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.006801227203515523 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006544187747774615 // Train Acc: 0.02856489945155393\n",
      "Val Loss: 0.006428208818440051 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006294590903069804 // Train Acc: 0.034277879341864714\n",
      "Val Loss: 0.0060965891941176615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006122331201802033 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.005887526155942503 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006004180627027135 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.005763406207894578 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00591315893252879 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005671747357529753 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005835622548844181 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.005592863089131082 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005767217104091654 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.005518781760817065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005705974028161446 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.005441618110874996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005649404803084949 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.005353634382652885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005594917772239043 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.005250468815896003 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005540657950049331 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.005131052717949976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005485732691495303 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.004996822968892315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005430155701873998 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.00485170426477185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0053746881544670016 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.0047019680907182835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005320465164105842 // Train Acc: 0.07998171846435101\n",
      "Val Loss: 0.004554565982235705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00526853493775345 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.004415221142527812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005219542118656863 // Train Acc: 0.09140767824497258\n",
      "Val Loss: 0.004287780841182479 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005173648288835289 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.004174326056861044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0051306065669329244 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.004075205501388101 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00508998575144902 // Train Acc: 0.07998171846435101\n",
      "Val Loss: 0.003989384776693495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005051411830601359 // Train Acc: 0.07998171846435101\n",
      "Val Loss: 0.0039151786819703 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005014652735553682 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.0038508985205279555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0049796185217426585 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.0037953429821166486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004946295661899436 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.0037475944400819786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004914708812023007 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.003707082281300031 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004884923602794033 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.0036733277237919323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004857024669534845 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.003645839388756191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004831105123646046 // Train Acc: 0.034277879341864714\n",
      "Val Loss: 0.0036240233491887063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004807216349509894 // Train Acc: 0.02856489945155393\n",
      "Val Loss: 0.0036071884165079717 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004785336145319868 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0035945298287140973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004765351619711912 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0035851866636863526 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004747084044733032 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.003578317803604638 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004730315255409812 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.003573180631707039 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [27:59<19:50, 297.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.08202824175997997 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07031611276461798 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.012400055272884368 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.012909354066804928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008748413581176366 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.008632656734655885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007805448749921603 // Train Acc: 0.07998171846435101\n",
      "Val Loss: 0.007548064500203028 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007259205767758858 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.006949170636396636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006756295894836186 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.006471961873638279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006343672707314234 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.0061168617651085645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00611290710227363 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.0060269436935948975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00594463582419649 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.00588182796833708 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0057988957937388005 // Train Acc: 0.034277879341864714\n",
      "Val Loss: 0.0056984522448414385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005670405520591034 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.005529709386310595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005558620811009293 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.005365535507307333 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005467570914526792 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.005230224225670099 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00539652004129708 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.005127118647043758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0053401205874298455 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.005040495656430721 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005293167365622967 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004963148679748616 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0052521957388450774 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004893085234524573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0052153360792090925 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.0048294688287355446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005181690669540653 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004771373679806643 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005150816300854093 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.00471784477806924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005122434654921358 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004667994607349529 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005096298194991789 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004621116862194065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00507215615006511 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.0045766664175864525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005049754046623032 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.004534236367737108 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00502884331443628 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.00449348460567896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005009194790103457 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004454118983737905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004990597619671694 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004415885908255244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004972870928793805 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004378561550230884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004955854376912512 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004341949859414907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004939416393910499 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004305912503589164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004923451499046393 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004270324480089852 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0049078870933737875 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004235116302665761 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004892679125877605 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.00420031781130306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004877812662621589 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004166015964823172 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004863298608922745 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.0041323677047758415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004849158871239579 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004099561302128303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004835421433889391 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004067830176448778 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004822106573303354 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004037357770892627 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004809224818842962 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004008326001520104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004796773219209546 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.003980856101192972 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [33:14<15:09, 303.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.14024080946135728 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0866513921714881 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.014014269222958283 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.015008892295608187 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.00990903260734552 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.00984500115394921 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008655903001751758 // Train Acc: 0.07998171846435101\n",
      "Val Loss: 0.008734763805370997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008422767635991616 // Train Acc: 0.07998171846435101\n",
      "Val Loss: 0.008406728786854622 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008262656741244361 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.00819283735472709 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008115122990744015 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.008037906160632916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00793419527660347 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.007847083369543886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007664717577007928 // Train Acc: 0.09140767824497258\n",
      "Val Loss: 0.00751473691643161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007361837839401264 // Train Acc: 0.08569469835466179\n",
      "Val Loss: 0.007185929612366154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007140880190108442 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.007028802366488997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006954145063586547 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.006980126703103238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00675477989535554 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.006970277069793905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006537124157884929 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.006917015491875217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006334262380335596 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.006740627454265076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006166990264957359 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.006515030591639087 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006032375686068149 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.006295153337037738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0059161143459573766 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.0060744087540489785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005804243063812963 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.00583715208888273 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005681797597936986 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.005581628675024737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005566926356370968 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.0053457480937461645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005482434981053106 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.00518371890682508 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0054178741734800095 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.005075435352730839 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005363329970792883 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.004992677237811115 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005314774270266485 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.004922288908239673 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005270561462155861 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.004857913423192632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005229860484826513 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.004796359602444093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005192171508349288 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.004736168361405897 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005157178468637039 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.004676956940195797 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005124695626427155 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.004618793054867317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005094613044048528 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.004562049207058461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0050668548461657624 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.004507257777731866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005041336468114455 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.004454965799060815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005017933716773612 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.004405764366417904 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004996471436033655 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.004360051733195125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004976730646439564 // Train Acc: 0.07998171846435101\n",
      "Val Loss: 0.004318046715685769 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004958472529301076 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.004279756264569347 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004941459771984647 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.00424497714608579 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004925473435472023 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.0042133497635779135 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0049103215987831555 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.004184469255372225 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [38:29<10:13, 306.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3132089689966475 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.09011674179312061 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.013706439522042244 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01395061327254071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.010611329200444317 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.010561093410668784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00896072862156917 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.009125736021601102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008402203053058938 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.008704000734724104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007455499804028782 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.007380702019230846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007138192296961162 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.007053136079133872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006920223020679459 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.006885982666383772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006685651427814209 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.0067343333571711 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006418662886790782 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.006578152526772636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006215945136702706 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.006416643079479828 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006073605378372918 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.006244555170483449 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005966740094467494 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.006102697334854919 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005883819273255009 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.0059873682980918705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005814950833275022 // Train Acc: 0.05712979890310786\n",
      "Val Loss: 0.005873805535135462 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005753648371271401 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.005750372115632191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005696491680003301 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.005616877988145193 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0056421682001278825 // Train Acc: 0.07426873857404022\n",
      "Val Loss: 0.005477858960683293 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005590650075820992 // Train Acc: 0.06284277879341865\n",
      "Val Loss: 0.005339311846696278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005542395735296576 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.0052064345012802415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005497747441622645 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.005082254400750732 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005456715775930393 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004967789090348079 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005419056908350091 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004863005987478092 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005384411266004629 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004767518018043655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00535239981858986 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.00468086199287106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005322675700801709 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004602486377253252 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005294935700315063 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004531774204224348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005268920059856436 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004468034240691101 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005244410169328745 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004410520353464081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005221220186568449 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004358504981021671 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005199195751882034 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004311326425522566 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005178206840209098 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.0042683651794076845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0051581409001736555 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004229114527868874 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0051389001661033065 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004193139221409664 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005120393236221361 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.004160082145758411 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005102526402791325 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004129610250375289 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005085210772712206 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004101437886300332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00506835991697185 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.0040753074161544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005051892133296844 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.0040509823079714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005035735572417143 // Train Acc: 0.04570383912248629\n",
      "Val Loss: 0.004028246121700196 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [43:43<05:09, 309.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.14997789787615512 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0689686438178315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.013315594767031329 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.014219977660104632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.00881095221576875 // Train Acc: 0.06855575868372943\n",
      "Val Loss: 0.009360804968951818 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007730305051942691 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.00768052415429231 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007032586102224069 // Train Acc: 0.05141681901279708\n",
      "Val Loss: 0.006498378749443766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006595166910642339 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.006016073769013233 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006354528338600706 // Train Acc: 0.034277879341864714\n",
      "Val Loss: 0.005871504627387314 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006176872564764043 // Train Acc: 0.039990859232175505\n",
      "Val Loss: 0.005783507292268469 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006006951572610171 // Train Acc: 0.034277879341864714\n",
      "Val Loss: 0.005685315529048881 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005835319741950855 // Train Acc: 0.034277879341864714\n",
      "Val Loss: 0.005537248456248027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005673533606247626 // Train Acc: 0.034277879341864714\n",
      "Val Loss: 0.005350466043797924 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0055368462891592024 // Train Acc: 0.02856489945155393\n",
      "Val Loss: 0.00516246396514094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005428429626475788 // Train Acc: 0.02856489945155393\n",
      "Val Loss: 0.004995773751836489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005342448887481632 // Train Acc: 0.02856489945155393\n",
      "Val Loss: 0.004852726069443366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0052718760951599225 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.004728363676215796 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00521153755042484 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.004617266877390006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005158295770221725 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.004515510343480855 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00511058606718151 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.004421493206310141 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005067897210760856 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.004335881625165176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005030175163606403 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.004260312998667359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004997293236690967 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.004195911121582065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004968797787172649 // Train Acc: 0.017138939670932357\n",
      "Val Loss: 0.004142305902753244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004943976594412689 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.004097848805352388 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0049220762160007364 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0040604843114338375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004902452022576589 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0040284502011833385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004884601873515933 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.004000466584008844 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004868155953628003 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.003975709006601178 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0048528426133459725 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.003953612683450475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004838461749044109 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.003933786366627935 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00482486316564338 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0039158993270522096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004811931121040908 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0038996922495939277 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004799579493065111 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0038849195869475163 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004787738021504444 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0038713775301242575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004776350499040973 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.00385888058842872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0047653709153386795 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0038472791168126554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004754759970117255 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.003836440436048981 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004744482817152988 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0038262303466634717 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004734510251736333 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0038165674538022895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004724814296626423 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.0038073515375692617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0047153719666608055 // Train Acc: 0.022851919561243144\n",
      "Val Loss: 0.003798478259020211 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [48:57<00:00, 293.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictive performance\n",
    "predictive_results = predictive_evaluation(\n",
    "    data_train_real=data_train_real_numpy, \n",
    "    data_test_real=data_test_real_numpy,\n",
    "    data_syn=data_syn_numpy, \n",
    "    hyperparameters=hyperparameters, \n",
    "    include_baseline=True, \n",
    "    verbose=True)\n",
    "\n",
    "# save results\n",
    "bidirectionality = \"bi\" if hyperparameters[\"bidirectional\"] else 'no_bi'\n",
    "predictive_results.to_csv(DATA_FOLDER / f\"results_{syn_data_type}_{hyperparameters['num_epochs']}_{hyperparameters['num_evaluation_runs']}_{bidirectionality}.csv\", index=False)\n",
    "\n",
    "# split in mse and mae results\n",
    "mse_results = predictive_results.loc[predictive_results['Metric'] == 'MSE']\n",
    "mae_results = predictive_results.loc[predictive_results['Metric'] == 'MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1cff47d36e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mse_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mse_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MSE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mae_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mae_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MAE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2*1e06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_data_augmentation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
