{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Füge das übergeordnete Verzeichnis zu sys.path hinzu\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "from utilities import split_data_into_sequences, load_sequential_time_series, reconstruct_sequential_data, Scaler, extract_features_and_targets_reg, get_discriminative_test_performance\n",
    "from data_evaluation.visual.visual_evaluation import visual_evaluation\n",
    "from predictive_evaluation import predictive_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"../../data\")\n",
    "REAL_DATA_FOLDER = DATA_FOLDER / \"real\"\n",
    "SYNTHETIC_DATA_FOLDER = DATA_FOLDER / \"synthetic\" / \"usable\" / \"1y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of loading data\n",
    "- Laden der Originaldaten: als pd dataframe \n",
    "- Laden der synthetischen, sequentiellen Daten: als np array (GAN, (V)AE)\n",
    "- Laden der synthetischen, sequentiellen Daten: als pd dataframe (brownian, algorithmit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible types: 'timegan_lstm', 'timegan_gru', 'jitter', 'timewarp', 'autoencoder', 'vae'\n",
    "syn_data_type = 'timegan_lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " syn data:\n",
      "\n",
      "       traffic_volume           temp        rain_1h        snow_1h  \\\n",
      "count   104964.000000  104964.000000  104964.000000  104964.000000   \n",
      "mean      3198.733834     283.343847       0.034189       0.000067   \n",
      "std       1932.971896      11.756874       0.209961       0.000296   \n",
      "min         47.574738     254.165840       0.000000       0.000000   \n",
      "25%       1184.408617     271.283047       0.000009       0.000000   \n",
      "50%       3710.620576     285.982410       0.000434       0.000000   \n",
      "75%       4926.550740     293.681594       0.004199       0.000044   \n",
      "max       6459.405012     304.359551       5.326140       0.007977   \n",
      "\n",
      "          clouds_all  \n",
      "count  104964.000000  \n",
      "mean       41.705959  \n",
      "std        38.665880  \n",
      "min         0.081840  \n",
      "25%         0.830981  \n",
      "50%        26.513073  \n",
      "75%        85.799234  \n",
      "max        97.120476  \n",
      "\n",
      "\n",
      "real train data:\n",
      "\n",
      "       traffic_volume         temp      rain_1h      snow_1h   clouds_all\n",
      "count     8759.000000  8759.000000  8759.000000  8759.000000  8759.000000\n",
      "mean      3244.668912   282.208136     0.086792     0.000233    44.397306\n",
      "std       1946.247953    12.114907     0.901360     0.006145    39.195308\n",
      "min          0.000000   243.390000     0.000000     0.000000     0.000000\n",
      "25%       1252.500000   273.605500     0.000000     0.000000     1.000000\n",
      "50%       3402.000000   283.650000     0.000000     0.000000    40.000000\n",
      "75%       4849.500000   292.060000     0.000000     0.000000    90.000000\n",
      "max       7260.000000   307.330000    42.000000     0.250000   100.000000\n",
      "\n",
      "\n",
      "real test data:\n",
      "\n",
      "       traffic_volume         temp  rain_1h  snow_1h   clouds_all\n",
      "count     2135.000000  2135.000000   2135.0   2135.0  2135.000000\n",
      "mean      3325.263700   270.553730      0.0      0.0    45.065105\n",
      "std       1996.851023     7.864566      0.0      0.0    40.781402\n",
      "min        216.000000   248.660000      0.0      0.0     0.000000\n",
      "25%       1222.500000   265.735000      0.0      0.0     1.000000\n",
      "50%       3563.000000   271.550000      0.0      0.0    40.000000\n",
      "75%       4946.000000   275.680000      0.0      0.0    90.000000\n",
      "max       7280.000000   290.150000      0.0      0.0    92.000000\n"
     ]
    }
   ],
   "source": [
    "# Load real time series\n",
    "data_train_real_df = pd.read_csv(REAL_DATA_FOLDER/'mitv_prep_1y.csv')\n",
    "data_train_real_numpy = dc(data_train_real_df).to_numpy()\n",
    "\n",
    "data_test_real_df = pd.read_csv(REAL_DATA_FOLDER/'mitv_prep_3mo.csv')\n",
    "data_test_real_numpy = dc(data_test_real_df).to_numpy()\n",
    "\n",
    "if syn_data_type == 'timegan_lstm':\n",
    "    # load sequential data (which should already be scaled)\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'8747_12_5_timegan_lstm_16_8k.csv', shape=(8747, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'timegan_gru':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'mitv_28499_12_5_gru_unscaled.csv', shape=(1, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'autoencoder':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'8726_12_5_lstm_autoencoder.csv', shape=(8726, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'vae':\n",
    "    data_syn_numpy = load_sequential_time_series(SYNTHETIC_DATA_FOLDER/'8759_12_5_fc_vae.csv', shape=(8759, 12, 5))\n",
    "\n",
    "elif syn_data_type == 'jitter':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'jittered_01.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "elif syn_data_type == 'timewarp':\n",
    "    data_syn_df = pd.read_csv(SYNTHETIC_DATA_FOLDER/f'time_warped.csv')\n",
    "    data_syn_numpy = dc(data_syn_df).to_numpy()\n",
    "\n",
    "# Loot at real and syn data\n",
    "df = pd.DataFrame(data_syn_numpy.reshape(-1, data_syn_numpy.shape[-1]), columns=data_train_real_df.columns)\n",
    "\n",
    "print('\\n\\n syn data:\\n')\n",
    "print(df.describe())\n",
    "\n",
    "print('\\n\\nreal train data:\\n')\n",
    "print(data_train_real_df.describe())\n",
    "\n",
    "print('\\n\\nreal test data:\\n')\n",
    "print(data_test_real_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Predictive Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"seq_len\": 12,\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 32,\n",
    "    \"hidden_size\": 12,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"num_evaluation_runs\": 10,\n",
    "    \"num_epochs\": 500,\n",
    "    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERPARAMETERS:\n",
      "seq_len :  12\n",
      "lr :  0.0001\n",
      "batch_size :  32\n",
      "hidden_size :  12\n",
      "num_layers :  1\n",
      "bidirectional :  True\n",
      "num_evaluation_runs :  10\n",
      "num_epochs :  500\n",
      "device :  cpu\n",
      "Synthetic Data is sequential: True\n",
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1057, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1056, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1663835884214644 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.13610835200490648 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.020800546695389888 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.022283075185602203 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012487148700781629 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013556882054271066 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00869062442262827 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009528831380200298 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007084969765324839 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007648569802233183 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006809658794298456 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0073008171679890335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0066708687358930105 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007105777998838355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006566066045082942 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0069513507392805285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006477264050533655 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006836679476477644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0063969255072391 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006754560156396645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006321806290669598 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006697085117647315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006250543378371691 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006655852899284047 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006182292796650997 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006623198689125916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006116242092748574 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006592887172968511 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006051702606729673 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006561385871678153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00598829599517146 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006528032067067483 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005926045085249102 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006494290695306571 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0058653758614439595 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006462566855856601 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005807000466870538 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006435091990758391 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005751671363857921 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006412938921986257 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0056998893311825055 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006395050256020006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0056516993595423165 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006378390750957324 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005606704414482262 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006359119611956617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005564250307324186 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0063341760816162124 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005523634449507443 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0063019230592009775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0054842596321305545 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006262139033745317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005445697079293674 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006215432487592539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005407684876916618 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0061628643615061745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005370143896995289 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006105546456049471 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0053331383323827145 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0060446725829559214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005296821952848457 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00598173902150901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005261388262750812 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005918368697166443 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00522701135610509 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005856124006266541 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005193796225085882 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005796410294030519 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005161754340851122 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005740159461685621 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005130820122486266 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005687836454907323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005100885281648595 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0056394510628545984 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005071833698303324 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005594711072797722 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005043578999504502 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005553215406561161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.00501607384542887 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005514424283961382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004989310283720303 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005477983360726605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0049633176741955025 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005443577004103538 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004938144590722628 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005410854262984632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004913844759610013 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005379563837777823 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004890452310303566 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005349533153040444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004867928701046732 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005320471009540865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004846378953330368 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005292467475074399 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004825744082187608 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005265376150525887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004805975884664804 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005239077610895038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004787020675666344 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005213591126341592 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fanny\\Documents\\ArnesShit\\time_series_data_augmentation\\data_evaluation\\predictive\\predictive_evaluation.py:277: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([{'Model': evaluation_method, 'Metric': 'MAE', 'Error': mae}])], ignore_index=True)\n",
      " 10%|█         | 1/10 [03:31<31:39, 211.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.40808229654157246 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.3464258427129072 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0291659686116189 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03128548528012984 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013591852145317098 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014715767749722171 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009633381161023013 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009965951218927169 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007812589983095544 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008008597336490364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007277323396827169 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007285406867809156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006986801330905993 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0069959687740158505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0067678816335268975 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006918330615222016 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00658424138932754 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006899343166664681 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006412626452740608 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006844327518004267 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006237725058934196 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006664453190751374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006069203742259746 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006389983957085539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005915423955030767 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006101993070093586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005781280834068048 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005880600533119458 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005667361535342669 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00573114216711153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005571041836141588 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005612508256864899 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00548973708646372 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0054974251714370705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0054196126968939756 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005386187078650384 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005356340058552387 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005285232177223353 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005297448018304052 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005197652732022107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005241711029993629 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005121846614843782 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0051885919322949715 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0050539156723329245 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005137925241677852 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004988990915829644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0050896532144431485 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004923506814520806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005043642405067047 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004856762842631296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0049996809300161 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004789744274389437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00495750753567588 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004723610051715856 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004916826052332882 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004659240678473211 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004877345670689319 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0045971301767755955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0048388085539629476 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004537445249939885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00480102894493773 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.00448024899849449 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00476391018575642 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0044254746445564225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004727463419362223 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004373083582750577 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0046918058345590576 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0043230976238298946 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004657144316244381 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0042756370350937635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004623739799058888 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004230900879894548 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004591853687978166 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004189086499322644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00456170015670971 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004150316006202689 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004533406907550644 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004114593029953539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004507011678078327 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.00408173055963262 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.00448240293073043 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0040513538303511105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004459662854724163 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0040233309777891815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004438535809341859 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.003997151465976939 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004418913836898554 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.003972513649119612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004400648588691091 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0039491450845483035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004383604086312656 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.003926843154819354 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004367654037727281 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0039054296862827066 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004352683835449615 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.003884806263479678 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004338591227313864 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.003864908491617397 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004325281465481321 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0038456795326269723 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [07:05<28:22, 212.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.19489527921987712 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.14507277655887746 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03281139920254911 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03602672045064323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012344712899219462 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013694123043130864 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009993276390566552 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01070298528408303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00892923866826905 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009373010122491157 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008297192400583086 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008722013540511183 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007919079945745613 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008319328018628499 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00757424168613311 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00792479291832184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007270654141274111 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007607333711348474 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007003969143664831 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007392401923425496 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006721568513529063 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007214391384931172 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006499341600676523 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007071687846773249 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006338551170175664 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006983662099467919 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006202814927202289 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006934779086698066 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006071533107905764 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0068895380040082865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00593477079430663 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006828736726139837 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005783326707161745 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006759798982838059 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005627472603261933 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006679167368394487 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005491383944342361 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006567225990104763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00537741089976447 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0064360680579043485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005279477132292274 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006302119818214765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0051934115759039255 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006172897646833649 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005117667437000812 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006051233024610316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005051795859314692 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005939325030126116 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004995207799374241 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005838677259709905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0049466770431230754 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005748805336599403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004904517937064116 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005667542970246252 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0048670211201873575 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0055923098972176805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004832746500547051 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005520898157128078 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004800614293105561 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005451803547604119 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004769872007279718 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005384115368703052 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0047400348085186094 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0053173643845023915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004710794530807226 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005251405462759602 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004681974168260153 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005186256558826084 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004653488626588055 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005122037344228695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0046253206223643714 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0050589121802819565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0045975021579061515 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0049970935626119815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00457012059165328 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004936766485437094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004543298775547721 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004878136644359021 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0045171926162599916 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00482140006565982 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004491972482302668 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004766748029985191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004467807274675472 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004714425612131462 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004444828028382774 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004664750105482252 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004423135885183638 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004618039382073809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0044028014779502155 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004574237021562808 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004383823006281317 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004533445301657433 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004366154934366295 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00449570786440745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004349723428655178 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004460947416887125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0043344422398401885 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0044290232848759525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.00432021952175576 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004399776784018339 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [10:33<24:36, 210.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.24222830399517378 // Train Acc: 0.0\n",
      "Val Loss: 0.17911868361646638 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.021938117620378842 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.02415884128662155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013098338093306109 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.014687166747856228 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008873893923540838 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009675028956467834 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007411057646573269 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008047642761512715 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.006950297777840093 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007442813578938299 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006720965884942018 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007044307562276064 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0065633882832183185 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006774477386737571 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006433621021735407 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006581199783420957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006316931193429351 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00642979984609958 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006209873912148994 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006305546112879015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006112197068193588 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006202238248935079 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006022562695170895 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006115072228781441 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0059379080937577095 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006039524793296176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005854935133336859 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005974293337203562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005771806413039266 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005919669385013335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005689429434910281 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005870298776939949 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005610017835869569 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005815976436304695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005533606328926709 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005751907250241321 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005458691136156936 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005679408040390734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005384424540730887 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005602070349542534 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005310629870076358 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0055229927084463485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005237746592882993 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005444009870509891 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0051670055849185334 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005366683841737754 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00510004290462859 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005291679894606418 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0050380281933216225 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005217919406919356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0049812072598253015 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005143758060191484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004929677097222013 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0050693641224985615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004883636234423322 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004995894625180346 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004843242061120479 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004924877135850051 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004808298021514159 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004858011184671127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004778144600712796 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00479682075092569 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004751866842054143 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004742178285275312 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004728561691492864 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004694223424474544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004707487898685011 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00465244272559443 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004688089795029947 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004616001521593288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004669963036974837 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004583915760603678 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0046528218200185545 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004555224779965904 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004636454129554195 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004529080456396674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004620704223974574 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0045048276797923096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004605455782917076 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004481935134047971 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0045906179932958995 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004460008219158386 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0045761194064818924 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004438728518674479 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0045619003693947065 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004417889213929062 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004547916762539301 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0043972975690849125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004534130062428677 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004376820860403206 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004520511620484265 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004356377408839762 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004507040388688418 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0043358857887249224 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004493702268465864 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.004315313287353253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0044803932414519305 // Train Acc: 0.034215328467153285\n",
      "Val Loss: 0.0042946816127582945 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [14:03<21:03, 210.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.14561759936113428 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.10760653358610238 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02730935438107835 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.030979789300438237 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011314243469540247 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012329555390512241 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009006198156577447 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009708206566488919 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008198278391458447 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008638707160785356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007850338402374165 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008147285402040272 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0076266132217804716 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007924739107051316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007439896845034439 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0077825716682983674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007275765733691408 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0076506637338110625 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0071332937567052945 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007524823044043253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007009194860486382 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007408768986351788 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006897972266607829 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0073014398598495655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0067909854994474975 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00719443175886922 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006603614502351894 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006912447964115178 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0063134594285099285 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0063883204780080735 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006114831515485896 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006105886176502442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005977468716361335 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005923311698579174 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005880908504804855 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005825147717533743 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005799953954933334 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005757960792192642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00572335765566201 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005700865308480228 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005648002580375866 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005652662181733724 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005576743501286111 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005615427110837225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005512595012270757 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00558796657167156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005454724868691533 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005568660734056988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005400881953843373 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005556499893667505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0053493829731265236 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005549541688250268 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005299281161728852 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005544312469496885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005250109067733951 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005535413483705591 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005201744053973375 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0055131565382265865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0051543214422120395 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005457064917530207 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005108193406855592 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005331607569721253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005063380670043755 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005123637260540444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005019781207767771 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005021587904433117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004981091305491154 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0050139614819165535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004947262202613872 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005003674524178838 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004917529801860193 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004978523843045182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004891255169207856 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004942549570627949 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004867645340205177 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004900493850821958 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004845965874167227 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004855875042267144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004825654990688972 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004810902831035063 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004806307424879531 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004766600946074023 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004787636765093631 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004723126726115451 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0047694583264957895 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00468043378992554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004751673198124924 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004638826118453461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004734240817951283 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004598964087884216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004717164890658804 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004561412928845076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004700475403814692 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004526359768693938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004684213335399794 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004493701949660831 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004668408267014749 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0044632329441168725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004653073052674307 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00443472879637471 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [17:34<17:32, 210.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.13392097369026745 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.10891816504847478 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.029186651987587884 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03235902078449726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012350013155411984 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013576089719529538 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008800312671487485 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00960671334691784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007659278127677521 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008170948124161977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0069536315149756785 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007352102253421703 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0066621956996701275 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00719513836563291 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006428262730805462 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007113732315380783 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0062220011154583985 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006988588193267146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006018920365659806 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006811505416408181 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005833973044216851 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006637099678354228 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005677407549228519 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006471096401048058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005556615592952627 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006298212950830074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005467850731916889 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006159407483852085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00539822371130177 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006062065894879839 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005340108342033668 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005987404414233477 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0052898638390164385 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005922912804902915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.00524557685143022 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005863302692031378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00520599495392653 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0058067204423851385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005170160684867143 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005752294478417539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005137329427848305 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00569938455352231 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00510692350667507 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005647440831286504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005078501155791995 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005595995606306721 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005051728809825111 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005544801845270044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005026342861998459 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005493751559269559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005002140029066711 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005442910800304483 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004978958961518182 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005392425457316944 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004956670391503348 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005342492676230476 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004935168419105518 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005293317830792683 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0049143654554971855 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005245136864045087 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0048941896962176965 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005198132938376683 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004874578164083149 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005152466501492788 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004855473995307311 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0051082573276396625 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004836816572347132 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005065595025799292 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0048185704789212806 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005024558522135895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004800696683818998 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004985200501430561 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004783155302577448 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0049474871484562755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004765913913529502 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004911368853166042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0047489412827417254 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004876795601483215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004732209152412893 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0048437081790967465 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004715690670435548 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004812037575688651 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004699362619919363 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004781718324760304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004683202840315592 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004752671524115345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004667192386942542 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004724832792656825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004651314244362573 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00469814012210597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004635555841420423 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00467251826707712 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0046199055950040075 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00464789624846376 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004604351900545133 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004624213365947499 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0045888885213266105 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004601402577998883 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004573507129722852 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004579411422395531 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [21:05<14:03, 210.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.09431249704755788 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.08810141638797872 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.023750761158451657 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.025923799805562284 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01213972572573753 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013529107087593088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00956465501464006 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010037546686362475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008209074615844166 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008502822091994697 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0075830451206875165 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008010708035298568 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006994746883746481 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00739775547374259 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006485555929821801 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006693264948861564 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006124395664686161 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006223904078497607 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.005929005496929584 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006009987117174794 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005788520581843535 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005907335243297412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005673541819777027 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005863616790841608 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 118\n",
      "INFO: Validation loss did not improve in epoch 119\n",
      "INFO: Validation loss did not improve in epoch 120\n",
      "INFO: Validation loss did not improve in epoch 121\n",
      "Epoch: 121\n",
      "Train Loss: 0.005578641699419703 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005858587354476399 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [21:57<07:56, 158.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 122\n",
      "Early stopping after 122 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.1746854621740262 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.12235826903762405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.026930667276419427 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.02953013325767482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011907487695129167 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.012887932637514657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010173829567251578 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.010941791950779803 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009335747614917584 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009925143611754346 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008572614424178771 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009016242795897758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007834197986908393 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008292019867119105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0072879735116989615 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007859692135003997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006965799455150255 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007698671371840379 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006762382392958486 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007612596026293057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006601879723713373 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0075255131335271635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006473278546923377 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007435068351161831 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00636153824901018 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007349830778206096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006260157996163207 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0072733620181679726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006166712445526445 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007201696787139072 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006080032620293489 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007126121069578563 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0059996680836669125 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007039283909013166 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005925829607286375 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00694074370103943 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005859521625097841 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0068371124729952396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005801910051844851 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00673690066426335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005753043306104322 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006644870935227065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0057114817327004 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006561401676770081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005675224046032522 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006484990028719253 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005642630415628698 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006414079935947324 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005612622619348232 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0063475208726766355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005584528191307437 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006284425035119057 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005557894871576968 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00622408079695614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005532389619289115 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006165868945091087 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0055077576922273165 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006109261460711851 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0054837945847853635 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006053792285349439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005460338279975402 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005999050082584076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005437259949373258 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0059446797993801094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0054144677070877 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005890385254614931 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0053919093372354655 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0058359275366563134 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.00536957647023471 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005781122942126411 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005347508236291703 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005725916077875916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005325792390040809 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005670367959229385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005304533580996191 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005614722969339174 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005283860982579254 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005559373790781726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005263885800951045 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005504828562740894 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0052446807332313106 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005451636197155013 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005226261551571792 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005400298286558074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005208593585025383 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005351178745246109 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005191614911262707 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005304490079117172 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005175246932946594 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005260274026488119 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005159410726437073 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005218459491837113 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005144035368248562 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005178881453021485 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005129060134194778 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005141333334476632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0051144327013243505 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005105598802294801 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005100111753244742 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005071467795299695 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [25:28<05:50, 175.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.22234131490648554 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.17113299034270527 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.032392486831322856 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03596222565016326 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012650248882646271 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.013963668504455948 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010573515445452836 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.011421761129681459 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009571687063460585 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01007985376396819 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008783983223031472 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009080105981625178 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00798736465415054 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008335283238385968 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007179313278536102 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007816342582159182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006815963665645461 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007570505032644552 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006668987164756514 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007451778520857368 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006558483102389469 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007358205199981218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00646433035374014 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007278411546448136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006380768702705357 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007211632382891634 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006302795158342261 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007153052076532999 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006226171000544525 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007094952496973907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0061475810272802675 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007029442253577358 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006064534095969106 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006950539379271076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005975484050535019 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006855680189533707 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005880159469080042 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006746168262051309 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005780083571312555 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006627101971603492 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005679080862701483 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006506692077142789 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005582619508572031 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006393608096165254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0054953901074235294 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006293377750005354 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0054188939852361315 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006206756861716071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005351393088446862 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00613036220345427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0052898752118161725 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006059450378148433 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005231621571736628 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0059890838422100336 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005174787869094021 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0059150879695901975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0051184424135857776 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005834365270429236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005062318234925369 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005744804593953578 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0050066193811303126 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005645578179289313 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004951768334237791 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00553745381287573 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004898280603173834 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005422657817903468 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004846641680193439 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00530439498149516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004797287338224654 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0051862875287256695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004750582553109793 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005071733592023306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004706761618119318 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004963243345949142 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004665914738132707 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004862274139133447 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004628036372375464 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004769383943365777 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004593159520555029 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004684787137461279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004561152736389547 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004608206327675897 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0045318551222202335 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004539255857440259 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004505050899061531 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004477378823246588 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004480492479034468 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004421870172818136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004457924785653306 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004371989501284107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004437098354160663 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0043269675601657264 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00441778440518733 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004286085772465038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004399776185546614 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004248694516718388 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004382890297344675 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0042142301955369905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004366970256507995 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.004182234844740699 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [28:59<03:06, 186.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3494212408991952 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.26421366336152835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.028622950039749597 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.03096781232777764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012780875578967949 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01394311708795345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.0106559843687429 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.01162413479280932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00911164813612105 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.009899194660486983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008095324951662498 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008949064738664995 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007682789485463125 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.008411003940957873 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007307050261904832 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007932308280621381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006997647077052508 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007613926311023533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006809563951712972 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007480688043870032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00666712313112089 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.00739580493502538 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006533251279951447 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007303282769177766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006395182184395754 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.007178157800808549 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006251561172038018 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0070070299621233165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0061126964835511235 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006803006057024878 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005990281147902736 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006606481099665603 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005887563356086204 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006437771261998397 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005803104840987192 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0062973184154971554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005733524088027214 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006182564237593289 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005674921347926214 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006090298233389416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0056238730903714895 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.006015758310882922 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005577913425560279 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0059536600841538 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0055355182356017565 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005899550968452412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00549581794690232 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005850277395973749 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005458314790489003 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005803795236929813 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00542269839886508 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005758808617589667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0053887467578148645 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0057144868393045134 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005356283013557635 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005670288036686971 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005325168373537365 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005625894019270644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005295285295880311 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005581148434430361 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005266540424633146 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005536051206838559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005238856688816522 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005490747164003551 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005212169874546179 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.005445421851404449 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005186426637687442 // Train Acc: 0.011405109489051095\n",
      "Val Loss: 0.0054002867477452934 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005161580072850138 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00535557877190192 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005137587189003078 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0053115199005012124 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005114408355014208 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005268305170294993 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005092005872500748 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005226092686986222 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005070338817653212 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005185001440729727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005049363687892791 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005145114565761212 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0050290322076124785 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0051064792856135785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005009293716390664 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005069109104464159 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004990093109204712 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.005033002515761729 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004971372333897386 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004998104372883544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.00495307153472371 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0049643715679207265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.00493513359743456 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.00493175134418861 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0049174993025329315 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004900153849602622 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004900117029595685 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.004869517863399404 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004882936205937574 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0048397669623441554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004865911563843094 // Train Acc: 0.02281021897810219\n",
      "Val Loss: 0.0048108065350200326 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [32:30<00:00, 195.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (6996, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1741, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.15845979037418212 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.09614819347519767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.030899900581568617 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.028389881263402375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014811357296684324 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.013558025802062317 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010952222084447946 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009503958137197928 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009327071362183553 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007952105956660075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008651714295693185 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0071399485480717634 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008290961054132477 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00664562282406471 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008058028751729082 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006318153597583825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007860884398379316 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006101193181662397 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007656103444443037 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006020307151431387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 94\n",
      "INFO: Validation loss did not improve in epoch 95\n",
      "INFO: Validation loss did not improve in epoch 96\n",
      "INFO: Validation loss did not improve in epoch 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:34<05:07, 34.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 98\n",
      "Early stopping after 98 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.46138461357722543 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.33221684138883245 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03705132484742223 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03159212666479024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014550721905402601 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01199410811743953 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011990151949095026 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009928247040476311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010663855110176043 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008878337939015844 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009829855508587778 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008088496958159587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009283599836568956 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007460189585319974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008867091397206261 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0069582907186651775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00848814155098442 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0065449022391641685 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008139808449073435 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006220462372187863 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007790159569632211 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00591327819803899 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007410516691039057 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005598811961879785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007028055427804351 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005393460274420002 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006804037243010876 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0053500412328338085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006677521815838199 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005308067320253362 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006581331221025318 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005261932182210413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00649511231456088 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005212244924835184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006410706385897821 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005160351914607666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006324704239978433 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005107499828392809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006236812906216439 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005053092780607668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006149510377231288 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004996580084447156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006066331644138517 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004938979290256446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005989145355184295 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004881519062275236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0059177033091814485 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004825184578922662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005851011890726648 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047710002950308 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005788287160105867 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004719845963303338 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005729114436374374 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00467238464308056 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005673455579427483 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004628882586786693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005621472930165697 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004589281061833555 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005573292312288835 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004553528896278956 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005528834664377652 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00452179265293208 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005487820421681488 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004494109847159548 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005449897180934389 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00447030811231922 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005414679740049523 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004449990618211979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005381812119312142 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004432603484019637 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005350987348263258 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044176226820458065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005321949414194446 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004404552820646627 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0052944797226206555 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004393021533773704 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005268400944567383 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004382709108970382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005243562429305591 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00437334964517504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005219834884330971 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004364710438742556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005197111058705933 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004356622365726666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0051752997516573635 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004348910629579966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005154323482143876 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043414291396567765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005134123202081616 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004334069600074806 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005114648110003606 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004326831530356272 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005095843642207977 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004319467264312235 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005077691470457828 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004312063196927987 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005060149921257406 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043045618359676815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0050431841563913955 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004296908813359385 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [03:30<15:40, 117.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.17881353896910743 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.1183706659823656 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03221713371370753 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.028068699179725213 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015878245972318055 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01422825361686674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012155658068968893 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010069208769974384 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.01014313696815577 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00822223573923111 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008834691193554397 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007139189088378441 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008213345085793775 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00652286254546859 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007719209291185487 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006224635802209377 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007336911917600321 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005947323384779421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00710764759726244 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00575607345354828 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006956345107742334 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005627149707553062 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006837747752938641 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005531379055570472 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006734087737383466 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005446254229173064 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006636694160324083 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005362149510024623 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006542064896976091 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052780933635817335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0064501804496336 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00519510051658885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00636246051484098 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005114874434234066 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006279941903933305 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00503936798565767 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006202930767943904 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004969642840495164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006131206942109825 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004905397961424155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006064222997998538 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004845630115067417 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006001352122027988 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004789486510509794 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.00594205568920923 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004736550910059701 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0058858924567673915 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004686667029322548 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005832468755925967 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004639724631455134 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00578140094929447 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045954885172911665 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0057322913238600105 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004553594715385275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005684730868476133 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004513562989251857 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005638326197165196 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004474894600835714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005592722883745687 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0044371402885934165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005547624114596765 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004399939819069749 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005502792391564578 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004363065204498443 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0054580465019744345 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004326392214914615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005413267506312018 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042899021646007895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005368405404196601 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004253730990669944 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0053234824003889885 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004218315714123574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005278590622200678 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004183797506530854 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005233926892856808 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041504248066551305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005189776900075546 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004118497684513303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005146481261533349 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004088294580155476 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.00510440208522824 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004060054231773723 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005063885781163409 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004033987348983911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0050252399757636295 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004010390265929428 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004988751330728681 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003989533953030001 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004954669513866161 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003971563798206097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004923148774680124 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003956445852633227 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004894221192502234 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0039439667630094015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004867802555598403 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003933797747066075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004843719963959235 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003925520882264457 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0048217614876110634 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003918900762007318 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [06:25<16:49, 144.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.21623541266746718 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.13910014595497738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03579247596176111 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.030527005649425766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.015069443605984795 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012767818049443038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01170721976139655 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009725928585976363 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010134353925958786 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00837953332811594 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009341331262069172 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007692309380085631 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008724651695441744 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007318946287374605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007840107400267976 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006918215125121854 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007281870380805202 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00604465686800805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0070985027176400295 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005707001017237251 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007005250981379487 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005569150582463904 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006939400288331658 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005488920880650932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006884182644391339 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005429624977775596 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006833841365583445 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005379961828955195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0067859633329226616 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005335458084432916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006739277660540522 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005293840453536673 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006692960958813049 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00525373013436117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006646395959151424 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005214161171831868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006599073823473361 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005174326210875403 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00655054525508371 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005133482382040132 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0065004030741181345 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005090909836475145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006448261957024501 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005045898483050141 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006393776426716924 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004997652333060449 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006336684760288867 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004945300862362439 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006276785729869724 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004887693828310479 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006213840349687936 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004823427503420548 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0061473165570702995 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004751179587434639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.006076514150003252 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004671055328270251 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.006002831757047371 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00458777332466773 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00593370077525559 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004513082835315304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0058770219058640405 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004456966465593062 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005832343974116696 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004417727606117048 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0057960013262544794 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004389088478108699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005765313386266464 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043666083027016035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005738573037215496 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043477069565349006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005714645265860985 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004330896666612138 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005692751431246565 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004315342713909393 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0056723661239144125 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043005760907280175 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0056531436254427725 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004286338515918363 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005634841477268771 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004272567654367197 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0056173027680174676 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042592776410112325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005600366856771006 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00424650505777787 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005583975992227169 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004234293321232227 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005568055718548205 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00422268075953153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005552562225299577 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004211716605773704 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005537458549287663 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004201427502134307 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005522719132565132 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004191818803718144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005508331529589872 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004182904499413615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005494278061977635 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004174680283970453 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0054805466265217765 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004167172962545671 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [09:22<15:40, 156.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.09756513085369378 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.06624281386082823 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.028322398237305688 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.023461165380748834 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.013354088717191289 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.011744148957289077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011134753661276985 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009323029241270639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009792034797313023 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007950474613938819 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008622184895472304 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006954346931624142 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007994998137458104 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006472014339471405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007669903718564472 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006315976470200853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007411491419252468 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006162012194875967 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007190800371333174 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005972642121328549 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006992817652777484 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005762459350411187 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006797816954684959 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005549409164285118 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00659581208463384 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005362159077247435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006406179117889369 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052199850959534 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006255179626620523 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005115172525190495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006143565720784984 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005036033035933294 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006054913618742195 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004968155020932582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005979129505560618 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004905534855259413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005911855974349461 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004847129188816655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005850668991256727 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004791901861740784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005793972936518422 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00473858537575738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005740617753801757 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004686369078064506 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005689754747768617 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00463513614482839 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0056408263783323575 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00458535095888444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005593537385405369 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004537766564383426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00554778114212124 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004493180505762046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005503542197136778 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004452225493944504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005460828570882922 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004415240826677869 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005419637012235888 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004382238646080209 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005379953459394271 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004352971226696602 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005341768464909177 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004327061260119081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005305076751817596 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00430411662991074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005269866659450579 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004283785512035882 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005236103657907359 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004265775347382508 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005203718907990786 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004249815684108233 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005172612632999035 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004235649293034592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005142663531090435 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004223011151506481 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005113756479472177 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042116337085396725 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005085795682155395 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004201354656834156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0050587173900566995 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00419204829218374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005032457554549166 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004183618190952323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005006946365510594 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0041760016793639144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004982107801609524 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004169163379860534 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004957856410544485 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004163084197154438 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004934100048259309 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004157748086039316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004910742066381712 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004153127113187855 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004887693837501925 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004149196945681152 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004864881156686455 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.00414590653963387 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.00484224800876295 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0041432125642049044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0048197683621491365 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.004141082087616352 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [12:17<13:37, 163.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.29417136160988516 // Train Acc: 0.0\n",
      "Val Loss: 0.18572988496585327 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.038918148441299726 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.03358148170465773 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.016375214633314986 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.01380441224371845 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.012580613787477352 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010631449778818271 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.011249932779964554 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009241521278057586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.010315592847785755 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008346318381584504 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009606049157951248 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007700681843032891 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.009002573972091622 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00714869446713816 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008447366277580937 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006610705809329044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007998565126308262 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006178380925716325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007724457966869688 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005991433222185482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00752869869962964 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005936174805868756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00735563440641737 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005887345918877559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007214754383445433 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0058235508855432275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.007097794250138614 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005744704841212793 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0069959157903478845 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005660102812742645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006904811278351298 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005576514990322969 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0068207761633197125 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005496064387261868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006739817581513735 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005418333059854128 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006657935690122006 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005342272262681615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006571568241959531 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0052671405571428215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006478730309541979 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005192634632641619 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006380305385757153 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005118750670755451 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006279577601586201 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0050454759250648995 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006181172727815569 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004973536717112769 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006089926518703976 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004904976410960609 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.006009160231088701 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004842304053123702 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005939779994524505 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004787438917397098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005880642551014447 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004741069589826194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005829658577767239 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0047026007774878635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005784837346200802 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004670680396851491 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005744659742680655 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004643740267916159 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005708020404789206 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004620300224897536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005674158070410802 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004599218485368923 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005642554575479568 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045796826778149065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005612853777884042 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004561189422383904 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005584804350765396 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004543395196510987 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005558220510685723 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004526101586154916 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005532955487160922 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004509151798926971 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.00550888181604513 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004492434711110863 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005485878066589299 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004475854436697607 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005463860157162767 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004459383211691271 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005442764012559772 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004443074085495689 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005422503017654566 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004426930084909228 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005403010587135678 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004410998095673593 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005384229107113496 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004395341701720926 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005366119182696269 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004380048347891055 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005348662935095291 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004365238462659446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005331856098672434 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004351018068634651 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005315705096736108 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004337495605630631 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [15:13<11:10, 167.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.5265531100706967 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.3633921605619517 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04735035672103433 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.04116662242873149 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.017632485758598264 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.014828530614348976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.013309628777120892 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.010954783983867277 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010951625917863831 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008808477087454362 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.010319979362228043 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008353717786005951 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009853532799353746 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007982701918279583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.009416274266576896 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007592036896808581 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008988918352247 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007214919710531831 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008593471027106966 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006883966359733181 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.008321557540316816 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006647211732342839 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.008167634343890022 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00647375384260985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.008067246220823054 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0063381759873168035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007983269072648087 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006229849049652164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.007898945491612127 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00613884422081438 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.007804386481721396 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006054425061765042 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.007699620961020254 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005968189269134944 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.007591678871247985 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00588053471239453 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.007482257097817056 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005797772854566574 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.007370409674274833 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005724688737907193 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.007256277570007666 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0056625831999223344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.007143233899031362 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005611825285648758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.007036136060349269 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005572211645035581 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.006937140544672331 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005541748528114774 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.006845728579279222 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0055173274552957575 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.006761313652615467 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005496193303472617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.006684190519239124 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005476531932469119 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.006614758996675684 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005457040041007779 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.006552676697363694 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005436688670041886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.006496777940333128 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005414921621030027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.006445573899601522 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005391437175091017 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.006397744295714619 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005366276356984268 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0063521922002525225 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005339690890501846 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0063079841765596376 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005311872822825204 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.006264297998633627 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005282861531966112 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.006220407286429242 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005252595364370129 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.006175655831761454 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00522087526677007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.006129446737128033 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0051874271170659495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0060812433491584305 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005151941534131765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.006030552282176708 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005113978298719634 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005976927531011335 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005072982075878165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005919984945831778 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005028287156231024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005859433040745795 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004979156004264951 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.00579512761065898 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004924880383028226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005727096916320968 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004864775033837015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0056555654096617 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004798187268897891 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005581045262281651 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004724670682017776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005504642716889614 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004644713485190137 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005428422327986897 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00456090562087907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005355009017086941 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004477698544294319 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [18:08<08:30, 170.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.222485225613667 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.14759590480137955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04223453130181794 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.036644103140993554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01499156113799032 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.013015615406700156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011798350268562397 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009806569602171128 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.01009135241630874 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.008245454914867879 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008920593220436495 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007149258615787734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008337678190014542 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006750161920420148 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007935300568005997 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006522833822633732 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007563497754745185 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006280875866386024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007275370799630166 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005971387155692686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00707970487780988 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005711720409718427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006934808172736334 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00553611807779155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006816001649899196 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0054123102306303655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006711456685998906 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005312867246737535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006613597205784767 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00522249117070301 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0065169460889691975 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005132304847409779 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006417691732595194 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005036530478603461 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0063140769042585 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004932493763044476 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006206990202642195 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004821477089585228 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006100074798995671 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004708868277851831 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005998919803285588 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0046032826966521416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005908740586845723 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004512994052757594 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005831694352965734 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004441161673854698 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00576620596778881 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004385443914427676 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005708942719528954 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004341427901421081 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0056569543188718496 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004305176371285184 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005608295862346071 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004273831156421114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005561820220746542 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00424543001096357 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.00551690345632176 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004218661437996409 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005473243951857022 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004192672650837763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005430770201765885 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041669439884241335 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00538959770546333 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004141264779239215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005349931736385288 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004115633437917991 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005311927677357578 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040900965835052455 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005275638455091274 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040647348325530235 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005241036201140431 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004039695420810445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005208052783570843 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004015218959697945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005176614780461663 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003991640875623985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005146662837275002 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003969341804358093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.00511815634260558 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003948691074567085 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0050910764621553695 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003930025289512493 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005065420865491628 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003913593793880533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005041187792829319 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038995537864552304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.005018366821276101 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003887963891876015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004996934770627721 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003878773981705308 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004976853685768228 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003871802215210416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004958042533659976 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038669208995997907 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004940411886604307 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003863865689543838 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004923873539795019 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003862341142005541 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 490\n",
      "INFO: Validation loss did not improve in epoch 491\n",
      "Epoch: 491\n",
      "Train Loss: 0.004908328032552703 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038620452057908884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 492\n",
      "INFO: Validation loss did not improve in epoch 493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [21:02<05:42, 171.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 494\n",
      "Early stopping after 494 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.13351334027571765 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.08618030365217816 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.04013344472844035 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.034244894405657596 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.014559012694182256 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.012616276330399242 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.011115744921004561 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009642675548622553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009445756705707475 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00814197408458726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008655981239905266 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007245783622122624 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008102188081922835 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006641701426865025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007537689923253506 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0060924600471149795 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006959169980544665 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005525476574389772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006765193990580567 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005236262349750508 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006651208451608851 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005097002739256079 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0065607992559033506 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005008787522092462 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006481156280991456 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004937075662680647 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0064088006596374385 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004871139421381733 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0063426512294580744 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004808240324597467 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00628215547695705 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004748246628283099 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006226815338050904 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004691706792536107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0061760757253484025 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004639163130724972 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006129298117288205 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004590887326577847 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006085774098995001 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00454681776040657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006044791215078131 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004506641888821667 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006005699827641963 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004469924271953377 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005967966087383631 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004436233449219303 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005931175526520705 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00440519327213141 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005895018294776819 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004376476173373786 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005859275717930401 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004349821348759261 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00582380822357896 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004325000167062337 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005788541818888152 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004301817892965945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005753455113738639 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004280117878013036 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005718576253527886 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004259778436442668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0056839688302583345 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0042407113762403074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005649722590574731 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004222860333340412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005615944518031527 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004206202505156398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005582746707028433 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004190744129432873 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005550236129159423 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004176485267552463 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005518512003236454 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004163441684266383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005487628579862654 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0041516088626601475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005457639002898224 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004140892891551961 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005428567143331482 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004131206326101991 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005400437486387755 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00412240025841377 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005373257885582864 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004114268591034819 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005347010909715778 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004106566997837614 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005321663581621599 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004099043371917849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0052971654932001885 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004091450779444792 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.005273463990173587 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004083568105389449 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005250623587893161 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004075137047435749 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0052284432902557935 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004066121457567947 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0052069042613687305 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004056572478095239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.00518596234474093 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040465263332324945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0051655774049941434 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004036046003668823 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [23:58<02:52, 172.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.12860038537174873 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.08179876521568406 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.03041009784816471 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.02606923214413903 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01494479028201879 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.014151270492849024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010945941065863354 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.009951540116559375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009128039242427576 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.007950731489638036 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008194047792869957 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0068274267017841336 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007534480241199607 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.006165563095022332 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007106082907826831 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0057573496829718355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006813096888278309 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005464876620945605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0065985384634291945 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005246988125145435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006438257596902039 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.005080454047261314 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006315774640991229 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004952070066197352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0062190576453297714 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00485144893808121 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00613892363218514 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.00477094310335815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006069044328983618 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004704750286923213 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006005546382147836 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004648537880910391 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.00594620510338291 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004599227862093936 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005889807641718826 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004554692188023844 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005835765432385669 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0045135507233102215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.0057838778941760255 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004474963922984898 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005734116700956317 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004438475580801341 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005686449078623988 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004403805472380058 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005640749362952259 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004370686090127988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005596801424377755 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0043387977639213204 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.00555436274001681 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004307795641943812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005513218176746783 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004277362405661155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005473176248107939 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004247250003655526 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005434066657886998 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004217318159697408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005395738985527681 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004187532573599707 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005358050683392084 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004157935544340448 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0053208629772403026 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004128629305738617 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005284049033376571 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004099745347841897 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00524750093745544 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040714109993793745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005211138987241815 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.004043751370839097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005174923898677712 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0040168718540702355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005138855021910478 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003990850756368176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005102976049630388 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003965739584104581 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005067362490197278 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003941567784005945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005032121717608768 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003918353049084544 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004997378001234082 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038961031050844626 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004963267898637818 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003874835651367903 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004929933014706406 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.0038545704235068774 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0048975217959856335 // Train Acc: 0.014269406392694063\n",
      "Val Loss: 0.003835358673875982 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004866178847315666 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0038172588048672133 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0048360352728145884 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0038003270789472893 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004807198296841149 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003784621638161215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004779741918077038 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0037701785141094164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004753700417379283 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.003756999607536603 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004729069360068332 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0037450628376311875 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004705813832179172 // Train Acc: 0.028538812785388126\n",
      "Val Loss: 0.0037343170747838237 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [26:54<00:00, 161.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.19375230919688805 // Train Acc: 0.0\n",
      "Val Loss: 0.138653356785124 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.022048238422466466 // Train Acc: 0.0\n",
      "Val Loss: 0.01972160263156349 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01046901435131998 // Train Acc: 0.0\n",
      "Val Loss: 0.010708041362125765 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00988467758495922 // Train Acc: 0.0\n",
      "Val Loss: 0.010195409896021539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00959432195578638 // Train Acc: 0.0\n",
      "Val Loss: 0.009916087057949467 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009361545974176088 // Train Acc: 0.0\n",
      "Val Loss: 0.009677935378964652 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009137964156800753 // Train Acc: 0.0\n",
      "Val Loss: 0.009442700648849662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008892146026881488 // Train Acc: 0.0\n",
      "Val Loss: 0.009181463709947739 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008601704037208195 // Train Acc: 0.0\n",
      "Val Loss: 0.008871124959974126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008276120240196212 // Train Acc: 0.0\n",
      "Val Loss: 0.008520069074901668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007965976213419716 // Train Acc: 0.0\n",
      "Val Loss: 0.008181638452647761 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007683741682545031 // Train Acc: 0.0\n",
      "Val Loss: 0.007873698353598064 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007408401429159864 // Train Acc: 0.0\n",
      "Val Loss: 0.0075755026995797045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00713564515843154 // Train Acc: 0.0\n",
      "Val Loss: 0.007281823176890611 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006874113768385936 // Train Acc: 0.0\n",
      "Val Loss: 0.007002481805499304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006633901206848794 // Train Acc: 0.0\n",
      "Val Loss: 0.006748993826014074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006419896067345524 // Train Acc: 0.0\n",
      "Val Loss: 0.006526637394827875 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006232004289485549 // Train Acc: 0.0\n",
      "Val Loss: 0.006334901448677887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006068891893485433 // Train Acc: 0.0\n",
      "Val Loss: 0.006171865619465032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005928913508775776 // Train Acc: 0.0\n",
      "Val Loss: 0.006035203844393519 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005808939844727108 // Train Acc: 0.0\n",
      "Val Loss: 0.0059208463263613256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005704352786322231 // Train Acc: 0.0\n",
      "Val Loss: 0.00582302118207074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005610287952827944 // Train Acc: 0.0\n",
      "Val Loss: 0.005735725721090355 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005522872403170488 // Train Acc: 0.0\n",
      "Val Loss: 0.005654273502824997 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005439731750114221 // Train Acc: 0.0\n",
      "Val Loss: 0.005576127912552858 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005359696255960676 // Train Acc: 0.0\n",
      "Val Loss: 0.005500569957604801 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005282231139021258 // Train Acc: 0.0\n",
      "Val Loss: 0.0054276349128816615 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005207033723711832 // Train Acc: 0.0\n",
      "Val Loss: 0.005357375938910991 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005133778350130064 // Train Acc: 0.0\n",
      "Val Loss: 0.0052895582736131146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005061939163888341 // Train Acc: 0.0\n",
      "Val Loss: 0.005223516041455282 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004990564118358455 // Train Acc: 0.0\n",
      "Val Loss: 0.005157955935944549 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0049180837157267445 // Train Acc: 0.0\n",
      "Val Loss: 0.005091276500289413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004843287235317267 // Train Acc: 0.0\n",
      "Val Loss: 0.00502304541666738 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004767615338266491 // Train Acc: 0.0\n",
      "Val Loss: 0.004953808187168431 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004693162780878087 // Train Acc: 0.0\n",
      "Val Loss: 0.0048848056932911275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004620693075432353 // Train Acc: 0.0\n",
      "Val Loss: 0.004817257969724861 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004549848198301699 // Train Acc: 0.0\n",
      "Val Loss: 0.004751194386997006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004480155677902491 // Train Acc: 0.0\n",
      "Val Loss: 0.0046862979948689995 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004411522473940056 // Train Acc: 0.0\n",
      "Val Loss: 0.004622385814912956 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004344117884060632 // Train Acc: 0.0\n",
      "Val Loss: 0.004559345381461423 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0042781753252370954 // Train Acc: 0.0\n",
      "Val Loss: 0.004497135460736569 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004213882924895308 // Train Acc: 0.0\n",
      "Val Loss: 0.004435804330701516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004151382398223887 // Train Acc: 0.0\n",
      "Val Loss: 0.004375515455401249 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004090814156453529 // Train Acc: 0.0\n",
      "Val Loss: 0.004316531492672353 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004032327831812101 // Train Acc: 0.0\n",
      "Val Loss: 0.004259170293384655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.003976052108148461 // Train Acc: 0.0\n",
      "Val Loss: 0.004203723483740098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.003922056751198148 // Train Acc: 0.0\n",
      "Val Loss: 0.00415039898626591 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.003870319428621923 // Train Acc: 0.0\n",
      "Val Loss: 0.0040992725709326225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0038207238348685717 // Train Acc: 0.0\n",
      "Val Loss: 0.004050274707630954 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0037730846566798753 // Train Acc: 0.0\n",
      "Val Loss: 0.004003252173689279 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:51<25:46, 171.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.17995593356623496 // Train Acc: 0.0\n",
      "Val Loss: 0.13719125403599305 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.018370840081851504 // Train Acc: 0.0\n",
      "Val Loss: 0.018065492385490375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011005432898876125 // Train Acc: 0.0\n",
      "Val Loss: 0.011217501149936155 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009890727463845997 // Train Acc: 0.0\n",
      "Val Loss: 0.010197303732010452 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009057408028609677 // Train Acc: 0.0\n",
      "Val Loss: 0.009344705388965932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007969174409693438 // Train Acc: 0.0\n",
      "Val Loss: 0.008134395578368144 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006982742193724841 // Train Acc: 0.0\n",
      "Val Loss: 0.007023557813160799 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0065468001103913195 // Train Acc: 0.0\n",
      "Val Loss: 0.006540666052817621 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0062973252109562354 // Train Acc: 0.0\n",
      "Val Loss: 0.0062817064410244875 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00610280217609228 // Train Acc: 0.0\n",
      "Val Loss: 0.006094707576134666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005931694170068865 // Train Acc: 0.0\n",
      "Val Loss: 0.00593737588814375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005773639587630182 // Train Acc: 0.0\n",
      "Val Loss: 0.005796598917169666 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005623785707117249 // Train Acc: 0.0\n",
      "Val Loss: 0.005666833687362007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005479028692987819 // Train Acc: 0.0\n",
      "Val Loss: 0.0055446839573877775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005336678037249811 // Train Acc: 0.0\n",
      "Val Loss: 0.005427268598313359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005193883299053599 // Train Acc: 0.0\n",
      "Val Loss: 0.005311658149796792 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005048479568840485 // Train Acc: 0.0\n",
      "Val Loss: 0.0051957756820642815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004901117866871598 // Train Acc: 0.0\n",
      "Val Loss: 0.005080069955015047 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004754385868136316 // Train Acc: 0.0\n",
      "Val Loss: 0.004966369046914307 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004610605576164918 // Train Acc: 0.0\n",
      "Val Loss: 0.004855947410264475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004472041316543064 // Train Acc: 0.0\n",
      "Val Loss: 0.004749423704660413 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004341252226558978 // Train Acc: 0.0\n",
      "Val Loss: 0.004647146881854331 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004220099982563151 // Train Acc: 0.0\n",
      "Val Loss: 0.004549474197185852 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004108956872249558 // Train Acc: 0.0\n",
      "Val Loss: 0.004456824112937532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004007177461963572 // Train Acc: 0.0\n",
      "Val Loss: 0.004369386488741095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.003913730287767063 // Train Acc: 0.0\n",
      "Val Loss: 0.004286938615735959 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.003827451075508034 // Train Acc: 0.0\n",
      "Val Loss: 0.004208871434358033 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.003747159713500745 // Train Acc: 0.0\n",
      "Val Loss: 0.004134379645851864 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.003671743465294066 // Train Acc: 0.0\n",
      "Val Loss: 0.00406260366752659 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0036001861532384606 // Train Acc: 0.0\n",
      "Val Loss: 0.00399271560176699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0035315669211080578 // Train Acc: 0.0\n",
      "Val Loss: 0.003923971982228316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0034650400686013984 // Train Acc: 0.0\n",
      "Val Loss: 0.003855727141490206 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.003399830459448037 // Train Acc: 0.0\n",
      "Val Loss: 0.003787476481573487 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0033352714312075717 // Train Acc: 0.0\n",
      "Val Loss: 0.0037189294091976165 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.003270887065685214 // Train Acc: 0.0\n",
      "Val Loss: 0.00365011867728423 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0032065206438039334 // Train Acc: 0.0\n",
      "Val Loss: 0.003581496028081429 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0031424399253223347 // Train Acc: 0.0\n",
      "Val Loss: 0.003513902688229626 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.003079315307273712 // Train Acc: 0.0\n",
      "Val Loss: 0.0034483562657524917 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0030179944198491415 // Train Acc: 0.0\n",
      "Val Loss: 0.0033856953494250776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.002959201711875314 // Train Acc: 0.0\n",
      "Val Loss: 0.0033263520189476285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.002903354950366142 // Train Acc: 0.0\n",
      "Val Loss: 0.0032703466427681798 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0028505909404016767 // Train Acc: 0.0\n",
      "Val Loss: 0.003217483352107758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.002800872267479583 // Train Acc: 0.0\n",
      "Val Loss: 0.0031674790058538994 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.002754069523344701 // Train Acc: 0.0\n",
      "Val Loss: 0.003120001761073416 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0027099497711709085 // Train Acc: 0.0\n",
      "Val Loss: 0.003074610049159012 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.002668203205533632 // Train Acc: 0.0\n",
      "Val Loss: 0.0030307779501361604 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0026284988122231 // Train Acc: 0.0\n",
      "Val Loss: 0.0029879929646002977 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0025905402583632506 // Train Acc: 0.0\n",
      "Val Loss: 0.002945841098500585 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.002554100028879209 // Train Acc: 0.0\n",
      "Val Loss: 0.0029040272591042927 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.002519013206982139 // Train Acc: 0.0\n",
      "Val Loss: 0.0028623823573897507 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [05:44<22:58, 172.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.19936666409718937 // Train Acc: 0.0\n",
      "Val Loss: 0.15442660423842344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.026559615967004267 // Train Acc: 0.0\n",
      "Val Loss: 0.025450492633337325 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011864858236138339 // Train Acc: 0.0\n",
      "Val Loss: 0.012159447769888422 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010685096493790344 // Train Acc: 0.0\n",
      "Val Loss: 0.011042588610540737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.010204566866967435 // Train Acc: 0.0\n",
      "Val Loss: 0.010584866827015172 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009809216680047719 // Train Acc: 0.0\n",
      "Val Loss: 0.010201202040876855 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.009463984406548956 // Train Acc: 0.0\n",
      "Val Loss: 0.009850531160323457 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00915066223244978 // Train Acc: 0.0\n",
      "Val Loss: 0.00951382527127862 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00883393236659212 // Train Acc: 0.0\n",
      "Val Loss: 0.009164578454907644 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.00849360887537451 // Train Acc: 0.0\n",
      "Val Loss: 0.008785681028596379 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.008134045802932827 // Train Acc: 0.0\n",
      "Val Loss: 0.008382386629554359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007766163296100245 // Train Acc: 0.0\n",
      "Val Loss: 0.007969447983090173 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.00739954071827488 // Train Acc: 0.0\n",
      "Val Loss: 0.007560326793993061 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007047158670739258 // Train Acc: 0.0\n",
      "Val Loss: 0.007170853933150118 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006725056231400363 // Train Acc: 0.0\n",
      "Val Loss: 0.006818979148837653 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006447139625080338 // Train Acc: 0.0\n",
      "Val Loss: 0.006519785637713291 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006218961699967821 // Train Acc: 0.0\n",
      "Val Loss: 0.006278988520022143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006035033166387276 // Train Acc: 0.0\n",
      "Val Loss: 0.006090056341649456 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00588292054530046 // Train Acc: 0.0\n",
      "Val Loss: 0.00593885057605803 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005750590984254531 // Train Acc: 0.0\n",
      "Val Loss: 0.005811681972012262 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005630526276513696 // Train Acc: 0.0\n",
      "Val Loss: 0.005699722729199989 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.00551922301034609 // Train Acc: 0.0\n",
      "Val Loss: 0.0055985285381955855 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005414987572460312 // Train Acc: 0.0\n",
      "Val Loss: 0.005505780792075463 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005316398613639686 // Train Acc: 0.0\n",
      "Val Loss: 0.005419660863381895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005221928377873924 // Train Acc: 0.0\n",
      "Val Loss: 0.005338360924824056 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005130123561003658 // Train Acc: 0.0\n",
      "Val Loss: 0.005260208504147489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005039777579881064 // Train Acc: 0.0\n",
      "Val Loss: 0.005183827722529796 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004949977280695382 // Train Acc: 0.0\n",
      "Val Loss: 0.005108188598586077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004860051284059372 // Train Acc: 0.0\n",
      "Val Loss: 0.005032562849704515 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00476951096058062 // Train Acc: 0.0\n",
      "Val Loss: 0.004956473779483613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004677973054275001 // Train Acc: 0.0\n",
      "Val Loss: 0.004879623801786114 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004585120042883813 // Train Acc: 0.0\n",
      "Val Loss: 0.004801824288866059 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004490719097358127 // Train Acc: 0.0\n",
      "Val Loss: 0.004722943801475181 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004394667388682071 // Train Acc: 0.0\n",
      "Val Loss: 0.004642880709037523 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004297028096620265 // Train Acc: 0.0\n",
      "Val Loss: 0.004561558302322572 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004198030757767757 // Train Acc: 0.0\n",
      "Val Loss: 0.004478952047330412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004098056945269374 // Train Acc: 0.0\n",
      "Val Loss: 0.004395117594818161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.003997620119666468 // Train Acc: 0.0\n",
      "Val Loss: 0.004310184811898084 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0038973485288682254 // Train Acc: 0.0\n",
      "Val Loss: 0.004224353485783054 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.003797950209267267 // Train Acc: 0.0\n",
      "Val Loss: 0.004137884920716963 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.003700140099781119 // Train Acc: 0.0\n",
      "Val Loss: 0.004051076282683591 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.003604554232596909 // Train Acc: 0.0\n",
      "Val Loss: 0.003964245132043619 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0035116964784774026 // Train Acc: 0.0\n",
      "Val Loss: 0.0038777342828159984 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.003421954024948826 // Train Acc: 0.0\n",
      "Val Loss: 0.003791954881109467 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0033356348084781283 // Train Acc: 0.0\n",
      "Val Loss: 0.003707435717065395 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.003252961952053223 // Train Acc: 0.0\n",
      "Val Loss: 0.0036248085597022014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.003174071904797039 // Train Acc: 0.0\n",
      "Val Loss: 0.003544717976315455 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.003099107033079788 // Train Acc: 0.0\n",
      "Val Loss: 0.0034677917423488742 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.003028263016306204 // Train Acc: 0.0\n",
      "Val Loss: 0.0033945752085525202 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.002961729087116788 // Train Acc: 0.0\n",
      "Val Loss: 0.003325354323764755 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [08:36<20:06, 172.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.14539554004119412 // Train Acc: 0.0\n",
      "Val Loss: 0.10323042259974913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0208206227363925 // Train Acc: 0.0\n",
      "Val Loss: 0.01996149378405376 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011567291817479005 // Train Acc: 0.0\n",
      "Val Loss: 0.011741961860521273 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010148604078320404 // Train Acc: 0.0\n",
      "Val Loss: 0.010399819978258826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009030024252074971 // Train Acc: 0.0\n",
      "Val Loss: 0.009278322290629148 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007753634866280269 // Train Acc: 0.0\n",
      "Val Loss: 0.007912740238349546 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006833880456147468 // Train Acc: 0.0\n",
      "Val Loss: 0.006923944492485713 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0064224580219908525 // Train Acc: 0.0\n",
      "Val Loss: 0.006479095506735823 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006204300931097778 // Train Acc: 0.0\n",
      "Val Loss: 0.006250847677107561 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0060398407561117655 // Train Acc: 0.0\n",
      "Val Loss: 0.006089468607255681 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005891151819803399 // Train Acc: 0.0\n",
      "Val Loss: 0.005951422402127223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005745887352880856 // Train Acc: 0.0\n",
      "Val Loss: 0.005821853647516533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0055996960424499155 // Train Acc: 0.0\n",
      "Val Loss: 0.005694975386458364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0054524369781292535 // Train Acc: 0.0\n",
      "Val Loss: 0.0055695670402862805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005307347684371451 // Train Acc: 0.0\n",
      "Val Loss: 0.005447315676561134 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.00516867922825237 // Train Acc: 0.0\n",
      "Val Loss: 0.005330854357982224 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005040516409189462 // Train Acc: 0.0\n",
      "Val Loss: 0.005223634011450816 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004925285990150075 // Train Acc: 0.0\n",
      "Val Loss: 0.005128220536492088 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004822724317274903 // Train Acc: 0.0\n",
      "Val Loss: 0.00504455826329914 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004730968904621967 // Train Acc: 0.0\n",
      "Val Loss: 0.004970901528246362 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004647900222016064 // Train Acc: 0.0\n",
      "Val Loss: 0.0049052042018791495 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004571711266579183 // Train Acc: 0.0\n",
      "Val Loss: 0.004845667072698813 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004500986400355913 // Train Acc: 0.0\n",
      "Val Loss: 0.004790833858053454 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004434648772582548 // Train Acc: 0.0\n",
      "Val Loss: 0.004739560177338056 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004371884516302122 // Train Acc: 0.0\n",
      "Val Loss: 0.0046909679129550404 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004312086613055693 // Train Acc: 0.0\n",
      "Val Loss: 0.00464439465960657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00425481680779586 // Train Acc: 0.0\n",
      "Val Loss: 0.00459935928652571 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004199770251055116 // Train Acc: 0.0\n",
      "Val Loss: 0.0045555198797956106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004146742838600727 // Train Acc: 0.0\n",
      "Val Loss: 0.004512656185860661 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0040956021918592015 // Train Acc: 0.0\n",
      "Val Loss: 0.004470627848058939 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004046262809092426 // Train Acc: 0.0\n",
      "Val Loss: 0.004429353279357945 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00399865908362184 // Train Acc: 0.0\n",
      "Val Loss: 0.004388780912003396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.003952743106289899 // Train Acc: 0.0\n",
      "Val Loss: 0.004348851963666014 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.003908469175577555 // Train Acc: 0.0\n",
      "Val Loss: 0.004309483234431933 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.003865775345244177 // Train Acc: 0.0\n",
      "Val Loss: 0.0042705590572123505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.003824568442366677 // Train Acc: 0.0\n",
      "Val Loss: 0.004231965082528239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.00378472250530105 // Train Acc: 0.0\n",
      "Val Loss: 0.00419360226422379 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.003746090410330405 // Train Acc: 0.0\n",
      "Val Loss: 0.004155388950708915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.003708515406615998 // Train Acc: 0.0\n",
      "Val Loss: 0.0041172605547630655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0036718403050340033 // Train Acc: 0.0\n",
      "Val Loss: 0.004079164617525583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0036359118060081278 // Train Acc: 0.0\n",
      "Val Loss: 0.004041052245619623 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0036005811400596716 // Train Acc: 0.0\n",
      "Val Loss: 0.004002873523330146 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0035657039269396703 // Train Acc: 0.0\n",
      "Val Loss: 0.0039645800493996255 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0035311319234304674 // Train Acc: 0.0\n",
      "Val Loss: 0.003926111893220382 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0034967099651843137 // Train Acc: 0.0\n",
      "Val Loss: 0.003887389185415073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0034622633460266983 // Train Acc: 0.0\n",
      "Val Loss: 0.0038482939030721106 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0034275938566459435 // Train Acc: 0.0\n",
      "Val Loss: 0.0038086638775315473 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0033924758519879522 // Train Acc: 0.0\n",
      "Val Loss: 0.003768283202820881 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.003356676898762737 // Train Acc: 0.0\n",
      "Val Loss: 0.00372692537705668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0033200154236207404 // Train Acc: 0.0\n",
      "Val Loss: 0.003684424050152302 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [11:29<17:14, 172.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.4971055206098513 // Train Acc: 0.0\n",
      "Val Loss: 0.38979951576753097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.0273531072147905 // Train Acc: 0.0\n",
      "Val Loss: 0.02564255813644691 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.012810972302335582 // Train Acc: 0.0\n",
      "Val Loss: 0.012888789244673468 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010492288434163553 // Train Acc: 0.0\n",
      "Val Loss: 0.01069821740754626 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009396478618265525 // Train Acc: 0.0\n",
      "Val Loss: 0.009611351538280194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.008514543054370235 // Train Acc: 0.0\n",
      "Val Loss: 0.008712292686951431 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007613491241450179 // Train Acc: 0.0\n",
      "Val Loss: 0.007755016670985655 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006798808586775202 // Train Acc: 0.0\n",
      "Val Loss: 0.006876630300063301 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0062305266321976945 // Train Acc: 0.0\n",
      "Val Loss: 0.006268638628534972 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.0058848023947251785 // Train Acc: 0.0\n",
      "Val Loss: 0.005905440911142664 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.005644533370693797 // Train Acc: 0.0\n",
      "Val Loss: 0.005662944614463909 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.005448057044607897 // Train Acc: 0.0\n",
      "Val Loss: 0.005472845643420111 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005273469845492171 // Train Acc: 0.0\n",
      "Val Loss: 0.005308519359889694 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005112130883028603 // Train Acc: 0.0\n",
      "Val Loss: 0.005158629935150119 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.00495968889615171 // Train Acc: 0.0\n",
      "Val Loss: 0.005017184785736556 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.004813628229227546 // Train Acc: 0.0\n",
      "Val Loss: 0.004880794580094517 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.004672643300634915 // Train Acc: 0.0\n",
      "Val Loss: 0.004747923093170605 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.004536340999487175 // Train Acc: 0.0\n",
      "Val Loss: 0.004618443322198635 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004404904390206841 // Train Acc: 0.0\n",
      "Val Loss: 0.004493133783001792 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004278760421734099 // Train Acc: 0.0\n",
      "Val Loss: 0.004373138358774171 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004158274986628086 // Train Acc: 0.0\n",
      "Val Loss: 0.004259433481969278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004043561949705084 // Train Acc: 0.0\n",
      "Val Loss: 0.004152457722970708 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.003934433021408306 // Train Acc: 0.0\n",
      "Val Loss: 0.004052036775233732 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0038304836802890516 // Train Acc: 0.0\n",
      "Val Loss: 0.003957574344663458 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0037312019920958945 // Train Acc: 0.0\n",
      "Val Loss: 0.003868300069800832 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0036360040210298003 // Train Acc: 0.0\n",
      "Val Loss: 0.0037833710730245168 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0035442230387348426 // Train Acc: 0.0\n",
      "Val Loss: 0.0037018756720830093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.003455195592165623 // Train Acc: 0.0\n",
      "Val Loss: 0.003622918437362056 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.003368378907110389 // Train Acc: 0.0\n",
      "Val Loss: 0.0035457757428627125 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0032834886045930863 // Train Acc: 0.0\n",
      "Val Loss: 0.0034700464055111464 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.003200578978141668 // Train Acc: 0.0\n",
      "Val Loss: 0.0033957265941849486 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0031200398730335237 // Train Acc: 0.0\n",
      "Val Loss: 0.0033231802993792703 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0030424911269682064 // Train Acc: 0.0\n",
      "Val Loss: 0.003252978676887737 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.002968602436759454 // Train Acc: 0.0\n",
      "Val Loss: 0.00318569346987219 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.002898874321711126 // Train Acc: 0.0\n",
      "Val Loss: 0.003121653002348136 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.002833505432609443 // Train Acc: 0.0\n",
      "Val Loss: 0.0030608358718878168 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0027723989354289185 // Train Acc: 0.0\n",
      "Val Loss: 0.0030028857407160105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.002715269655290115 // Train Acc: 0.0\n",
      "Val Loss: 0.0029472949692386794 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0026617695782897082 // Train Acc: 0.0\n",
      "Val Loss: 0.002893573945303532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0026115319097827117 // Train Acc: 0.0\n",
      "Val Loss: 0.0028413812259466134 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.002564185294726668 // Train Acc: 0.0\n",
      "Val Loss: 0.0027905691510320386 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0025193630425327514 // Train Acc: 0.0\n",
      "Val Loss: 0.0027411470740017567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0024767469834816986 // Train Acc: 0.0\n",
      "Val Loss: 0.0026932330812666225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0024360929674003273 // Train Acc: 0.0\n",
      "Val Loss: 0.0026469733571337366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.00239724697681786 // Train Acc: 0.0\n",
      "Val Loss: 0.0026025216301522134 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.002360137931542199 // Train Acc: 0.0\n",
      "Val Loss: 0.0025600453023798763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.002324766202082381 // Train Acc: 0.0\n",
      "Val Loss: 0.0025197242450138383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0022911662227882404 // Train Acc: 0.0\n",
      "Val Loss: 0.002481745575046675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0022593593942777566 // Train Acc: 0.0\n",
      "Val Loss: 0.0024462615012783894 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0022293149801151887 // Train Acc: 0.0\n",
      "Val Loss: 0.0024133395461831244 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [14:22<14:22, 172.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.2773463657321451 // Train Acc: 0.0\n",
      "Val Loss: 0.1995942378585989 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.026877756553731823 // Train Acc: 0.0\n",
      "Val Loss: 0.024458302608267826 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0116239999927002 // Train Acc: 0.0\n",
      "Val Loss: 0.011979742534458637 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010429113412267405 // Train Acc: 0.0\n",
      "Val Loss: 0.01081379336220297 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00986154607526924 // Train Acc: 0.0\n",
      "Val Loss: 0.0102428284262053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009393326240723535 // Train Acc: 0.0\n",
      "Val Loss: 0.009755129684609445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008939700416973942 // Train Acc: 0.0\n",
      "Val Loss: 0.009265980238772252 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00844040093571373 // Train Acc: 0.0\n",
      "Val Loss: 0.008712448391386053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007862528156676249 // Train Acc: 0.0\n",
      "Val Loss: 0.00806508605707098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007255212172066376 // Train Acc: 0.0\n",
      "Val Loss: 0.007391903223469854 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.0067860422561319385 // Train Acc: 0.0\n",
      "Val Loss: 0.006888130463829095 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0064965474278959505 // Train Acc: 0.0\n",
      "Val Loss: 0.006584238222884861 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006310203193993091 // Train Acc: 0.0\n",
      "Val Loss: 0.006395127092996104 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006167622305958885 // Train Acc: 0.0\n",
      "Val Loss: 0.006257762237113308 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006040259267067426 // Train Acc: 0.0\n",
      "Val Loss: 0.006139710494740443 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005917090630226824 // Train Acc: 0.0\n",
      "Val Loss: 0.006027763874524019 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0057939408521367804 // Train Acc: 0.0\n",
      "Val Loss: 0.0059172532351856886 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005669589952319078 // Train Acc: 0.0\n",
      "Val Loss: 0.005806837681765584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005544303397044165 // Train Acc: 0.0\n",
      "Val Loss: 0.005696297544901344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005419361364674758 // Train Acc: 0.0\n",
      "Val Loss: 0.005586243295957419 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0052962286836897195 // Train Acc: 0.0\n",
      "Val Loss: 0.005477862678129565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005175399932091775 // Train Acc: 0.0\n",
      "Val Loss: 0.005372084819034419 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005056286322646115 // Train Acc: 0.0\n",
      "Val Loss: 0.005269069569608704 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004937957465163853 // Train Acc: 0.0\n",
      "Val Loss: 0.005168377482120625 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0048197062811537825 // Train Acc: 0.0\n",
      "Val Loss: 0.00506919982038777 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0047012958287571864 // Train Acc: 0.0\n",
      "Val Loss: 0.004970438702201301 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004583145699625384 // Train Acc: 0.0\n",
      "Val Loss: 0.004871138622349298 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004466451656394404 // Train Acc: 0.0\n",
      "Val Loss: 0.004771288312886926 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0043531775234807195 // Train Acc: 0.0\n",
      "Val Loss: 0.004672436933668161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004245734892915254 // Train Acc: 0.0\n",
      "Val Loss: 0.0045773771792565554 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004146195911976709 // Train Acc: 0.0\n",
      "Val Loss: 0.00448894094760445 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.004055414644663209 // Train Acc: 0.0\n",
      "Val Loss: 0.004408664221409708 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.003972861110222139 // Train Acc: 0.0\n",
      "Val Loss: 0.004336410644464194 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0038972128444185405 // Train Acc: 0.0\n",
      "Val Loss: 0.004271040454676206 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0038270693400993807 // Train Acc: 0.0\n",
      "Val Loss: 0.004211259027943015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0037613503356383406 // Train Acc: 0.0\n",
      "Val Loss: 0.004156079765578562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0036993846398263694 // Train Acc: 0.0\n",
      "Val Loss: 0.004104884760454297 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.003640813462767903 // Train Acc: 0.0\n",
      "Val Loss: 0.004057311612731693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00358544183245405 // Train Acc: 0.0\n",
      "Val Loss: 0.004013095143124122 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.003533121369317701 // Train Acc: 0.0\n",
      "Val Loss: 0.0039719852544790645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0034836925115419304 // Train Acc: 0.0\n",
      "Val Loss: 0.0039336997094902805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.003436980330221542 // Train Acc: 0.0\n",
      "Val Loss: 0.003897931415122002 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.003392796222966794 // Train Acc: 0.0\n",
      "Val Loss: 0.0038643654467622663 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0033509496617509416 // Train Acc: 0.0\n",
      "Val Loss: 0.0038326979635960674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0033112515973939337 // Train Acc: 0.0\n",
      "Val Loss: 0.003802640137093311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0032735242139969073 // Train Acc: 0.0\n",
      "Val Loss: 0.0037739419981583275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0032375944766628484 // Train Acc: 0.0\n",
      "Val Loss: 0.003746382258197462 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0032032995036118516 // Train Acc: 0.0\n",
      "Val Loss: 0.0037197812291031532 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.003170490874345393 // Train Acc: 0.0\n",
      "Val Loss: 0.003693977986801077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0031390284707973875 // Train Acc: 0.0\n",
      "Val Loss: 0.00366884355369786 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [17:14<11:29, 172.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.24403086005280553 // Train Acc: 0.0\n",
      "Val Loss: 0.170496320318092 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.024720876571216268 // Train Acc: 0.0\n",
      "Val Loss: 0.02374607642943209 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011425162532651738 // Train Acc: 0.0\n",
      "Val Loss: 0.011682642962444912 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.009863937816893062 // Train Acc: 0.0\n",
      "Val Loss: 0.010076172996989706 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009379659358360996 // Train Acc: 0.0\n",
      "Val Loss: 0.00954969950358976 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009071576107637829 // Train Acc: 0.0\n",
      "Val Loss: 0.009215916756709868 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008812870890897226 // Train Acc: 0.0\n",
      "Val Loss: 0.008932321810756217 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008568579214093484 // Train Acc: 0.0\n",
      "Val Loss: 0.008662252767350186 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008327151829436353 // Train Acc: 0.0\n",
      "Val Loss: 0.008395466940816153 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008085534242343276 // Train Acc: 0.0\n",
      "Val Loss: 0.008130611064420505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007844724501568194 // Train Acc: 0.0\n",
      "Val Loss: 0.007870028993453492 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007607879681134231 // Train Acc: 0.0\n",
      "Val Loss: 0.007617489949122749 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007379236072939757 // Train Acc: 0.0\n",
      "Val Loss: 0.007377050732347098 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0071629928424954414 // Train Acc: 0.0\n",
      "Val Loss: 0.007152247644791549 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006961965327976127 // Train Acc: 0.0\n",
      "Val Loss: 0.006945197517052293 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006776773080029886 // Train Acc: 0.0\n",
      "Val Loss: 0.006756093780594793 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0066062933196564585 // Train Acc: 0.0\n",
      "Val Loss: 0.006583632753145966 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006448872595255744 // Train Acc: 0.0\n",
      "Val Loss: 0.006426076214252548 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006303205095831151 // Train Acc: 0.0\n",
      "Val Loss: 0.006282058254476975 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006168568499926393 // Train Acc: 0.0\n",
      "Val Loss: 0.006150841979648579 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006044623409036428 // Train Acc: 0.0\n",
      "Val Loss: 0.006032130549746481 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0059311005024273444 // Train Acc: 0.0\n",
      "Val Loss: 0.00592572636986998 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005827553994468835 // Train Acc: 0.0\n",
      "Val Loss: 0.005831206484105099 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.00573323614689092 // Train Acc: 0.0\n",
      "Val Loss: 0.005747727905823426 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005647067594571575 // Train Acc: 0.0\n",
      "Val Loss: 0.005673995205539871 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005567702658784886 // Train Acc: 0.0\n",
      "Val Loss: 0.005608416261913424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005493669847736654 // Train Acc: 0.0\n",
      "Val Loss: 0.005549302419901572 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005423517703167992 // Train Acc: 0.0\n",
      "Val Loss: 0.005495035158343275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005355937398712844 // Train Acc: 0.0\n",
      "Val Loss: 0.005444182521155612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005289821016398093 // Train Acc: 0.0\n",
      "Val Loss: 0.005395564913157035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005224271210710003 // Train Acc: 0.0\n",
      "Val Loss: 0.005348249677230011 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005158565199315191 // Train Acc: 0.0\n",
      "Val Loss: 0.005301501958588646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005092116958912479 // Train Acc: 0.0\n",
      "Val Loss: 0.005254706097978421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005024465233391602 // Train Acc: 0.0\n",
      "Val Loss: 0.005207289087543772 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004955330809108385 // Train Acc: 0.0\n",
      "Val Loss: 0.0051587424209256745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004884714635459017 // Train Acc: 0.0\n",
      "Val Loss: 0.005108796791385182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004812783862785202 // Train Acc: 0.0\n",
      "Val Loss: 0.005057471538682214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004739541972915059 // Train Acc: 0.0\n",
      "Val Loss: 0.005004613319496539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.004664817041415534 // Train Acc: 0.0\n",
      "Val Loss: 0.004949635453522205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0045885090303148 // Train Acc: 0.0\n",
      "Val Loss: 0.004891876600132408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004510713328945463 // Train Acc: 0.0\n",
      "Val Loss: 0.004831006665798751 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004431704113464037 // Train Acc: 0.0\n",
      "Val Loss: 0.004767111320556565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004351823047158567 // Train Acc: 0.0\n",
      "Val Loss: 0.004700518364552408 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004271393367270067 // Train Acc: 0.0\n",
      "Val Loss: 0.004631565776865252 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004190671026970357 // Train Acc: 0.0\n",
      "Val Loss: 0.00456040394174951 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004109776762237594 // Train Acc: 0.0\n",
      "Val Loss: 0.004487146971620281 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004028911702069093 // Train Acc: 0.0\n",
      "Val Loss: 0.004412112119925124 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.003948691544509982 // Train Acc: 0.0\n",
      "Val Loss: 0.004335915227420628 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.003869934888879798 // Train Acc: 0.0\n",
      "Val Loss: 0.004259197026575831 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.00379318729113175 // Train Acc: 0.0\n",
      "Val Loss: 0.0041825836286245085 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [20:06<08:37, 172.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.19628753683202343 // Train Acc: 0.0\n",
      "Val Loss: 0.14496512683955107 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.025635763794421878 // Train Acc: 0.0\n",
      "Val Loss: 0.024380468475547704 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011832433717631518 // Train Acc: 0.0\n",
      "Val Loss: 0.012107761974700473 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01038984835177626 // Train Acc: 0.0\n",
      "Val Loss: 0.010659756646914915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009810893620963986 // Train Acc: 0.0\n",
      "Val Loss: 0.010078358624807812 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.00936985708301096 // Train Acc: 0.0\n",
      "Val Loss: 0.00963133463043381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.00894998383312805 // Train Acc: 0.0\n",
      "Val Loss: 0.009199745347723364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008525333193994072 // Train Acc: 0.0\n",
      "Val Loss: 0.008754816634411161 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.008094276366946734 // Train Acc: 0.0\n",
      "Val Loss: 0.008293131493370642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007631185061625762 // Train Acc: 0.0\n",
      "Val Loss: 0.007786534815518694 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007121443524148706 // Train Acc: 0.0\n",
      "Val Loss: 0.007218221989883618 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0066186970213564225 // Train Acc: 0.0\n",
      "Val Loss: 0.006655448035929691 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0062254476876606206 // Train Acc: 0.0\n",
      "Val Loss: 0.006224977796558629 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0059662495525000985 // Train Acc: 0.0\n",
      "Val Loss: 0.00595428579359908 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005785288114300568 // Train Acc: 0.0\n",
      "Val Loss: 0.005777631796346131 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005637415285013854 // Train Acc: 0.0\n",
      "Val Loss: 0.0056436694089577275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0055028057160689476 // Train Acc: 0.0\n",
      "Val Loss: 0.005529924507506869 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005373761993960465 // Train Acc: 0.0\n",
      "Val Loss: 0.005426986348307268 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005247228641220321 // Train Acc: 0.0\n",
      "Val Loss: 0.005330031320706688 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.00512211465278904 // Train Acc: 0.0\n",
      "Val Loss: 0.005235820554662496 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004998344078566454 // Train Acc: 0.0\n",
      "Val Loss: 0.005141779198311269 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0048761894314052305 // Train Acc: 0.0\n",
      "Val Loss: 0.0050457464128902015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004755569927045676 // Train Acc: 0.0\n",
      "Val Loss: 0.004945942842062901 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004635781868888261 // Train Acc: 0.0\n",
      "Val Loss: 0.004841120391872457 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004515987322570067 // Train Acc: 0.0\n",
      "Val Loss: 0.004730912924490191 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.004396009361254451 // Train Acc: 0.0\n",
      "Val Loss: 0.004616111973088912 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004276623825551134 // Train Acc: 0.0\n",
      "Val Loss: 0.004498354184695266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0041590502034324186 // Train Acc: 0.0\n",
      "Val Loss: 0.004379159380385483 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004044168617854907 // Train Acc: 0.0\n",
      "Val Loss: 0.0042592857488092375 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.003932426614499732 // Train Acc: 0.0\n",
      "Val Loss: 0.004139333656481044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0038242388977010023 // Train Acc: 0.0\n",
      "Val Loss: 0.004020622817122123 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.003720034373930515 // Train Acc: 0.0\n",
      "Val Loss: 0.00390501826768741 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0036200352016300574 // Train Acc: 0.0\n",
      "Val Loss: 0.0037941490003669805 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0035241716264929946 // Train Acc: 0.0\n",
      "Val Loss: 0.003688947356898676 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.003432156809405785 // Train Acc: 0.0\n",
      "Val Loss: 0.003589730004949326 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0033435475107252634 // Train Acc: 0.0\n",
      "Val Loss: 0.0034964170518585226 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.003257796181616752 // Train Acc: 0.0\n",
      "Val Loss: 0.003408673201391304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.003174324450561496 // Train Acc: 0.0\n",
      "Val Loss: 0.0033259599268521097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0030926273193223122 // Train Acc: 0.0\n",
      "Val Loss: 0.003247643001800911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0030123396289160635 // Train Acc: 0.0\n",
      "Val Loss: 0.003173063159920275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0029332662103846497 // Train Acc: 0.0\n",
      "Val Loss: 0.0031015950138680638 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.002855445801325091 // Train Acc: 0.0\n",
      "Val Loss: 0.0030327412649057807 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0027793562132758876 // Train Acc: 0.0\n",
      "Val Loss: 0.0029663153966380793 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0027061287082729597 // Train Acc: 0.0\n",
      "Val Loss: 0.002902637388218533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0026373998393553425 // Train Acc: 0.0\n",
      "Val Loss: 0.002842476679308509 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.002574651685201023 // Train Acc: 0.0\n",
      "Val Loss: 0.0027866685106842357 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.002518575350949583 // Train Acc: 0.0\n",
      "Val Loss: 0.002735684829002077 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.002468957441734558 // Train Acc: 0.0\n",
      "Val Loss: 0.002689487219322473 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0024249335072113872 // Train Acc: 0.0\n",
      "Val Loss: 0.0026475794126533648 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0023853575992057756 // Train Acc: 0.0\n",
      "Val Loss: 0.0026091852648691695 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [22:59<05:45, 172.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.3901680009822323 // Train Acc: 0.0\n",
      "Val Loss: 0.29415478001941336 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.02219706967251894 // Train Acc: 0.0\n",
      "Val Loss: 0.02113243266940117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.011420678007140007 // Train Acc: 0.0\n",
      "Val Loss: 0.011552443177523937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.010252939244800302 // Train Acc: 0.0\n",
      "Val Loss: 0.010422445308755744 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.009617165671916653 // Train Acc: 0.0\n",
      "Val Loss: 0.009802026877349074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009167239958735088 // Train Acc: 0.0\n",
      "Val Loss: 0.009345990453254092 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008786900621758067 // Train Acc: 0.0\n",
      "Val Loss: 0.008943882148543542 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.008465734556036657 // Train Acc: 0.0\n",
      "Val Loss: 0.008597171581773595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.00821163068891218 // Train Acc: 0.0\n",
      "Val Loss: 0.008325588688339021 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.008003857447638188 // Train Acc: 0.0\n",
      "Val Loss: 0.008108909320170906 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.007819407078390728 // Train Acc: 0.0\n",
      "Val Loss: 0.007919086079875176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.007645497507879247 // Train Acc: 0.0\n",
      "Val Loss: 0.007740160708569668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.007475566936520749 // Train Acc: 0.0\n",
      "Val Loss: 0.007564574759453535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.007305931338729958 // Train Acc: 0.0\n",
      "Val Loss: 0.007388509875586764 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.007134304381906986 // Train Acc: 0.0\n",
      "Val Loss: 0.007209713486107913 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0069591374259833335 // Train Acc: 0.0\n",
      "Val Loss: 0.007026610813442279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006779352983366273 // Train Acc: 0.0\n",
      "Val Loss: 0.00683804155242714 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006594363927466956 // Train Acc: 0.0\n",
      "Val Loss: 0.00664345711808313 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006404432545194921 // Train Acc: 0.0\n",
      "Val Loss: 0.006443647224329073 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006211413809451271 // Train Acc: 0.0\n",
      "Val Loss: 0.006242042729123072 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006019277753814477 // Train Acc: 0.0\n",
      "Val Loss: 0.006045272720935331 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005833202823369283 // Train Acc: 0.0\n",
      "Val Loss: 0.005860607959965075 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005657555055687604 // Train Acc: 0.0\n",
      "Val Loss: 0.005692111055197364 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0054946202068742725 // Train Acc: 0.0\n",
      "Val Loss: 0.005540341165297749 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005344403552297571 // Train Acc: 0.0\n",
      "Val Loss: 0.005403963617176156 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005205416943735473 // Train Acc: 0.0\n",
      "Val Loss: 0.0052807250240055675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005075587873500198 // Train Acc: 0.0\n",
      "Val Loss: 0.005167795476419004 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.004952804418546509 // Train Acc: 0.0\n",
      "Val Loss: 0.005062070159791884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0048352171080590966 // Train Acc: 0.0\n",
      "Val Loss: 0.004960655575533482 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004721330649423578 // Train Acc: 0.0\n",
      "Val Loss: 0.004861265604003248 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004609990519733003 // Train Acc: 0.0\n",
      "Val Loss: 0.004762445936318149 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0045003646511061444 // Train Acc: 0.0\n",
      "Val Loss: 0.0046636208225126295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004392001150360918 // Train Acc: 0.0\n",
      "Val Loss: 0.004565023541958494 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004284649747413196 // Train Acc: 0.0\n",
      "Val Loss: 0.0044673012685962025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004177912328308278 // Train Acc: 0.0\n",
      "Val Loss: 0.004370954465544359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.00407155350286016 // Train Acc: 0.0\n",
      "Val Loss: 0.0042762041144835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.003966254493398124 // Train Acc: 0.0\n",
      "Val Loss: 0.00418316632382233 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0038636515142138807 // Train Acc: 0.0\n",
      "Val Loss: 0.004092315983408215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0037656246420563746 // Train Acc: 0.0\n",
      "Val Loss: 0.004004580039657991 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.003673393732881956 // Train Acc: 0.0\n",
      "Val Loss: 0.003920649954075502 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0035870676629243155 // Train Acc: 0.0\n",
      "Val Loss: 0.003840342075140639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.003505941991509283 // Train Acc: 0.0\n",
      "Val Loss: 0.0037628040617247198 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.003429040337086668 // Train Acc: 0.0\n",
      "Val Loss: 0.00368712944698266 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0033554231365781157 // Train Acc: 0.0\n",
      "Val Loss: 0.003612678982740776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.003284263252477336 // Train Acc: 0.0\n",
      "Val Loss: 0.0035390442771710117 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0032148326487444153 // Train Acc: 0.0\n",
      "Val Loss: 0.0034659229949201373 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0031464968245321265 // Train Acc: 0.0\n",
      "Val Loss: 0.003393051679119129 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0030787138690423544 // Train Acc: 0.0\n",
      "Val Loss: 0.0033201644461686637 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.003011054716064471 // Train Acc: 0.0\n",
      "Val Loss: 0.003247042153221132 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0029432386580905685 // Train Acc: 0.0\n",
      "Val Loss: 0.0031735481085806067 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [25:52<02:52, 172.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.18534760027443437 // Train Acc: 0.0\n",
      "Val Loss: 0.12670345807617361 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.018318856271094384 // Train Acc: 0.0\n",
      "Val Loss: 0.01790426850996234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.01150312644671904 // Train Acc: 0.0\n",
      "Val Loss: 0.011715565892783079 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.01021201095275887 // Train Acc: 0.0\n",
      "Val Loss: 0.010478677414357662 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00958671929850562 // Train Acc: 0.0\n",
      "Val Loss: 0.009862476443363861 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.009007309920677584 // Train Acc: 0.0\n",
      "Val Loss: 0.009272157866507768 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.008259756588929898 // Train Acc: 0.0\n",
      "Val Loss: 0.008474634727463126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007433797776078184 // Train Acc: 0.0\n",
      "Val Loss: 0.007555049435574223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006848123237665011 // Train Acc: 0.0\n",
      "Val Loss: 0.006879581375555559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006417177056204783 // Train Acc: 0.0\n",
      "Val Loss: 0.006388446032492952 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.00609795596199663 // Train Acc: 0.0\n",
      "Val Loss: 0.006042212000201372 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.0058702174058507224 // Train Acc: 0.0\n",
      "Val Loss: 0.005807843730276959 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.005697508401037555 // Train Acc: 0.0\n",
      "Val Loss: 0.005644142424518412 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.005553527138212943 // Train Acc: 0.0\n",
      "Val Loss: 0.005519746176221154 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.005422749876610288 // Train Acc: 0.0\n",
      "Val Loss: 0.0054132015253840525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0052988927717826685 // Train Acc: 0.0\n",
      "Val Loss: 0.005315068042413755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005179696110370633 // Train Acc: 0.0\n",
      "Val Loss: 0.005221735027788037 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005064445155190057 // Train Acc: 0.0\n",
      "Val Loss: 0.0051317280983891 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.004953319303580819 // Train Acc: 0.0\n",
      "Val Loss: 0.005044663497458466 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.004847821036670527 // Train Acc: 0.0\n",
      "Val Loss: 0.004961662481284954 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.004750395401542555 // Train Acc: 0.0\n",
      "Val Loss: 0.0048849633757279 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.004661589695329961 // Train Acc: 0.0\n",
      "Val Loss: 0.004814984176350249 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.004579480726550414 // Train Acc: 0.0\n",
      "Val Loss: 0.004749792853412642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.004501881801361669 // Train Acc: 0.0\n",
      "Val Loss: 0.004687394648367031 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.004427376049347477 // Train Acc: 0.0\n",
      "Val Loss: 0.004626648646610027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.00435517440001442 // Train Acc: 0.0\n",
      "Val Loss: 0.004567011624616994 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.004284803007652805 // Train Acc: 0.0\n",
      "Val Loss: 0.004508176593067632 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00421588452847581 // Train Acc: 0.0\n",
      "Val Loss: 0.004449873735641383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.004148038285710002 // Train Acc: 0.0\n",
      "Val Loss: 0.0043917894474527035 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.004080889657507629 // Train Acc: 0.0\n",
      "Val Loss: 0.004333596356975084 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.004014139417420699 // Train Acc: 0.0\n",
      "Val Loss: 0.004275037527126683 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.003947631222595669 // Train Acc: 0.0\n",
      "Val Loss: 0.004215991503389722 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00388140146983107 // Train Acc: 0.0\n",
      "Val Loss: 0.004156510051424531 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0038156537425472533 // Train Acc: 0.0\n",
      "Val Loss: 0.004096793198682876 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0037507097493032137 // Train Acc: 0.0\n",
      "Val Loss: 0.004037129335020754 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.0036869192978082285 // Train Acc: 0.0\n",
      "Val Loss: 0.003977816249243915 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.003624581129446447 // Train Acc: 0.0\n",
      "Val Loss: 0.00391908808590167 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.003563883548791703 // Train Acc: 0.0\n",
      "Val Loss: 0.003861083148512989 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.003504892686657293 // Train Acc: 0.0\n",
      "Val Loss: 0.0038038445499048315 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.003447564567155396 // Train Acc: 0.0\n",
      "Val Loss: 0.003747353620763699 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.0033917598644022714 // Train Acc: 0.0\n",
      "Val Loss: 0.0036915476142894478 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.0033372463041822934 // Train Acc: 0.0\n",
      "Val Loss: 0.003636291758051481 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.0032836563663810204 // Train Acc: 0.0\n",
      "Val Loss: 0.0035813119167207993 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0032303859802658274 // Train Acc: 0.0\n",
      "Val Loss: 0.0035260389707135885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0031763929509574795 // Train Acc: 0.0\n",
      "Val Loss: 0.003469303803285584 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0031199872272607942 // Train Acc: 0.0\n",
      "Val Loss: 0.003409024635964835 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.0030599709886730093 // Train Acc: 0.0\n",
      "Val Loss: 0.003344000632536005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.0029991702558036857 // Train Acc: 0.0\n",
      "Val Loss: 0.0032792133596641097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.002941655678160012 // Train Acc: 0.0\n",
      "Val Loss: 0.00322023510763591 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0028883678048953294 // Train Acc: 0.0\n",
      "Val Loss: 0.003166946199913086 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [28:45<00:00, 172.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data after splitting into sequences: (8748, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1057, 12, 5)\n",
      "Shape of the data after splitting into sequences: (1056, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.1254850729952619 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07837150585563744 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.010997177498803306 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.014639687560060444 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008892379313583061 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.011009372770786285 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007756685711046718 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009386776672566639 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007404763548327627 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008551846641828032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007201074724514684 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008169384474646957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007063236015519362 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007967460304772592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006960490294110704 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007833657916361356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006874439987810278 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007744166726136909 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006796076417716741 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007673134910874069 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006720840096557972 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007597252034910899 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006645051342770593 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007502296350567657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006564350087038996 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007388792000710964 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006476746002251327 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007270401617621674 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0063865133708043255 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007181426284232122 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006299926937575634 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007141973309711937 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006219216159411231 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007127647883916164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006143471381139228 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007112421170698807 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006071351963594229 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0070826421656152785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006001721872114687 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007035474649027866 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005934256147376178 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006981859894414597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005869738504454882 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006938637960154344 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005809067333371666 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006913161720149219 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005752355992748067 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0069009644828517645 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005699069473306756 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006893499124356929 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0056484239197198225 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006883336148936959 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00559961440899772 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0068651005310718625 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005551926176177791 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006834418608752244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.0055046133498892775 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006787579171085621 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005457066707044901 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006721988881883376 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005408972691913975 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006638463956358678 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.0053604148884629065 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006543080589514883 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.00531165635871311 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006445112008163158 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005262877236752919 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006352269131799831 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.00521419962810206 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006268175496884128 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005165833580904424 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006193547660265775 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.00511808151258343 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006128057118450456 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.0050712782713811045 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006071311045054565 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0050257457199104644 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006023021353244343 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0049817570235318005 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005982880145037437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004939503799736588 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005950104500002721 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004899055964864239 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005923361181938911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004860358992382931 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0059008632156559646 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0048232522581991705 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005880717955091421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004787502095905189 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005861208356423851 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0047528360879016355 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005841040292152148 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004718969201382209 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005819361704839941 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004685622656182003 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005795728613841622 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.00465255868608401 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005770042620818405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.00461955865454763 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005742810358402922 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [06:45<1:00:50, 405.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.08907709934048941 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.06949826329946518 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.011055745348722506 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.015012634561999756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008736423660074813 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.011741514091708642 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007829977072804308 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010100134518271423 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00731976879794675 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00894783016102498 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0070570174534847685 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00834778807235553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0068509647240896305 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008010343217071803 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0066815185329963675 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007758425021379748 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006551069964614732 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007569578729624695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006445874921339896 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007403934073141392 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006360911211816992 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007265825933941147 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006287110417675213 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007162710850346176 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.0062192849688473715 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0070929620620411105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0061552361522416435 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00705174238914076 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006093869244461024 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007034607895869105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 145\n",
      "INFO: Validation loss did not improve in epoch 146\n",
      "INFO: Validation loss did not improve in epoch 147\n",
      "INFO: Validation loss did not improve in epoch 148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [08:47<31:47, 238.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 149\n",
      "Early stopping after 149 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.2150667706569163 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.07930485904216766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.010698846116120599 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01395563205347999 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.0092412674321982 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.011675739297917223 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008362495445010424 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010542796219370383 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.00793366743038005 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009943972588242853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007585919808410789 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009263712084194756 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007276584749694035 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008615957901758306 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007067066601135341 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008281211245476323 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.0068845873936104985 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008030974641716218 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006714778156015491 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007833287996404311 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006564065686323198 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0076119988293880045 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006427198887348918 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007350591812556719 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006309199527457656 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007088089060476597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006220041657372342 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006876528180916519 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006152350237589649 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006729995626408388 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006094629371281666 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006625335115720244 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006039907849650026 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006546276518801118 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005985223730430995 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00648631011683713 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005930013696269781 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006442664958098356 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005874735439644013 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00641232863178148 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.00582033454272828 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006389986244304215 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005767861033736181 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006369804052690811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005717668874198622 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006348188175573288 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005669321048552753 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006323647870243911 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.0056220450216369665 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006295744809048141 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.0055751120152336695 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006264667988836984 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.00552801275796085 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006230801717131673 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005480517611763963 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006194565006915261 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005432681946387818 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006156219758422059 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005384780805054848 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006115753535756513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005337174858713193 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006072967522092821 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.00529019084335109 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0060277861844309985 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005244103803467925 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005980509970172802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005199195851377761 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0059319500407368385 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0051557836989008935 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005883332239190007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005114166087652276 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0058360094588030785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005074471742331035 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0057909804574378275 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005036574098519075 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0057484222137752705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.00500021398602091 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005707576826406533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.00496531739033728 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005667460274279994 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004932118587726233 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005628337244749726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004900818475491454 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005591625282677877 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004871403079750646 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005558442218256567 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004843719277324043 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005529041032252067 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004817531525750748 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005503136794740225 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0047925934005040045 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005480219075894531 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004768681229548478 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00545975861742216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004745604447493504 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0054412372376057595 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0047232094932717 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005424191021387849 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004701377199094409 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0054082117687143825 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [15:34<36:47, 315.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.19985887883963163 // Train Acc: 0.0\n",
      "Val Loss: 0.08481612582417096 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.011207651820032086 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.015351652790901853 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008858656168700805 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01154689013492316 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007977522726718105 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010257997960947892 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007478821147646128 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009180953589213245 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007160690169882751 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00841590079396744 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006943544665550715 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007977191139669979 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006823401174433972 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007822234073982519 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006740186033366926 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007749764207640991 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006667892994713277 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00765539740677923 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006604688251974194 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007540551377186442 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006547672147104956 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007425399668350378 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006493349789988277 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007320437059902093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006439903965436937 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007232736248303862 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006386550337244745 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007166909341536024 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006333009360619468 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007121904102592345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.0062789679032459285 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007091900484417291 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0062237883222262145 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007070025168907116 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006166722354035607 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007050909800454974 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006107440223969294 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007031052304870065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006046404262601996 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0070080013658084415 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005984613024910582 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006980136975043398 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005923025199796126 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006947104120627046 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005862379207942376 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006909740308080526 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005803363237247361 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006868991733747809 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005746664357000462 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006825507940341006 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.0056928800991685275 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006779819261282682 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005642346407510715 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006732608801137437 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005595092965699426 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006684888003613143 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.0055509131841048245 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006638051257194842 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005509534768938552 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006593551349771374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005470727604194127 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006552544203312958 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0054343158342591204 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006515546366298462 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005400097402039544 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0064824830521555505 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005367833260430612 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006452740779530038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005337244309816966 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006425370455390829 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005308041295740699 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006399422253975097 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.00527995442622554 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0063740144798751265 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0052527287811929635 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006348492447560763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.0052261577186049546 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006322459920364267 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005200066049151221 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006295733112732277 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005174313003084089 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006268283050945576 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005148797464651487 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006240218453218832 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0051234406841556175 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006211718599147657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0050981817313706605 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0061829553999225885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005072975217143816 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006154092651901438 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00504777346333528 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006125270327388802 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005022527749973249 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006096631521359086 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004997169819317411 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006068275200531763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004971614917608274 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.006040417637182947 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [22:21<35:09, 351.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.08799710451970916 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.06950333303607562 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01110539600292265 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.014716879505773677 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008775168955981582 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010905930977862547 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007925452270342456 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009512790622573127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.0075919771771511945 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00897964250351138 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007348424441439068 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008530082119464436 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007157595101271465 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008184492279885007 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006998757549439824 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00795666007188094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006856714420968078 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007765661267673268 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006727655095148702 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0075866981779279955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006609788062127556 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007427126915632363 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00650109121564459 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007286519421648015 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006398405584121903 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0071621478179141 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006298803660075369 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007053933257017942 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006201081542860907 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006962630651233827 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0061055348280066426 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006885608644498622 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006012773891154104 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006816622486118884 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005923218794432647 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0067489753915544815 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005837194411078948 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006677509668995352 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005754990499201478 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006598783485755762 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0056767599706979 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006510916316662641 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.0056024190125488075 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006413629079949768 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005531597261272806 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006308177787372295 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005463726915184143 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006197159404537696 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005398246961078816 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0060841560993781865 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005334816745165702 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005973189884303685 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005273399791034139 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005867980376077706 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00521416372291393 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005771275370053071 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005157319727350346 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005684390681220547 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.00510294730025914 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0056076029705924586 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.00505092501024404 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005539831373265342 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005001040902354804 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005479237787178992 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.004953018565542172 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005423767803757287 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.004906586693579389 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0053714553907732755 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.004861510264788545 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005320598662365228 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.004817588176383335 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005269941726617296 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.004774644284000896 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0052188569405937895 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.004732488174846147 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005167302134556367 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.0046909169072176325 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00511573112624533 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.00464975584589517 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005064916540183784 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004608905965069026 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005015758396920693 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004568381846130704 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004969050090842168 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004528313492751686 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004925461099072195 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004488912789330697 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004885584085166235 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.00445038970664971 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004849905278259779 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004412884735442712 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0048188100678517535 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.004376449341178315 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004792676468188053 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004341058513822997 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004771917983067825 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004306639504441872 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0047568333958385185 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004273110438798856 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.0047475867425365486 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [29:08<30:58, 371.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.09764694280486547 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0659405485014705 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.010799281377008208 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.014498319976743968 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.009064513678114242 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.011314978823065758 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00853960307368624 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010136010895674938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.008236389810119805 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009562531808007728 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007866312512383887 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009197763934293213 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.0075668715274896155 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008911666035761727 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00733225872657999 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008650306249311304 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007168285301916295 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008419088101671898 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.007028740722391502 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008200127508162576 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006904230357834752 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008035156149964999 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.00679403330125783 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007929078303277493 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006692918518922501 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007838457446161877 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.00659873752041347 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00773450595271938 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006512169863039481 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007635397186009761 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006438871313646688 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007549954078379361 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006373015965885863 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007467902025810498 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006312569435511357 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007387815734974164 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.0062556305244289186 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00730879617827561 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006203240135444542 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007233370896702742 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006154701326525952 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007165073649957776 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006108588051800897 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007105115191627513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.006064022362991568 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0070524269865606636 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0060204733460009355 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00700472791761379 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005977615625857183 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006959704202873742 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005935263071061066 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0069156883092706695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005893238808536626 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006871907800600371 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.0058514132403634095 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006828507497523199 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005809670281524121 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006786299821482424 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005767885107339328 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006746332955491894 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005725925612277002 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006709796225871234 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005683701434955887 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006677745642852695 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005641237115704538 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0066511358052273005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.0055987587136044495 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006630598471554763 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.0055566965320345075 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006616338772480102 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005515566216488037 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006607540848884074 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.0054757341913201285 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006602776428575025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005437338260454475 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006600404829334687 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005400324242561544 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006598995724583373 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005364555906282203 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006597423460334539 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.00532988488944026 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0065949367263409145 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005296177234819033 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00659110803184483 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005263321474960697 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006585734875817948 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.00523118527767451 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006578736609834082 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0051996270585183725 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0065701214788372025 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.0051684994737905415 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00655993354260264 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005137661102285755 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006548193776432206 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.005106982541563401 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006534910041784101 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.005076352771843223 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006520046612850446 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.005045673351201111 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006503533975988188 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [35:58<25:39, 384.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.11479363356201043 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.07537364576231032 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.011234373400554783 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.015355485731133205 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.008285955457830669 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009939500541590592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.007616287364962407 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008792055154438405 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007301284924257629 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008323477266137214 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007097850106758707 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008128373835728887 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.006937684007418909 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00803726922501536 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.006815521548999307 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007981803096995196 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006716584522231147 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0079244602307239 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006629525339575672 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007843818871633094 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006546239876631014 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007719402489088038 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006459200320598521 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007480490535004612 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006367638431552464 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007171295008019489 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006276609438430634 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006890046786900391 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.0061874202322989525 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006633648963864236 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006103153093960613 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006422684938811204 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006025590354017402 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006267183953348328 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.005954785084767332 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006154123734792366 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.00588991179410198 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006068302797810996 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005829989865858114 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00600123347933678 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005774262736439092 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005947811686543419 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.005722202787876394 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005903881248634528 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.0056733867258660945 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005865860737257582 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.005627386784968451 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005830812507637721 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005583754494203973 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005796642200199559 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005542027012111323 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005761944038300391 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005501757173286496 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005725856541710741 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.005462533164594044 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005688014792908421 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005423938422642607 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005648146873093484 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005385436855620063 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005605731097816983 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.0053466043919852685 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005560299802763278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005307214409627967 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005511405460043427 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.0052673421894630906 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005459070051609374 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005227253587160538 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005404167240211631 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005187189474534175 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005348030484610182 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005147245632236827 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005291837342905209 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005107346074370467 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005236327990560848 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005067287630872321 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.00518190669690204 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005026859814008286 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005128971119548249 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.004986000715302035 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005078049903899869 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.004944897067483678 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.005029867673703634 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.004903889129979458 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004984916862346889 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.004863343719061859 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.004943115218980785 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.004823572052684417 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004903991504654507 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.004784812983908645 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004867004939382348 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.004747236932209017 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004831745672751875 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.00471095893103408 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004797992049990331 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.004676046552774165 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004765713875017622 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.004642549765673407 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004735018903910018 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.004610489630263803 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.004706027256735765 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [42:47<19:37, 392.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 0.23170945554004618 // Train Acc: 0.0\n",
      "Val Loss: 0.10311489300254513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.01199345020479412 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.01678057385416811 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.009333775292995165 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.011676968773826957 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008276775785359184 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009729166549411328 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007834255655544726 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00918853365103988 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.007611459272639516 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008891759385519168 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007400335955784283 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008603769217563026 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.0072100572692404006 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008291427771944334 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007084007807934939 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00811462732963264 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006983759994656903 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008030974251382491 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006895642559055982 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007985813830814818 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006814580613434812 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00796091923599734 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006736467554535329 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00793972481316065 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006658961312097056 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007905006388147525 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006581990144379825 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007846595733152592 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.0065056223105605165 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007768126451136435 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006429214220131378 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007678470663342844 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006352153793360418 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0075808193137430965 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006274452598142341 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007475259029032553 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006196758739856264 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007369935074273278 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.006119143470319012 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007286262821734828 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006040033551594464 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007252176073105896 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 212\n",
      "INFO: Validation loss did not improve in epoch 213\n",
      "INFO: Validation loss did not improve in epoch 214\n",
      "INFO: Validation loss did not improve in epoch 215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [45:43<10:47, 323.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 216\n",
      "Early stopping after 216 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.1262512127877344 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.08734116571791031 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.011921159190107035 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.016221074421670947 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.009402040976365275 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.012032933190793675 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.008503524650618693 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010460101355634192 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007937029740578194 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009660436223973246 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0076532441142851135 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.009142340427976759 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007446984544977838 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008779013796490343 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.00720157375783954 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008365804548649228 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.006944107595031785 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00800983011311687 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006753079864959244 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007788104518754955 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006582601997559512 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007577587371034657 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006430096699880909 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0073903635185321465 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006300104434229093 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007238366332946017 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.0061858391924080195 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007111161693875843 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006082251747360136 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006995466797996093 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.005989123606680632 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0068806289243237935 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.005907547601219437 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00676230904098381 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.0058363727698276255 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0066474909099805005 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.005772515296913901 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006549391607918283 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.005712752894479679 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006477594991926761 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.005654466833380414 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006435505895163207 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 211\n",
      "Epoch: 211\n",
      "Train Loss: 0.005595608343245969 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006422939114546513 // Val Acc: 0.0\n",
      "**************************************************\n",
      "INFO: Validation loss did not improve in epoch 212\n",
      "INFO: Validation loss did not improve in epoch 213\n",
      "INFO: Validation loss did not improve in epoch 214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [48:38<04:37, 277.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Validation loss did not improve in epoch 215\n",
      "Early stopping after 215 epochs\n",
      "Epoch: 1\n",
      "Train Loss: 0.22015774957290843 // Train Acc: 0.011425959780621572\n",
      "Val Loss: 0.09425593299024246 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 11\n",
      "Train Loss: 0.011586998526127821 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.015848984503570723 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 21\n",
      "Train Loss: 0.009075763596177537 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.011707923799643622 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 31\n",
      "Train Loss: 0.00797923800294582 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.010025969679083894 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 41\n",
      "Train Loss: 0.007669242370811544 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00937223042060128 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 51\n",
      "Train Loss: 0.0074606032163531295 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008916550199501216 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 61\n",
      "Train Loss: 0.007300988917977088 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008542345405337127 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 71\n",
      "Train Loss: 0.007187829615967093 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008278811384704621 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 81\n",
      "Train Loss: 0.007087988062526339 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.008108828497557518 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 91\n",
      "Train Loss: 0.006982170353972198 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007994513582054745 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 101\n",
      "Train Loss: 0.006863586216402228 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00792442963403814 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 111\n",
      "Train Loss: 0.006743422378249589 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00786346008124597 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 121\n",
      "Train Loss: 0.006626818861584205 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007814139764591613 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 131\n",
      "Train Loss: 0.006523836804515558 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00778132927713587 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 141\n",
      "Train Loss: 0.006435913902429023 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0077475912608754105 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 151\n",
      "Train Loss: 0.006357430039340599 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007707807618905516 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 161\n",
      "Train Loss: 0.006285184044607999 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00766489260336932 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 171\n",
      "Train Loss: 0.006217913869410835 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007620087859001668 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 181\n",
      "Train Loss: 0.006154240125733202 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007575229392386973 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 191\n",
      "Train Loss: 0.006097843174795387 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007544412715908359 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 201\n",
      "Train Loss: 0.0060478678517770605 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007511843105449396 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 211\n",
      "Train Loss: 0.006002460381120845 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007475127335911726 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 221\n",
      "Train Loss: 0.005960673273313736 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007434385924545281 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 231\n",
      "Train Loss: 0.0059218095115154395 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007389734752530999 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 241\n",
      "Train Loss: 0.005885236985961034 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0073414563803988345 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 251\n",
      "Train Loss: 0.005850313899726173 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007289867827614003 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 261\n",
      "Train Loss: 0.005816401971756928 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00723511357243885 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 271\n",
      "Train Loss: 0.00578288206876591 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007176528417724459 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 281\n",
      "Train Loss: 0.005749268575624258 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007112559122855172 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 291\n",
      "Train Loss: 0.005715346651686393 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.007042366025202414 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 301\n",
      "Train Loss: 0.005681090047106256 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0069667721126119 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 311\n",
      "Train Loss: 0.005646447774846025 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006887091467540492 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 321\n",
      "Train Loss: 0.005611488242315254 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006804798380471766 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 331\n",
      "Train Loss: 0.005576546003418878 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006721853805870256 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 341\n",
      "Train Loss: 0.005541480684051055 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.00663763505872339 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 351\n",
      "Train Loss: 0.005506099519699512 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006550892078153351 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 361\n",
      "Train Loss: 0.005470486750044746 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006460855673889027 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 371\n",
      "Train Loss: 0.005434890169142941 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006367721540086409 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 381\n",
      "Train Loss: 0.005399476274421483 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006272934089579126 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 391\n",
      "Train Loss: 0.005364037669962698 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006178099593585905 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 401\n",
      "Train Loss: 0.005328283356089772 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.006084946374518468 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 411\n",
      "Train Loss: 0.005292570363377754 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005995602559243494 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 421\n",
      "Train Loss: 0.005257403191351 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0059117991443011254 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 431\n",
      "Train Loss: 0.0052231938656508055 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005834858905633583 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 441\n",
      "Train Loss: 0.0051902914590030306 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005764560089172686 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 451\n",
      "Train Loss: 0.005158877731054013 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005699627841001048 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 461\n",
      "Train Loss: 0.005128972610425531 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005638828042291981 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 471\n",
      "Train Loss: 0.00510054757887535 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0055814468582599044 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 481\n",
      "Train Loss: 0.0050735629860258966 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.005527194928350475 // Val Acc: 0.0\n",
      "**************************************************\n",
      "Epoch: 491\n",
      "Train Loss: 0.0050479392263565 // Train Acc: 0.005712979890310786\n",
      "Val Loss: 0.0054760993088540785 // Val Acc: 0.0\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [55:28<00:00, 332.90s/it]\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictive performance\n",
    "predictive_results = predictive_evaluation(\n",
    "    data_train_real=data_train_real_numpy, \n",
    "    data_test_real=data_test_real_numpy,\n",
    "    data_syn=data_syn_numpy, \n",
    "    hyperparameters=hyperparameters, \n",
    "    include_baseline=True, \n",
    "    verbose=True)\n",
    "\n",
    "# save results\n",
    "bidirectionality = \"bi\" if hyperparameters[\"bidirectional\"] else 'no_bi'\n",
    "predictive_results.to_csv(DATA_FOLDER / f\"results_{syn_data_type}_{hyperparameters['num_epochs']}_{hyperparameters['num_evaluation_runs']}_{bidirectionality}.csv\", index=False)\n",
    "\n",
    "# split in mse and mae results\n",
    "mse_results = predictive_results.loc[predictive_results['Metric'] == 'MSE']\n",
    "mae_results = predictive_results.loc[predictive_results['Metric'] == 'MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x284690e4dd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mse_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mse_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MSE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Model', y='Error', hue='Model', data=mae_results)\n",
    "sns.stripplot(x='Model', y='Error', hue='Metric', data=mae_results, dodge=True, jitter=True, palette='dark:black', alpha=0.7)\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Metric')\n",
    "plt.title(f'MAE | {syn_data_type} | {hyperparameters[\"num_evaluation_runs\"]} Training Runs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2*1e06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_data_augmentation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
