{"cells":[{"cell_type":"markdown","metadata":{},"source":["# LSTM Baseline Model Testing"]},{"cell_type":"markdown","metadata":{},"source":["## 0 Imports & Constants"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["import sys\n","import os\n","\n","# Füge das übergeordnete Verzeichnis zu sys.path hinzu\n","parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n","sys.path.insert(0, parent_dir)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","import pandas as pd\n","from pathlib import Path\n","import itertools\n","from tqdm import tqdm\n","import json\n","from datetime import datetime\n","import re\n","\n","from TimeSeriesDataset import TimeSeriesDataset\n","from utils import load_time_series\n","from LSTM import add_lagged_data, scale_data, train_test_split_to_tensor\n","from baseline_model.LSTM import LSTM, train_model"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["DATA_FOLDER = Path(\"../data\")\n","REAL_DATA_FOLDER = DATA_FOLDER / \"real\"\n","SYNTHETIC_DATA_FOLDER = DATA_FOLDER / \"synthetic\"\n","BENCHMARK = True"]},{"cell_type":"markdown","metadata":{},"source":["## 1 Data"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loading"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["# Load data from csv\n","# -> convert Date column to datetime\n","data = load_time_series(f'{REAL_DATA_FOLDER}/AAPL_10_24_real.csv')"]},{"cell_type":"markdown","metadata":{},"source":["## 2 Benchmark Loop"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["hyperparameters = {\n","    'lag': [7, 14, 21],\n","    'lr': [0.002, 0.001],\n","    'hidden_size': [2, 4, 6, 8, 12],\n","    'num_layers': [1, 2],\n","    'batch_size': [8, 16, 32, 64],\n","}"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["# get all combinations of hyperparameters\n","keys, values = zip(*hyperparameters.items())\n","possible_hyperparameters = [dict(zip(keys, v)) for v in itertools.product(*values)]"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["possible_features = [\n","    ['Close', 'Volume'],\n","    ['Close', 'Open', 'Volume'],\n","    ['Close', 'Open', 'High', 'Low', 'Volume']\n","]"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/240 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["**************************************************\n","**************************************************\n","STARTING NEW BENCHMARK RUN:\n","Features: ['Close', 'Volume']\n","Hyperparameters: {'lag': 7, 'lr': 0.002, 'hidden_size': 2, 'num_layers': 1, 'batch_size': 8}\n","**************************************************\n","**************************************************\n","Adding lagged data for columns: ['Close', 'Volume']\n","Shape of the numpy array wit lagged data: (3611, 8, 2)\n","Shape of X_train: torch.Size([3430, 7, 2]) \n"," Shape of y_train: torch.Size([3430, 1]) \n"," Shape of X_test: torch.Size([181, 7, 2]) \n"," Shape of y_test: torch.Size([181, 1])\n","Epoch: 1\n","Validation Loss: 0.23082415111686871\n","**************************************************\n","Epoch: 2\n","Validation Loss: 0.012904002432427977\n","**************************************************\n","Epoch: 3\n","Validation Loss: 0.00529647696479831\n","**************************************************\n","Epoch: 4\n","Validation Loss: 0.002451465474618801\n","**************************************************\n","Epoch: 5\n","Validation Loss: 0.0012924718697752762\n","**************************************************\n","Epoch: 6\n","Validation Loss: 0.0011807848095486406\n","**************************************************\n","Epoch: 7\n","Validation Loss: 0.0007900533673819155\n","**************************************************\n","Epoch: 8\n","Validation Loss: 0.0005951794126936559\n","**************************************************\n","Epoch: 9\n","Validation Loss: 0.0013151412283110878\n","INFO: Validation loss did not improve in epoch 9\n","**************************************************\n","Epoch: 10\n","Validation Loss: 0.000578682195855831\n","**************************************************\n","Epoch: 11\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/240 [00:26<?, ?it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     43\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m---> 45\u001b[0m validation_loss, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# save loss for each run\u001b[39;00m\n\u001b[1;32m     54\u001b[0m validation_losses\u001b[38;5;241m.\u001b[39mappend(validation_loss)\n","File \u001b[0;32m~/time_series_data_augmentation/baseline_model/LSTM.py:124\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, device, patience, num_epochs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     current_validation_loss \u001b[38;5;241m=\u001b[39m validate_one_epoch(model, test_loader, criterion, device)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# early stopping\u001b[39;00m\n","File \u001b[0;32m~/time_series_data_augmentation/baseline_model/LSTM.py:65\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device, log_interval, scheduler)\u001b[0m\n\u001b[1;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     64\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 65\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     68\u001b[0m     \n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# log training loss \u001b[39;00m\n\u001b[1;32m     70\u001b[0m     avg_train_loss_across_batches \u001b[38;5;241m=\u001b[39m running_train_loss \u001b[38;5;241m/\u001b[39m log_interval\n","File \u001b[0;32m~/time_series_data_augmentation/ba_venv_3_8_10/lib/python3.8/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m~/time_series_data_augmentation/ba_venv_3_8_10/lib/python3.8/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n","File \u001b[0;32m~/time_series_data_augmentation/ba_venv_3_8_10/lib/python3.8/site-packages/torch/optim/adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;129m@_use_grad_for_differentiable\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform a single optimization step.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m        closure (Callable, optional): A closure that reevaluates the model\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m            and returns the loss.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_graph_capture_health_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m~/time_series_data_augmentation/ba_venv_3_8_10/lib/python3.8/site-packages/torch/optim/optimizer.py:339\u001b[0m, in \u001b[0;36mOptimizer._cuda_graph_capture_health_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_graph_capture_health_check\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;66;03m# Note [torch.compile x capturable]\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;66;03m# If we are compiling, we try to take the capturable path automatically by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m# Thus, when compiling, inductor will determine if cudagraphs\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# can be enabled based on whether there is input mutation or CPU tensors.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_built() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 339\u001b[0m         capturing \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_current_stream_capturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m capturing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups):\n\u001b[1;32m    342\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting CUDA graph capture of step() for an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    343\u001b[0m                                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    344\u001b[0m                                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but param_groups\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m capturable is False.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/time_series_data_augmentation/ba_venv_3_8_10/lib/python3.8/site-packages/torch/cuda/graphs.py:28\u001b[0m, in \u001b[0;36mis_current_stream_capturing\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_current_stream_capturing\u001b[39m():\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    If a CUDA context does not exist on the current device, returns False without initializing the context.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cuda_isCurrentStreamCapturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["RESULTS = {}\n","if BENCHMARK:\n","    for features in possible_features:\n","\n","        for hyperparameters in tqdm(possible_hyperparameters):\n","            \n","            print('*'*50)\n","            print('*'*50)\n","            print('STARTING NEW BENCHMARK RUN:')\n","            print(f\"Features: {features}\")\n","            print(f\"Hyperparameters: {hyperparameters}\")\n","            print('*'*50)\n","            print('*'*50)\n","\n","            ### Select features\n","            features_incl_date = features+['Date']\n","            data_only_features = data[features_incl_date]\n","            \n","            ### Data Preprocessing\n","            data_lagged = add_lagged_data(data_only_features, hyperparameters['lag'], features)\n","            data_lagged_scaled, scaler_close = scale_data(data_lagged)\n","            X_train, y_train, X_test, y_test = train_test_split_to_tensor(data_lagged_scaled)\n","\n","            ### Create datasets and DataLoaders\n","            train_dataset = TimeSeriesDataset(X_train, y_train)\n","            test_dataset = TimeSeriesDataset(X_test, y_test)\n","            train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], shuffle=True)\n","            test_loader = DataLoader(test_dataset, batch_size=hyperparameters['batch_size'], shuffle=False)\n","\n","            ### Train model\n","            validation_losses = [] # reset validation losses\n","            for i in range(2): # train 2 times because sometimes the model converges to a local minimum\n","                ### Instantiate model\n","                model = LSTM(\n","                    device=device,\n","                    input_size=len(features),\n","                    hidden_size=hyperparameters['hidden_size'],\n","                    num_stacked_layers=hyperparameters['num_layers']\n","                ).to(device)\n","\n","                ### Optimizer, Criterion\n","                optimizer = torch.optim.Adam(model.parameters(), lr=hyperparameters['lr'])\n","                criterion = nn.MSELoss()\n","\n","                validation_loss, model = train_model(\n","                    model=model,\n","                    train_loader=train_loader,\n","                    test_loader=test_loader,\n","                    optimizer=optimizer,\n","                    criterion=criterion,\n","                    device=device)\n","                \n","                # save loss for each run\n","                validation_losses.append(validation_loss)\n","            \n","            ### Save results to dict\n","            feature_acronym = ''.join([feature[0] for feature in features])\n","            RESULTS[f'{feature_acronym}_lag{hyperparameters[\"lag\"]}_lr{hyperparameters[\"lr\"]}_bs{hyperparameters[\"batch_size\"]}_hs{hyperparameters[\"hidden_size\"]}_nl{hyperparameters[\"num_layers\"]}'] = min(validation_losses) # only save min loss\n","        \n","    ### Save results to json\n","    print('Saving results to json...')\n","    with open(f'./benchmark_results/LSTM_benchmark_results_{datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")}.json', 'w') as json_file:\n","        json.dump(RESULTS, json_file, indent=4)\n","    print('Results saved!')\n","\n","    print('#'*50)\n","    print('#'*50)\n","    print('#'*50)\n","    print('TRAINING FINISHED')\n","    print('#'*50)\n","    print('#'*50)\n","    print('#'*50)"]},{"cell_type":"markdown","metadata":{},"source":["## 3 Evaluate Results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open('./benchmark_results/LSTM_benchmark_results_2024_06_19_205109.json', 'r') as json_file:\n","    results = json.load(json_file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# get results as array of dicts (bc i saved them shitty in the first place)\n","result_array = []\n","for key, value in results.items():\n","    result_dict = {}\n","    result_dict['error'] = value\n","    result_dict['features'] = key.split('_')[0]\n","    for feature in ['lag', 'lr', 'bs', 'hs', 'nl']:\n","        pattern = rf\"{feature}(\\d+\\.?\\d*)\" # regex pattern to extract the value of the hyperparameter\n","        match = re.search(pattern, key) # search for the value in the key\n","        result_dict[feature] = match.group()[len(feature):]\n","    result_array.append(result_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# turn into dataframe and select best 20\n","result_df = pd.DataFrame(result_array)\n","result_df_sorted = result_df.sort_values(by='error', ascending=True)\n","best_20 = result_df_sorted.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# extract best hyperparameters from best 20 results\n","best_hyperparameters = {}\n","for feature in ['features', 'lag', 'lr', 'bs', 'hs', 'nl']:\n","    best_hyperparameters[feature] = best_20[feature].value_counts().idxmax()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["{'features': 'CV',\n"," 'lag': '14',\n"," 'lr': '0.001',\n"," 'bs': '32',\n"," 'hs': '12',\n"," 'nl': '1'}"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["best_hyperparameters"]}],"metadata":{"kernelspec":{"display_name":"time_series_data_augmentation_venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":2}
