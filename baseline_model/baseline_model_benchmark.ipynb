{"cells":[{"cell_type":"markdown","metadata":{},"source":["# LSTM Baseline Model Testing"]},{"cell_type":"markdown","metadata":{},"source":["## 0 Imports & Constants"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import sys\n","import os\n","\n","# Füge das übergeordnete Verzeichnis zu sys.path hinzu'\n","parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n","sys.path.insert(0, parent_dir)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from pathlib import Path\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from copy import deepcopy as dc\n","from sklearn.preprocessing import MinMaxScaler\n","import itertools\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","from utils.utils import load_time_series, add_lagged_data, scale_data, train_test_split_to_tensor, inverse_scale_data\n","\n","from baseline_model.TimeSeriesDataset import TimeSeriesDataset\n","from baseline_model.LSTM import LSTM, train_model"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2.3.1+cu121\n","12.1\n","True\n","NVIDIA TITAN RTX\n"]}],"source":["print(torch.__version__)\n","print(torch.version.cuda)\n","print(torch.cuda.is_available())\n","print(torch.cuda.get_device_name(0))"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["DATA_FOLDER = Path(\"../data\")\n","MULTIVARIATE_DATA_FOLDER = DATA_FOLDER / \"multivariate\"\n","UNIVARIATE_DATA_FOLDER = DATA_FOLDER / \"univariate\"\n","BENCHMARK = False"]},{"cell_type":"markdown","metadata":{},"source":["## 1 Data"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loading"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Load data from csv\n","# -> convert Date column to datetime\n","data = load_time_series(f'{UNIVARIATE_DATA_FOLDER}/NVDA_open_high_low_close_adjClose_volume_99_24.csv')"]},{"cell_type":"markdown","metadata":{},"source":["## 2 Benchmark Loop"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["hyperparameters = {\n","    'lag': [7, 14, 21],\n","    'lr': [0.002, 0.001],\n","    'hidden_size': [2, 4, 6, 8, 12],\n","    'num_layers': [1, 2],\n","    'batch_size': [64, 8, 16, 32],\n","}"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# get all combinations of hyperparameters\n","keys, values = zip(*hyperparameters.items())\n","possible_hyperparameters = [dict(zip(keys, v)) for v in itertools.product(*values)]"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["possible_features = [\n","    ['Close', 'Volume'],\n","    ['Close', 'Open', 'Volume'],\n","    ['Close', 'Open', 'High', 'Low', 'Volume']\n","]"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/240 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["**************************************************\n","**************************************************\n","STARTING NEW BENCHMARK RUN:\n","Features: ['Close', 'Volume']\n","Hyperparameters: {'lag': 7, 'lr': 0.002, 'hidden_size': 2, 'num_layers': 1, 'batch_size': 64}\n","**************************************************\n","**************************************************\n","Adding lagged data for columns: ['Close', 'Volume']\n","Shape of the numpy array wit lagged data: (6384, 8, 2)\n","Shape of X_train: torch.Size([6064, 7, 2]) \n"," Shape of y_train: torch.Size([6064, 1]) \n"," Shape of X_test: torch.Size([320, 7, 2]) \n"," Shape of y_test: torch.Size([320, 1])\n","Epoch: 1\n","Validation Loss: 0.08576316554099321\n","**************************************************\n","Epoch: 2\n","Validation Loss: 0.05997884273529053\n","**************************************************\n","Epoch: 3\n","Validation Loss: 0.048535099253058434\n","**************************************************\n","Epoch: 4\n","Validation Loss: 0.04020353402011097\n","**************************************************\n","Epoch: 5\n","Validation Loss: 0.0336088047362864\n","**************************************************\n","Epoch: 6\n","Validation Loss: 0.02822754569351673\n","**************************************************\n","Epoch: 7\n","Validation Loss: 0.023810411244630812\n","**************************************************\n","Epoch: 8\n","Validation Loss: 0.020204178895801304\n","**************************************************\n","Epoch: 9\n","Validation Loss: 0.01729070539586246\n","**************************************************\n","Epoch: 10\n","Validation Loss: 0.014964509138371796\n","**************************************************\n","Epoch: 11\n","Validation Loss: 0.01312535029137507\n","**************************************************\n","Epoch: 12\n","Validation Loss: 0.01167847797041759\n","**************************************************\n","Epoch: 13\n","Validation Loss: 0.010537977458443492\n","**************************************************\n","Epoch: 14\n","Validation Loss: 0.009630684333387762\n","**************************************************\n","Epoch: 15\n","Validation Loss: 0.008898242091527208\n","**************************************************\n","Epoch: 16\n","Validation Loss: 0.008297299518017099\n","**************************************************\n","Epoch: 17\n","Validation Loss: 0.007797782693523913\n","**************************************************\n","Epoch: 18\n","Validation Loss: 0.0073801417282084005\n","**************************************************\n","Epoch: 19\n","Validation Loss: 0.00703230508370325\n","**************************************************\n","Epoch: 20\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/240 [00:22<?, ?it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     42\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m---> 44\u001b[0m validation_loss, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# save loss for each run\u001b[39;00m\n\u001b[1;32m     53\u001b[0m validation_losses\u001b[38;5;241m.\u001b[39mappend(validation_loss)\n","File \u001b[0;32m~/time_series_data_augmentation/baseline_model/LSTM.py:118\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, device, patience, num_epochs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     current_validation_loss \u001b[38;5;241m=\u001b[39m validate_one_epoch(model, test_loader, criterion, device)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# early stopping\u001b[39;00m\n","File \u001b[0;32m~/time_series_data_augmentation/baseline_model/LSTM.py:54\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device, log_interval, scheduler)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_index, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     52\u001b[0m     x_batch, y_batch \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[0;32m---> 54\u001b[0m     train_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m criterion(train_pred, y_batch)\n\u001b[1;32m     56\u001b[0m     running_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m~/time_series_data_augmentation/ba_venv_3_8_10/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/time_series_data_augmentation/ba_venv_3_8_10/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/time_series_data_augmentation/baseline_model/LSTM.py:33\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_stacked_layers, batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     32\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x, (h0, c0)) \u001b[38;5;66;03m# get output of LSTM layer\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# run output through fully connected layer\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","File \u001b[0;32m~/time_series_data_augmentation/ba_venv_3_8_10/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/time_series_data_augmentation/ba_venv_3_8_10/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/time_series_data_augmentation/ba_venv_3_8_10/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["RESULTS = {}\n","for features in possible_features:\n","\n","    for hyperparameters in tqdm(possible_hyperparameters):\n","        \n","        print('*'*50)\n","        print('*'*50)\n","        print('STARTING NEW BENCHMARK RUN:')\n","        print(f\"Features: {features}\")\n","        print(f\"Hyperparameters: {hyperparameters}\")\n","        print('*'*50)\n","        print('*'*50)\n","\n","        ### Select features\n","        features_incl_date = features+['Date']\n","        data_only_features = data[features_incl_date]\n","        \n","        ### Data Preprocessing\n","        data_lagged = add_lagged_data(data_only_features, hyperparameters['lag'], features)\n","        data_lagged_scaled, scaler_close = scale_data(data_lagged)\n","        X_train, y_train, X_test, y_test = train_test_split_to_tensor(data_lagged_scaled)\n","\n","        ### Create datasets and DataLoaders\n","        train_dataset = TimeSeriesDataset(X_train, y_train)\n","        test_dataset = TimeSeriesDataset(X_test, y_test)\n","        train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], shuffle=False, pin_memory=True, num_workers=8)\n","        test_loader = DataLoader(test_dataset, batch_size=hyperparameters['batch_size'], shuffle=False, pin_memory=True, num_workers=8)\n","\n","        ### Train model\n","        validation_losses = [] # reset validation losses\n","        for i in range(2): # train 2 times because sometimes the model converges to a local minimum\n","            ### Instantiate model\n","            model = LSTM(\n","                device=device,\n","                input_size=len(features),\n","                hidden_size=hyperparameters['hidden_size'],\n","                num_stacked_layers=hyperparameters['num_layers']\n","            ).to(device)\n","\n","            ### Optimizer, Criterion\n","            optimizer = torch.optim.Adam(model.parameters(), lr=hyperparameters['lr'])\n","            criterion = nn.MSELoss()\n","\n","            validation_loss, model = train_model(\n","                model=model,\n","                train_loader=train_loader,\n","                test_loader=test_loader,\n","                optimizer=optimizer,\n","                criterion=criterion,\n","                device=device)\n","            \n","            # save loss for each run\n","            validation_losses.append(validation_loss)\n","        \n","        ### Save results\n","        feature_acronym = ''.join([feature[0] for feature in features])\n","        RESULTS[f'{feature_acronym}_lag{hyperparameters[\"lag\"]}_lr{hyperparameters[\"lr\"]}_bs{hyperparameters[\"batch_size\"]}_hs{hyperparameters[\"hidden_size\"]}_nl{hyperparameters[\"num_layers\"]}'] = min(validation_losses)\n","\n","print('#'*50)\n","print('#'*50)\n","print('#'*50)\n","print('TRAINING FINISHED')\n","print('#'*50)\n","print('#'*50)\n","print('#'*50)"]},{"cell_type":"markdown","metadata":{},"source":["## 3 Evaluate Results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["{'CV_lag7_lr0.002_bs4_hs2_nl1': 0.003400171762939408,\n"," 'CV_lag7_lr0.002_bs8_hs2_nl1': 0.010209126127210765,\n"," 'CV_lag7_lr0.002_bs16_hs2_nl1': 0.005297926136427123,\n"," 'CV_lag7_lr0.002_bs32_hs2_nl1': 0.0056508751447836405,\n"," 'CV_lag7_lr0.002_bs4_hs2_nl2': 0.014161826070790084,\n"," 'CV_lag7_lr0.002_bs8_hs2_nl2': 0.012155452139359113,\n"," 'CV_lag7_lr0.002_bs16_hs2_nl2': 0.017698677682892593}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["RESULTS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["'CV_lag7_lr0.002_bs4_hs2_nl1'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["best_params = min(RESULTS, key=RESULTS.get)\n","best_params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["0.003400171762939408"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["RESULTS[best_params]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"time_series_data_augmentation_venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":2}
